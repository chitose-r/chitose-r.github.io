<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Resnet-paper | Chitose-Blog</title><meta name="author" content="Chitose"><meta name="copyright" content="Chitose"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Deep Residual Learning for Image Recognition 图像识别的深度残差学习  2016 摘要更深的神经网络往往更难以训练，我们在此提出一个残差学习的框架，以减轻网络的训练负担，这是个比以往的网络要深的多的网络。我们明确地将层作为输入学习残差函数，而不是学习未知的函数。我们提供了非常全面的实验数据来证明，残差网络更容易被优化，并且可以在深度增加的情况下让精度也增">
<meta property="og:type" content="article">
<meta property="og:title" content="Resnet-paper">
<meta property="og:url" content="https://www.chitose.cn/Resnet-paper/index.html">
<meta property="og:site_name" content="Chitose-Blog">
<meta property="og:description" content="Deep Residual Learning for Image Recognition 图像识别的深度残差学习  2016 摘要更深的神经网络往往更难以训练，我们在此提出一个残差学习的框架，以减轻网络的训练负担，这是个比以往的网络要深的多的网络。我们明确地将层作为输入学习残差函数，而不是学习未知的函数。我们提供了非常全面的实验数据来证明，残差网络更容易被优化，并且可以在深度增加的情况下让精度也增">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_4.png">
<meta property="article:published_time" content="2024-02-06T13:41:16.000Z">
<meta property="article:modified_time" content="2024-02-06T13:41:16.000Z">
<meta property="article:author" content="Chitose">
<meta property="article:tag" content="演示">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_4.png"><link rel="shortcut icon" href="https://www.fomal.cc/favicon.ico"><link rel="canonical" href="https://www.chitose.cn/Resnet-paper/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体中文","cht_to_chs":"你已切换为简体中文","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Resnet-paper',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-02-06 21:41:16'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://www.fomal.cc/static/css/runtime.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.css" /><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Face.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">52</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_4.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Chitose-Blog"><span class="site-name">Chitose-Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Resnet-paper</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-02-06T13:41:16.000Z" title="发表于 2024-02-06 21:41:16">2024-02-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-02-06T13:41:16.000Z" title="更新于 2024-02-06 21:41:16">2024-02-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">10.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>34分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Resnet-paper"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Deep-Residual-Learning-for-Image-Recognition"><a href="#Deep-Residual-Learning-for-Image-Recognition" class="headerlink" title="Deep Residual Learning for Image Recognition"></a>Deep <a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=Residual&amp;spm=1001.2101.3001.7020">Residual</a> Learning for Image Recognition</h1><blockquote>
<h1 id="图像识别的深度残差学习-2016"><a href="#图像识别的深度残差学习-2016" class="headerlink" title="图像识别的深度残差学习  2016"></a>图像识别的深度残差学习  2016</h1></blockquote>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>更深的神经网络往往更难以训练，我们在此提出一个残差学习的框架，以减轻网络的训练负担，这是个比以往的网络要深的多的网络。我们明确地将层作为输入学习残差函数，而不是学习未知的函数。我们提供了非常全面的实验数据来证明，残差网络更容易被优化，并且可以在深度增加的情况下让精度也增加。在ImageNet的数据集上我们评测了一个深度152层（是VGG的8倍）的残差网络，但依旧拥有比VGG更低的复杂度。残差网络整体达成了3.57%的错误率，这个结果获得了ILSVRC2015的分类任务第一名，我们还用CIFAR-10数据集分析了100层和1000层的网络。</p>
<p>在一些计算机视觉识别方向的任务当中，深度表示往往是重点。我们极深的网络让我们得到了28%的相对提升（对COCO的对象检测数据集）。我们在深度残差网络的基础上做了提交的版本参加ILSVRC和COCO2015的比赛，我们还获得了ImageNet对象检测，Imagenet对象定位，COCO对象检测和COCO图像分割的第一名。</p>
<blockquote>
<h3 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h3><p><strong>背景：</strong> 神经网络的深度越深，越难以训练</p>
<p><strong>本文贡献：</strong> 本文展示了一种残差学习框架，能够简化使那些非常深的网络的训练，该框架能够将层作为输入学习残差函数，而不是学习未知的函数。</p>
<p><strong>结果：</strong> 本文提供了全面的依据表明，这些残差网络更容易被优化，而且可以在深度增加的情况下让精度也增加。</p>
<p><strong>成绩：</strong> 2015年的ILSVRC分类任务上以及获得了第一名的成绩，后来在ImageNet检测、ImageNet定位、COCO检测以及COCO分割上均获得了第一名的成绩。</p>
</blockquote>
<h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h2><p>深度卷积神经网络在图像分类领域取得了一系列的突破 。 深度网络很好的将一个端到端的多层模型中的低/中/高级特征以及分类器整合起来，特征的等级可以通过所堆叠层的数量（深度）来丰富。最近有结果显示，模型的深度发挥着至关重要的作用，这样导致了ImageNet竞赛的参赛模型都趋向于“非常深”——16 层 到30层 。许多其它的视觉识别任务的都得益于非常深的模型。</p>
<p>在深度的重要性的驱使下，出现了一个新的问题：训练一个更好的网络是否和堆叠更多的层一样简单呢？解决这一问题的障碍便是困扰人们很久的梯度消失/梯度爆炸，这从一开始便阻碍了模型的收敛。归一初始化（normalized initialization）和中间归一化（intermediate normalization）在很大程度上解决了这一问题，它使得数十层的网络在反向传播的随机梯度下降（SGD）上能够收敛。</p>
<p>当深层网络能够收敛时，一个退化问题又出现了：随着网络深度的增加，准确率达到饱和（不足为奇）然后迅速退化。意外的是，这种退化并不是由过拟合造成的，并且在一个合理的深度模型中增加更多的层却导致了更高的错误率，我们的实验也证明了这点。</p>
<p>退化的出现（训练准确率）表明了并非所有的系统都是很容易优化的。让我们来比较一个浅层的框架和它的深层版本。对于更深的模型，这有一种通过构建的解决方案：恒等映射（identity mapping）来构建增加的层，而其它层直接从浅层模型中复制而来。这个构建的解决方案也表明了，一个更深的模型不应当产生比它的浅层版本更高的训练错误率。实验表明，我们目前无法找到一个与这种构建的解决方案相当或者更好的方案（或者说无法在可行的时间内实现）。</p>
<p>本文中，我们提出了一种深度残差学习框架来解决这个退化问题。我们明确的让这些层来拟合残差映射（residual mapping），而不是让每一个堆叠的层直接来拟合所需的底层映射（desired underlying mapping）。假设所需的底层映射为 H(x)H(x)，我们让堆叠的非线性层来拟合另一个映射： F(x):=H(x)−xF(x):=H(x)−x。 因此原来的映射转化为： F(x)+xF(x)+x。我们推断残差映射比原始未参考的映射（unreferenced mapping）更容易优化。在极端的情况下，如果某个恒等映射是最优的，那么将残差变为0 比用非线性层的堆叠来拟合恒等映射更简单。</p>
<p>公式 F(x)+xF(x)+x 可以通过前馈神经网络的“shortcut连接”来实现(Fig.2)。Shortcut连接就是跳过一个或者多个层。在我们的例子中，shortcut 连接只是简单的执行恒等映射，再将它们的输出和堆叠层的输出叠加在一起(Fig.2)。恒等的shortcut连接并不增加额外的参数和计算复杂度。完整的网络仍然能通过端到端的SGD反向传播进行训练，并且能够简单的通过公共库（例如，Caffe）来实现而无需修改求解器（solvers）。</p>
<p>我们在ImageNet数据集上进行了综合性的实验来展示这个退化问题并评估了我们提出的方法。本文表明了： 1) 我们极深的残差网络是很容易优化的，但是对应的“plain”网络（仅是堆叠了层）在深度增加时却出现了更高的错误率。 2) 我们的深度残差网络能够轻易的由增加层来提高准确率，并且结果也大大优于以前的网络。</p>
<p>CIFAR-10数据集上也出现了类似的现象，这表明了我们提出的方法的优化难度和效果并不仅仅是对于一个特定数据集而言的。我们在这个数据集上成功的提出了超过100层的训练模型，并探索了超过1000层的模型。</p>
<p>在ImageNet分类数据集上，极深的残差网络获得了优异的成绩。我们的152层的残差网络是目前ImageNet尚最深的网络，并且别VGG网络的复杂度还要低。在ImageNet测试集上，我们的组合模型(ensemble)的top-5错误率仅为3.57%，并赢得了ILSVRC 2015分类竞赛的第一名。这个极深的模型在其他识别任务上同样也具有非常好的泛化性能，这让我们在ILSVRC &amp; COCO 2015 竞赛的ImageNet检测、ImageNet定位、COCO检测以及COCO分割上均获得了第一名的成绩。这强有力的证明了残差学习法则的通用性，因此我们将把它应用到其他视觉甚至非视觉问题上。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/ResNet-Figure1.png" alt=""></p>
<blockquote>
<p>图1：在CIFAR-10数据集上使用20层和56层“普通”网络的训练误差（左）和测试误差（右）。较深的网络具有更高的训练误差，因而也有更高的测试误差。在ImageNet上的类似现象见图4。</p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/ResNet-Figure2.png" alt=""></p>
<blockquote>
<p>图2：残差学习的构建模块。</p>
<p><strong>背景：</strong> 模型的深度发挥着至关重要的作用，这样导致了ImageNet竞赛的参赛模型都趋向于“非常深”——16 层 到30层</p>
<p><strong>问题一：</strong> <strong>模型深度太大时，会存在梯度消失/梯度爆炸的问题</strong></p>
<p><strong>梯度消失/梯度爆炸概念：</strong> 二者问题问题都是因为网络太深,网络权值更新不稳定造成的。本质上是因为梯度反向传播中的连乘效应（小于1连续相乘多次）。梯度消失时，越靠近输入层的参数w越是几乎纹丝不动；梯度爆炸时，越是靠近输入层的参数w越是上蹿下跳。</p>
<p><strong>解决方法：</strong> 归一初始化（normalized initialization）和中间归一化（intermediate normalization）＋BN，加快网络收敛。</p>
<p><strong>问题二： 随着网络深度的增加，准确率达到饱和然后迅速退化</strong></p>
<p><strong>网络退化概念：</strong> 神经网络随着层数加深，首先训练准确率会逐渐趋于饱和；若层数继续加深，反而训练准确率下降，效果不好了，而这种下降不是由过拟合造成的（因为如果是过拟合的话，训练时误差应该很低而测试时很高）。</p>
<blockquote>
<p><strong>Q：为啥会出现网络退化？</strong></p>
<p>由于非线性激活函数Relu的存在，每次输入到输出的过程都几乎是不可逆的，这也造成了许多不可逆的信息损失。一个特征的一些有用的信息损失了，得到的结果肯定不尽人意。说通俗一点就是中间商赚差价。层数增多之后，信息在中间层损失掉了。</p>
</blockquote>
<p> <strong>解决方法：</strong> 深度残差学习</p>
<p>（具体方法会在3.1章节讲解）</p>
<p><strong>结果：</strong></p>
<p>（1）残差网络的结构更利于优化收敛</p>
<p>（2）解决了退化问题</p>
<p>（3）残差网络可以在扩展网络深度的同时，提高网络性能</p>
</blockquote>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a><strong>2 相关工作</strong></h2><h3 id="2-1-残差表达"><a href="#2-1-残差表达" class="headerlink" title="2.1 残差表达"></a>2.1 残差表达</h3><p><strong>残差表达。</strong> 在图像识别中，VLAD是残差向量对应于字典进行编码的一种表达形式，Fisher Vector可以看做是VLAD 的一个概率版本。对于图像检索和分类它们都是强力的浅层表达。对于向量量化，残差向量编码比原始向量编码更加有效。</p>
<p>在低级视觉和计算机图形学中，为了求解偏微分方程（PDEs），通常使用Multigrid法将系统重新表达成多尺度的子问题来解决，每一个子问题就是解决粗细尺度之间的残差问题。Multigrid的另外一种方式是分层基预处理，它依赖于代表着两个尺度之间残差向量的变量。实验证明 这些求解器比其他标准求解器的收敛要快得多，却并没有意识到这是该方法的残差特性所致。这些方法表明了一个好的重新表达或者预处理能够简化优化问题。</p>
<blockquote>
<h3 id="主要内容-1"><a href="#主要内容-1" class="headerlink" title="主要内容"></a>主要内容</h3><p>（1）对于向量量化，残差向量编码比原始向量编码更加有效。</p>
<p>（2）Multigrid的残差特性使得求解器比其他标准求解器的收敛要快得多，表明了一个好的重新表达或者预处理能够简化优化问题。</p>
</blockquote>
<h3 id="2-2-短路连接"><a href="#2-2-短路连接" class="headerlink" title="2.2 短路连接"></a>2.2 短路连接</h3><p><strong>Shortcut连接</strong>。Shortcut连接已经经过了很长的一段实践和理论研究过程。训练多层感知器（MLPs）的一个早期实践就是添加一个连接输入和输出的线性层。在Szegedy2015Going及Lee2015deeply中，将一些中间层直接与辅助分类器相连接可以解决梯度消失/爆炸问题。在 Szegedy2015Going中，一个“inception”层由一个shortcut分支和一些更深的分支组合而成。</p>
<p>与此同时，“highway networks”将shortcut连接与门控函数 结合起来。这些门是数据相关并且是有额外参数的，而我们的恒等shortcuts是无参数的。当一个门的shortcut是“closed”（接近于0）时，highway网络中的层表示非残差函数。相反的，我们的模型总是学习残差函数；我们的恒等shortcuts从不关闭，在学习额外的残差函数时，所有的信息总是通过的。此外，highway网络并不能由增加层的深度（例如， 超过100层）来提高准确率。</p>
<blockquote>
<h3 id="主要内容-2"><a href="#主要内容-2" class="headerlink" title="主要内容"></a>主要内容</h3><p>（1）Shortcut连接已经经过了很长的一段实践和理论研究过程，证明是有效的。</p>
<p>（2）和highway networks（门控函数）对比：当一个门的shortcut是“closed”（接近于0）时，highway networks中的层表示非残差函数。相反的，我们的模型总是学习残差函数；我们的恒等shortcuts从不关闭，是无参数的，在学习额外的残差函数时，所有的信息总是通过的。此外，highway networks并不能由增加层的深度（例如，超过100层）来提高准确率。</p>
</blockquote>
<h2 id="3-深度残差学习"><a href="#3-深度残差学习" class="headerlink" title="3 深度残差学习"></a>3 深度残差学习</h2><h3 id="3-1-残差学习"><a href="#3-1-残差学习" class="headerlink" title="3.1 残差学习"></a>3.1 残差学习</h3><p>我们将H(x)看作一个由部分堆叠的层（并不一定是全部的网络）来拟合的底层映射，其中x是这些层的输入。假设多个非线性层能够逼近复杂的函数，这就等价于这些层能够逼近复杂的残差函数，例如, H(x)−x（假设输入和输出的维度相同）。所以我们明确的让这些层来估计一个残差函数：F(x)=H(x)−x而不是H(x)。因此原始函数变成了：F(x)+x。尽管这两个形式应该都能够逼近所需的函数（正如假设），但是学习的难易程度并不相同。</p>
<p>这个重新表达的动机是由退化问题这个反常的现象(Fig.1，左)。正如我们在Introduction中讨论的，如果增加的层能以恒等映射来构建，一个更深模型的训练错误率不应该比它对应的浅层模型的更大。退化问题表明了，求解器在通过多个非线性层来估计恒等映射上可能是存在困难的。而伴随着残差学习的重新表达，如果恒等映射是最优的，那么求解器驱使多个非线性层的权重趋向于零来逼近恒等映射。</p>
<p>在实际情况下，恒等映射不太可能达到最优，但是我们的重新表达对于这个问题的预处理是有帮助的。如果最优函数更趋近于恒等映射而不是0映射，那么对于求解器来说寻找关于恒等映射的扰动比学习一个新的函数要容易的多。通过实验(Fig.7)表明，学习到的残差函数通常只有很小的响应，说明了恒等映射提供了合理的预处理。</p>
<blockquote>
<h3 id="ResNet目的"><a href="#ResNet目的" class="headerlink" title="ResNet目的"></a>ResNet目的</h3><p>我们选择加深网络的层数，是希望深层的网络的表现能比浅层好，或者是希望它的表现至少和浅层网络持平（相当于直接复制浅层网络的特征）</p>
<h3 id="以前方法"><a href="#以前方法" class="headerlink" title="以前方法"></a>以前方法</h3><p>在正常的网络中，应该传递给下一层网络的输入是 H(x)=F(x)，即直接拟合H(x)</p>
<h3 id="本文改进"><a href="#本文改进" class="headerlink" title="本文改进"></a>本文改进</h3><p>在ResNet中，传递给下一层的输入变为H(x)=F(x)+x，即拟合残差F(x)=H(x)－x</p>
<p><strong>残差模块：</strong> 一条路不变（恒等映射）；另一条路负责拟合相对于原始网络的残差，去纠正原始网络的偏差，而不是让整体网络去拟合全部的底层映射，这样网络只需要纠正偏差。</p>
<h3 id="本质"><a href="#本质" class="headerlink" title="本质"></a>本质</h3><p>（1）加了残差结构后，给了输入x一个多的选择。若神经网络学习到这层的参数是冗余的时候，它可以选择直接走这条“跳接”曲线（shortcut connection），跳过这个冗余层，而不需要再去拟合参数使得H(x)=F(x)=x</p>
<p>（2）加了恒等映射后，深层网络至少不会比浅层网络更差。</p>
<p>（3）而在Resnet中，只需要把F(x)变为0即可，输出变为F(x)+x=0+x=x很明显，将网络的输出优化为0比将其做一个恒等变换要容易得多。</p>
<blockquote>
<p><strong>Q：为什么H(x)=F(x)+x中F(x)为0才有效？</strong></p>
<p>模型在训练过程中，F(x)是训练出来的，如果F(x)对于提高模型的训练精度无作用，自然梯度下降算法就调整该部分的参数，使该部分的效果趋近于0.这样整个模型就不会出现深度越深反而效果越差的情况了。</p>
</blockquote>
</blockquote>
<h3 id="3-2-通过短路连接进行恒等映射"><a href="#3-2-通过短路连接进行恒等映射" class="headerlink" title="3.2 通过短路连接进行恒等映射"></a>3.2 通过短路连接进行恒等映射</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/ResNet-3.2.png" alt=""></p>
<blockquote>
<h3 id="Shortcuts-Connection的两种方式："><a href="#Shortcuts-Connection的两种方式：" class="headerlink" title="Shortcuts Connection的两种方式："></a>Shortcuts Connection的两种方式：</h3><p><strong>（1）shortcuts同等维度映射，F(x)与x相加就是就是逐元素相加</strong></p>
<ul>
<li>y=F(x,Wi)+x</li>
<li>F=W2σ(W1x)</li>
</ul>
<p>其中 x 和 y 分别表示层的输入和输出。函数 F(x,Wi)代表着学到的残差映射，σ 代表ReLU</p>
<p>这种方式通过shortcuts直接传递输入x，不会引入额外的参数也不会增加模块的计算复杂性，因此可以公平地将残差网络和plain网络作比较。</p>
<p><strong>（2）如果两者维度不同（改变了输入/输出的通道），需要给x执行一个线性映射来匹配维度</strong></p>
<ul>
<li>y=F(x,Wi)+Wsx.</li>
<li>F=W2σ(W1x)</li>
</ul>
<p>这种方式的目的仅仅是为了保持x与F(x)之间的维度一致，所以通常只在相邻残差块之间通道数改变时使用，绝大多数情况下仅使用第一种方式。</p>
<p>用卷积层进行残差学习：以上的公式表示为了简化，都是基于全连接层的，实际上当然可以用于卷积层。加法随之变为对应channel间的两个feature map逐元素相加。</p>
</blockquote>
<h3 id="3-3-网络架构"><a href="#3-3-网络架构" class="headerlink" title="3.3 网络架构"></a>3.3 网络架构</h3><p>我们在多个plain网络和残差网络上进行了测试，并都观测到了一致的现象。接下来我们将在ImageNet上对两个模型进行讨论。</p>
<h4 id="Plain网络"><a href="#Plain网络" class="headerlink" title="Plain网络"></a>Plain网络</h4><p>我们的plain网络结构(Fig.3，中)主要受VGG网络 (Fig.3，左)的启发。</p>
<p>卷积层主要为3*3的滤波器，并遵循以下两点要求：(i) 输出特征尺寸相同的层含有相同数量的滤波器；(ii) 如果特征尺寸减半，则滤波器的数量增加一倍来保证每层的时间复杂度相同。我们直接通过stride 为2的卷积层来进行下采样。在网络的最后是一个全局的平均pooling层和一个1000 类的包含softmax的全连接层。加权层的层数为34，如Fig.3(中)所示。</p>
<p>值得注意的是，我们的模型比VGG网络(Fig.3，左)有更少的滤波器和更低的计算复杂度。我们34层的结构含有36亿个FLOPs（乘-加），而这仅仅只有VGG-19 （196亿个FLOPs）的18%。</p>
<h4 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h4><p>基于上述plain网络，我们插入了快捷连接（图3，右侧），将网络变成其残差版本。恒等快捷连接（公式（1））在输入和输出具有相同维度时可以直接使用（图3中的实线快捷方式）。当维度增加时（图3中的虚线快捷方式），我们考虑两个选项：（A）快捷方式仍然执行恒等映射，为增加的维度填充额外的零条目。此选项不引入额外的参数；（B）使用公式（2）中的投影快捷方式来匹配维度（由1×1卷积完成）。对于这两个选项，当快捷方式穿过两个大小的特征图时，它们会以步幅2执行。<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Resnet-Figure3.png" alt=""></p>
<blockquote>
<p>图3。ImageNet的示例网络架构。左侧：作为参考的VGG-19模型[41]（19.6亿浮点运算）。中间：一个具有34个参数层的简单网络（3.6亿浮点运算）。右侧：一个具有34个参数层的残差网络（3.6亿浮点运算）。点状的快捷方式增加了维度。表1显示了更多细节和其他变体。</p>
<h3 id="Plain网络-1"><a href="#Plain网络-1" class="headerlink" title="Plain网络"></a>Plain网络</h3><p>plain网络结构主要受VGG网络的启发。 卷积层主要为3*3的卷积核，直接通过stride为2的卷积层来进行下采样。在网络的最后是一个全局的平均pooling层和一个1000类的包含softmax的全连接层。加权层的层数为34。</p>
<p><strong>两条设计准则：</strong></p>
<p>（i）同样输出大小的特征图，有着相同数量的卷积核；</p>
<p>（ii）如果特征图大小减半，为了保证相同的时间复杂度，卷积核个数加倍。</p>
<p><strong>与VGG对比：</strong></p>
<p>我们的模型比VGG有更少的卷积核和更低的计算复杂度。我们34层的结构含有36亿个FLOPs（乘-加），而这仅仅只有VGG-19 （196亿个FLOPs）的18%。</p>
<h3 id="残差网络-1"><a href="#残差网络-1" class="headerlink" title="残差网络"></a>残差网络</h3><p>在plain网络的基础上，加入shortcuts连接，就变成了相应的残差网络</p>
<p>如上图，实线代表维度一样，直接相加 。虚线代表维度不一样（出现了下采样，步长为2的卷积），使用残差网络</p>
<p><strong>调整维度的方法有两种：</strong></p>
<p><strong>（1）zero-padding：对多出来的通道padding补零填充，这种方法不会引入额外的参数；</strong></p>
<p><strong>（2）线性投影变换：用1*1卷积升维，是需要学习的参数，精度比zero-padding更好，但是耗时更长，占用更多内存。</strong></p>
<p>这两种方法都使用stride为2的卷积。</p>
</blockquote>
<h3 id="3-4-实现"><a href="#3-4-实现" class="headerlink" title="3.4 实现"></a>3.4 实现</h3><p>我们在ImageNet上的实现遵循[21, 41]中的实践。为了进行尺度增强[41]，图像被调整大小，其较短的一侧在[256, 480]范围内进行随机采样。从图像或其水平翻转中随机采样一个224×224的裁剪，减去每像素均值[21]。使用[21]中的标准颜色增强。我们在每个卷积层之后和激活之前采用批量归一化（BN）[16]，按照[16]的做法。我们将权重初始化为[13]中的值，并从头开始训练所有普通/残差网络。我们使用小批量大小为256的SGD。当错误达到平稳状态时，学习速率从0.1开始，每次除以10，模型最多训练60×104次迭代。我们使用0.0001的权重衰减和0.9的动量。我们不使用[14]中的dropout，遵循[16]的实践。</p>
<p>在测试中，为了进行比较研究，我们采用标准的10-crop测试[21]。为了获得最佳结果，我们采用与[41, 13]相同的完全卷积形式，并在多个尺度上平均分数（调整图像大小，使较短的一侧在{224, 256, 384, 480, 640}中）</p>
<blockquote>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>（1）图像分别随机被压缩到256到480之间，之后做图像增强</p>
<p>（2）输出处理过程：用224 * 224 随机裁出一个小图，在做水平的镜像来做图像增强（不同尺度维度），10个小图汇总成一个大图（可使用多尺度裁剪和结果融合）。</p>
<p>（3）每个卷积层后面或者激活层之前都使用BN</p>
<p><strong>参数：</strong> mini-batch为256，学习率为0.1，训练60万的迭代次数，正则化0.0001，动量是0.9。没有使用dropout（BN和dropout不能混合使用，单独使用效果更佳，原因：方差偏移）</p>
</blockquote>
<h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4 实验"></a>4 实验</h2><h3 id="4-1-ImageNet分类"><a href="#4-1-ImageNet分类" class="headerlink" title="4.1. ImageNet分类"></a>4.1. ImageNet分类</h3><p>本文在1000类的ImageNet2012数据集上对我们的方法进行评估。训练集包含128万张图像，验证集包含5万张图像。我们在10万张测试图像上进行测试，并对top-1和top-5 的错误率进行评估。</p>
<h4 id="Plain网络-2"><a href="#Plain网络-2" class="headerlink" title="Plain网络"></a>Plain网络</h4><p>我们首先评估了18层和34层的plain网络。34层的网络如图Fig.3(中)所示。18层的结构很相似，具体细节参见Table 1。</p>
<p>Table 2中展示的结果表明了34层的网络比18层的网络具有更高的验证错误率。为了揭示产生这种现象的原因，在Fig.4(左)中我们比较了整个训练过程中的训练及验证错误率。从结果中我们观测到了明显的退化问题——在整个训练过程中34 层的网络具有更高的训练错误率，即使18层网络的解空间为34层解空间的一个子空间。</p>
<p>我们认为这种优化上的困难不太可能是由梯度消失所造成的。因为这些plain网络的训练使用了BN，这能保证前向传递的信号是具有非零方差的。我们同样验证了在反向传递阶段的梯度由于BN而具有良好的范式，所以在前向和反向阶段的信号不会存在消失的问题。事实上34层的plain网络仍然具有不错的准确率(Table 3)，这表明了求解器在某种程度上也是有效的。我们推测，深层的plain网络的收敛率是指数衰减的，这可能会影响训练错误率的降低。这种优化困难的原因我们将在以后的工作中进行研究。</p>
<blockquote>
<p>首先进行的实验是18层和34层的plain网络，实验结果如下表所示，产生了一种退化现象：<strong>在训练过程中34层的网络比18层的网络有着更高的训练错误率。</strong></p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="C:/Users/17145/AppData/Roaming/Typora/typora-user-images/image-20240214120618120.png" alt="image-20240214120618120"></p>
<blockquote>
<p>表1。ImageNet的架构。构建块在括号中显示（也见图5），以及堆叠的构建块数量。通过使用步长为2的conv3_1、conv4_1和conv5_1执行下采样。</p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/ResNet-Figure4.png" alt=""></p>
<blockquote>
<p>图4。在ImageNet上的训练。细曲线表示训练误差，粗曲线表示中心裁剪的验证误差。左侧：18层和34层的简单网络。右侧：18层和34层的残差网络（ResNets）。在此图中，相比于它们的简单对应网络，残差网络没有额外的参数。</p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Resnet-Table2.png" alt=""></p>
<blockquote>
<p>表2。ImageNet验证集上的Top-1错误率（%，10次裁剪测试）。在这里，与它们的简单对应网络相比，残差网络（ResNets）没有额外的参数。图4展示了训练过程。</p>
<p>首先进行的实验是18层和34层的plain网络，实验结果如下表所示，产生了一种退化现象：<strong>在训练过程中34层的网络比18层的网络有着更高的训练错误率。</strong>（细线：训练集上的误差； 粗线：测试集上的误差）</p>
</blockquote>
<h4 id="残差网络-2"><a href="#残差网络-2" class="headerlink" title="残差网络"></a><strong>残差网络</strong></h4><p>接下来我们对18层和34层的残差网络ResNets进行评估。如Fig.3 (右)所示，ResNets的基本框架和plain网络的基本相同，除了在每一对3*3的滤波器上添加了一个shortcut连接。在Table 2以及Fig.4(右)的比较中，所有的shortcuts都是恒等映射，并且使用0对增加的维度进行填充(选项 A)。因此他们并没有增加额外的参数。</p>
<p>我们从Table 2和Fig.4中观测到以下三点：</p>
<p>第一，与plain网络相反，34层的ResNet比18层ResNet的结果更优(2.8%)。更重要的是，34 层的ResNet在训练集和验证集上均展现出了更低的错误率。这表明了这种设置可以很好的解决退化问题，并且我们可以由增加的深度来提高准确率。</p>
<p>第二，与对应的plain网络相比，34层的ResNet在top-1 错误率上降低了3.5% (Table 2)，这得益于训练错误率的降低(Fig.4 右 vs 左)。这也验证了在极深的网络中残差学习的有效性。</p>
<p>最后，我们同样注意到，18层的plain网络和残差网络的准确率很接近 (Table 2)，但是ResNet 的收敛速度要快得多。(Fig.4 右 vs 左)。<br>如果网络“并不是特别深” (如18层)，现有的SGD能够很好的对plain网络进行求解，而ResNet能够使优化得到更快的收敛。</p>
<blockquote>
<p>接着对18层和34层的残差网络进行评估，为了保证变量的一致性，其基本框架结构和plain网络的结构相同，只是在每一对卷积层上添加了shortcuts连接来实现残差结构，对于维度不匹配的情况，使用0来填充维度（即3.3介绍过的方法1），因此也并没有添加额外的参数。训练结果如下图所示</p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>（1）与plain网络相反，34层的resnet网络比18层的错误率更低，表明可以通过增加深度提高准确率，解决了退化问题。</p>
<p>（2）与plain网络相比，层次相同的resnet网络上错误率更低，表明残差网络在深层次下仍然有效。</p>
<p>（3）对于18层的plain网络，它和残差网络的准确率很接近，但是残差网络的收敛速度要更快。</p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Resnet-Table3.png" alt=""></p>
<blockquote>
<p>表3。ImageNet验证集上的错误率（%，10次裁剪测试）。VGG-16基于我们的测试。ResNet-50/101/152属于选项B，仅使用投影来增加维度。</p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Resnet-Table4.png" alt=""></p>
<blockquote>
<p>表4。ImageNet验证集上单模型结果的错误率（%）（除了†报告在测试集上）。</p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Resnet-Table5.png" alt=""></p>
<blockquote>
<p>表5。集成的错误率（%）。Top-5错误率是在ImageNet测试集上，并由测试服务器报告。</p>
</blockquote>
<h4 id="恒等-vs-映射-Shortcuts"><a href="#恒等-vs-映射-Shortcuts" class="headerlink" title="恒等 vs 映射 Shortcuts"></a>恒等 vs 映射 Shortcuts</h4><p>我们已经验证了无参数的恒等shortcuts是有助于训练的。接下来我们研究映射shortcut(Eq.2)。在Table 3中，我们比较了三种选项：<br>(A) 对增加的维度使用0填充，所有的shortcuts是无参数的(与Table 2 和 Fig.4 (右)相同)；<br>(B) 对增加的维度使用映射shortcuts，其它使用恒等shortcuts；<br>(C) 所有的都是映射shortcuts。</p>
<p>Table 3表明了三种选项的模型都比对于的plain模型要好。B略好于A，我们认为这是因为A中的0填充并没有进行残差学习。C略好于B，我们把这个归结于更多的（13个）映射shortcuts所引入的参数。在A、B、C三个结果中细小的差距也表明了映射shortcuts对于解决退化问题并不是必需的。所以我们在本文接下来的内容中，为了减少复杂度和模型尺寸，并不使用选项C的模型。恒等shortcuts因其无额外复杂度而对以下介绍的瓶颈结构尤为重要。</p>
<blockquote>
<p>无参数的恒等shortcuts肯定是有助于提高训练效果的，针对映射shortcuts，有三种方法可供选择：</p>
<p><strong>（1）ResNet - 34 A：</strong> 所有的shortcut都使用恒等映射，也就是多出来的通道补0，没有额外的参</p>
<p><strong>（2）ResNet - 34 B：</strong> 对需要调整维度的使用卷积映射shortcut来实现，不需要调整维度的使用恒等shortcut，升维的时候使用1 * 1卷积</p>
<p><strong>（3）ResNet - 34 C：</strong> 所有的shortcut都使用1 * 1卷积（效果最好，但引入更多的参数，不经济）</p>
<p>B比A好，因为A在升维的时候用padding补零，丢失了shortcut学习，没有进行残差学习</p>
<p>C比B好，因为C的13个非下采样残差模块的shortcut都有参数，模型能力比较强</p>
<p>但是ABC都差不多，说明恒等映射的shortcut可以解决退化问题</p>
</blockquote>
<h4 id="深度瓶颈结构"><a href="#深度瓶颈结构" class="headerlink" title="深度瓶颈结构"></a>深度瓶颈结构</h4><p>接下来我们介绍更深的模型。考虑到训练时间的限制，我们将构建块修改成瓶颈的设计。对于每一个残差函数F，我们使用了三个叠加层而不是两个(Fig.5)。 这三层分别是1<em>1、3</em>3 和1<em>1 的卷积，1</em>1 的层主要负责减少然后增加（恢复）维度，剩下的3*3的层来减少输入和输出的维度。Fig.5展示了一个例子，这两种设计具有相似的时间复杂度。</p>
<p><strong>无参数</strong> 的恒等shortcuts对于瓶颈结构尤为重要。如果使用映射shortcuts来替代Fig.5(右)中的恒等shortcuts，将会发现时间复杂度和模型尺寸都会增加一倍，因为shortcut连接了两个高维端，所以恒等shortcuts对于瓶颈设计是更加有效的。</p>
<p><strong>50层 ResNet</strong>：我们将34层网络中2层的模块替换成3层的瓶颈模块，整个模型也就变成了50层的ResNet (Table 1)。对于增加的维度我们使用选项B来处理。整个模型含有38亿个FLOPs。</p>
<p><strong>101层和152层 ResNets：</strong> 我们使用更多的3层模块来构建101层和152层的ResNets (Table 1)。值得注意的是，虽然层的深度明显增加了，但是152层ResNet的计算复杂度(113亿个FLOPs)仍然比VGG-16(153 亿个FLOPs)和VGG-19(196亿个FLOPs）的小很多。</p>
<p>50/101/152层ResNets比34层ResNet的准确率要高得多(Table 3 和4)。而且我们并没有观测到退化问题。所有的指标都证实了深度带来的好处。 (Table 3 和4)。</p>
<blockquote>
<p>接下来介绍层次更多的模型，对于每一个残差块，不再使用两层卷积，而是使用三层卷积来实现，如下图所示。</p>
<p><strong>50层的残差网络：</strong> 将其34层的残差网络的2个卷积层替换成了3个bottleneck残差块，就变成了50层残差网络，下采样使用的是1 * 1 的卷积</p>
<h3 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h3><p>50/101/152层的resnet比34层resnet的准确率要高很多，解决了深层的退化问题。同时即使是152层resnet的计算复杂度仍然比VGG-16和VGG-19要小。</p>
</blockquote>
<h4 id="与最优秀方法的比较"><a href="#与最优秀方法的比较" class="headerlink" title="与最优秀方法的比较"></a>与最优秀方法的比较</h4><p>在Table 4中我们比较了目前最好的单模型结果。我们的34层ResNets取得了非常好的结果，152层的ResNet的单模型top-5验证错误率仅为 4.49%，甚至比先前组合模型的结果还要好 (Table 5)。我们将6个不同深度的ResNets合成一个组合模型(在提交结果时只用到2个152层的模型)。这在测试集上的top-5错误率仅为3.57% (Table 5)，这一项在ILSVRC 2015 上获得了第一名的成绩。</p>
<blockquote>
<p>将6个不同深度的ResNets合成一个组合模型(在提交结果时只用到2个152层的模型)。这在测试集上的top-5错误率仅为3.57% (Table 5)，这一项在ILSVRC 2015 上获得了第一名的成绩。</p>
</blockquote>
<h3 id="4-2-CIFAR-10和分析"><a href="#4-2-CIFAR-10和分析" class="headerlink" title="4.2. CIFAR-10和分析"></a>4.2. CIFAR-10和分析</h3><p>我们在包含5万张训练图像和1万张测试图像的10类CIFAR-10数据集上进行了更多的研究。我们在训练集上进行训练，在测试集上进行验证。我们关注的是验证极深模型的效果，而不是追求最好的结果，因此我们只使用简单的框架如下。</p>
<p>Plain网络和残差网络的框架如 Fig.3(中/右)所示。网络的输入是<code>32*32</code>的减掉像素均值的图像。第一层是3<em>3的卷积层。然后我们使用6n个`3</em>3`的卷积层的堆叠，卷积层对应的特征图有三种：{32,16,8}，每一种卷积层的数量为2n 个，对应的滤波器数量分别为{16,32,64}。使用strde为2的卷积层进行下采样。在网络的最后是一个全局的平均pooling层和一个10类的包含softmax的全连接层。一共有6n+2个堆叠的加权层。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Resnet-4.2.png" alt=""></p>
<p>权重的衰减设置为0.0001，动量为0.9，采用了He2015Delving中的权值初始化以及BN，但是不使用Dropout，mini-batch的大小为128，模型在2块GPU 上进行训练。学习率初始为0.1，在第32000和48000次迭代时将其除以10，总的迭代次数为64000，这是由45000/5000的训练集/验证集分配所决定的。我们在训练阶段遵循Lee2015deeply中的数据增强法则：在图像的每条边填充4个像素，然后在填充后的图像或者它的水平翻转图像上随机采样一个<code>32*32</code> 的crop。在测试阶段，我们只使用原始<code>32*32</code>的图像进行评估。</p>
<p>我们比较了n={3,5,7,9}，也就是20、32、44以及56层的网络。Fig.6(左) 展示了plain网络的结果。深度plain网络随着层数的加深，训练错误率也变大。这个现象与在ImageNet(Fig.4, 左)和MNIST上的结果很相似，表明了优化上的难度确实是一个很重要的问题。</p>
<p>Fig.6(中)展示了ResNets的效果。与ImageNet(Fig.4, 右)中类似，我们的ResNets能够很好的克服优化难题，并且随着深度加深，准确率也得到了提升。</p>
<p>我们进一步探索了n=18，也就是110层的ResNet。在这里，我们发现0.1的初始学习率有点太大而不能很好的收敛。所以我们刚开始使用0.01的学习率，当训练错误率在80%以下(大约400次迭代)之后，再将学习率调回0.1继续训练。剩余的学习和之前的一致。110层的ResNets很好的收敛了 (Fig.6, 中)。它与其他的深层窄模型，如FitNet和 Highway (Table 6)相比，具有更少的参数，然而却达到了最好的结果 (6.43%, Table 6)。</p>
<blockquote>
<p><strong>CIFAR-10 数据集：</strong> 50w的训练集，10w的测试集，一共10个类别</p>
<h4 id="对比plain网络和残差网络的做法"><a href="#对比plain网络和残差网络的做法" class="headerlink" title="对比plain网络和残差网络的做法"></a>对比plain网络和残差网络的做法</h4><p>（1）输入的图像为32*32的像素，此时的图像做了预处理（每个像素减去均值）</p>
<p>（2）第一个卷积层为3<em>3 ，使用6n的卷积层，分别都是3</em>3的，feature map为（3232/1616/ 8*8)。一共有6n+2的卷积层（最后一层为池化层：1 +2n，2n，2n，1）</p>
<p>（3）卷积核个数分别为16/32/64，feature map个数减半，channel数翻倍</p>
<blockquote>
<p><strong>Q：为什么下采样之后feature map尺寸减半，通道个数翻倍？</strong></p>
<p>因为池化会让长宽方向减半，卷积核个数对应通道加倍（详情见《MobileNet》）</p>
</blockquote>
<p>下采样用的是步长为2的卷积，最后加一个全局池化，10个神经元的全连接层和softmax</p>
<p>（1）残差是由2层神经网络（每一个shortcut都由3 * 3的卷积组成）来拟合的，总共有6n，所以一共有3n的shortcut。</p>
<p>（2）下采样是由0补充（下采样的残差和不带残差的计算量是一样的）</p>
<p>（3）训练过程中的正则化为0.0001 ，动量化为0.9 ，论文中提出的权重进行初始化，使用了BN没有使用dropout，批次处理为128，起始的学习率为0.1，在3.2w和4.8w迭代时除以10，最终在6.4w终止训练</p>
<p>（4）把训练集划分为4.5w训练和5k的验证，使用图像增强方法，分别在图像外边补4个pixel，再用32 <em>32 的图像进行剪裁（水平翻转的图像增强）。测试的时候，直接使用32 </em> 32的图像进行测试即可</p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Resnet-Table6.png" alt=""></p>
<blockquote>
<p> 表6。CIFAR-10测试集上的分类错误率。所有方法都使用了数据增强。对于ResNet-110，我们运行了5次，并显示“最好的（平均值±标准差）”，如[43]中所示。</p>
</blockquote>
<h4 id="分析每一层的网络的响应分布"><a href="#分析每一层的网络的响应分布" class="headerlink" title="分析每一层的网络的响应分布"></a>分析每一层的网络的响应分布</h4><p>Fig.7展示了层响应的标准方差(std)。 响应是每一个3*3卷积层的BN之后、非线性层(ReLU/addition)之前的输出。对于ResNets，这个分析结果也揭示了残差函数的响应强度。Fig.7表明了ResNets的响应比它对应的plain网络的响应要小。这些结果也验证了我们的基本动机(Sec3.1)，即残差函数比非残差函数更接近于0。从Fig.7中ResNet-20、56和110的结果，我们也注意到，越深的ResNet的响应幅度越小。当使用更多层是，ResNets中单个层对信号的改变越少。</p>
<blockquote>
<p>残差网络是修正输入。响应的标准差如下图7</p>
<h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p>BN处理，均值已被调整为0。标准差衡量数据的离散程度（标准差越大，表明响应越大） 响应是每一层都是3 * 3的卷积层，介于BN后和激活之前。</p>
<h3 id="结论-2"><a href="#结论-2" class="headerlink" title="结论"></a>结论</h3><p>（1）ResNets的响应比它对应的plain网络的响应要小</p>
<p>（2）残差函数比非残差函数更接近于0</p>
<p>（3）越深的ResNet的响应幅度越小</p>
<p>（4）越靠近起始层，输出越大</p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Resnet-Figure6.png" alt=""></p>
<blockquote>
<p>图6。在CIFAR-10上训练。虚线表示训练误差，粗线表示测试误差。左侧：简单网络。plain-110的误差高于60%，未显示。中间：ResNets。右侧：具有110层和1202层的ResNets。</p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Resnet-Figure7.png" alt=""></p>
<blockquote>
<p>图7。CIFAR-10上层响应的标准偏差（std）。这些响应是每个3×3层的输出，在批量归一化（BN）之后和非线性操作之前。顶部：层按它们原始的顺序显示。底部：响应按降序排列。</p>
</blockquote>
<h4 id="探索超过1000层网络"><a href="#探索超过1000层网络" class="headerlink" title="探索超过1000层网络"></a>探索超过1000层网络</h4><p>我们探索了一个超过1000层的极其深的模型。我们设置n=200，也就是1202层的网络模型，按照上述进行训练。我们的方法对103103层的模型并不难优化，并且达到了&lt;0.1%的训练错误率(Fig.6, 右)，它的测试错误率也相当低(7.93%, Table 6)。</p>
<p>但是在这样一个极其深的模型上，仍然存在很多问题。1202层模型的测试结果比110层的结果要差，尽管它们的训练错误率差不多。我们认为这是过拟合导致的。这样一个1202层的模型对于小的数据集来说太大了(19.4M)。在这个数据集上应用了强大的正则化方法，如maxout或者 dropout，才获得了最好的结果。</p>
<p>本文中，我们并没有使用maxout/dropout，只是简单的通过设计深层窄模型来进行正则化，而且不用担心优化的难度。但是通过强大的正则化或许能够提高实验结果，我们会在以后进行研究。</p>
<blockquote>
<p>取n等于200 ，也就是1202的残差卷积网络（6 * 200 + 2），和之前的训练方式一样，误差小于0.1，表明了没有退化，没优化困难。</p>
<p>但测试集的性能没有110层的好，文中表明这是过拟合了（模型太深参数过多，对于这个小数据集没有必要）</p>
<p>此论文没有使用maxout或者是dropout来正则化，因为核心任务是为了解决退化问题。</p>
</blockquote>
<h3 id="4-3-PASCAL和MS-COCO上的对象检测"><a href="#4-3-PASCAL和MS-COCO上的对象检测" class="headerlink" title="4.3 PASCAL和MS COCO上的对象检测"></a>4.3 PASCAL和MS COCO上的对象检测</h3><p>我们的方法在其它识别任务上展现出了很好的泛化能力。Table 7和8展示了在PASCAL VOC 2007 和 2012以及 COCO上的目标检测结果。我们使用Faster R-CNN作为检测方法。在这里，我们比较关注由ResNet-101 替换VGG-16所带来的的提升。使用不同网络进行检测的实现是一样的，所以检测结果只能得益于更好的网络。最值得注意的是，在COCO数据集上，我们在COCO的标准指标(mAP@[.5, .95])上比先前的结果增加了6.0%，这相当于28%的相对提升。而这完全得益于所学到的表达。</p>
<p>基于深度残差网络，我们在ILSVRC &amp; COCO 2015竞赛的ImageNet检测、ImageNet定位、COCO检测以及COCO分割上获得了第一名。</p>
<blockquote>
<p>【Table 7 在PASCAL VOC 2007/2012测试集上使用Faster R-CNN的目标检测 mAP (%)。有关更好的结果，请参见附录。】</p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Resnet-Table7.png" alt=""></p>
<blockquote>
<p>表7。使用基线Faster R-CNN在PASCAL VOC 2007/2012测试集上的对象检测mAP（%）。更好的结果见表10和表11。</p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Resne-Table8.png" alt=""></p>
<blockquote>
<p>表8。使用基线Faster R-CNN在COCO验证集上的对象检测mAP（%）。更好的结果见表9。</p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://www.chitose.cn">Chitose</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://www.chitose.cn/Resnet-paper/">https://www.chitose.cn/Resnet-paper/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.chitose.cn" target="_blank">Chitose-Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_4.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/LeNet-paper/" title="LeNet-paper"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_6.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">LeNet-paper</div></div></a></div><div class="next-post pull-right"><a href="/VGG-paper/" title="VGG-paper"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_9.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">VGG-paper</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Face.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Chitose</div><div class="author-info__description">Hahaha</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">52</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/chitose-r"><i class="fab fa-github"></i><span>🛴/前往小家..</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/chitose-r" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:171450290@qq.com" target="_blank" title="Email"><i class="fa-solid fa-square-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="/qq/" target="_blank" title="QQ"><i class="fab fa-qq" style="color: #12b7f5;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Deep-Residual-Learning-for-Image-Recognition"><span class="toc-text">Deep Residual Learning for Image Recognition</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E7%9A%84%E6%B7%B1%E5%BA%A6%E6%AE%8B%E5%B7%AE%E5%AD%A6%E4%B9%A0-2016"><span class="toc-text">图像识别的深度残差学习  2016</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-text">摘要</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9"><span class="toc-text">主要内容</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%8B%E7%BB%8D"><span class="toc-text">1 介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-text">2 相关工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%AE%8B%E5%B7%AE%E8%A1%A8%E8%BE%BE"><span class="toc-text">2.1 残差表达</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9-1"><span class="toc-text">主要内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E7%9F%AD%E8%B7%AF%E8%BF%9E%E6%8E%A5"><span class="toc-text">2.2 短路连接</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9-2"><span class="toc-text">主要内容</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%B7%B1%E5%BA%A6%E6%AE%8B%E5%B7%AE%E5%AD%A6%E4%B9%A0"><span class="toc-text">3 深度残差学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%AE%8B%E5%B7%AE%E5%AD%A6%E4%B9%A0"><span class="toc-text">3.1 残差学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ResNet%E7%9B%AE%E7%9A%84"><span class="toc-text">ResNet目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A5%E5%89%8D%E6%96%B9%E6%B3%95"><span class="toc-text">以前方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E6%94%B9%E8%BF%9B"><span class="toc-text">本文改进</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E8%B4%A8"><span class="toc-text">本质</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E9%80%9A%E8%BF%87%E7%9F%AD%E8%B7%AF%E8%BF%9E%E6%8E%A5%E8%BF%9B%E8%A1%8C%E6%81%92%E7%AD%89%E6%98%A0%E5%B0%84"><span class="toc-text">3.2 通过短路连接进行恒等映射</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Shortcuts-Connection%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F%EF%BC%9A"><span class="toc-text">Shortcuts Connection的两种方式：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc-text">3.3 网络架构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Plain%E7%BD%91%E7%BB%9C"><span class="toc-text">Plain网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C"><span class="toc-text">残差网络</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Plain%E7%BD%91%E7%BB%9C-1"><span class="toc-text">Plain网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C-1"><span class="toc-text">残差网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E5%AE%9E%E7%8E%B0"><span class="toc-text">3.4 实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-text">方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%AE%9E%E9%AA%8C"><span class="toc-text">4 实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-ImageNet%E5%88%86%E7%B1%BB"><span class="toc-text">4.1. ImageNet分类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Plain%E7%BD%91%E7%BB%9C-2"><span class="toc-text">Plain网络</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C-2"><span class="toc-text">残差网络</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-text">结论</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%81%92%E7%AD%89-vs-%E6%98%A0%E5%B0%84-Shortcuts"><span class="toc-text">恒等 vs 映射 Shortcuts</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E7%93%B6%E9%A2%88%E7%BB%93%E6%9E%84"><span class="toc-text">深度瓶颈结构</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA-1"><span class="toc-text">结论</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8E%E6%9C%80%E4%BC%98%E7%A7%80%E6%96%B9%E6%B3%95%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-text">与最优秀方法的比较</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-CIFAR-10%E5%92%8C%E5%88%86%E6%9E%90"><span class="toc-text">4.2. CIFAR-10和分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E6%AF%94plain%E7%BD%91%E7%BB%9C%E5%92%8C%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C%E7%9A%84%E5%81%9A%E6%B3%95"><span class="toc-text">对比plain网络和残差网络的做法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E6%9E%90%E6%AF%8F%E4%B8%80%E5%B1%82%E7%9A%84%E7%BD%91%E7%BB%9C%E7%9A%84%E5%93%8D%E5%BA%94%E5%88%86%E5%B8%83"><span class="toc-text">分析每一层的网络的响应分布</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95-1"><span class="toc-text">方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA-2"><span class="toc-text">结论</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%A2%E7%B4%A2%E8%B6%85%E8%BF%871000%E5%B1%82%E7%BD%91%E7%BB%9C"><span class="toc-text">探索超过1000层网络</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-PASCAL%E5%92%8CMS-COCO%E4%B8%8A%E7%9A%84%E5%AF%B9%E8%B1%A1%E6%A3%80%E6%B5%8B"><span class="toc-text">4.3 PASCAL和MS COCO上的对象检测</span></a></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By Chitose</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"></div><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script defer src="/js/cursor.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Python/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🥩 Python (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/C/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🕶️ C (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Embedded/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">💳 Embedded (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Pytorch/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📯 Pytorch (9)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Paper/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📰 Paper (18)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/others/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🤡 others (14)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/segmentation/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🔪 segmentation (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Model/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🗞️ Model (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item" style="visibility: hidden"></div><a class="magnet_link_more"  href="https://www.chitose.cn/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(33.333333333333336% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v6.2.0" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly_v4.3.1" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" data-title="本站采用多线部署，主线路托管于Vercel" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-Vercel-brightgreen?style=flat&amp;logo=Vercel" alt=""/></a><a class="github-badge" target="_blank" href="https://dashboard.4everland.org/" style="margin-inline:5px" data-title="本站采用多线部署，备用线路托管于4EVERLAND" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-4EVERLAND-22DDDD?style=flat&amp;logo=IPFS" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="/js/runtime.js"></script><script data-pjax>
  function butterfly_clock_anzhiyu_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock_anzhiyu')
    if(parent_div_git) {
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var qweather_key = '6be604177b8a4c3e97c78a352ee324f7';
  var gaud_map_key = '17b299fafade134736e6a1d4acb5ef18';
  var baidu_ak_key = 'undefined';
  var flag = 0;
  var clock_rectangle = '113.34532,23.15624';
  var clock_default_rectangle_enable = 'false';

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_anzhiyu_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_anzhiyu_injector_config();
  }
  </script><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script data-pjax src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.js"></script><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023-12-17-2/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_8.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023-12-17-2/" alt="">第二篇文章</a><div class="blog-slider__text">这是第二篇文章</div><a class="blog-slider__button" href="2023-12-17-2/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Pytorch-6/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_6.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-19</span><a class="blog-slider__title" href="Pytorch-6/" alt="">Pytorch(6)-张量可微性</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="Pytorch-6/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023-12-17-3/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_10.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023-12-17-3/" alt="">第三篇文章</a><div class="blog-slider__text">这是第三篇文章</div><a class="blog-slider__button" href="2023-12-17-3/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body></html>