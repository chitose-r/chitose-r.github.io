<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Todo</title>
      <link href="/Todo/"/>
      <url>/Todo/</url>
      
        <content type="html"><![CDATA[<h2 id="论文篇"><a href="#论文篇" class="headerlink" title="论文篇"></a>论文篇</h2><h3 id="图像分类论文"><a href="#图像分类论文" class="headerlink" title="图像分类论文"></a>图像分类论文</h3><ul><li>[ ] LeNet</li><li>[x] NiN</li><li>[ ] AlexNet</li><li>[ ] Vgg</li><li>[ ] AlexNet</li><li>[ ] ResNet</li><li>[ ] DenseNet</li><li>[ ] ResNeXt</li><li>[ ] SENet</li><li>[x] Inception-v1</li><li>[x] Inception-v2</li><li>[x] Inception-v3</li><li>[ ] Inception-v4</li><li>[ ] Inception-v5</li><li><h3 id="目标检测论文"><a href="#目标检测论文" class="headerlink" title="目标检测论文"></a>目标检测论文</h3></li><li>[ ] SPP</li><li>[ ] RCNN</li><li>[ ] Fast RCNN</li><li>[ ] Faster RCNN</li><li>[ ] Mask RCNN</li><li>[ ] Yolov1</li><li>[ ] Yolov2</li><li>[ ] Yolov3</li><li>[ ] Yolov4</li><li>[ ] Yolov6</li><li>[ ] Yolov7</li><li>[ ] MobileNet-v1</li><li>[ ] MobileNet-v2</li><li>[ ] MobileNet-v3</li><li>[ ] ShuffleNet-v1</li><li>[ ] ShuffleNet-v2</li><li>[ ] EfficientNet-v1</li><li>[ ] EfficientNet-v2</li><li>[ ] EfficientDet(EfficientNet+BiFPN)<h3 id="语义分割篇"><a href="#语义分割篇" class="headerlink" title="语义分割篇"></a>语义分割篇</h3></li><li>[ ] DeepLabV1</li><li>[ ] DeepLabV2</li><li>[ ] DeepLabV3</li><li>[ ] UNet</li><li>[ ] U2Net</li><li>[ ] LR-ASPP<h3 id="点云论文"><a href="#点云论文" class="headerlink" title="点云论文"></a>点云论文</h3></li><li>[ ] PointNet</li><li>[ ] PointNet++</li><li>[ ] PointNetMAE</li><li>[ ] DGCNN<h3 id="事件相机论文"><a href="#事件相机论文" class="headerlink" title="事件相机论文"></a>事件相机论文</h3></li><li>[ ] EVSegNet</li><li>[ ] DVS-OUTLAB</li><li>[ ] Evaluation of Deep Learning based 3D-Point-Cloud</li><li>[ ] Semantic Segmentation on NVS<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3></li><li>[ ] </li></ul>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>LeNet-paper</title>
      <link href="/LeNet-paper/"/>
      <url>/LeNet-paper/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Resnet-paper</title>
      <link href="/Resnet-paper/"/>
      <url>/Resnet-paper/</url>
      
        <content type="html"><![CDATA[<h1 id="Deep-Residual-Learning-for-Image-Recognition"><a href="#Deep-Residual-Learning-for-Image-Recognition" class="headerlink" title="Deep Residual Learning for Image Recognition"></a>Deep <a href="https://so.csdn.net/so/search?q=Residual&amp;spm=1001.2101.3001.7020">Residual</a> Learning for Image Recognition</h1><blockquote><h1 id="图像识别的深度残差学习-2016"><a href="#图像识别的深度残差学习-2016" class="headerlink" title="图像识别的深度残差学习  2016"></a>图像识别的深度残差学习  2016</h1></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>更深的神经网络往往更难以训练，我们在此提出一个残差学习的框架，以减轻网络的训练负担，这是个比以往的网络要深的多的网络。我们明确地将层作为输入学习残差函数，而不是学习未知的函数。我们提供了非常全面的实验数据来证明，残差网络更容易被优化，并且可以在深度增加的情况下让精度也增加。在ImageNet的数据集上我们评测了一个深度152层（是VGG的8倍）的残差网络，但依旧拥有比VGG更低的复杂度。残差网络整体达成了3.57%的错误率，这个结果获得了ILSVRC2015的分类任务第一名，我们还用CIFAR-10数据集分析了100层和1000层的网络。</p><p>在一些计算机视觉识别方向的任务当中，深度表示往往是重点。我们极深的网络让我们得到了28%的相对提升（对COCO的对象检测数据集）。我们在深度残差网络的基础上做了提交的版本参加ILSVRC和COCO2015的比赛，我们还获得了ImageNet对象检测，Imagenet对象定位，COCO对象检测和COCO图像分割的第一名。</p><blockquote><h3 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h3><p><strong>背景：</strong>神经网络的深度越深，越难以训练</p><p><strong>本文贡献：</strong>本文展示了一种残差学习框架，能够简化使那些非常深的网络的训练，该框架能够将层作为输入学习残差函数，而不是学习未知的函数。</p><p><strong>结果：</strong>本文提供了全面的依据表明，这些残差网络更容易被优化，而且可以在深度增加的情况下让精度也增加。</p><p><strong>成绩：</strong>2015年的ILSVRC分类任务上以及获得了第一名的成绩，后来在ImageNet检测、ImageNet定位、COCO检测以及COCO分割上均获得了第一名的成绩。</p></blockquote><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h2><p>深度卷积神经网络在图像分类领域取得了一系列的突破 。 深度网络很好的将一个端到端的多层模型中的低/中/高级特征以及分类器整合起来，特征的等级可以通过所堆叠层的数量（深度）来丰富。最近有结果显示，模型的深度发挥着至关重要的作用，这样导致了ImageNet竞赛的参赛模型都趋向于“非常深”——16 层 到30层 。许多其它的视觉识别任务的都得益于非常深的模型。</p><p>在深度的重要性的驱使下，出现了一个新的问题：训练一个更好的网络是否和堆叠更多的层一样简单呢？解决这一问题的障碍便是困扰人们很久的梯度消失/梯度爆炸，这从一开始便阻碍了模型的收敛。归一初始化（normalized initialization）和中间归一化（intermediate normalization）在很大程度上解决了这一问题，它使得数十层的网络在反向传播的随机梯度下降（SGD）上能够收敛。</p><p>当深层网络能够收敛时，一个退化问题又出现了：随着网络深度的增加，准确率达到饱和（不足为奇）然后迅速退化。意外的是，这种退化并不是由过拟合造成的，并且在一个合理的深度模型中增加更多的层却导致了更高的错误率，我们的实验也证明了这点。</p><p>退化的出现（训练准确率）表明了并非所有的系统都是很容易优化的。让我们来比较一个浅层的框架和它的深层版本。对于更深的模型，这有一种通过构建的解决方案：恒等映射（identity mapping）来构建增加的层，而其它层直接从浅层模型中复制而来。这个构建的解决方案也表明了，一个更深的模型不应当产生比它的浅层版本更高的训练错误率。实验表明，我们目前无法找到一个与这种构建的解决方案相当或者更好的方案（或者说无法在可行的时间内实现）。</p><p>本文中，我们提出了一种深度残差学习框架来解决这个退化问题。我们明确的让这些层来拟合残差映射（residual mapping），而不是让每一个堆叠的层直接来拟合所需的底层映射（desired underlying mapping）。假设所需的底层映射为 H(x)H(x)，我们让堆叠的非线性层来拟合另一个映射： F(x):=H(x)−xF(x):=H(x)−x。 因此原来的映射转化为： F(x)+xF(x)+x。我们推断残差映射比原始未参考的映射（unreferenced mapping）更容易优化。在极端的情况下，如果某个恒等映射是最优的，那么将残差变为0 比用非线性层的堆叠来拟合恒等映射更简单。</p><p>公式 F(x)+xF(x)+x 可以通过前馈神经网络的“shortcut连接”来实现(Fig.2)。Shortcut连接就是跳过一个或者多个层。在我们的例子中，shortcut 连接只是简单的执行恒等映射，再将它们的输出和堆叠层的输出叠加在一起(Fig.2)。恒等的shortcut连接并不增加额外的参数和计算复杂度。完整的网络仍然能通过端到端的SGD反向传播进行训练，并且能够简单的通过公共库（例如，Caffe）来实现而无需修改求解器（solvers）。</p><p>我们在ImageNet数据集上进行了综合性的实验来展示这个退化问题并评估了我们提出的方法。本文表明了： 1) 我们极深的残差网络是很容易优化的，但是对应的“plain”网络（仅是堆叠了层）在深度增加时却出现了更高的错误率。 2) 我们的深度残差网络能够轻易的由增加层来提高准确率，并且结果也大大优于以前的网络。</p><p>CIFAR-10数据集上也出现了类似的现象，这表明了我们提出的方法的优化难度和效果并不仅仅是对于一个特定数据集而言的。我们在这个数据集上成功的提出了超过100层的训练模型，并探索了超过1000层的模型。</p><p>在ImageNet分类数据集上，极深的残差网络获得了优异的成绩。我们的152层的残差网络是目前ImageNet尚最深的网络，并且别VGG网络的复杂度还要低。在ImageNet测试集上，我们的组合模型(ensemble)的top-5错误率仅为3.57%，并赢得了ILSVRC 2015分类竞赛的第一名。这个极深的模型在其他识别任务上同样也具有非常好的泛化性能，这让我们在ILSVRC &amp; COCO 2015 竞赛的ImageNet检测、ImageNet定位、COCO检测以及COCO分割上均获得了第一名的成绩。这强有力的证明了残差学习法则的通用性，因此我们将把它应用到其他视觉甚至非视觉问题上。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/ResNet-Figure1.png" alt=""></p><blockquote><p>图1：在CIFAR-10数据集上使用20层和56层“普通”网络的训练误差（左）和测试误差（右）。较深的网络具有更高的训练误差，因而也有更高的测试误差。在ImageNet上的类似现象见图4。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/ResNet-Figure2.png" alt=""></p><blockquote><p>图2：残差学习的构建模块。</p><p><strong>背景：</strong>模型的深度发挥着至关重要的作用，这样导致了ImageNet竞赛的参赛模型都趋向于“非常深”——16 层 到30层</p><p><strong>问题一：</strong> <strong>模型深度太大时，会存在梯度消失/梯度爆炸的问题</strong></p><p><strong>梯度消失/梯度爆炸概念：</strong>二者问题问题都是因为网络太深,网络权值更新不稳定造成的。本质上是因为梯度反向传播中的连乘效应（小于1连续相乘多次）。梯度消失时，越靠近输入层的参数w越是几乎纹丝不动；梯度爆炸时，越是靠近输入层的参数w越是上蹿下跳。</p><p><strong>解决方法：</strong>归一初始化（normalized initialization）和中间归一化（intermediate normalization）＋BN，加快网络收敛。</p><p><strong>问题二： 随着网络深度的增加，准确率达到饱和然后迅速退化</strong></p><p><strong>网络退化概念：</strong>神经网络随着层数加深，首先训练准确率会逐渐趋于饱和；若层数继续加深，反而训练准确率下降，效果不好了，而这种下降不是由过拟合造成的（因为如果是过拟合的话，训练时误差应该很低而测试时很高）。</p><blockquote><p><strong>Q：为啥会出现网络退化？</strong></p><p>由于非线性激活函数Relu的存在，每次输入到输出的过程都几乎是不可逆的，这也造成了许多不可逆的信息损失。一个特征的一些有用的信息损失了，得到的结果肯定不尽人意。说通俗一点就是中间商赚差价。层数增多之后，信息在中间层损失掉了。</p></blockquote><p> <strong>解决方法：</strong>深度残差学习</p><p>（具体方法会在3.1章节讲解）</p><p><strong>结果：</strong></p><p>（1）残差网络的结构更利于优化收敛</p><p>（2）解决了退化问题</p><p>（3）残差网络可以在扩展网络深度的同时，提高网络性能</p></blockquote><h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a><strong>2 相关工作</strong></h2><h3 id="2-1-残差表达"><a href="#2-1-残差表达" class="headerlink" title="2.1 残差表达"></a>2.1 残差表达</h3><p><strong>残差表达。</strong>在图像识别中，VLAD是残差向量对应于字典进行编码的一种表达形式，Fisher Vector可以看做是VLAD 的一个概率版本。对于图像检索和分类它们都是强力的浅层表达。对于向量量化，残差向量编码比原始向量编码更加有效。</p><p>在低级视觉和计算机图形学中，为了求解偏微分方程（PDEs），通常使用Multigrid法将系统重新表达成多尺度的子问题来解决，每一个子问题就是解决粗细尺度之间的残差问题。Multigrid的另外一种方式是分层基预处理，它依赖于代表着两个尺度之间残差向量的变量。实验证明 这些求解器比其他标准求解器的收敛要快得多，却并没有意识到这是该方法的残差特性所致。这些方法表明了一个好的重新表达或者预处理能够简化优化问题。</p><blockquote><h3 id="主要内容-1"><a href="#主要内容-1" class="headerlink" title="主要内容"></a>主要内容</h3><p>（1）对于向量量化，残差向量编码比原始向量编码更加有效。</p><p>（2）Multigrid的残差特性使得求解器比其他标准求解器的收敛要快得多，表明了一个好的重新表达或者预处理能够简化优化问题。</p></blockquote><h3 id="2-2-短路连接"><a href="#2-2-短路连接" class="headerlink" title="2.2 短路连接"></a>2.2 短路连接</h3><p><strong>Shortcut连接</strong>。Shortcut连接已经经过了很长的一段实践和理论研究过程。训练多层感知器（MLPs）的一个早期实践就是添加一个连接输入和输出的线性层。在Szegedy2015Going及Lee2015deeply中，将一些中间层直接与辅助分类器相连接可以解决梯度消失/爆炸问题。在 Szegedy2015Going中，一个“inception”层由一个shortcut分支和一些更深的分支组合而成。</p><p>与此同时，“highway networks”将shortcut连接与门控函数 结合起来。这些门是数据相关并且是有额外参数的，而我们的恒等shortcuts是无参数的。当一个门的shortcut是“closed”（接近于0）时，highway网络中的层表示非残差函数。相反的，我们的模型总是学习残差函数；我们的恒等shortcuts从不关闭，在学习额外的残差函数时，所有的信息总是通过的。此外，highway网络并不能由增加层的深度（例如， 超过100层）来提高准确率。</p><blockquote><h3 id="主要内容-2"><a href="#主要内容-2" class="headerlink" title="主要内容"></a>主要内容</h3><p>（1）Shortcut连接已经经过了很长的一段实践和理论研究过程，证明是有效的。</p><p>（2）和highway networks（门控函数）对比：当一个门的shortcut是“closed”（接近于0）时，highway networks中的层表示非残差函数。相反的，我们的模型总是学习残差函数；我们的恒等shortcuts从不关闭，是无参数的，在学习额外的残差函数时，所有的信息总是通过的。此外，highway networks并不能由增加层的深度（例如，超过100层）来提高准确率。</p></blockquote><h2 id="3-深度残差学习"><a href="#3-深度残差学习" class="headerlink" title="3 深度残差学习"></a>3 深度残差学习</h2><h3 id="3-1-残差学习"><a href="#3-1-残差学习" class="headerlink" title="3.1 残差学习"></a>3.1 残差学习</h3><p>我们将H(x)看作一个由部分堆叠的层（并不一定是全部的网络）来拟合的底层映射，其中x是这些层的输入。假设多个非线性层能够逼近复杂的函数，这就等价于这些层能够逼近复杂的残差函数，例如, H(x)−x（假设输入和输出的维度相同）。所以我们明确的让这些层来估计一个残差函数：F(x)=H(x)−x而不是H(x)。因此原始函数变成了：F(x)+x。尽管这两个形式应该都能够逼近所需的函数（正如假设），但是学习的难易程度并不相同。</p><p>这个重新表达的动机是由退化问题这个反常的现象(Fig.1，左)。正如我们在Introduction中讨论的，如果增加的层能以恒等映射来构建，一个更深模型的训练错误率不应该比它对应的浅层模型的更大。退化问题表明了，求解器在通过多个非线性层来估计恒等映射上可能是存在困难的。而伴随着残差学习的重新表达，如果恒等映射是最优的，那么求解器驱使多个非线性层的权重趋向于零来逼近恒等映射。</p><p>在实际情况下，恒等映射不太可能达到最优，但是我们的重新表达对于这个问题的预处理是有帮助的。如果最优函数更趋近于恒等映射而不是0映射，那么对于求解器来说寻找关于恒等映射的扰动比学习一个新的函数要容易的多。通过实验(Fig.7)表明，学习到的残差函数通常只有很小的响应，说明了恒等映射提供了合理的预处理。</p><blockquote><h3 id="ResNet目的"><a href="#ResNet目的" class="headerlink" title="ResNet目的"></a>ResNet目的</h3><p>我们选择加深网络的层数，是希望深层的网络的表现能比浅层好，或者是希望它的表现至少和浅层网络持平（相当于直接复制浅层网络的特征）</p><h3 id="以前方法"><a href="#以前方法" class="headerlink" title="以前方法"></a>以前方法</h3><p>在正常的网络中，应该传递给下一层网络的输入是 H(x)=F(x)，即直接拟合H(x)</p><h3 id="本文改进"><a href="#本文改进" class="headerlink" title="本文改进"></a>本文改进</h3><p>在ResNet中，传递给下一层的输入变为H(x)=F(x)+x，即拟合残差F(x)=H(x)－x</p><p><strong>残差模块：</strong>一条路不变（恒等映射）；另一条路负责拟合相对于原始网络的残差，去纠正原始网络的偏差，而不是让整体网络去拟合全部的底层映射，这样网络只需要纠正偏差。</p><h3 id="本质"><a href="#本质" class="headerlink" title="本质"></a>本质</h3><p>（1）加了残差结构后，给了输入x一个多的选择。若神经网络学习到这层的参数是冗余的时候，它可以选择直接走这条“跳接”曲线（shortcut connection），跳过这个冗余层，而不需要再去拟合参数使得H(x)=F(x)=x</p><p>（2）加了恒等映射后，深层网络至少不会比浅层网络更差。</p><p>（3）而在Resnet中，只需要把F(x)变为0即可，输出变为F(x)+x=0+x=x很明显，将网络的输出优化为0比将其做一个恒等变换要容易得多。</p><blockquote><p><strong>Q：为什么H(x)=F(x)+x中F(x)为0才有效？</strong></p><p>模型在训练过程中，F(x)是训练出来的，如果F(x)对于提高模型的训练精度无作用，自然梯度下降算法就调整该部分的参数，使该部分的效果趋近于0.这样整个模型就不会出现深度越深反而效果越差的情况了。</p></blockquote></blockquote><h3 id="3-2-通过短路连接进行恒等映射"><a href="#3-2-通过短路连接进行恒等映射" class="headerlink" title="3.2 通过短路连接进行恒等映射"></a>3.2 通过短路连接进行恒等映射</h3><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/ResNet-3.2.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>VGG-paper</title>
      <link href="/VGG-paper/"/>
      <url>/VGG-paper/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Inceptionv3-paper</title>
      <link href="/Inceptionv3-paper/"/>
      <url>/Inceptionv3-paper/</url>
      
        <content type="html"><![CDATA[<h1 id="《Rethinking-the-Inception-Architecture-for-Computer-Vision》"><a href="#《Rethinking-the-Inception-Architecture-for-Computer-Vision》" class="headerlink" title="《Rethinking the Inception Architecture for Computer Vision》"></a>《<strong>Rethinking the Inception Architecture for Computer Vision</strong>》</h1><blockquote><h1 id="重新思考计算机视觉的Inception体系结构-2015"><a href="#重新思考计算机视觉的Inception体系结构-2015" class="headerlink" title="重新思考计算机视觉的Inception体系结构 2015"></a>重新思考计算机视觉的Inception体系结构 2015</h1></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><strong>摘要</strong></h2><p>对许多任务而言，卷积网络是目前最新的计算机视觉解决方案的核心。从2014年开始，<strong>深度卷积网络开始变成主流</strong>，在各种<strong>基准数据集</strong>上都取得了实质性成果。对于大多数任务而言，虽然增加的模型大小和计算成本都趋向于转化为直接的质量收益（只要提供足够的标注数据去训练），但<strong>计算效率和低参数计数</strong>仍是各种应用场景的限制因素，例如移动视觉和大数据场景。目前，我们正在探索增大网络的方法，目标是通过适当的<strong>分解卷积</strong>和积极的<strong>正则化</strong>来尽可能地有效利用增加的计算。我们在ILSVRC 2012分类挑战赛的验证集上评估了我们的方法，结果证明我们的方法超过了目前最先进的方法并取得了实质性收益：对于<strong>单一框架(只看全图)</strong>评估错误率为：21.2% top-1和5.6% top-5，使用的网络计算代价为每次推断需要进行50亿次乘加运算并使用不到2500万的参数。通过四个模型组合和多次评估，我们报告了3.5% top-5和17.3% top-1的错误率。</p><blockquote><h3 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h3><p><strong>背景：</strong>自2014年开始，网络变得又大又深，计算复杂度高，无法在移动场景(边缘计算)和大数据场景下使用。</p><ul><li>VGG（深）和GoogleNet（宽）</li></ul><p><strong>目的：</strong>在加宽和加深网络的同时，我们要考虑计算的效率</p><p><strong>引出本文主旨：</strong>探索了<u>可分离卷积</u>和<u>正则化</u>去提高计算效率</p></blockquote><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h2><p>从2012年Krizhevsky等人[9]赢得了ImageNet竞赛[16]起，他们的网络“AlexNet”已经成功了应用到了许多计算机视觉任务中，例如目标检测[5]，分割[12]，行人姿势评估[22]，视频分类[8]，目标跟踪[23]和超分辨率[3]。</p><p>这些成功推动了一个新研究领域，这个领域主要专注于<strong>寻找更高效运行的卷积神经网络</strong>。从2014年开始，通过利用<strong>更深(层数)更宽(神经元/参数量)</strong>的网络，网络架构的质量得到了明显改善。VGGNet[18]和GoogLeNet[20]在2014 ILSVRC [16]分类挑战上取得了类似的高性能。<strong>一个有趣的发现是在分类性能上的收益趋向于转换成各种应用领域上的显著质量收益。</strong>这意味着深度卷积架构上的架构改进可以用来改善大多数越来越多地依赖于高质量、可学习视觉特征的其它计算机视觉任务的性能。网络质量的改善也导致了卷积网络在新领域的应用，在AlexNet特征不能与手工精心设计的解决方案竞争的情况下，例如，检测时的候选区域生成[4]。</p><p>尽管VGGNet[18]具有架构简洁的强有力特性，但它的成本很高：评估网络需要大量的计算。另一方面，GoogLeNet[20]的Inception架构也被设计为在内存和计算预算严格限制的情况下也能表现良好。例如，<strong>GoogleNet只使用了500万参数，与其前身AlexNet相比减少了12倍，AlexNet使用了6000万参数。此外，VGGNet使用了比AlexNet大约多3倍的参数。</strong></p><p>Inception的计算成本也远低于VGGNet或其更高性能的后继者[6]。这使得可以在大数据场景中[17]，[13]，在大量数据需要以合理成本处理的情况下或在内存或计算能力固有地受限情况下，利用Inception网络变得可行，例如在移动视觉设定中。通过应用<strong>针对内存使用的专门解决方案</strong>[2]，[15]或<strong>通过计算技巧优化某些操作的执行[10]，可以减轻部分这些问题。但是这些方法增加了额外的复杂性</strong>。此外，这些方法也可以应用于优化Inception架构，再次扩大效率差距。</p><p>然而，Inception架构的复杂性使得更难以对网络进行更改。如果单纯地放大架构，大部分的计算收益可能会立即丢失。此外，[20]并没有提供关于导致GoogLeNet架构的各种设计决策的贡献因素的明确描述。这使得它更难以在适应新用例的同时保持其效率。例如，如果认为有必要增加一些Inception模型的能力，<strong>将滤波器组大小的数量加倍的简单变换将导致计算成本和参数数量增加4倍。这在许多实际情况下可能会被证明是禁止或不合理的</strong>，尤其是在相关收益适中的情况下。在本文中，我们从描述一些<strong>一般原则和优化思想</strong>开始，对于以有效的方式扩展卷积网络来说，这被证实是有用的。虽然我们的原则不局限于Inception类型的网络，但是在这种情况下，它们更容易观察，因为Inception类型构建块的通用结构足够灵活，可以自然地合并这些约束。这通过大量使用降维和Inception模块的并行结构来实现，这允许减轻结构变化对邻近组件的影响。但是，对于这样做需要谨慎，因为应该遵守一些指导原则来保持模型的高质量。</p><blockquote><h3 id="主要内容-1"><a href="#主要内容-1" class="headerlink" title="主要内容"></a>主要内容</h3><p>（1）深度卷积网络性能的提升可以应用在其它计算机视觉任务中，这些任务有着共同点：它们都依赖学习到的高质量视觉特征（visual features）。</p><p>（2）卷积网络性能的提升会产生新的应用领域，比如AlexNet features 无法与手工工程相比，比如目标检测中候选框的生成。</p><p>（3）在参数上对比其他模型展现GoogLeNet的优势(参数量Alexnet：6000W，GoogLeNet：500W，VGG16：1.3E)</p><p>（4）一味的堆叠Inception模块将使得计算量爆炸，换来的精准度并不划算。</p><p>这篇论文中，先会介绍通用设计原则和一些优化的思想，然后再探索新的Inception结构</p></blockquote><h2 id="2-通用的设计原则"><a href="#2-通用的设计原则" class="headerlink" title="2 通用的设计原则"></a>2 通用的设计原则</h2><p>这里我们将介绍一些具有卷积网络的、具有各种架构选择的、基于大规模实验的设计原则。在这一点上，以下原则的效用是推测性的，另外将来的实验证据将对于评估其准确性和有效领域是必要的。然而，严重偏移这些原则往往会导致网络质量的恶化，修正检测到的这些偏差状况通常会导致改进的架构。</p><p>1.<strong>避免表征瓶颈，尤其是在网络的前面。</strong>前馈网络可以由从输入层到分类器或回归器的非循环图表示。这为信息流定义了一个明确的方向。对于分离输入输出的任何切口，可以访问通过切口的信息量。应该避免极端压缩的瓶颈。一般来说，在达到用于着手任务的最终表示之前，表示大小应该从输入到输出缓慢减小。理论上，信息内容不能仅通过表示的维度来评估，因为它丢弃了诸如相关结构的重要因素；维度仅提供信息内容的粗略估计。</p><ul><li>降维会造成各通道之间的相关性信息丢失，仅反应了致密的嵌入信息。（高维的稀疏特征-&gt;低维的稠密特征）</li></ul><p>2.<strong>更高维度的表示在网络中更容易局部处理。</strong>在卷积网络中增加每个图块的激活允许更多解耦的特征。所产生的网络将训练更快。</p><ul><li>相互独立的稀疏特征（猫脸、猫尾、猫牙…）</li></ul><p>3.<strong>空间聚合可以在较低维度嵌入上完成，而不会在表示能力上造成许多或任何损失。</strong>例如，在执行更多展开（例如3×3）卷积之前，可以在空间聚合之前减小输入表示的维度，没有预期的严重不利影响。我们假设，如果在空间聚合上下文中使用输出，则相邻单元之间的强相关性会导致维度缩减期间的信息损失少得多。鉴于这些信号应该易于压缩，因此尺寸减小甚至会促进更快的学习。</p><ul><li>3x3or5x5大卷积核卷积之前用1x1的卷积降维， 不会造成表示能力的丢失。</li></ul><p>4.<strong>平衡网络的宽度和深度。</strong>通过平衡每个阶段的滤波器数量和网络的深度可以达到网络的最佳性能。增加网络的宽度和深度可以有助于更高质量的网络。然而，如果两者并行增加，则可以达到恒定计算量的最佳改进。因此，计算预算应该在网络的深度和宽度之间以平衡方式进行分配。</p><p>虽然这些原则可能是有意义的，但并不是开箱即用的直接使用它们来提高网络质量。我们的想法是仅在不明确的情况下才明智地使用它们。</p><blockquote><p>这一章主要是介绍了作者想到的四种设计原则，论文中说道，这几种设计原则虽然没有严格的证明或者实验加持，但你要大致上遵守，如果你背离这几个原则太多，则必然会造成较差的实验结果。</p><h4 id="原则一：要避免过度的降维和压缩特征导致特征表达瓶颈，特别是在网络的浅层。"><a href="#原则一：要避免过度的降维和压缩特征导致特征表达瓶颈，特别是在网络的浅层。" class="headerlink" title="原则一：要避免过度的降维和压缩特征导致特征表达瓶颈，特别是在网络的浅层。"></a>原则一：要避免过度的降维和压缩特征导致特征表达瓶颈，特别是在网络的浅层。</h4><p><strong>做法：</strong>feature map 长宽大小应随网络加深缓慢减小（不能猛减）。</p><p><strong>原因：</strong>过度的降维或者收缩特征将造成一定程度的信息丢失（信息相关性丢失）</p><blockquote><p><strong>为何特别是网络的浅层？</strong></p><p>因为在网络的浅层丢失的原图信息还不是很多，仍然保留信息的稀疏性。如果在浅层就进行过度地压缩和降维，会对后面提取特征等工作是有负面影响的。</p></blockquote><h4 id="原则二：特征（神经元数量）越多，收敛越快"><a href="#原则二：特征（神经元数量）越多，收敛越快" class="headerlink" title="原则二：特征（神经元数量）越多，收敛越快"></a>原则二：特征（神经元数量）越多，收敛越快</h4><p><strong>理解：</strong>这里的特征多不是说特征图像多，而是说相互独立的特征多，特征分解的彻底。</p><p><strong>举例：</strong>人脸特征分解成人脸、人左眼、人右眼、鼻子、嘴巴、眉毛等等独立特征会比单纯的一张大脸特征收敛的快。（赫布原理）</p><h4 id="原则三：大卷积核的卷积之前可以先降维，这个操作并不会造成太多的损失。"><a href="#原则三：大卷积核的卷积之前可以先降维，这个操作并不会造成太多的损失。" class="headerlink" title="原则三：大卷积核的卷积之前可以先降维，这个操作并不会造成太多的损失。"></a>原则三：大卷积核的卷积之前可以先降维，这个操作并不会造成太多的损失。</h4><p><strong>空间聚合</strong>：即通过聚集相邻特征的信息来降低输入数据的维度，同时尽量保持表示能力不受损失。</p><p><strong>原因：</strong>InceptionV1中使用的模块里有 3×3和5×5的卷积，并且在他们之前有使用1×1的卷积，这样的降维操作对于邻近单元的强相关性中损失的特征信息很少。</p><p><strong>举例：</strong>每一层输出的feature map上每一个相邻的像素，它们的感受野是相邻且有一部分是重合的，即它们高度相关；若将它们进行1x1卷积后，特征信息是可以得到保证，因为1x1卷积后的feature map是能够实现跨通道的信息交融。</p><h4 id="原则四：均衡网络中的深度和宽度。"><a href="#原则四：均衡网络中的深度和宽度。" class="headerlink" title="原则四：均衡网络中的深度和宽度。"></a>原则四：均衡网络中的深度和宽度。</h4><ul><li>深度就是指层数的多少，宽度指每一层中卷积核的个数，也就是提取到的特征数。</li><li>让深度和宽度并行提升，成比例的提升，能提高性能和计算效率。</li><li>可以让计算量在每一层上均匀分配，VGG那第一层全连接层的数据堆积就属于不均匀分配的情况。</li></ul><blockquote><p>举例：输入特征是10x6x6，卷积层是in=10,out=4,size=2的卷积核，输出层是4x5x5</p><p>过程：存在4个10x2x2的卷积核，对于每个卷积核：生成1个5x5的map（10个5x5的map按位求和）。</p><p>每个位置的参数都不一样。</p></blockquote><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul><li>渐进降维</li><li>丰富特征：参数量多（宽度多）</li><li>智能降维：大卷积核前用1x1卷积核进行降维，可以减少计算量而不会对特征表达能力造成太大损失。</li><li>深宽同步</li></ul></blockquote><h2 id="3-分解大卷积核"><a href="#3-分解大卷积核" class="headerlink" title="3 分解大卷积核"></a>3 分解大卷积核</h2><p>​    GoogLeNet网络[20]的大部分<strong>初始收益来源于大量地使用降维</strong>。这可以被视为以计算有效的方式分解卷积的特例。考虑例如1×1卷积层之后接一个3×3卷积层的情况。在视觉网络中，预期相近激活的输出是高度相关的。因此，我们可以预期，它们的激活可以在聚合之前被减少，并且这应该会导致类似的富有表现力的局部表示。</p><p>  在这里，我们将在各种设定中探索卷积分解的其它方法，特别是为了提高解决方案的计算效率。由于Inception网络是全卷积的，每个权重对应每个激活的一次乘法。因此，任何计算成本的降低会导致参数数量减少。这意味着，通过适当的分解，我们可以得到更多的解耦参数，从而加快训练。此外，我们可以使用计算和内存节省来增加我们网络的滤波器组的大小，同时保持我们在单个计算机上训练每个模型副本的能力。</p><blockquote><h3 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h3><p>Googlenet的inception成功原因大部分得益于使用1 ×1 的卷积核，1 ×1 卷积核可以看作一个特殊的大卷积核分解过程，它通过降维大大降低计算量，增加非线性，跨通道交流造成损失少。</p><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><p>由上面的原则三，在使用大卷积核之前先用1 × 1卷积核，feature map信息不会丢失很多，因为感受野相邻的元素相关性很强，他们在卷积的过程中重合度很高，仅相差一个步长，所以feature map基本上不会丢失特征信息(元素特征相关性不变)。</p><h3 id="探究方法"><a href="#探究方法" class="headerlink" title="探究方法"></a>探究方法</h3><p>在这篇文章中，作者探究了不同设置下的分解卷积。通过分解卷积在减少参数数量的同时可以得到更多的解耦特征，加快网络的训练。同时，节省下来的空间、计算资源可以用于进一步增加卷积核的尺寸，以达到代价基本不变下的更高性能。</p><blockquote><h4 id="激活："><a href="#激活：" class="headerlink" title="激活："></a>激活：</h4><p>神经元（像素）；</p><h4 id="聚合："><a href="#聚合：" class="headerlink" title="聚合："></a>聚合：</h4><p>3x3卷积</p><h4 id="1x1卷积核的作用："><a href="#1x1卷积核的作用：" class="headerlink" title="1x1卷积核的作用："></a>1x1卷积核的作用：</h4><p>降维度、降参数、跨通道融合、增加非线性。</p><h4 id="解耦："><a href="#解耦：" class="headerlink" title="解耦："></a>解耦：</h4><p>将复杂的操作或模型参数分解为更简单、更小的部分。即在3x3卷积前使用1x1（原则三）可增加滤波器组的大小（卷积核的数量）</p><h3 id="卷积分解的例子：Inception模块"><a href="#卷积分解的例子：Inception模块" class="headerlink" title="卷积分解的例子：Inception模块"></a>卷积分解的例子：Inception模块</h3><p>假设你有一个使用标准卷积操作的CNN，其中一个卷积层使用了较大的卷积核，比如5x5。这个5x5的卷积核作用在输入特征图上，每个权重与输入特征图的激活相乘，然后求和得到输出特征图的一个像素点。</p><h4 id="未分解的卷积操作"><a href="#未分解的卷积操作" class="headerlink" title="未分解的卷积操作"></a>未分解的卷积操作</h4><ul><li>假设输入特征图有64个通道，你想使用5x5的卷积核生成256个通道的输出特征图。</li><li>参数量 = (5 <em> 5 </em> 64 + 1) * 256 = 102,656个参数</li></ul><h4 id="分解卷积操作"><a href="#分解卷积操作" class="headerlink" title="分解卷积操作"></a>分解卷积操作</h4><ol><li><strong>第一步：使用1x1的卷积核降维</strong>。假设你首先用1x1的卷积核将输入的64个通道降维到16个通道。<ul><li>参数量 = (1 <em> 1 </em> 64 + 1) * 16 = 1,040个参数</li></ul></li><li><strong>第二步：在降维后的特征图上应用3x3的卷积核</strong>。然后，对这16个通道的特征图应用3x3的卷积核，以生成最终的256个通道的输出特征图。<ul><li>参数量 = (3 <em> 3 </em> 16 + 1) * 256 = 37,120个参数</li></ul></li></ol><p>通过这种分解，总的参数量变为1,040 + 37,120 = 38,160个参数，远少于未分解前的102,656个参数。</p></blockquote></blockquote><h3 id="3-1-分解成更小的卷积"><a href="#3-1-分解成更小的卷积" class="headerlink" title="3.1 分解成更小的卷积"></a>3.1 分解成更小的卷积</h3><p>​    具有较大空间滤波器（例如5×5或7×7）的卷积在计算方面往往不成比例地昂贵。例如，具有n个滤波器的5×5卷积在具有m个滤波器的网格上比具有相同数量的滤波器的3×3卷积的计算量高25/9=2.78倍。<strong>当然，5×5滤波器在更前面的层可以捕获更远的单元激活之间、信号之间的依赖关系，因此滤波器几何尺寸的减小带来了很大的表现力损失。</strong>然而，我们可以询问5×5卷积是否可以被具有相同输入尺寸和输出深度的参数较小的多层网络所取代。如果我们放大5×5卷积的计算图，我们看到每个输出看起来像一个小的完全连接的网络，在其输入上滑过5×5的块（见图1）。由于我们正在构建视觉网络，所以通过两层的卷积结构再次利用平移不变性来代替全连接的组件似乎是很自然的：第一层是3×3卷积，第二层是在第一层的3×3输出网格之上的一个全连接层（见图1）。通过在输入激活网格上滑动这个小网络，用两层3×3卷积来替换5×5卷积（比较图4和5）。<br>​        该设定通过相邻块之间共享权重明显减少了参数数量。为了分析预期的计算成本节省，我们将对典型的情况进行一些简单的假设：我们可以假设n=αm，也就是我们想通过常数α因子来改变激活/单元的数量。由于5×5卷积是聚合的，α通常比1略大（在GoogLeNet中大约是1.5）。用两个层替换5×5层，似乎可以通过两个步骤来实现扩展：在两个步骤中通过√α增加滤波器数量。为了简化我们的估计，通过选择α=1（无扩展），如果我们单纯地滑动网络而不重新使用相邻网格图块之间的计算，我们将增加计算成本。滑动该网络可以由两个3×3的卷积层表示，其重用相邻图块之间的激活。<strong>这样，我们最终得到一个计算量减少到(9+9)/25的网络，通过这种分解导致了28％的相对增益。</strong>每个参数在每个单元的激活计算中只使用一次，所以参数计数具有完全相同的节约。不过，这个设置提出了两个一般性的问题：<strong>这种替换是否会导致任何表征力的丧失？</strong>如果我们的主要目标是对计算的线性部分进行分解，<strong>是不是建议在第一层保持线性激活？</strong>我们已经进行了几个控制实验（例如参见图2），并且在分解的所有阶段中使用线性激活总是逊于使用修正线性单元。我们将这个收益归因于网络可以学习的增强的空间变化，特别是如果我们对输出激活进行批标准化[7]。当对维度减小组件使用线性激活时，可以看到类似的效果。<br>​        图2。两个Inception模型间几个控制实验中的一个，其中一个分解为线性层+ ReLU层，另一个使用两个ReLU层。在三亿八千六百万次运算后，在验证集上前者达到了76.2% top-1准确率，后者达到了77.2% top-1的准确率。</p><blockquote><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>将一个5×5的卷积换成两个3×3卷积，同理7×7卷积可以用3个3×3卷积。（大大减少参数数量，感受野不变）</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>相同卷积核个数核feature map尺寸的情况下，5 × 5 卷积核比3 ×3卷积核计算量高了2.78倍。</p><ul><li>假设输入和输出的尺寸不变的情况下</li></ul><h3 id="原因-1"><a href="#原因-1" class="headerlink" title="原因"></a>原因</h3><p>相邻感受野的权值共享（共享卷积核，即卷积核内的数值都是一样的，用同一个卷积核），所以减少了很多计算量。</p><blockquote><p><strong>问题一：这种替代会影响网络的表达能力吗？</strong></p><p>直观的看是可行的，从结果看也是可行的。但是要问严谨的数学原理，确实难以解释。</p><p><strong>问题二：如果我们的目标是分解计算的线性部分，是否会建议保留第一层的线性激活性？</strong></p><p>对于分解后的激活函数，作者通过实验证明，保留对于原图的第一次3 ×3卷积的激活函数有较好效果（一层卷积变成两层了，增加了非线性变换，增强模型非线性表达能力），用BN后效果更好。</p></blockquote></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv3-Figure1.png" alt=""></p><blockquote><p>图1。迷你网络取代了5x5卷积</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv3-Figure2.png" alt=""></p><blockquote><p>图2。两个Inception模型之间的几个控制实验之一，其中一个使用因子分解为线性+ReLU层，另一个使用两个ReLU层。在386万次运行后，前者稳定在76.2%，而后者达到77.2%验证集中排名前1的准确性。</p></blockquote><h3 id="3-2-非对称分解卷积"><a href="#3-2-非对称分解卷积" class="headerlink" title="3.2 非对称分解卷积"></a>3.2 非对称分解卷积</h3><p>上述结果表明，大于3×3的卷积滤波器可能不是通常有用的，因为它们总是可以简化为3×3卷积层序列。我们仍然可以问这个问题，是否应该把它们分解成更小的，例如2×2的卷积。然而，<strong>通过使用非对称卷积，可以做出甚至比2×2更好的效果，即n×1。</strong>例如使用3×1卷积后接一个1×3卷积，相当于以与3×3卷积相同的感受野滑动两层网络（参见图3）。如果输入和输出滤波器的数量相等，那么对于相同数量的输出滤波器，两层解决方案节省33％。相比之下，将3×3卷积分解为两个2×2卷积表示仅节省了11％的计算量。</p><p>在理论上，我们可以进一步论证，<strong>可以通过1×n卷积和后面接一个n×1卷积替换任何n×n卷积，并且随着n增长，计算成本节省显著增加</strong>（见图6）。实际上，我们发现，采用这种分解在前面的层次上不能很好地工作，但是<strong>对于中等网格尺寸（在m×m特征图上，其中m范围在12到20之间），其给出了非常好的结果。</strong>在这个水平上，通过使用1×7卷积，然后是7×1卷积可以获得非常好的结果。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv3-Figure3.png" alt=""></p><blockquote><p>图3。取代3×3卷积的迷你网络。这个该网络的下层由3×1卷积和3输出单位。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv3-Figure4.png" alt=""></p><blockquote><p>图4。原来的Inception模型所述。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv3-Figure5.png" alt=""></p><blockquote><p>图5。Inception模型，其中每个5×5卷积被两个3×3卷积重新放置，如的原理3所建议的第2节。</p><h3 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h3><p>将3x3卷积可分解为1x3和3x1两个不对称卷积（空间可分离卷积）——宽度上的分解</p><p>假设原始为3×3的图，使用3× 1的卷积核去卷积，得到一个feature map，然后再用1× 3的卷积核对刚才得到的feature map做卷积得到1×1的feature map（相当于全连接层）。</p><h3 id="结果："><a href="#结果：" class="headerlink" title="结果："></a>结果：</h3><p>在输入和输出等同的情况下，参数降低33%（将3x3卷积核分解为两个2x2卷积核，只是降低了11%）</p><ul><li>公式如下，n为卷积核尺寸</li></ul><script type="math/tex; mode=display">\frac{n^2 - 2n}{n^2}</script><h3 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h3><p>（1）这种分解 (n ×n 分解成了 n×1 和1 ×n) ，n 越大节省的运算量越大。</p><p>（2）这种分解在前面的层效果不好，使用feature map大小在12-20之间。</p><h3 id="变种："><a href="#变种：" class="headerlink" title="变种："></a>变种：</h3><p>可以理解成不对称卷积是在深度上分解。而扩展滤波器组是在宽度上分解，应用在最后的输出分类层之前，用该模块扩展特征维度生成高维稀疏特征（增加特征个数，符合原则二）。</p></blockquote><h2 id="4-辅助分类器"><a href="#4-辅助分类器" class="headerlink" title="4 辅助分类器"></a>4 <strong>辅助分类器</strong></h2><p>[20]引入了辅助分类器的概念，以改善非常深的网络的收敛。<strong>最初的动机是将有用的梯度推向较低层，使其立即有用，并通过抵抗非常深的网络中的消失梯度问题来提高训练过程中的收敛。</strong>Lee等人[11]也认为辅助分类器促进了更稳定的学习和更好的收敛。有趣的是，我们发现辅助分类器在训练早期并没有导致改善收敛：在两个模型达到高精度之前，有无侧边网络的训练进度看起来几乎相同。接近训练结束，辅助分支网络开始超越没有任何分支的网络的准确性，达到了更高的稳定水平。</p><p>另外，[20]在网络的不同阶段使用了两个侧分支。<strong>移除更下面的辅助分支对网络的最终质量没有任何不利影响。</strong>再加上前一段的观察结果，这意味着[20]最初的假设，<strong>这些分支有助于演变低级特征很可能是不适当的。</strong>相反，我们认为辅助分类器起着正则化项的作用。事实证明了这一点如果辅助分类器被批量归一化[7]或具有丢弃层。这也为这个猜想提供了一个弱的支持性证据该批处理规范化充当正则化子。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv3-Figure6.png" alt=""></p><blockquote><p>图6。n×n卷积分解后的Inception模块。在我们提出的架构中，对17×17的网格我们选择n=7。（滤波器尺寸可以通过原则3选择）</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv3-Figure7.png" alt=""></p><blockquote><p>图7：具有扩展滤波器组输出的Inception模块。此架构用于最粗糙的（8×8）网格，以促进高维表征，正如第2节的第二原则所建议的。我们仅在最粗糙的网格上使用此解决方案，因为在这一层产生高维稀疏表征最为关键，相比于空间聚合，局部处理（通过1×1卷积）的比例增加了。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv3-Figure8.png" alt=""></p><blockquote><p>图8：最后一个17×17层顶部的辅助分类器。侧头部中层的批量归一化[7]在top-1准确率上带来了0.4%的绝对增益。下轴显示了执行的迭代次数，每次迭代的批量大小为32。</p><h3 id="较之前的改进"><a href="#较之前的改进" class="headerlink" title="较之前的改进"></a>较之前的改进</h3><p>在GoogLeNet里面用了两个辅助分类器（4a和4b两个模块后面），但是事后实验证明，辅助分类器并未在训练初期改善收敛性，第一个没什么用，在v2，v3里面去掉了。</p></blockquote><h2 id="5-高效下降特征图尺寸"><a href="#5-高效下降特征图尺寸" class="headerlink" title="5 高效下降特征图尺寸"></a>5 高效下降特征图尺寸</h2><p>传统上，卷积网络使用一些池化操作来缩减特征图的网格大小。<strong>为了避免表示瓶颈，在应用最大池化或平均池化之前，需要扩展网络滤波器的激活维度。</strong>例如，开始有一个带有k个滤波器的d×d网格，如果我们想要达到一个带有2k个滤波器的网格，我们首先需要用2k个滤波器计算步长为1的卷积，然后应用一个额外的池化步骤。这意味着总体计算成本由在较大的网格上使用次运算的昂贵卷积支配。一种可能性是转换为带有卷积的池化，因此导致次运算，将计算成本降低为原来的四分之一。然而，由于表示的整体维度下降到会导致表示能力较弱的网络（参见图9），这会产生一个表示瓶颈。<strong>我们建议另一种变体，其甚至进一步降低了计算成本，同时消除了表示瓶颈</strong>（见图10），而不是这样做。我们可以使用<strong>两个平行的步长为2的块：P和C。</strong>P是一个池化层（平均池化或最大池化）的激活，两者都是步长为2，其滤波器组连接如图10所示。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv3-Figure9.png" alt=""></p><blockquote><p>图9。减少网格尺寸的两种替代方式。左边的解决方案违反了表示瓶颈的原则1。右边的版本计算量昂贵3倍。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv3-Figure10.png" alt=""></p><blockquote><p>图10。缩减网格尺寸的同时扩展滤波器组的Inception 模块。它不仅廉价并且避免了原则1中提出的表示瓶颈。右侧的图表示相同的解决方案，但是从网格大小而不是运算的角度来看。</p><h3 id="传统降维方法"><a href="#传统降维方法" class="headerlink" title="传统降维方法"></a>传统降维方法</h3><p><strong>方法一：</strong>先对feature map池化会导致表征瓶颈，信息量丢失很多；（<strong>池化-&gt;卷积</strong>）</p><p><strong>方法二：</strong>信息保留下了，但计算量比较大； （<strong>卷积-&gt;池化</strong>）</p><ul><li>传统池化为降低宽度而定，而深宽同步，深度也要上去。上去的方法有两个：↑各有缺点。</li></ul><h3 id="计算量比较"><a href="#计算量比较" class="headerlink" title="计算量比较"></a>计算量比较</h3><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv3-computevs.png" alt=""></p><h3 id="文本改进"><a href="#文本改进" class="headerlink" title="文本改进"></a>文本改进</h3><p>并行执行（卷积C+池化P），再进行feature map的堆叠。</p><p>inception并行模块结构图: 左边两路卷积，右边池化。最后再叠加，可以在不丢失信息的情况下减小参数量、升维、降宽。</p></blockquote><h2 id="6-Inception-v2"><a href="#6-Inception-v2" class="headerlink" title="6 Inception-v2"></a>6 Inception-v2</h2><p>在这里，我们连接上面的点，并提出了一个新的架构，在ILSVRC 2012分类基准数据集上提高了性能。我们的网络布局在表1中给出。注意，基于与3.1节中描述的同样想法，我们<strong>将传统的7×7卷积分解为3个3×3卷积</strong>。对于网络的Inception部分，我们在35×35处有3个传统的Inception模块，每个模块有288个滤波器。使用第5节中描述的网格缩减技术，这将缩减为17×17的网格，具有768个滤波器。这之后是图5所示的5个分解的Inception模块实例。使用图10所示的网格缩减技术，这被缩减为8×8×1280的网格。在最粗糙的8×8级别，我们有两个如图6所示的Inception模块，每个块连接的输出滤波器组的大小为2048。网络的详细结构，包括Inception模块内滤波器组的大小，在补充材料中给出，在提交的tar文件中的model.txt中给出。然而，我们已经观察到，只要遵守第2节的原则，对于各种变化网络的质量就相对稳定。虽然我们的网络深度是42层，但我们的计算成本仅比GoogLeNet高出约2.5倍，它仍比VGGNet要高效的多。</p><p>表1。提出的网络架构的轮廓。每个模块的输出大小是下一模块的输入大小。我们正在使用图10所示的缩减技术的变种，以缩减应用时Inception块间的网格大小。我们用0填充标记了卷积，用于保持网格大小。这些Inception模块内部也使用0填充，不会减小网格大小。所有其它层不使用填充。选择各种滤波器组大小来观察第2节的原理4。<br>图7。具有扩展的滤波器组输出的Inception模块。这种架构被用于最粗糙的（8×8）网格，以提升高维表示，如第2节原则2所建议的那样。我们仅在最粗的网格上使用了此解决方案，因为这是产生高维度的地方，稀疏表示是最重要的，因为与空间聚合相比，局部处理（1×1 卷积）的比率增加。</p><blockquote><p>作者基于以上的分析与所提出的通用性准则提出了改进后的Inception-v2架构：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv3-Table1.png" alt=""></p><ul><li>figure5 ——把5×5卷积用两个3×3卷积代替</li><li>figure6 ——应用的那个不对称卷积</li><li>figure7 ——用在输出分类前的那个扩展滤波器组</li></ul><p>相比起Inception-v1结构，这里将开始的7x7卷积使用3个3x3卷积替代，在后面的Inception模块中分别使用了三种不同的模式：</p><ul><li>第一部分输入的特征图尺寸为35x35x288,采用了图5中的架构，将5x5以两个3x3代替。</li><li>第二部分输入特征图尺寸为17x17x768，采用了图6中nx1+1xn的结构</li><li>第三部分输入特征图尺寸为8x8x1280, 采用了图7中所示的并行模块的结构</li></ul><h3 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h3><p>这个网络是符合上面说的设计的四大原则的， 网络有42层深，计算量是googlenet的2.5倍(仍比VGG高效)。</p></blockquote><h2 id="7-通过标签平滑进行模型正则化"><a href="#7-通过标签平滑进行模型正则化" class="headerlink" title="7 通过标签平滑进行模型正则化"></a>7 通过标签平滑进行模型正则化</h2><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv3-7.png" alt=""></p><blockquote><p>(这一节非常复杂，涉及大量公式，本人水平有限就不再推理，这里只解释相关概念和算法效果。相关计算推导推荐大家看b站同济子豪兄)</p><blockquote><p><strong>Q1：one-hot是什么？</strong></p><p>独热编码即 One-Hot 编码，又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候，其中只有一位有效。（简单地说，就是对于多分类向量，计算机中往往用[0, 1, 3]等此类离散的、随机的而非有序(连续)的向量表示，而one-hot vector 对应的向量便可表示为[0, 1, 0]，即对于长度为n 的数组，只有一个元素是1，其余都为0。因此表征我们已知样本属于某一类别的概率是为1的确定事件，属于其他类别的概率则均为0。）</p><p><strong>Q2：one-hot缺点？</strong></p><p>1.过拟合：因为模型使得正确标签的分数足够大</p><p>2.降低模型的泛化能力，降低了网络的适应性</p><p><strong>Q3：LSR是什么？</strong></p><p>在深度学习样本训练的过程中，我们采用one-hot标签去进行计算交叉熵损失时，只考虑到训练样本中正确的标签位置（one-hot标签为1的位置）的损失，而忽略了错误标签位置（one-hot标签为0的位置）的损失。这样一来，模型可以在训练集上拟合的很好，但由于其他错误标签位置的损失没有计算，导致预测的时候，预测错误的概率增大。Label Smoothing Regularization（LSR）标签平滑正则化,机器学习中的一种正则化方法，是一种通过在输出y中添加噪声，实现对模型进行约束，降低模型过拟合（overfitting）程度的一种约束方法（regularization methed）。</p><p><strong>Q4：LSR的实质？</strong></p><p>标签平滑的实质就是促使神经网络中进行softmax激活函数激活之后的分类概率结果向正确分类靠近，即正确的分类概率输出大（对应的one-hot标签为1位置的softmax概率大），并且同样尽可能的远离错误分类（对应的one-hot标签为0位置的softmax概率小），即错误的分类概率输出小。</p></blockquote><h3 id="Label-Smooth实现"><a href="#Label-Smooth实现" class="headerlink" title="Label Smooth实现"></a>Label Smooth实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_labels = (<span class="number">1.0</span> - label_smoothing) * one_hot_labels + label_smoothing / （num_classes-<span class="number">1</span>）</span><br></pre></td></tr></table></figure><h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><p>label_smoothing = 0.1</p><p>num_classes = 1000</p><h4 id="结果-2"><a href="#结果-2" class="headerlink" title="结果"></a>结果</h4><p>Label smooth提高了网络精度0.2%，在正确的答案上添加一点错误，担责写错误是可以相互抵消的，正确的就会凸显出来，不会出现过拟合。</p></blockquote><h2 id="8-Training-Methodology—训练方法"><a href="#8-Training-Methodology—训练方法" class="headerlink" title="8 Training Methodology—训练方法"></a>8 Training Methodology—训练方法</h2><p>我们在TensorFlow[1]分布式机器学习系统上使用<strong>随机梯度方法</strong>训练了我们的网络，使用了50个副本，每个副本在一个NVidia Kepler GPU上运行，<strong>bs为32</strong>，<strong>100个epoch</strong>。我们之前的实验使用动量方法[19]，<strong>衰减值为0.9</strong>，而我们最好的模型是用<strong>RMSProp</strong> [21]实现的，<strong>衰减值为0.9，ϵ=1.0</strong>。我们使用<strong>0.045</strong>的学习率，<strong>每两个epoch以0.94的指数速率衰减</strong>。此外，<strong>阈值为2.0的梯度裁剪</strong>[14]被发现对于稳定训练是有用的。使用随时间计算的运行参数的平均值来执行模型评估。</p><blockquote><p>最优模型的优化方法：RMSProp + learning rate decay(0.9) , 同时使用了阈值为2的梯度截断使得训练更加稳定。</p><h3 id="具体参数"><a href="#具体参数" class="headerlink" title="具体参数"></a>具体参数</h3><div class="table-container"><table><thead><tr><th>初始模型</th><th>tesnsorflow</th></tr></thead><tbody><tr><td>优化器</td><td>stochastic gradient</td></tr><tr><td>机器个数</td><td>50</td></tr><tr><td>batch size</td><td>32</td></tr><tr><td>epoch</td><td>100</td></tr><tr><td>最好模型</td><td>RMSProp</td></tr><tr><td>decay</td><td>0.9</td></tr><tr><td>初始学习率</td><td>0.045</td></tr><tr><td>指数衰减</td><td>0.94</td></tr></tbody></table></div></blockquote><h2 id="9-小分辨率输入性能"><a href="#9-小分辨率输入性能" class="headerlink" title="9 小分辨率输入性能"></a>9 小分辨率输入性能</h2><p>视觉网络的典型用例是用于检测的后期分类，例如在Multibox [4]上下文中。这包括分析在某个上下文中包含单个对象的相对较小的图像块。任务是确定图像块的中心部分是否对应某个对象，如果是，则确定该对象的类别。这个挑战的是对象往往比较小，分辨率低。这就提出了如何正确处理低分辨率输入的问题。</p><p>普遍的看法是，使用更高分辨率感受野的模型倾向于导致显著改进的识别性能。然而，区分第一层感受野分辨率增加的效果和较大的模型容量、计算量的效果是很重要的。如果我们只是改变输入的分辨率而不进一步调整模型，那么我们最终将使用计算上更便宜的模型来解决更困难的任务。当然，由于减少了计算量，这些解决方案很自然就出来了。为了做出准确的评估，模型需要分析模糊的提示，以便能够“幻化”细节。这在计算上是昂贵的。因此问题依然存在：如果计算量保持不变，更高的输入分辨率会有多少帮助。确保不断努力的一个简单方法是在较低分辨率输入的情况下减少前两层的步长，或者简单地移除网络的第一个池化层。为了这个目的我们进行了以下三个实验：</p><p>1.步长为2，大小为299×299的感受野和最大池化。</p><p>2.步长为1，大小为151×151的感受野和最大池化。</p><p>3.步长为1，大小为79×79的感受野和第一层之后没有池化。</p><p>所有三个网络具有几乎相同的计算成本。虽然第三个网络稍微便宜一些，但是池化层的成本是无足轻重的（在总成本的1％以内）。在每种情况下，网络都进行了训练，直到收敛，并在ImageNet ILSVRC 2012分类基准数据集的验证集上衡量其质量。结果如表2所示。虽然分辨率较低的网络需要更长时间去训练，但最终结果却与较高分辨率网络的质量相当接近。表2。当感受野尺寸变化时，识别性能的比较，但计算代价是不变的。但是，如果只是单纯地按照输入分辨率减少网络尺寸，那么网络的性能就会差得多。然而，这将是一个不公平的比较，因为我们将在比较困难的任务上比较一个便宜16倍的模型。表2的这些结果也表明，有人可能会考虑在R-CNN [5]的上下文中对更小的对象使用专用的高成本低分辨率网络。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv3-Table2.png" alt=""></p><blockquote><h3 id="目标检测难点："><a href="#目标检测难点：" class="headerlink" title="目标检测难点："></a>目标检测难点：</h3><p>图像中低分辨率的目标难以检测，如何处理低分辨率的输入？</p><h3 id="常规方法："><a href="#常规方法：" class="headerlink" title="常规方法："></a>常规方法：</h3><p>对这些使用更大感受野来进行卷积，以提高在低分辨率输入上的识别准确率。</p><h3 id="不足："><a href="#不足：" class="headerlink" title="不足："></a>不足：</h3><p>这很难说明性能的提升是来自于感受野的扩大还是模型能力及计算量的增加。</p><h3 id="引出问题："><a href="#引出问题：" class="headerlink" title="引出问题："></a>引出问题：</h3><p>在保持计算代价不变的情况下，如何最大限度的提高分辨率？</p><h3 id="方法：-1"><a href="#方法：-1" class="headerlink" title="方法："></a>方法：</h3><p>保持模型复杂度不变，降低前两层的步长，或者移除第一个池化层。</p><h3 id="实验验证："><a href="#实验验证：" class="headerlink" title="实验验证："></a>实验验证：</h3><p>三组实验感受野逐步下降而计算量保持不变：</p><p>（1）79x79的感受野，stride = 1，不进行maxpooling</p><p>（2）151x151的感受野，strde = 1，加上maxpooling</p><p>（3）299x299的感受野，stride = 2，加上maxpooling</p><p><strong>结论：</strong>实验表明虽然感受野增大，但是在保持计算量不变的情况下模型性能相差不大</p></blockquote><h2 id="10-实验结果和比较"><a href="#10-实验结果和比较" class="headerlink" title="10 实验结果和比较"></a>10 实验结果和比较</h2><p>表3显示了我们提出的体系结构（Inception-v2）识别性能的实验结果，架构如第6节所述。每个Inception-v2行显示了累积变化的结果，包括突出显示的新修改加上所有先前修改的结果。标签平滑是指在第7节中描述的方法。分解的7×7包括将第一个7×7卷积层分解成3×3卷积层序列的改变。BN-auxiliary是指辅助分类器的全连接层也批标准化的版本，而不仅仅是卷积。<strong>我们将表3最后一行的模型称为Inception-v3，</strong>并在多裁剪图像和组合设置中评估其性能。我们所有的评估都在ILSVRC-2012验证集上的48238个非黑名单样本中完成，如[16]所示。我们也对所有50000个样本进行了评估，结果在top-5错误率中大约为0.1%，在top-1错误率中大约为0.2%。在本文即将出版的版本中，我们将在测试集上验证我们的组合结果，但是我们上一次对BN-Inception的春季测试[7]表明测试集和验证集错误趋于相关性很好。11. 结论我们提供了几个设计原则来扩展卷积网络，并在Inception体系结构的背景下进行研究。这个指导可以导致高性能的视觉网络，与更简单、更单一的体系结构相比，它具有相对适中的计算成本。Inception-v3的最高质量版本在ILSVR 2012分类上的单裁剪图像评估中达到了21.2\％<br>的top-1错误率和5.6％的top-5错误率，达到了新的水平。与Ioffe等[7]中描述的网络相比，这是通过增加相对适中（2.5/times）的计算成本来实现的。尽管如此，我们的解决方案所使用的计算量比基于更密集网络公布的最佳结果要少得多：我们的模型比He等[6]的结果更好——将top-5(top-1)的错误率相对分别减少了25% (14%)，然而在计算代价上便宜了六倍，并且使用了至少减少了五倍的参数（估计值）。我们的四个Inception-v3模型的组合效果达到了3.5\％，多裁剪图像评估达到了3.5\％的top-5的错误率，这相当于比最佳发布的结果减少了25\％以上，几乎是ILSVRC 2014的冠军GoogLeNet组合错误率的一半。我们还表明，可以通过感受野分辨率为79×79的感受野取得高质量的结果。这可能证明在检测相对较小物体的系统中是有用的。我们已经研究了在神经网络中如何分解卷积和积极降维可以导致计算成本相对较低的网络，同时保持高质量。较低的参数数量、额外的正则化、批标准化的辅助分类器和标签平滑的组合允许在相对适中大小的训练集上训练高质量的网络。<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv3-Table3.png" alt=""></p><blockquote><p>表3：<strong>单次裁剪实验</strong>结果比较了各种贡献因素的累积效应。我们将我们的数据与Ioffe等人[7]发布的最佳单次裁剪推理结果进行了比较。对于“Inception-v2”行，更改是累积的，每个后续行除了前面的更改外，还包括新的更改。最后一行指的是所有的更改，这就是我们下面称为“Inception-v3”的内容。遗憾的是，He等人[6]仅报告了10次裁剪评估结果，而没有报告单次裁剪结果，这在下面的表4中有报告。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv3-Table4.png" alt=""></p><blockquote><p>表4：单模型，多裁剪实验结果比较了各种贡献因素的累积效应。我们将我们的数据与在ILSVRC 2012分类基准测试上发布的最佳单模型推理结果进行了比较。</p><h3 id="对InceptionV2-进行改进："><a href="#对InceptionV2-进行改进：" class="headerlink" title="对InceptionV2 进行改进："></a>对InceptionV2 进行改进：</h3><p>（1）InceptionV2 加入RMSProp(一种计算梯度的方法)</p><p>（2）在上面的基础上加入Label Smoothing(LSR,标签平滑正则化)</p><p>（3）在上面的基础上再加入7×7的卷积核分解(分解成3×3)</p><p>（4）在上面的基础上再加入含有BN的辅助分类器</p><p>所以本文最终提出的<strong>InceptionV3=inceptionV2+RMSProp优化+LSR+BN-auxilary</strong></p><p>同时也将Inception-v3进行了多尺度的训练，在单模型下的对比结果如下: Table4</p><p>进一步地，Inception-v3进行模型融合（ensemble)后与其他模型做对比：Table2</p><p><strong>结论：</strong>可以看到Inception-v3在分类任务上都获得了更好的性能 </p></blockquote><h2 id="11-结论"><a href="#11-结论" class="headerlink" title="11 结论"></a>11 结论</h2><p>我们提供了几个设计原则来扩展卷积网络，并在Inception体系结构的背景下进行研究。这个指导可以导致高性能的视觉网络，与更简单、更单一的体系结构相比，它具有相对适中的计算成本。</p><p>Inception-v3的最高质量版本在ILSVR 2012分类上的单裁剪图像评估中达到了21.2％的top-1错误率和5.6％的top-5错误率，达到了新的水平。与Ioffe等[7]中描述的网络相比，这是通过增加相对适中（2.5/times）的计算成本来实现的。尽管如此，我们的解决方案所使用的计算量比基于更密集网络公布的最佳结果要少得多：我们的模型比He等[6]的结果更好——将top-5(top-1)的错误率相对分别减少了25%(14%)，然而在计算代价上便宜了六倍，并且使用了至少减少了五倍的参数（估计值）。我们的四个Inception-v3模型的组合效果达到了3.5％，多裁剪图像评估达到了3.5％的top-5的错误率，这相当于比最佳发布的结果减少了25％以上，几乎是ILSVRC 2014的冠军GoogLeNet组合错误率的一半。</p><p>我们还表明，可以通过感受野分辨率为79×79的感受野取得高质量的结果。这可能证明在检测相对较小物体的系统中是有用的。我们已经研究了在神经网络中如何分解卷积和积极降维可以导致计算成本相对较低的网络，同时保持高质量。较低的参数数量、额外的正则化、标准化的辅助分类器和标签平滑的组合允许在相对适中大小的训练集上训练高质量的网络。</p><blockquote><p> 总结一句话：我们的模型很好！</p></blockquote><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p><strong>Q1：论文试图解决什么问题？</strong></p><p>在加宽和加深网络的同时，探索了可分离卷积和正则化去提高计算效率，从而达到优化原有GooogLeNet模型，提高网络性能。</p><p><strong>Q2：这是否是一个新的问题？</strong></p><p>不是，是对原有InceptionV1的优化</p><p><strong>Q3：这篇文章要验证一个什么科学假设？</strong></p><p>探索了可分离卷积和正则化去提高计算效率</p><p><strong>Q4：有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</strong></p><p>无</p><p><strong>Q5：论文中提到的解决方案之关键是什么？</strong></p><p>1.卷积核分解：（1）将大的卷积核分解成小的（2）非对称分解</p><p>2.并行执行（卷积C+池化P），再进行feature map的堆叠</p><p>3.标签平滑（LSR）进行模型正则化</p><p>4.辅助分类器：在本文中，作者纠正了辅助分类器的作用，靠近输入辅助分类器（加不加没有影响），但是靠近输出的辅助分类器（加上BN后）可以起到正则化的作用。</p><p><strong>Q6：论文中的实验是如何设计的？</strong></p><p>在原有Inception-v2逐步改进，形成对照实验组，引出V3</p><p>Inception-v3进行了多尺度的训练，在单模型下的对比结果</p><p>Inception-v3进行模型融合（ensemble)后与其他模型做对比</p><p><strong>Q7：用于定量评估的数据集是什么？代码有没有开源？</strong></p><p>ImageNet2012，有开源</p><p><strong>Q8：论文中的实验及结果有没有很好地支持需要验证的科学假设？</strong></p><p>有，通过对照实验证明了结论</p><p><strong>Q9：这篇论文到底有什么贡献？</strong></p><p>改进原有的Inception-V1结构，使得CNN模型在分类任务上获得更高的性能<br>提出了四大设计原则<br>分解大filters，使其小型化、多层化，提出了“非对称卷积”<br>优化inception v1的auxiliary classifiers，并造了新的带BN的辅助分类器<br>演示了即使是低分辨率的输入也可以达到较好的识别效果<br>使用新的 inception并行模块解决了传统采样层的问题<br>inceptionV3 = inceptionV2+RMSProp+LSR+BN-aux<br><strong>Q10：下一步呢？有什么工作可以继续深入？</strong></p><p>如何在计算量不增加的情况下，解决由于信息压缩造成的信息损失问题。<br>如何在计算量不增加的情况下，增加模型的拓扑结构，以提高模型的表达能力。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Inceptionv2-paper</title>
      <link href="/Inceptionv2-paper/"/>
      <url>/Inceptionv2-paper/</url>
      
        <content type="html"><![CDATA[<h1 id="Batch-Normalization-Accelerating-Deep-Network-Trainingby-Reducing-Internal-Covariate-Shift"><a href="#Batch-Normalization-Accelerating-Deep-Network-Trainingby-Reducing-Internal-Covariate-Shift" class="headerlink" title="Batch Normalization: Accelerating Deep Network Trainingby Reducing Internal Covariate Shift"></a>Batch Normalization: Accelerating Deep Network Trainingby Reducing Internal Covariate Shift</h1><blockquote><h1 id="批归一化-通过减少内部协变量移位加速深度网络训练"><a href="#批归一化-通过减少内部协变量移位加速深度网络训练" class="headerlink" title="批归一化:通过减少内部协变量移位加速深度网络训练"></a>批归一化:通过减少内部协变量移位加速深度网络训练</h1></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>由于每层输入的分布在训练过程中随着前一层的参数发生变化而发生变化，因此训练深度神经网络很复杂。由于需要较低的学习率和仔细的参数初始化，这会减慢训练速度，并且使得训练具有饱和非线性的模型变得非常困难。我们将这种现象称为内部协变量偏移，并通过归一化层输入来解决该问题。我们的方法的优势在于将标准化作为模型架构的一部分，并为每个训练小批量执行标准化。Batch <a href="https://so.csdn.net/so/search?q=Normalization&amp;spm=1001.2101.3001.7020">Normalization</a>允许我们使用更高的学习率，并且在初始化时不那么小心。它还充当正则化器，在某些情况下消除了Dropout的需要。应用于最先进的图像分类模型，批量归一化在训练步骤减少14倍的情况下实现了相同的精度，并以显着的优势击败了原始模型。使用一组批量归一化网络，我们改进了ImageNet分类的最佳发布结果：达到4.9%的top-5验证错误（和4.8%的测试错误），超过了人工评估员的准确性。</p><blockquote><p><strong>主要描述了批量归一化（Batch Normalization，简称BN）技术在深度学习中的作用和好处。</strong></p><ul><li><strong>加速训练</strong>：通过减少内部协变量偏移，BN层帮助网络更快地收敛。</li><li><strong>提高泛化能力</strong>：BN层还能充当一种正则化的角色，有助于提高模型的泛化能力。</li></ul><h3 id="内部协变量偏移："><a href="#内部协变量偏移：" class="headerlink" title="内部协变量偏移："></a>内部协变量偏移：</h3><p>网络的每一层都需要不断适应前一层输出分布的变化，这使得训练变得复杂并减慢了学习速度。</p><h3 id="批量归一化的提出："><a href="#批量归一化的提出：" class="headerlink" title="批量归一化的提出："></a><strong>批量归一化的提出：</strong></h3><p>通过在模型中加入特定的归一化层<strong>来解决内部协变量偏移</strong>问题。这种归一化层对每个训练小批量（batch）的数据进行标准化处理，即调整数据使其均值为0，方差为1，以稳定每一层的输入分布。</p><h3 id="批量归一化的优势："><a href="#批量归一化的优势：" class="headerlink" title="批量归一化的优势："></a><strong>批量归一化的优势</strong>：</h3><ul><li><strong>允许使用更高的学习率</strong>：由于每层输入的分布更加稳定，因此可以使用更高的学习率而不会导致训练不稳定。</li><li><strong>初始化不那么敏感</strong>：归一化减少了模型对参数初始化的依赖。</li><li><strong>充当正则化器</strong>：批量归一化还具有正则化的效果，有时可以替代Dropout等正则化技术。</li></ul><h3 id="实际应用中的效果："><a href="#实际应用中的效果：" class="headerlink" title="实际应用中的效果："></a><strong>实际应用中的效果</strong>：</h3><p>训练步骤减少14倍的情况下实现了相同的精度，并以显着的优势击败了原始模型。</p><h3 id="在ImageNet分类上的成果："><a href="#在ImageNet分类上的成果：" class="headerlink" title="在ImageNet分类上的成果："></a><strong>在ImageNet分类上的成果</strong>：</h3><p>在ImageNet分类任务上取得了4.9%的top-5验证错误率，这个结果甚至超过了人类评审员的准确性，显示了批量归一化技术的强大能力。</p></blockquote><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h2><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv2-1.png" alt=""></p><blockquote><h3 id="随机梯度下降（SGD）："><a href="#随机梯度下降（SGD）：" class="headerlink" title="随机梯度下降（SGD）："></a><strong>随机梯度下降（SGD）</strong>：</h3><p>SGD是一种流行的优化算法，用于训练深度网络，以最小化损失函数，这通常表示为预测错误的度量。</p><h3 id="小批量处理："><a href="#小批量处理：" class="headerlink" title="小批量处理："></a><strong>小批量处理</strong>：</h3><p>在实际应用中，SGD通常不会使用全部训练数据来计算梯度，而是使用一个小批量（<code>m</code> 个样本）来近似梯度。这样做可以提高计算效率(并行)。</p><h3 id="训练中的挑战："><a href="#训练中的挑战：" class="headerlink" title="训练中的挑战："></a><strong>训练中的挑战</strong>：</h3><p>尽管SGD是一个有效的方法，但它对模型的<strong>超参数（如学习率）</strong>和<strong>初始参数值</strong>的选择非常敏感。</p><h3 id="网络深度和训练复杂性："><a href="#网络深度和训练复杂性：" class="headerlink" title="网络深度和训练复杂性："></a><strong>网络深度和训练复杂性</strong>：</h3><p>受到内部协变量偏移的影响。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv2-2.png" alt=""></p><blockquote><h3 id="协变量偏移："><a href="#协变量偏移：" class="headerlink" title="协变量偏移："></a>协变量偏移：</h3><p>当学习系统的输入分布发生变化时，系统需要适应这些变化。</p><h3 id="输入分布的稳定性："><a href="#输入分布的稳定性：" class="headerlink" title="输入分布的稳定性："></a>输入分布的稳定性：</h3><p>如果子网络的输入分布稳定，则学习过程会更有效，因为参数不需要频繁调整来适应输入分布的变化。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv2-3.png" alt=""></p><blockquote><h3 id="批归一化（Batch-Normalization）："><a href="#批归一化（Batch-Normalization）：" class="headerlink" title="批归一化（Batch Normalization）："></a><strong>批归一化（Batch Normalization）</strong>：</h3><p>批归一化是一种减少内部协变量偏移的技术，通过规范化（归一化）每一层输入的均值和方差来实现。这有助于保持每层输入分布的稳定，从而加速训练过程、模型泛化。</p><h3 id="非线性激活函数和饱和问题："><a href="#非线性激活函数和饱和问题：" class="headerlink" title="非线性激活函数和饱和问题："></a><strong>非线性激活函数和饱和问题</strong>：</h3><p>一些激活函数（如sigmoid）在其输入非常高或非常低时会饱和，导致梯度消失，进而减慢训练。批归一化通过保持输入分布的稳定性，可以减轻饱和问题。</p><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>将批归一化应用到ImageNet分类网络时，他们发现可以在使用更少训练步骤的情况下达到与原始模型相同的性能。</p></blockquote><h2 id="2-减少内部协变量偏移"><a href="#2-减少内部协变量偏移" class="headerlink" title="2 减少内部协变量偏移"></a>2 减少内部协变量偏移</h2><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv2-4.png" alt=""></p><blockquote><h3 id="归一化激活值-白化-："><a href="#归一化激活值-白化-：" class="headerlink" title="归一化激活值(白化)："></a><strong>归一化激活值(白化)</strong>：</h3><p>历史上，人们已经发现如果输入数据被“白化”（每个特征都减去其均值并除以其标准差，使得特征分布具有零均值和单位方差），神经网络会更快地收敛。这种技术也可以应用到每一层的输入上，使每一层都得到白化的输入，从而提高训练速度。</p><h3 id="批归一化的原理："><a href="#批归一化的原理：" class="headerlink" title="批归一化的原理："></a><strong>批归一化的原理</strong>：</h3><p>批归一化就是在每一层对激活值进行归一化处理，以确保它们的分布保持稳定。具体来说，就是对每一批数据的激活值减去它们的均值并除以它们的标准差。这可以减少内部协变量偏移，使网络的每一层不必不断调整自己来适应前一层的输出分布变化。</p><h3 id="梯度下降与归一化的冲突："><a href="#梯度下降与归一化的冲突：" class="headerlink" title="梯度下降与归一化的冲突："></a><strong>梯度下降与归一化的冲突</strong>：</h3><p>在实际实验中，如果归一化步骤和梯度下降步骤没有正确地结合，模型可能会出现问题，例如参数爆炸。这表明，在实施批归一化时，需要谨慎计算归一化参数，确保它们与梯度下降步骤协同工作。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv2-5.png" alt=""></p><blockquote><p>为了<strong>确保深度学习模型中的激活值分布稳定</strong>，并避免训练过程中的问题，批归一化被提出作为一种有效的归一化技术。</p></blockquote><h2 id="3-通过小批量统计进行归一化"><a href="#3-通过小批量统计进行归一化" class="headerlink" title="3 通过小批量统计进行归一化"></a>3 通过小批量统计进行归一化</h2><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv2-6.png" alt=""></p><blockquote><p>批归一化通过<strong>对每个特征独立归一化</strong>，并通过<strong>引入可学习的缩放和平移参数</strong>，解决了<strong>传统激活值规范化方法可能带来的限制</strong>。这使得深度网络在保持快速收敛的同时，也保留了表征能力。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv2-7.png" alt=""></p><blockquote><p>BN算法</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv2-8.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv2-9.png" alt=""></p><blockquote><p>BN算法</p></blockquote><h3 id="3-1-批归一化网络的训练与推理"><a href="#3-1-批归一化网络的训练与推理" class="headerlink" title="3.1 批归一化网络的训练与推理"></a>3.1 批归一化网络的训练与推理</h3><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/InV2-10.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv2-11.png" alt=""></p><h3 id="3-2-批归一化卷积网络"><a href="#3-2-批归一化卷积网络" class="headerlink" title="3.2 批归一化卷积网络"></a>3.2 批归一化卷积网络</h3><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv2-12.png" alt=""></p><h3 id="3-3-批归一化允许更高的学习率"><a href="#3-3-批归一化允许更高的学习率" class="headerlink" title="3.3 批归一化允许更高的学习率"></a>3.3 批归一化允许更高的学习率</h3><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv2-13.png" alt=""></p><h3 id="3-4-批归一化使模型正则化"><a href="#3-4-批归一化使模型正则化" class="headerlink" title="3.4 批归一化使模型正则化"></a>3.4 批归一化使模型正则化</h3><p>​    当使用批归一化进行训练时，可以看到一个训练样本与小批量中的其他样本结合使用，训练网络不再为给定的训练样本生成确定值。在我们的实验中，我们发现这种效应有利于网络的泛化。虽然Dropout通常被用来减少过度拟合，在批归一化网络中，我们发现它可以被删除或大量减少。</p><h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h2><h3 id="4-1-Activations-over-time"><a href="#4-1-Activations-over-time" class="headerlink" title="4.1 Activations over time"></a>4.1 Activations over time</h3><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv2-14.png" alt=""></p><h3 id="4-2-ImageNet分类"><a href="#4-2-ImageNet分类" class="headerlink" title="4.2 ImageNet分类"></a>4.2 ImageNet分类</h3><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv2-15.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv2-16.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv2-17.png" alt=""></p><h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h2><p>我们提出了一种新的机制，可以显著加快深度网络的训练。基于众所周知的Covariate Shift使机器学习系统的训练复杂化的前提，Covariate Shift也适用于子网和层，从网络的内部激活中将其删除可能有助于训练。我们提出的方法从归一化激活以及将归一化合并到网络体系结构本身中汲取了力量。这可以确保任何用于训练网络的优化方法都能恰当地处理归一化。为了实现深度网络训练中常用的随机优化方法，我们对每个小批量进行归一化，并通过归一化参数对梯度进行反向传播。批归一化对每个激活只添加两个额外的参数，这样做保留了网络的表征能力。提出了一种利用批归一化网络构造、训练和执行推理的算法。得到的网络可以用饱和非线性进行训练，对增加的训练率更有容忍度，而且通常不需要dropout用于正则化。</p><p>仅仅在最先进的图像分类模型中添加批归一化，就可以大大加快训练速度。通过进一步提高学习率、去除Dropout和应用批归一化所提供的其他修改，我们只需一小部分训练步骤就达到了先前的SOTA，然后在单网络图像分类中击败了SOTA。此外，通过结合使用批归一化训练的多个模型，我们在ImageNet上的表现要比ImageNet上最知名的系统要好。</p><p>有趣的是，我们的方法与[Knowledge matters: Importance of prior information for optimization]的标准化层有相似之处，尽管这两种方法的目标非常不同，执行的任务也不同。批归一化的目标是在整个训练过程中实现激活值的稳定分布，在我们的实验中，我们将其应用于非线性之前，因为在非线性之前，匹配一阶和二阶矩更有可能得到稳定的分布。相反，另一个方法将标准化层应用于非线性的输出，导致更稀疏的激活。在我们的大规模图像分类实验中，无论有没有批归一化，我们都没有观察到非线性输入是稀疏的。批归一化的另一个显著区别特征包括学习尺度和平移，从而允许BN变换来表示恒等变换（标准化层不需要这个，因为它后面跟着的是学习的线性变换，从概念上讲，它吸收了必要的尺度和平移），卷积层的处理，不依赖于小批量的确定性推理和在网络中批归一化每个卷积层。</p><p>在这项工作中，我们还没有探索批归一化可能实现的所有可能性。我们未来的工作包括将我们的方法应用于递归神经网络，其中Internal Covariate Shift和梯度消失或爆炸可能特别严重，这将使我们能够更彻底地检验归一化改善梯度传播的假设（第3.3节）。我们计划研究批归一化是否可以帮助领域适应，从传统意义上说，即网络执行的归一化是否可以使其更轻松地推广到新的数据分布，也许只需对总体均值和方差重新计算即可（算法2）。最后,我们相信的进一步理论分析算法将允许更多的改进和应用。</p><blockquote><h3 id="总结几个点"><a href="#总结几个点" class="headerlink" title="总结几个点:"></a>总结几个点:</h3><ul><li>可以放大学习率。</li><li>免去使用dropout。</li><li>大幅降低L2，且提高精准度。</li><li>加速lr衰减。</li><li>不必用LRN。</li><li>更彻底的shuffling，相当于正则,提高精准度。</li></ul></blockquote><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>显著的架构变化包括:</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inv2-18.png" alt=""></p><blockquote><p>这篇文章提出了BN也就是批归一化操作，基本上在现在的CNN中是一个必备操作了。然后对Inception所作的一些修改中，比较有启示作用的还是用了两个3 × 3卷积代替了原本的5 × 5 卷积。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>NiN-Paper</title>
      <link href="/NiN-Paper/"/>
      <url>/NiN-Paper/</url>
      
        <content type="html"><![CDATA[<h1 id="《Going-deeper-with-convolutions》"><a href="#《Going-deeper-with-convolutions》" class="headerlink" title="《Going deeper with convolutions》"></a>《Going deeper with convolutions》</h1><blockquote><h1 id="深入卷积-2014-09"><a href="#深入卷积-2014-09" class="headerlink" title="深入卷积 2014.09"></a>深入卷积 2014.09</h1></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们提出了一种<strong>新式深度网络结构</strong>称之为NIN，<strong>增强模型对感受野内局部块的辨别力</strong>。 传统的卷积层使用线性过滤器后跟非线性激活函数来扫描输入。 与之相反的是我们构建更加复杂结构的<strong>微神经网络</strong>来使得感受野内的数据抽象化。 我们使用<strong>多层感知器</strong>来实例化微神经网络，这是一种强大的函数逼近器。 特征图是通过以<strong>与 CNN 类似</strong>的方式在输入上滑动微网络获得的，然后将特征图送入下一层。 <strong>深度 NIN 可以通过堆叠多个上述结构来实现。</strong> 通过微网络增强的局部建模，我们能够在分类层的特征图上利用全局平均池化，这比传统的全连接层更容易解释并且更不容易过拟合。同时我们展示了 NIN 在 CIFAR-10 和 CIFAR-100 上的最新分类性能，以及在 SVHN 和 MNIST 数据集上的合理性能。</p><blockquote><h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><p><strong>MLP：</strong>小型神经网络替换了传统CNN中的线性卷积滤波器。NiN在这些卷积层中嵌入了多层感知器，每个感受野对应的位置上都有一个MLP。这些MLP由多个全连接层组成，并且每层都有自己的非线性激活函数。这允许每个感受野内的数据经历更复杂的非线性变换，从而能够提取更丰富的特征。</p><ul><li>对局部特征进行更细致处理的方法</li></ul><p><strong>GAP：</strong>计算特征图中每个特征通道的平均值，使得每个通道的输出都直接对应到最终的分类结果，提高了模型在分类任务中的解释性。</p><ul><li>大大减少了模型参数的数量</li><li>这不仅降低了过拟合的风险，而且让模型的决策过程更加清晰（因为我们可以直接看到每个特征通道对于最终决策的贡献）。</li><li>不容易过拟合：全局平均池化相比全连接层在直观理解和泛化性能上的优势</li></ul></blockquote><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h2><p>卷积神经网络(CNNs)[1]由交替的卷积层和池化层组成。卷积层在输入的每个局部部分取线性滤波器和底层接收场的内积，然后是一个非线性激活函数。结果输出称为特征映射。</p><blockquote><p>线性滤波器指的是卷积核，底层接收场指的是感受野。</p></blockquote><p>CNN的卷积滤波器是底层数据块的广义线性模型（generalized linear model ）（GLM），而且我们认为它的抽象程度较低。抽象是指特征对同一概念的变体是不变的[2]。用更强（有效）的非线性函数逼近器代替GLM，可以提高局部模型的抽象能力。当样本的隐含概念（latent concept）是线性可分时，GLM可以达到很好的抽象程度，例如：这些概念的变体都在GLM分割平面的同一边。因此，传统的CNN隐含地假设潜在的概念是线性可分的。然而，同一概念的数据通常是非线性流形的（nonlinear manifold），捕捉这些概念的表达通常都是输入的高维非线性函数。在NIN中，用一种一般非线性函数逼近器的“微网络结构”代替了GLM。在本文中，我们选择多层感知器[3]作为微网络的实例化，它是一种通用函数逼近器，是一个可通过反向传播进行训练的神经网络。</p><blockquote><h3 id="传统vs实际"><a href="#传统vs实际" class="headerlink" title="传统vs实际"></a>传统vs实际</h3><p><strong>传统：</strong>局部感受野的广义线性模型(GLM)，即使用线性过滤器后跟非线性激活函数来扫描输入。这一般只会学习到较低抽象层级（只能识别简单的图案和边缘），即可能只能理解输入数据的基本结构。</p><ul><li>传统的CNN隐含地假设潜在的概念是线性可分的。</li><li>但同一概念的数据通常是非线性流形的（nonlinear manifold），这是GLM面临的挑战。</li></ul><p><strong>实际：</strong>通过使用更有效的非线性函数逼近器（比如多层感知器），可以提高卷积层的抽象能力。这就像是用一套更复杂的算法来解释数据中的模式，而不是简单地将它们视为像素的集合。</p><ul><li>能够对局部感受野内的信息进行更深层次的非线性处理。</li></ul></blockquote><p>我们称之为mlpconv层的结果结构在图1中与CNN进行了比较。线性卷积层和mlpconv层都将局部接收场映射到输出特征向量。 mlpconv 层将局部块（ input local patch）的输入通过一个<strong>由<a href="https://so.csdn.net/so/search?q=全连接层&amp;spm=1001.2101.3001.7020">全连接层</a>和非线性激活函数组成的多层感知器（MLP）</strong>映射到了输出的特征向量。 <strong>MLP在所有局部感受野中共享。</strong>特征图通过用像CNN一样的方式在输入上滑动MLP得到，<strong>NIN的总体结构是一系列mplconv层的堆叠。</strong> 它被称为“网络中的网络”(NIN)，因为我们有微型网络(MLP)，它在mlpconv层中构成整个深层网络的组成部分。</p><blockquote><p>线性卷积层和mlpconv都是将局部感受野(local receptive field)映射到输出特征向量。</p><p>Mlpconv由若干全连接层和非线性激活函数组成，目的是在每个局部感受野内进行更复杂的特征抽象。</p><p>Mlpconv核使用带非线性激活函数的MLP，跟传统的CNN一样，MLP在各个局部感受野中共享参数的，滑动MLP核可以最终得到输出特征图。</p><p>NIN通过多个mlpconv的堆叠得到。</p></blockquote><p>我们不采用在CNN中传统的完全连通层进行分类，而是通过全局平均池层（global average pooling layer）直接输出最后一个mlpconv层的特征映射的空间平均值作为类别的可信度，然后将得到的向量输入到Softmax层。 在传统的CNN中，很难解释如何将来自分类层（objective cost layer）的分类信息传递回前一个卷积层，因为全连接层像一个黑盒一样。相反，全局平均池更有意义和可解释，因为它强制特征映射和类别之间的对应，这是通过使用微型网络进行更强的局部建模而实现的。此外，完全连接层容易过度拟合，严重依赖于Dropout正则化[4][5]，而全局平均池本身就是一个结构正则化，这在本质上防止了对整体结构的过度拟合。</p><blockquote><ul><li>GAP通过计算特征映射的空间平均值，为每个类别生成一个数值，然后这些数值被输入到Softmax层以进行最终的分类。</li><li>全局平均池化层自身就具有结构正则化的效果，能够在本质上减少对模型整体结构的过拟合风险。</li><li>全局平均池化层的有效性部分源自于NIN中微型网络的使用，从而使得通过GAP层计算得到的类别可信度更加准确。</li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/NiN-Figure1.png" alt=""></p><blockquote><p>图1:线性卷积层与mlpconv层的对比。线性卷积层包含一个线性滤波器，而mlpconv层包含一个微网络(本文选择多层感知器)。两层都将局部接受野映射到潜在概念的置信度值。</p></blockquote><h2 id="2-卷积神经网络"><a href="#2-卷积神经网络" class="headerlink" title="2 卷积神经网络"></a>2 卷积神经网络</h2><p>经典卷积神经元网络[1]由交替叠加的卷积层和空间汇聚层组成。卷积层通过线性卷积滤波器和非线性激活函数(整流器（rectifier）、Sigmoid、tanh等)生成特征映射。以线性整流器为例，可以按以下方式计算特征图：</p><script type="math/tex; mode=display">f_{i,j,k} = \max(w_k^T x_{i,j}, 0).</script><p>这里的((i, j))是特征图像素的索引，xij代表以位置((i, j))为中心的输入块，(k)用来索引特征图的颜色通道。</p><blockquote><p>CNN_ReLu的计算公式</p></blockquote><p>当隐性概念（ the latent concepts ）的实例是线性可分的时，这种线性卷积就足以进行抽象。然而，实现良好抽象的表示通常是输入数据的高度非线性函数。 在传统的CNN中，这可以通过利用一套完整的滤波器来弥补，覆盖所有隐含概念的变化。也就是说，可以学习独立的线性滤波器来检测同一概念的不同变化。然而，对单个概念有太多的过滤器会给下一层带来额外的负担，（因为）下一层需要考虑上一层的所有变化组合[7]。在CNN中， 来自更高层的滤波器会映射到原始输入的更大区域。它通过结合下面层中的较低级别概念来生成更高级别的概念。因此，我们认为，在将每个本地块（local patch）合并为更高级别的概念之前，对每个本地快进行更好的抽象是有益的。</p><blockquote><p>如果数据中的隐性概念（比如图片中的猫、狗等物体）在特征空间中能够通过一个直线或平面轻易地区分开来，那么传统的线性卷积操作就足够有效了。</p><p>数据的高级特征通常是非线性分布的，这就需要非线性方法来进行有效的特征抽象。</p><p>微型网络（如MLP）来替代传统的线性卷积操作，即在将较低级别的概念（从局部区域抽取的特征）组合成更高级别的概念（全局的、更复杂的特征）之前，如果能在每个局部区域（本地块）实现更好的抽象，那么整个网络的表现会更好。</p></blockquote><p>在最近的Maxout网络[8]中，特征映射的数目通过仿射特征映射上的最大池来减少(仿射特征映射是线性卷积不应用激活函数的直接结果)。线性函数的极大化使分段线性逼近器( piecewise linear approximator)能够逼近任意凸函数。与进行线性分离的传统卷积层相比，最大输出网络更强大，因为它能够分离凸集中的概念。 这种改进使maxout网络在几个基准数据集上有最好的表现。</p><p>但是，Maxout网络强制要求潜在概念的实例存在于输入空间中的凸集中，这并不一定成立。当隐性概念的分布更加复杂时，需要使用更一般的函数逼近器。我们试图通过引入一种新颖的“Network In Network”结构来实现这一目标，即在每个卷积层中引入一个微网络来计算局部块的更抽象的特征。</p><blockquote><p>Maxout网络假定数据的分布形成了凸集。然而，这个假设在现实世界的数据中并不总是成立，特别是当数据的分布更加复杂时。</p><p>了处理更加复杂的数据分布，NIN提出了在卷积层内部使用微型网络来计算特征的方法，即更加一般的函数逼近器。</p></blockquote><p>在以前的一些工作中，已经提出了在输入端滑动一个微网络。例如，结构化多层感知器(SMLP)[9]在输入图像的不同块上应用共享多层感知器；在另一项工作中，基于神经网络的滤波器被训练用于人脸检测[10]。然而，它们都是针对特定的问题而设计的，而且都只包含一层滑动网络结构。NIN是从更一般的角度提出的，将微网络集成到CNN结构中，对各个层次的特征进行更好的抽象。</p><blockquote><p>先前的工作如SMLP或基于神经网络的滤波器通常是为了解决特定的任务（如人脸检测）而设计的，它们通常只包含一层这样的滑动网络结构。</p><p>NIN提出了一个更通用的概念，将微型网络集成到CNN的多个层次中。</p><p>NIN不仅仅在输入层使用微网络，而是在网络的多个层次中都使用微型网络来进行特征抽象。</p></blockquote><h2 id="3-网络中的网络"><a href="#3-网络中的网络" class="headerlink" title="3 网络中的网络"></a>3 网络中的网络</h2><p> 我们首先强调了我们提出的“网络中的网络”结构的关键组成部分：MLP卷积层和全局平均池层分别在3.1节和3.2节中。然后我们在3.3节中详细描述整个NiN。</p><blockquote><p>MLP+GAP+NIN</p></blockquote><h3 id="3-1-MLP卷积层"><a href="#3-1-MLP卷积层" class="headerlink" title="3.1 MLP卷积层"></a>3.1 MLP卷积层</h3><p>由于没有关于潜在概念分布的先验信息，使用通用函数逼近器来提取局部块的特征是可取的，因为它能够逼近潜在概念的更抽象的表示。径向基网络和多层感知器是两种著名的通用函数逼近器。我们在这项工作中选择多层感知器有两个原因。首先，多层感知器与采用反向传播训练的卷积神经网络的结构兼容；第二，多层感知器本身可以是一个深层次的模型，这与特征重用的精神是一致的[2]。在本文中，这种新型的层称为mlpconv，其中MLP取代GLM，在输入上进行转换。图1说明了线性卷积层和mlpconv层之间的区别。由mlpconv层执行的计算如下：</p><script type="math/tex; mode=display">f_{i,j,k_1}^1 = \max(w_{k_1}^1 T x_{i,j} + b_{k_1}, 0).</script><script type="math/tex; mode=display">\vdots</script><script type="math/tex; mode=display">f_{i,j,k_n}^n = \max(w_{k_n}^n T f_{i,j}^{n-1} + b_{k_n}, 0).</script><p>这里n是多层感知器中的层数。在多层感知器中，采用整流线性单元作为激活函数。</p><p>从跨信道(跨特征图—cross feature map)池的角度看，方程2等价于正规卷积层上的级联跨信道参数池( cascaded cross channel parametric pooling)。每个池层对输入特征映射执行加权线性重组，然后通过整流线性单元。跨通道池功能映射在下一层中一次又一次地跨通道池。这种级联的跨信道参数池结构允许复杂且可学习的交叉信道信息交互。</p><p>跨信道参数池层也等价于具有1x1卷积核的卷积层。这一解释使得理解nin的结构更为直观。</p><blockquote><p>径向基函数：一种使用径向基函数（如高斯函数）作为激活函数的神经网络结构，能够处理非线性问题。</p><p>跨信道池：不同特征图上相同位置的特征值进行加权和（类似于传统的卷积操作），然后应用非线性激活函数（如ReLU）来完成。</p><p>跨信道参数池层：1x1卷积核来实现</p></blockquote><p><strong>与maxout层的比较</strong>：maxout网络中的maxout层跨多个仿射特征映射执行max池化[8]。最大层的特征图计算如下：</p><script type="math/tex; mode=display">f_{i,j,k} = \max_m \left( w_{k_m}^T x_{i,j} \right).</script><p>线性函数上的极大值形成一个分段线性函数，它能够建模任何凸函数。对于凸函数，函数值低于特定阈值的样本构成凸集。因此，通过逼近局部块的凸函数，maxout对样本位于凸集(即L2球、凸锥)的概念具有形成分离超平面的能力。mlpconv层与maxout层的不同之处在于，凸函数逼近器被一个通用函数逼近器所代替，它在建模各种隐性概念分布方面具有更强的能力。</p><blockquote><p>相比处理假定数据的分布形成了凸集的数据，MLP更加泛化</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/NiN-Figure2.png" alt=""></p><blockquote><p>图2：网络中网络的总体结构。在本文中，NINS包括三个mlpconv层和一个全局平均池层的叠加。</p></blockquote><h3 id="3-2-全局平均池化"><a href="#3-2-全局平均池化" class="headerlink" title="3.2 全局平均池化"></a>3.2 全局平均池化</h3><p>传统的卷积神经网络在网络的底层进行卷积。在分类方面，将上一卷积层的特征映射矢量化，并将其输入到完全连通的层中，然后是Softmax Logistic回归层[4][8][11]。该结构将卷积结构与传统的神经网络分类器连接起来。它将卷积层作为特征提取器，并对生成的特征进行传统的分类。</p><p>然而，全连通层容易过度拟合，从而阻碍了整个网络的泛化能力。Dropout是由Hinton等人提出的[5]。它作为一个正则化，在训练期间随机地将半个完全连接的层的激活设置为零。它提高了泛化能力，在很大程度上防止了过度拟合[4]。</p><blockquote><p>传统：CNN+Softmax Logistic回归层</p><p>缺点：全连接层阻碍泛化能力</p></blockquote><p>在本文中，我们提出了另一种策略，称为全局平均池，以取代CNN中传统的全连通层。其思想是为最后一个mlpconv层中的分类任务的每个对应类别生成一个特征映射。我们没有在特征映射的顶部添加完全连通的层，而是取每个特征映射的平均值，并将得到的向量直接输入到Softmax层。全局平均池取代完全连通层上的一个优点是，通过增强特征映射和类别之间的对应关系，它更适合于卷积结构。因此，特征映射可以很容易地解释为类别信任映射。另一个优点是在全局平均池中没有优化参数，从而避免了这一层的过度拟合。此外，全局平均池综合了空间信息，从而对输入的空间平移具有更强的鲁棒性。</p><blockquote><p>GAP：为最后一个mlpconv层中的分类任务的每个对应类别生成一个特征映射。</p><p>优点：</p><ul><li><p>特征映射可以很容易地解释为类别信任映射。</p></li><li><p>没有优化参数，从而避免了这一层的过度拟合。</p></li><li>综合了空间信息，泛化性强。</li></ul></blockquote><p>我们可以看到全局平均池是一个结构正则化器，它显式地将特征映射强制为概念(类别)的信任映射。这是由mlpconv层实现的，因为它们比GLMS更接近置信度图。</p><blockquote><p>显式地将特征映射强制为概念(类别)的信任映射。</p></blockquote><h3 id="3-3-网络中的网络结构"><a href="#3-3-网络中的网络结构" class="headerlink" title="3.3 网络中的网络结构"></a>3.3 网络中的网络结构</h3><p>NIN的总体结构是一个由mlpconv层组成的堆栈，上面是全局平均池和目标成本层。在mlpconv 层之间可以添加下采样层，如cnn和maxout网络中的那样。图2显示了一个包含三个mlpconv层的nin。在每个mlpconv层中，有一个三层感知器。NIN和微网络中的层数都是灵活的，可以根据特定的任务进行调整。</p><blockquote><p>NiN架构：</p><ul><li>无全连接层</li><li>交替使用NiN块和步幅为2的最大池化层（逐渐减小高宽和增大通道数）</li><li>最后使用全局平均池化层得到输出（其输入的通道数是类别数，相当于每一个通道提取一个类别的信息）</li></ul></blockquote><h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4 实验"></a>4 实验</h2><h3 id="4-1-概观"><a href="#4-1-概观" class="headerlink" title="4.1 概观"></a>4.1 概观</h3><p>我们对四个基准数据集进行了评估：CIFAR-10[12]、CIFAR-100[12]、Svhn[13]和Mnist[1]。用于数据集的网络都由三个层叠的mlpconv层组成，所有实验中的mlpconv层随后都是一个空间最大池层，它对输入图像进行二倍的向下采样。作为正则化器，除最后一个mlpconv层外，所有输出都应用了Dropout。除非具体说明，实验部分中使用的所有网络都使用全局平均池，而不是网络顶部完全连接的层。另一个应用的正则化方法是Krizhevsky等人使用的权重衰减[4]。图2说明了本节中使用的nin网络的总体结构。补充材料中提供了详细的参数设置。我们在AlexKrizhevsky[4]开发的超快Cuda-ConvNet代码上实现我们的网络。据集的预处理、训练和验证集的分割都遵循GoodFelt等人的观点[8]。</p><p>我们采用Krizhevsky等人使用的训练过程[4]。也就是说，我们手动设置适当的初始化权值和学习率。该网络是使用规模为128的小型批次进行训练的。训练过程从初始权重和学习率开始，一直持续到训练集的准确性停止提高，然后学习率降低10。这个过程被重复一次，（因此）最终的学习率是初始值的百分之一。</p><blockquote><p>Net：均使用三个层叠的mlpconv层，每个mlpconv层后面都跟随一个空间最大池化层进行下采样，以便提取和缩减图像特征</p><p>正则化：Dropout和权重衰减</p><p>训练过程：使用大小为128的小批量，手动设置了初始权重和学习率。然后将学习率降低10倍进行进一步训练，最终lr是原来的1/100</p></blockquote><h3 id="4-2-Cifar-10"><a href="#4-2-Cifar-10" class="headerlink" title="4.2 Cifar-10"></a>4.2 Cifar-10</h3><p>CIFAR-10数据集[12]由10类自然图像组成，总共有<strong>50000幅训练</strong>图像和<strong>10000幅测试</strong>图像。每个图像都是大小为<strong>32x32</strong>的<strong>RGB</strong>图像。对于这个数据集，我们应用了古德费罗等人使用的相同的<strong>全局对比规范化</strong>和<strong>ZCA白化</strong>（global contrast normalization and ZCA whitening）。我们使用最后10000张培训集的图像作为验证数据。</p><p>本实验中每个mlpconv层的特征映射数被设置为与相应的maxout网络中相同的数目。使用验证集对两个超参数进行了调整，即<strong>局部接收场大小</strong>和<strong>权重衰减</strong>（the local receptive field size and the weight decay）。在此之后，超参数是固定的，我们用训练集和验证集从零开始对网络进行重新训练。结果模型用于测试。在此数据集上，我们获得了10.41%的测试误差，与最新的数据集相比，测试误差提高了1%以上。与以往方法的比较见表1。</p><blockquote><ul><li>在Cifar-10进行训练预测</li><li>采用全局对比规范化和ZCA白化预处理</li><li>调整局部接收场大小（卷积核大小）和权重衰减两个超参数</li><li>10.41%的测试误差，相比之前的方法测试误差提高了1%以上</li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/NiN-Table1.png" alt=""></p><p>实验结果表明，通过提高模型的泛化能力，NIN中的<strong>MLpconv层之间使用Dropout</strong>可以提高网络的性能。如图3所示，在mlpconv层间引用dropout层错误率减少了20%多。这一结果与Goodfellow等人的一致，所以本文的所有模型mlpconv层间都加了dropout。没有dropout的模型在CIFAR-10数据集上错误率是14.5%，已经超过之前最好的使用正则化的模型（除了maxout）。由于没有dropout的maxout网络不可靠，所以本文只与有dropout正则器的版本比较。</p><p>为了与以前的工作相一致，我们还对CIFAR-10数据集的平移和水平翻转增强的方法进行了评估。我们可以达到8.81%的测试误差，这就达到了新的最先进的性能。</p><blockquote><ul><li>MLPconv之间添加dropout</li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/NiN-Figure3.png" alt=""></p><blockquote><p>图3：在mlpconv层之间Dropout的正则化效果。给出了前200次训练中有无辍学的nin的训练和测试误差。</p></blockquote><h3 id="4-3-Cifar-100"><a href="#4-3-Cifar-100" class="headerlink" title="4.3 Cifar-100"></a>4.3 Cifar-100</h3><p>CIFAR-100数据集[12]的大小和格式与CIFAR-10数据集相同，但它包含100个类。因此，每个类中的图像数量仅为CIFAR-10数据集的十分之一。对于CIFAR-100，我们<strong>不调优超参数</strong>，而是使用与CIFAR-10数据集相同的设置。唯一的区别是最后一个mlpconv层输出100个功能映射。CIFAR-100的测试误差为35.68%，在不增加数据的情况下，其性能优于目前的最佳性能。</p><blockquote><ul><li>使用与与CIFAR-10数据集相同的设置</li><li>最后一个mlpconv层输出100个功能映射，因为一张图中每个通道代表一个类别的置信度分数</li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/NiN-Table2.png" alt=""></p><h3 id="4-4-街景房号-SVHM"><a href="#4-4-街景房号-SVHM" class="headerlink" title="4.4 街景房号(SVHM)"></a>4.4 街景房号(SVHM)</h3><p>SVHN数据集[13]由<strong>630420张32x32</strong>彩色图像组成，分为<strong>训练集、测试集和验证集</strong>。该数据集的任务是对位于每幅图像中心的数字进行分类。训练和测试程序遵循古德费罗等人的要求[8]。也就是说，从训练集中选择的每类400个样本和从验证集合中选出的每类200个样本用于验证。训练集的其余部分和验证集用于训练。验证集仅用作超参数选择的指导，而从未用于模型的训练。</p><p>数据集的预处理也同Goodfellow[8]，即使用局部对比度归一化（local contrast normalization）。Svhn中使用的结构和参数<strong>类似于CIFAR-10的结构和参数</strong>，也是由三个mlpconv层和之后的全局平均池组成。 我们在这个数据集上得到2.35%的错误率。我们将结果与其他没有做数据增强的方法结果进行比较，如表3所示。</p><blockquote><ul><li>添加了验证操作</li><li>类似于CIFAR-10的结构和参数</li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/NiN-Table3.png" alt=""></p><h3 id="4-5-Minst"><a href="#4-5-Minst" class="headerlink" title="4.5 Minst"></a>4.5 Minst</h3><p>MNIST[1]数据集由大小为28x28的手写数字0-9组成。共有6万张培训图像和1万张测试图像。对于这个数据集，采用了与CIFAR-10相同的网络结构.但是，从每个mlpconv层生成的特征映射的数量减少了。因为mnist是一个比CIFAR-10更简单的数据集，所以需要更少的参数。我们在这个数据集上测试我们的方法而不增加数据。计算结果与以往采用卷积结构的工作结果进行了比较，如表4所示。</p><p>我们得到了0.47%的表现，但是没有当前最好的0.45%好，因为MNIST的错误率已经非常低了。</p><blockquote><ul><li>与CIFAR-10相同的网络结构</li><li>每个MLP生成的特征映射的数量减少（因为MINST简单，所以更少的参数即可）</li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/NiN-Table4.png" alt=""></p><h3 id="4-6-作为正则化者的全局平均池"><a href="#4-6-作为正则化者的全局平均池" class="headerlink" title="4.6 作为正则化者的全局平均池"></a>4.6 作为正则化者的全局平均池</h3><p>全局平均池层与完全连接层相似，因为它们都执行矢量化特征映射的线性转换。差别在于变换矩阵。对于全局平均池，变换矩阵是前缀的（ prefixed）， 并且仅在共享相同值的块对角线元素上是非零的。完全连通的层可以有密集的变换矩阵，并且这些值要经过反向传播优化。为了研究全局平均池的正则化效应，我们将全局平均池层替换为完全连通层，而模型的其他部分保持不变。为了研究全局平均池的正则化效应，我们将全局平均池层替换为完全连通层，而模型的其他部分保持不变。这两种模型都在CIFAR-10数据集上进行了测试，性能比较见表5。</p><p>如表5所示，全连接层没有dropout的表现最差，11.59%，与预期一样，全连接层没有正则化器会过拟合。在完全连接层之前增加dropout，降低了测试误差(10.88%)。全局平均池的测试误差最低(10.41%)。</p><p>&gt;</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/NiN-Table5.png" alt=""></p><p>接着，我们探讨全局平均池是否对常规CNN具有相同的正则化效果。我们实例化了传统的CNN，如Hinton等人所描述的[5]，由三个卷积层和一个局部连接层组成。局部连接层（local connection layer）生成16个特征映射，这些特征映射被馈送给一个完全连接的具有dropout的层。为了使比较公平，我们将本地连接层的特征映射从16减少到10，因为在全局平均池方案中，每个类别只允许一个特征映射。然后，通过用全局平均池替换掉完全连接的层，创建具有全局平均池的等效网络。在CIFAR-10数据集上进行了性能测试。</p><p>这种全连通层CNN模型的误码率仅为17.56%。当加入Dropout时，我们实现了类似的性能(15.99%)，正如Hinton等人所报告的[5]。在该模型中，用全局平均池代替完全连通层，我们得到了16.46%的误差率，与无Dropout的CNN相比，误差提高了1%。再次验证了全局平均池层作为正则化层的有效性。虽然它略差于退出正则化结果，但我们认为对于线性卷积层来说，全局平均池可能要求过高，因为它需要经过修正激活的线性滤波器来建模类别的置信图。</p><blockquote><p>表明全局平均池化层能作为一种有效的正则化手段，尽管其性能略逊于Dropout。</p></blockquote><h3 id="4-7-NiN可视化"><a href="#4-7-NiN可视化" class="headerlink" title="4.7 NiN可视化"></a>4.7 NiN可视化</h3><p>我们在nin的最后一个mlpconv层显式地执行特征映射，通过全局平均池将其作为类别的置信度映射，这只有通过更强的局部接受域建模(如nin中的mlpconv)才能实现。为了了解这一目的实现程度，我们从CIFAR-10训练模型的最后一个mlpconv层中提取并直接可视化了特征映射。</p><p>图4显示了从CIFAR-10测试集中选择的10个类别中的每个类别的一些示例图像及其相应的特征图。预期最大的激活（activations ）是在对应于输入图像的地面真相类别（ the ground truth category）的特征图中观察到的，这是通过全局平均池显式执行的。在地面真实类别的特征图中，可以观察到最强烈的激活出现在原始图像中物体的同一区域。对于结构化对象尤其如此，例如图4第二行中的CAR。请注意，类别的特征映射仅使用类别信息进行训练。如果使用对象的包围框作为细粒度标签，则期望得到更好的结果。</p><blockquote><p>可视化结果显示，模型能够在与图像真实类别对应的特征图中产生最强烈的激活，特别是在图像中物体的位置上，表明NIN能有效识别图像中关键区域。</p><ul><li>即<strong>成功地将特征映射转换为类别置信度映射</strong>。</li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/NiN-Figure4.png" alt=""></p><blockquote><p>图4：最后一个mlpconv层的特征映射的可视化。只有前10%的激活功能地图显示。特征映射对应的分类如下：1.飞机，2.汽车，3.小鸟，4.猫，5.鹿，6.狗，7.青蛙，8.马，9.飞船，10.卡车。特征映射对应于输入图像的地面真相被突出显示。左面板和右面板只是不同的例子。</p></blockquote><h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h2><p>我们提出了一种新的用于分类任务的深度网络-“网络中的网络”(NetworkinNetwork，NIN)。这种新结构由多层感知器来转换输入的mlpconv层和一个全局平均池层组成，作为传统CNN中完全连接层的替代。mlpconv层更好地模型化了局部块，全局平均池充当了防止全局过度拟合的结构正则化器。使用这两个NIN组件，我们在CIFAR-10、CIFAR-100和Svhn数据集上演示了最新的性能。通过特征映射的可视化，证明了NIN最后一个mlpconv层的特征映射是类别的置信度映射，这就激发了通过nin进行目标检测的可能性。</p><blockquote><p>MLP：更好地模型化了局部块</p><p>GAP：防止全局过度拟合的结构正则化器</p><p>可视化：激发了通过nin进行目标检测的可能性</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Inceptionv1-paper</title>
      <link href="/Inceptionv1-paper/"/>
      <url>/Inceptionv1-paper/</url>
      
        <content type="html"><![CDATA[<h1 id="Going-deeper-with-convolutions"><a href="#Going-deeper-with-convolutions" class="headerlink" title="Going deeper with convolutions"></a>Going deeper with convolutions</h1><blockquote><h1 id="深入卷积"><a href="#深入卷积" class="headerlink" title="深入卷积"></a>深入卷积</h1></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们提出了一种代号为“GoogleNet”的深度卷积神经网络架构，它负责在2014 年ImageNet大规模视觉识别挑战(ILSVRC14)中设置分类和检测的新技术状态。 该体系结构的主要特点是提高了网络内部计算资源的利用率。这是通过一个精心设计的设计来实现的，它允许增加网络的深度和宽度，同时保持计算预算不变。为了优化质量，体系结构的决策是基于赫布原则和多尺度处理的直觉。 在我们提交给ILSVRC14时使用的一个特殊化身是GoogLeNet，这是一个22层的深度网络，其质量是在分类和检测的背景下进行评估的。</p><blockquote><h3 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h3><ul><li>引出Inception这个网络架构，并介绍它在ILSVRC14中提供新的技术水平。</li><li>介绍Inception的主要特点是提高了网络中计算资源的利用率。</li><li><strong>本文的设计目的：</strong>通过精细的设计，在保证计算量不变的情况下，增加深度和宽度。</li><li><strong>用到的理论知识：</strong>为了优化质量，架构基于Hebbian原则（生物神经方面）和多尺度处理的。</li><li>提交给ILSVRC14的是一个特例叫<strong>GoogLeNet</strong>，这是一个<strong>22层</strong>的深度网络，其质量是在分类和检测背景下评估的</li></ul><h3 id="其他内容"><a href="#其他内容" class="headerlink" title="其他内容"></a>其他内容</h3><p><strong>Hebbian原则：</strong>神经科学上的概念，简单讲就是神经元突触的‘用进废退’，两个神经元，如果总是同时兴奋，就会形成一个组合，其中一个神经元的兴奋会促进另一个的兴奋。反映在inception上就是把相关性强的特征汇聚的一起。</p><p><strong>多尺度处理：</strong>不同尺度的卷积核并行处理。</p></blockquote><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h2><p>在过去的三年里，主要是<strong>由于深度学习的进步</strong>，更具体地说，卷积神经网络[10]，图像识别和目标检测的质量一直在以惊人的速度发展。一个令人鼓舞的消息是，这些进展不仅仅是更强大的硬件、更大的数据集和更大的模型的结果，而且主要是新想法、算法和改进的网络架构的结果。 例如，除了ILSVRC2014竞赛的分类数据集外，没有使用新的数据源。我们提给ILSVRC 2014的谷歌网络实际上比克里热夫斯基等人[9]两年前的获奖架构少了12个参数，同时也更准确得多。目标检测的最大收益并不是仅仅来自使用深度网络或更大的模型，而是来自深度架构和经典计算机视觉的协同作用， 比如Girshick等人[6]的R-CNN算法。</p><p>另一个值得注意的因素是，随着移动计算和嵌入式计算的持续发展，我们的算法的效率——特别是它们的能力和内存使用——变得越来越重要。值得注意的是，导致本文中提出的深层架构设计的考虑包括了这个因素，而不是完全固定在精度数字上。对于大多数的实验，模型设计保持15亿的计算预算，所以他们不会成为一个纯粹的学术好奇心，但可以把现实世界使用，即使在大型数据集，在一个合理的成本。</p><p>在本文中，我们将重点研究一种高效的计算机视觉深度神经网络架构，代号GoogleNet， 它的名字来源于Lin等[12]在网络论文中发表的网络和著名的“we need to go deeper”网络模因[1]。 在我们的例子中，“deep”一词有两种不同的含义：首先，我们以“Inception module”的形式引 入了一个新的组织层次，也在更直接的意义上增加了网络深度。一般来说，我们可以从Arora等 人[2]等人的理论工作中获得[12]的灵感和指导。该架构的好处在ILSVRC2014的分类和检测挑战上得到了实验验证，其中它的性能显著优于当前的最新技术水平。</p><blockquote><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul><li>由于深度学习和卷积网络发现，目标分类和检测能力显著提高。</li><li>没有用到新的数据集</li><li>目标检测领域，最大的收获不是来自更深的网络和更大的模型，而是深度架构和经典计算机视觉的协同作用。</li><li>随着移动计算和嵌入式计算的持续发展，不能一味追求精读提升，算法效率尤其是功率和内存使用格外重要。</li></ul><h3 id="Inception的诞生"><a href="#Inception的诞生" class="headerlink" title="Inception的诞生"></a>Inception的诞生</h3><p>本文关注计算机视觉中的高效深度神经网络架构Inception</p><h3 id="两个重要启发文献"><a href="#两个重要启发文献" class="headerlink" title="两个重要启发文献"></a>两个重要启发文献</h3><p>（1）<strong>《NetWork in network》</strong>——1×1卷积降维/升维，Global Average pooling层取代全连接层。</p><p>（2）<strong>Arora et al</strong>——用稀疏、分散的网络取代以前庞大密集臃肿的网络</p><h3 id="deep的两种不同含义："><a href="#deep的两种不同含义：" class="headerlink" title="deep的两种不同含义："></a>deep的两种不同含义：</h3><ul><li>Inception模块的形式引入新的组织层次</li><li>网络深度的增加</li></ul><p>最后作者指出：本文不仅仅是实验室学术成果，应用于实际甚至更大的数据集。</p></blockquote><h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h2><p>从LeNet-5[10]开始，卷积神经网络(CNN)通常有一个标准的结构——堆叠卷积层 (可以是归一化和最大池化) ，然后是一个或多个完全连接的层。这种基本设计的变体在图像分类文献中普遍存在，并在MNIST、CIFAR和最显著的ImageNet分类挑战[9,21]上取得了最好的结果。对于更大的数据集，如Imagenet，最近的趋势是增加层数[12]和层大小[21,14]，同时使用dropout层[7] 来解决过拟合的问题。</p><p>尽管担心最大池化层会导致准确空间信息的丢失，相似的卷积网络架构也被成功地用于定位、 目标检测[6,14,18,5]和人姿态估计[19]。受灵长类动物视觉皮层的神经科学模型的启发，Serre等人。 [15]使用一系列不同大小的固定Gabor过滤器来处理多个尺度，类似于Inception model。然而，与[15]的固定2层深度模型相反，Inception model模型中的所有滤波器都是被学习的。此外，初始层重复多次，在GoogLeNet模型中形成了22层深度模型。</p><p>Network-in-Network是Lin等人提出的一种方法。 以增加神经网络的表征能力。当应用于卷积层时，该方法可以看作是额外的<code>1*1</code>个卷积层，然后通常是经过修正的线性激活[9]。这使得它能够很容易地集成到当前的CNN管道中。我们在体系结构中大量使用这种方法。然而，在我们的设置中，<code>1*1</code>个卷积有双重目的：最重要的是，它们主要被用作降维模块，以消除计算瓶颈，否则这将限制我们的网络的大小。这不仅允许增加深度，还可以增加网络的宽度，而不会造成显著的性能损失。</p><p>目前领先的目标检测方法是由Girshick等人提出的具有具有卷积神经网络的区域(R-CNN)的目标检测方法。 [6].R-CNN将整个检测问题分解为两个子问题：首先以类别无关的方式利用潜在的对象建议的颜色和超像素一致性寻找候选框，然后使用CNN分类器来识别这些位置的对象类别。这种两阶段的方法利用了具有低水平线索的边界盒分割的准确性，以及最先进的cnn的高度强大的分类能力。 我们在检测提交中采用了类似的管道，但在这两个阶段都探索了增强，如用于更高对象边界框 召回的多盒[5]预测，以及用于更好地对边界框建议进行分类的集成方法。</p><blockquote><h3 id="前人研究"><a href="#前人研究" class="headerlink" title="前人研究"></a>前人研究</h3><p>自LeNet-5开始，<strong>CNN标准结构：（卷积＋归一化+最大池化）×n＋（全连接层）×m。</strong></p><p>最近他们都在研究增加层数和层大小，同时使用dropout层来解决过拟合的问题。</p><h3 id="本文灵感来源"><a href="#本文灵感来源" class="headerlink" title="本文灵感来源"></a>本文灵感来源</h3><p>1.根据一系列大小不同的固定Gabor滤波器处理多尺度。本文使用类似策略，<strong>重复多次Inception层</strong>，直至22层深度模型GoogLeNet。</p><p>2.根据NiN网络中的 1×1卷积核，这个卷积核之前在VGG里是仅增加深度；在本文中除了<strong>增加深度还起到降维作用</strong>，消除计算瓶颈。</p><p>3.从R-CNN中，改进了R-CNN的两阶段方法(1.框出候选区域，2.每个区域卷积) 运用到自己的模型中了。</p></blockquote><h2 id="3-动机和高水平的考虑"><a href="#3-动机和高水平的考虑" class="headerlink" title="3 动机和高水平的考虑"></a>3 动机和高水平的考虑</h2><p>提高深度神经网络性能的最直接的方法是增加它们的规模。这包括增加网络的深度-级别的数量和它的宽度—每个级别的单位数量。这是一种训练更高质量模型的简单和安全的方法，特别是考虑到大量标记训练数据的可用性。然而，这个简单的解决方案也有两个主要的缺点。</p><p>更大的尺寸通常意味着大量的参数，这使得扩大后的网络更容易发生过拟合，特别是在训练集中标记的例子数量有限的情况下。这可能成为一个主要的瓶颈，因为创建高质量的训练集可能是很棘手的而且价格昂贵，特别是如果需要专家的人工评分员来区分细粒度的视觉类别。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/InceV1-Figure1.png" alt=""></p><blockquote><p>图1:ILSVRC 2014分类挑战中1000个类别中的两个不同类别。</p></blockquote><p>网络规模的另一个缺点是计算资源的使用显著增加。例如，在深度视觉网络中，如果两个卷积层被链接起来，它们的滤波器数量的任何均匀增加都会导致计算量的二次增加。如果增加的容量使用效率较低 (例如，如果大多数权重最终接近于零) ，那么就会浪费大量的计算。 由于在实践中，计算预算总是有限的，因此即使其主要目标是提高结果的质量，也会首选有效地分配 计算资源，而不是不加选择地增加计算资源的大小。</p><p>解决这两个问题的基本方法将是最终从完全连接到稀疏连接的架构，甚至在卷积内部。除了模拟生物系统外， 由于Arora等人的开创性工作，这也将具有更坚实的理论基础的优势。 [2].他们的主要结果表明，如果数据集的概率分布是由一个大的，非常稀疏的深度神经网络，那么最优网络拓扑可以构造一层通过分析的相关统计的激活最后一层和聚类神经元高度相关的输出。虽然严格的数学证明需要非常强的条件，但这一说法与众所周知的赫比原理：神经元一起放电，连接在一起，在实践中，即使在不那么严格的条件下，基本思想也适用。</p><p>缺点是，今天的计算基础设施在涉及到非均匀稀疏数据结构的数值计算方面非常低效。即使算 术运算的数量减少了100次，查找和缓存丢失的开销也是如此的主要，以至于切换到稀疏矩阵也 不会得到回报。通过使用稳步改进、高度调优的数字库，允许极其快速的密集矩阵乘法，利用 底层CPU或GPU硬件[16,9]的微小细节，进一步扩大了差距。此外，非均匀稀疏模型需要更复杂 的工程和计算基础设施。 目前大多数面向视觉的机器学习系统只是利用卷积来利用空间领域的 稀疏性。然而，卷积是实现为到早期层中补丁的密集连接的集合。 自[11]以来，ConvNets传统 上在特征维度上使用随机和稀疏连接表，为了打破对称性，提高学习能力，趋势转向与[9]完全 连接，以更好地优化并行计算。结构的均匀性和大量的过滤器和更大的批量大小允许利用有效的密集计算。</p><p>这就提出了一个问题，是否还有下一步中间步骤的希望：一个利用额外稀疏性的架构，即使是 在过滤器级别，就像该理论所建议的那样，但利用我们的硬件是在密集矩阵上的计算。大量关于稀疏矩阵计算的文献(e。g. [3])表明，将稀 疏矩阵聚类成相对密集的子矩阵，倾向于提供稀疏矩阵乘法的最先进的实际性能。认为在不久 的将来，类似的方法将被用于自动构建非统一的深度学习架构，这似乎并不牵强。</p><p>Inception架构最初是第一作者的一个案例研究，用于评估一个复杂的网络拓扑构造算法的假设输出 ，该算法试图近似[2]为视觉网络隐含的稀疏结构，并通过密集的、可用的组件覆盖假设的结果 。尽管这是一项高度推测的工作，但只有在对拓扑的精确选择进行两次迭代后，我们已经可以 看到对基于[12]的参考体系结构的适度收益。在进一步调整了学习速率、超参数和改进的训练 方法后，我们确定了生成的初始空间架构作为[6]和[5]的基础网络，在定位和目标检测中特别 有用。有趣的是，虽然大多数原始的架构选择都受到了彻底的质疑和测试，但它们至少在本地 是最优的。</p><p>但人们必须谨慎：尽管所提议的架构已经在计算机视觉领域取得了成功，但它的质量是否可以 归因于导致其构建的指导原则仍然值得怀疑。确保需要更彻底的分析和验证：例如，如果基于 下面描述的原则的自动化工具能够为视觉网络找到类似但更好的拓扑结构。最令人信服的证明 是，如果一个自动化系统能够创建网络拓扑，使用相同的算法，在其他领域产生类似的收益。 至少，Inception架构的最初成功为这个方向的未来工作带来了坚实的动力。</p><blockquote><h3 id="提高深度网络性能最直接的方法"><a href="#提高深度网络性能最直接的方法" class="headerlink" title="提高深度网络性能最直接的方法"></a>提高深度网络性能最直接的方法</h3><p><strong>增加网络尺寸：</strong></p><p>（1）增加深度（层数）</p><p>（2）增加宽度（卷积核个数）</p><p><strong>缺点：</strong></p><p>1.参数太多，如果训练数据集有限，很容易产生过拟合；</p><p>2 .计算量增加，可能浪费计算资源；</p><p>3.网络越深，容易出现梯度弥散问题（梯度越往后穿越容易消失），难以优化模型</p><h3 id="传统连接方法"><a href="#传统连接方法" class="headerlink" title="传统连接方法"></a>传统连接方法</h3><p><strong>传统上的连接：</strong></p><p>卷积网络传统上一直在特征维度上使用随机和稀疏连接表，以打破对称性并改善学习效果，然而趋势回到完全连接，以进一步优化并行计算。 当前用于计算机视觉的最新架构具有统一的结构。 大量的过滤器和更大的批处理大小允许高效使用密集计算。</p><p><strong>缺点：</strong></p><p>1.目前的计算基础架构在对非均匀稀疏数据结构进行数值计算时效率很低。</p><p>2.涉及非均匀稀疏模型需要更精细的引擎和计算架构</p><h3 id="解决方法（本文）"><a href="#解决方法（本文）" class="headerlink" title="解决方法（本文）"></a>解决方法（本文）</h3><p>引入稀疏性，用稀疏连接代替全连接层，甚至一般的卷积。（<strong>稀疏连接代替密集连接</strong>）</p><p><strong>原理：</strong>如果数据集的概率分布可以由大型的，非常稀疏的深度神经网络表示，可以通过如下方式逐层构建：分析上一层的激活输出的统计特性，并将具有高度相关性输出的神经元进行聚类，来获得一个稀疏的表示。</p><h3 id="稀疏连接两种方法："><a href="#稀疏连接两种方法：" class="headerlink" title="稀疏连接两种方法："></a>稀疏连接两种方法：</h3><p>1.<strong>空间（spatial） 维度</strong>上的稀疏连接，也就是 CNN 。其只对输入图像的局部进行卷积，而不是对整个图像进行卷积，同时参数共享降低了总参数的数目并减少了计算量。</p><p>2.在<strong>特征（feature）维度</strong>上的稀疏连接进行处理，也就是在通道的维度上进行处理。（Inception 结构的灵感来源）</p><blockquote><p><strong>从稀疏矩阵理解稀疏性：</strong>稀疏矩阵的定义：矩阵中非零元素的个数远远小于矩阵元素的总数，并且非零元素的分布没有规律，则称该矩阵为稀疏矩阵。由于稀疏矩阵的特殊性质，在对稀疏矩阵进行卷积运算时往往会产生很多不必要的计算（在0值密集的区域进行卷积），这时候为了提升运算效率就可以考虑忽略矩阵中0值聚集的区域而选取出非零值聚集的区域直接卷积。</p></blockquote><p><strong>问题引出：</strong>能否在仍旧利用现有硬件进行密集矩阵运算的条件下，改进模型结构，哪怕只在卷积层水平改进，从而能够利用额外的稀疏性呢？</p></blockquote><h2 id="4-架构细节"><a href="#4-架构细节" class="headerlink" title="4 架构细节"></a>4 架构细节</h2><h3 id="Inception初始结构"><a href="#Inception初始结构" class="headerlink" title="Inception初始结构"></a>Inception初始结构</h3><p>初始架构的主要思想是基于如何发现卷积视觉网络中的最优局部稀疏结构可以被现成的密集组件近似和覆盖。注意，假设平移不变性意味着我们的网络将由卷积构建块构建。我们所需要的就是找到最优的局部构造，并在空间上重复它。阿罗拉等人[2]提出了一种逐层的构造，其中应该分析最后一层的相关统计数据，并将其聚类为具有相关性高的单元组。这些集群构成了下一层的单元，并连接到前一层中的单元。我们假设前一层的每个单元对应于输入图像的某个区域，这些单元被分组为滤波器组。在较低的层 (靠近输入的层) 中，相关的单元将集中在局部区域。这意味着，我们最终会有许多集群集中在一个区域，它们可以在下一层被1<em>1个卷积所覆盖，如[12]所示。然而，我们也可以预期，将会有更少的空间上更分散的集群，可以在更大的卷积核上被卷积覆盖，而在越来越大的区域上，卷积核的数量将会减少。为了避免卷积核对齐问题，Inception架构的当前版本被限制在过滤器大小`1</em>1<code>、</code>3<em>3<code>和</code>5</em>5`以内，但是这个决定更多地是基于方便而不是必要。这也意味着，建议的架构是所有这些层的组合，它们的输出滤波器组连接成一个单一的输出向量，形成下一阶段的输入。此外， 由于池化操作对于在当前最先进的卷积网络中取得成功至关重要，因此它建议在每个这样的阶段添加一个替代的并行池化路径也应该产生额外的有益效果(见图2(a))。</p><p>这些“Inception modules”堆叠，他们的输出相关统计必然会不同：高抽象被更高的层，他们的空间浓 度将减少表明3<em>3和5</em>5卷积的比率应该增加我们搬到更高的层。</p><p><code>5*5</code>上述模块的一个大问题，至少在这种中殿形式中是，即使在具有大量滤波器的卷积层之上， 即使是少量的<code>5*5</code>个卷积也可能非常昂贵。一旦将池化单元添加到混合单元中，这个问题就会变 得更加明显：它们的输出过滤器的数量等于上一阶段的过滤器的数量。池化层的输出与卷积层 的输出的合并将不可避免地导致一个各阶段间输出数量的增加。即使这种体系结构可能覆盖最优稀疏结构，但它也会做到效率很低 ，在几个阶段内导致计算爆炸。<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/InceV1-Figure2.png" alt=""></p><blockquote><h3 id="Inception主要思想"><a href="#Inception主要思想" class="headerlink" title="Inception主要思想"></a>Inception主要思想</h3><p>1.用密集模块去近似局部稀疏结构，聚合高相关性的特征输入到下一层。</p><p>2.平移不变性意味着网络由卷积构建块构建。只需要找到最优局部结构（就是Inception块）并在空间上重复它。</p><h3 id="Inception初始结构-1"><a href="#Inception初始结构-1" class="headerlink" title="Inception初始结构"></a>Inception初始结构</h3><p>模块结构组成：1×1卷积，3×3卷积，5×5卷积，3×3最大池化。（非必须，只是为了对齐容易）</p><blockquote><h4 id="为什么这么搭建？"><a href="#为什么这么搭建？" class="headerlink" title="为什么这么搭建？"></a>为什么这么搭建？</h4><p><strong>Q1：为什么分三个卷积核？</strong></p><p>根据上一篇VGG中介绍感受野的例子，我们可以知道卷积核大的，感受野就越大。如果这个区域越大，说明我们获得的越是一个偏向全体的特征；而区域越小，说明我们获得的是一个偏向局部的特征。这样一来就可以获得不同尺度的特征。</p><p><strong>Q2：为什么有条路是池化层？</strong></p><p>引用论文原话：由于池化操作对于目前卷积网络的成功至关重要，因此建议在每个这样的阶段添加一个替代的并行池化路径应该具有额外的有益效果。</p><p><strong>Q3：四条路过后为什么要合一块？</strong></p><p>举个例子：有一个Inception结构的块，它的输入是16个通道的，输出是256个通道的。而有另外一个卷积层，设它的卷积核是5×5的，它的输入和输出也是16和256个通道。那么其实卷积层的这个输出是256个5×5尺度的信息，这样就造成了大量的冗余。但Inception输出同样是256个通道，但是它有四个尺度的信息，可能每个尺度占据64个通道，冗余信息就更少，这个输出信息也更加丰富。</p></blockquote><p><strong>过程：</strong>特征图先被复制成4份并分别被传至接下来的4个部分,将前一层用三个单独的卷积核扫描，并加一个池化操作，然后把这四个操作的输出串联到一起作为下一层的输入。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/InceV1-naive.png" alt=""></p><p><strong>不足：</strong></p><p>1.整个网络是通过大量堆叠这个模块来构建的，即使是适量的5×5卷积也会产生参数的爆炸式增长；</p><p>2.在这个单元中使用的最大池化层保留了输入数据的特征图的深度，所以在最后进行合并时，总的输出的特征图的深度增加，通道数多了，这样增加了该单元之后的网络结构的计算量。</p></blockquote><h3 id="InceptionV1结构"><a href="#InceptionV1结构" class="headerlink" title="InceptionV1结构"></a>InceptionV1结构</h3><p>这就导致了所提出的架构的第二个想法：在其他计算需求会增加过多的地方，明智地应用降维 和投影。这是基于嵌入的成功：即使是低维的嵌入也可能包含大量关于一个相对较大的图像补 丁的信息。然而，嵌入以密集的、压缩的形式表示信息，而压缩的信息更难建模。我们希望在 大多数地方保持我们的表示稀疏 (根据[2]的条件的要求) ，并且只在信号必须集体聚集时压缩 它们。也就是说，在昂贵的3<em>3和5</em>5个卷积之前，使用1*1个卷积来计算缩减。除了被用作减 少量外，它们还包括使用校正的线性激活，这使它们具有双重用途。最终的结果如图2(b).所示</p><p>一般来说，初始网络是由上述类型的模块堆叠组成的网络，偶尔有最大池化层，步幅为2，使网格 分辨率减半。出于技术上的原因 (训练期间的记忆效率) ，只在较高的层开始使用Inception模 块，同时以传统的卷积方式保持较低的层似乎是有益的。这并不是严格必要的，只是反映了我 们目前实施中的一些基础设施低低下。</p><p>这种体系结构的一个主要好处方面是，它允许在每个阶段显著增加单元的数量，而不会导致计 算复杂性的不受控制的爆炸。降维的普遍使用允许将最后阶段的大量输入滤波器屏蔽到下一层 ，首先减少它们的维数，然后与大的卷积核进行卷积。这种设计的另一个实际有用的方面是 ，它与直觉相一致，即视觉信息应该在不同的尺度上进行处理，然后进行聚合，以便下一阶段 可以同时从不同的尺度上提取特征。</p><p>计算资源的改进使用允许增加每个阶段的宽度和阶段的数量，而不遇到计算困难。利用初始架 构的另一种方法是创建质量稍低，但计算成本较低的版本。我们发现，所有包含的旋钮和杠杆 都允许计算资源的控制平衡，从而导致网络比非初始架构的网络快2-3倍，但在这一点上需要仔 细的手动设计。</p><blockquote><h3 id="InceptionV1结构-1"><a href="#InceptionV1结构-1" class="headerlink" title="InceptionV1结构"></a>InceptionV1结构</h3><p><strong>主要思想：</strong>降维。在的3×3和5×5卷积之前，使用1×1卷积核进行卷积，来计算减少量，达到降维效果还有增强非线性能力的作用。</p><p><strong>过程：</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/IncepV1-progress.png" alt=""></p><blockquote><h3 id="为什么这么搭建？-1"><a href="#为什么这么搭建？-1" class="headerlink" title="为什么这么搭建？"></a>为什么这么搭建？</h3><p><strong>Q1:1×1卷积核的作用？</strong></p><p>(1)降维或升维。</p><p>(2)减少参数量，减少计算量。</p><p>(3)实现跨通道信息交融。使用1×1卷积核，其实就是对不同的通道间的信息做线性组合的一个变换过程。</p><p>(4)增加模型深度，提高非线性。使用1×1卷积核的网络更深，而每一层网络后都会用Relu函数增加非线性，这样就增加了一层非线性特性。</p><p><strong>Q2:为什么要把1ⅹ1卷积核加在3×3、5×5卷积？</strong></p><p>为了进行降维从而减少后边大卷积核的参数量。</p><p><strong>Q3：既然5×5那么大，为何不全部替换成1×1？</strong></p><p>虽然1x1可以解决参数量的问题，但是它同时局限了感受野的范围，所以要做到感受野和参数量的权衡，虽然5x5和3x3的参数量大，但是他们卷积效果也好，我们只在某些时候引入1x1卷积，起到降维/通道聚合等作用。</p></blockquote><p>结果：这三部分因为1×1卷积层的加入，总的卷积参数量已经大大低于之前的初始化Inception模块，而且因为在最大池化层之前也加入了1×1的卷积层，所以最终输出的特征图的深度也降低了，这样也降低了该单元之后的网络结构的计算量。这个Inception是GoogLeNet的基本结构。很多个Inception的联合就最终组成了GoogLeNet。</p></blockquote><h2 id="5-GoogLeNet"><a href="#5-GoogLeNet" class="headerlink" title="5 GoogLeNet"></a><strong>5 GoogLeNet</strong></h2><p>较小的这里，表1描述了最成功的特定实例(名为GoogLeNet)。采用了完全相同的拓扑结构 (用 不同的采样方法进行训练)，用于我们集合中的7个模型中的6个。</p><p>所有的卷积，包括那些在初始模块内部的卷积，都使用了修正的线性激活。在我们的网络中， 接受域的大小是224<em>224采取RGB颜色通道，“#3×3 reduce”和“#5×5 reduce”表示 在3</em>3和5<em>5卷积之前，使用的1</em>1的卷积层。在池化层列中内置的最大池化之后， 我们可以在池化层中看到1*1个卷积核的数量。所有这些reduce也使用了ReLU激活函数。</p><p>该网络的设计考虑到了计算效率和实用性，因此推理可以在单个设备上运行，甚至包括那些计算资源有限的设备，特别是低内存占用的设备。当只计算带有参数的层时，该网络有22层深 ( 如果我们也计算池化，则是27层) 。用于建设网络的层 (独立构件) 约为100个。然而，这个数 字取决于所使用的机器学习基础设施系统。在分类器之前使用平均池化是基于[12]的，尽管我 们的实现的不同之处在于我们使用了一个额外的线性层。这使得我们的网络可以很容易地调整 和微调我们的网络，但它主要是方便的，我们不认为它会产生重大影响。研究发现，从全连 接层到平均池化的top-1精度提高了约0.6%，但即使去除完全连接层，使用dropout层仍然是必要的。</p><p>由于网络的深度相对较大，因此以有效的方式将梯度传播回所有层的能力是一个值得关注的问 题。一个有趣的观点是，相对较浅的网络在这一任务上的强大表现表明， 由网络中间的各层所 产生的特征应该是非常有区别的。通过添加连接到这些中间层的辅助分类器，我们将期望在分 类器的较低阶段鼓励区分，增加传播回的梯度信号，并提供额外的正则化。这些分类器采用了 较小的卷积网络的形式，并将它们放在初始空间(4a)和(4d)模块的输出之上。在训练过程中， 它们的损失以折扣权值加到网络的总损失中 (辅助分类器的损失加权为0.3) 。在推理时，这些 辅助网络被丢弃。</p><p>包括辅助分类器在内的侧面额外网络的确切结构如下：</p><p>具有5<em>5个过滤器大小和步幅3的平均池化层，导致(4a)输出4</em>4<em>512， (4d)阶段输出4</em>4*528</p><p>一个128个滤波器的1*1的卷积层用于降维和ReLU激活。</p><p>一个具有1024个单元和ReLU线性激活的全连接层。</p><p>一个有70%的dropout率的dropout层。</p><p>一个以softmax损失函数作为分类器的线性层 (预测与主分类器相同的1000个类，但在推理时删除 ) 。</p><blockquote><h3 id="GoogLeNet表格解读："><a href="#GoogLeNet表格解读：" class="headerlink" title="GoogLeNet表格解读："></a>GoogLeNet表格解读：</h3><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/InceV1-%E8%A1%A8%E6%A0%BC%E8%A7%A3%E8%AF%BB.png" alt=""></p><p><strong>第一列：</strong> 对应一系列层的名称</p><p><strong>蓝色框中的两列：</strong></p><ul><li>patch size/stride 列，对应卷积核、池化核大小和步距。</li><li>output size列，输出特征矩阵。</li></ul><p><strong>粉红框中的参数：</strong>对应Inception结构中的配置</p><ul><li>#1x1: 对应层次如图所示，表示该层卷积核的个数。</li><li>#3x3 reduce和#5×5 reduce:表示在3x3，5x5卷积操作之前使用了1x1卷积的数量。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/InceV1-%E7%BB%93%E6%9E%84%E8%AF%A6%E8%A7%A3.png" alt=""></p><h3 id="（1）输入"><a href="#（1）输入" class="headerlink" title="（1）输入"></a>（1）输入</h3><p>原始输入图像为224x224x3，且都进行了零均值化的预处理操作（图像每个像素减去均值）</p><h3 id="（2）第一模块"><a href="#（2）第一模块" class="headerlink" title="（2）第一模块"></a>（2）第一模块</h3><p>第一模块采用的是1个单纯的卷积层＋1个最大池化层。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/InceV1-No1.png" alt=""></p><p><strong>卷积层：</strong></p><p>卷积核大小7×7，步长为2，padding为3，输出通道数64，输出特征图尺寸为(224-7+3×2)/2+1=112.5(向下取整)=112，输出特征图维度为112x112x64，卷积后进行ReLU操作。</p><p><strong>池化层：</strong></p><p>窗口大小3×3，步长为2，输出特征图尺寸为((112 -3)/2)+1=55.5(向上取整)=56，输出特征图维度为56x56x64。\</p><h3 id="（3）第二模块"><a href="#（3）第二模块" class="headerlink" title="（3）第二模块"></a><strong>（3）第二模块</strong></h3><p>第二模块采用2个卷积层＋1个最大池化层。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/InceV1-No2.png" alt=""></p><p><strong>卷积层：</strong></p><ul><li>用64个1x1的卷积核（3x3卷积核之前的降维）将输入的特征图（56x56x64）变为56x56x64，然后进行ReLU操作。参数量是1×1×64×64=4096</li><li>再用卷积核大小3×3，步长为1，padding为1，输出通道数192，进行卷积运算，输出特征图尺寸为(56-3+1×2)/1+1=56，输出特征图维度为56x56x192，然后进行ReLU操作。参数量是3×3×64×192=110592</li><li>第二模块卷积运算总的参数量是110592+4096=114688，即114688/1024=112K</li></ul><p><strong>池化层：</strong></p><p>窗口大小3×3，步长为2，输出通道数192，输出为((56 - 3)/2)+1=27.5(向上取整)=28，输出特征图维度为28x28x192。</p><h3 id="（4）第三模块-Inception-3a层"><a href="#（4）第三模块-Inception-3a层" class="headerlink" title="（4）第三模块(Inception 3a层)"></a>（4）第三模块(Inception 3a层)</h3><p>Inception 3a层，分为四个分支，采用不同尺度， </p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/InceV1-No3.png" alt=""></p><p><strong>卷积层：</strong> </p><p>1.使用64个1x1的卷积核，运算后特征图输出为28x28x64，然后RuLU操作。参数量1×1×192×6412288</p><p>2.96个1x1的卷积核（3x3卷积核之前的降维）运算后特征图输出为28x28x96，进行ReLU计算，再进行128个3x3的卷积，输出28x28x128。参数量1×1×192×96+3×3×96×128＝129024</p><p>3.16个1x1的卷积核（5x5卷积核之前的降维）将特征图变成28x28x16，进行ReLU计算，再进行32个5x5的卷积，输出28x28x32。参数量1×1×192×16+5×5×16×32=15872</p><p><strong>池化层：</strong></p><p>使用3x3的卷积核，输出28x28x192，然后进行32个1x1的卷积，输出28x28x32.。总参数量1×1×192×32=6144</p><p><strong>输出结果：</strong></p><p>将四个结果进行连接，对这四部分输出结果的第三维并联，即64+128+32+32=256，最终输出28x28x256。总的参数量是12288+129024+15872+6144=163328，即163328/1024=159.5K，约等于159K。</p><h3 id="（5）第三模块-Inception-3b层"><a href="#（5）第三模块-Inception-3b层" class="headerlink" title="（5）第三模块(Inception 3b层)"></a>（5）第三模块(Inception 3b层)</h3><p>Inception 3b层，分为四个分支，采用不同尺度。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/InceV1-No3b.png" alt=""></p><p><strong>卷积层：</strong>  </p><p>1.128个1x1的卷积核，然后RuLU，输出28x28x128pool层，使用3x3的核，输出28x28x256，然后进行64个1x1的卷积，输出28x28x64</p><p>2.128个1x1的卷积核（3x3卷积核之前的降维）变成28x28x128，进行ReLU，再进行192个3x3的卷积，输出28x28x192</p><p>3.32个1x1的卷积核（5x5卷积核之前的降维）变成28x28x32，进行ReLU，再进行96个5x5的卷积，输出28x28x963.</p><p><strong>池化层：</strong></p><p>使用3x3的核，输出28x28x256，然后进行64个1x1的卷积，输出28x28x64</p><p><strong>输出结果：</strong></p><p>将四个结果进行连接，对这四部分输出结果的第三维并联，即128+192+96+64=480，最终输出输出为28x28x480。</p><h3 id="（6）第四模块-Inception-4a、4b、4c、4e"><a href="#（6）第四模块-Inception-4a、4b、4c、4e" class="headerlink" title="（6）第四模块(Inception 4a、4b、4c、4e)"></a>（6）第四模块(Inception 4a、4b、4c、4e)</h3><p>与Inception3a，3b类似</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/InceV1-No4.png" alt=""></p><h3 id="（7）第五模块-Inception-5a、5b"><a href="#（7）第五模块-Inception-5a、5b" class="headerlink" title="（7）第五模块(Inception 5a、5b)"></a>（7）第五模块(Inception 5a、5b)</h3><p>与Inception3a，3b类似</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/InceV1-No5.png" alt=""></p><h4 id="（8）输出层"><a href="#（8）输出层" class="headerlink" title="（8）输出层"></a>（8）输出层</h4><p>前面已经多次提到，在输出层GoogLeNet与AlexNet、VGG采用3个连续的全连接层不同，GoogLeNet采用的是全局平均池化层，得到的是高和宽均为1的卷积层，然后添加丢弃概率为40%的Dropout，输出层激活函数采用的是softmax。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Incev1-Out.png" alt=""></p><h3 id="（9）激活函数"><a href="#（9）激活函数" class="headerlink" title="（9）激活函数"></a>（9）激活函数</h3><p>GoogLeNet每层使用的激活函数为ReLU激活函数。</p><h3 id="（10）辅助分类器"><a href="#（10）辅助分类器" class="headerlink" title="（10）辅助分类器"></a>（10）辅助分类器</h3><p><strong>目的：</strong>根据实验数据，发现神经网络的中间层也具有很强的识别能力，为了利用中间层抽象的特征，在某些中间层中添加含有多层的分类器。</p><p>如下图所示，红色边框内部代表添加的辅助分类器。GoogLeNet中共增加了两个辅助的softmax分支。<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/InceV1-cls.png" alt=""></p><p><strong>作用：</strong></p><p>一是为了避免梯度消失，用于向前传导梯度。反向传播时如果有一层求导为0，链式求导结果则为0。</p><p>二是将中间某一层输出用作分类，起到模型融合作用。实际测试时，这两个辅助softmax分支会被去掉。</p><p>（更详细解读可参见： </p><p><a href="https://blog.csdn.net/qq_37555071/article/details/108214680?ops_request_misc=%7B%22request%5Fid%22%3A%22167003005016800184188396%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&amp;request_id=167003005016800184188396&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-9-108214680-null-null.142^v67^control,201^v3^control_1,213^v2^t3_esquery_v1&amp;utm_term=googlenet论文原文&amp;spm=1018.2226.3001.4187">深入解读GoogLeNet网络结构（附代码实现）_雷恩Layne的博客-CSDN博客_googlenet网络结构</a>）</p><blockquote><p><strong>总结</strong><br>（1）卷积 -&gt; 池化 -&gt;卷积 -&gt;池化 -&gt; 9个inception堆叠 (中间两个池化) -&gt; 池化 -&gt; dropout -&gt; 线性层 -&gt; softmax。用了7个模型，网络共22层。</p><p>（2）所有的卷积以及降维投影，都使用Relu作为激活函数。</p><p>（3）将全连接层用平均池化层代替后，top-1提高了大约0.6%，之前的VGG Alex等这里都是三个全连接层。（在平均池化层之后，还是添加了一个全连接层，是为了大家做finetune（微调））</p><p>（4）最后一层依旧使用dropout防止过拟合。</p><p>（5）为了在浅层使特征更加有区分性，帮助反向传播，解决梯度消失。增加了两个辅助分类器 L=L（最后）+0.3xL（辅1）+0.3xL（辅2），测试阶段去掉辅助分类器。</p></blockquote></blockquote><h2 id="6-训练方法"><a href="#6-训练方法" class="headerlink" title="6 训练方法"></a>6 训练方法</h2><p>我们的网络使用干扰信念[4]分布式机器学习系统进行训练，使用少量的模型和数据并行性。虽然我们只使用了基于CPU的实现，但一个粗略的估计表明，GoogLeNet网络可以在一周内使用少量的高端gpu来训练到收敛，主要的限制是内存的使用。我们的训练使用异步随机梯度下降，超参数为0.9的动量优化[17]，固定学习速率计划(每8个ch学习速率降低4%)。使用Polyak平均[13]用于创建推理时 使用的最终模型。</p><p>我们的图像采样方法已经大大改变了在比赛前的几个月，已经趋同的模型还接受了其他选项的训练，有时与已更改的超参数一起使用，如dropout和学习率，所以很难给一个明确的指导最有效的单一方法来训练这些网络。更复杂的是，一些模型主要在较小的图像上训练，另一些在较大的图像上，受[8]的 启发。尽管如此，在比赛后被验证非常有效的处方包括对不同大小的图像斑块进行采样，其大 小均匀分布在图像面积的8%到100%之间，其高宽比在3/4到4/3之间随机选择。此外，我们还发现AndrewHoward[8]的光度畸变在一定程度上有助于对抗过拟合。此外，我们开始使用随机插值 方法 (双线性、面积、最近邻和三次等概率) 来调整相对较晚的大小，并结合其他超参数变化，因此我们不能确定最终结果是否受到其使用的积极影响。</p><blockquote><ul><li>使用了DistBelief分布式机器学习系统，并伴随着标准数量的模型以及数据并行对GoogLeNet网络进行训练。</li><li>使用基于CPU的实现，但使用GPU会更快。</li><li>训练过程使用异步随机梯度下降，动量参数为0.9，固定的学习率计划（每8次遍历下降学习率4%）。</li><li>赛后发现一个调参玄学，包括各种尺寸的图像块的采样，它的尺寸均匀分布在图像区域的8%—100%之间，方向角限制为[3/4, 4/3] 之间。（但谁都说不清这些玄学是否真的有用）</li><li>我们发现Andrew Howard的光度扭曲对于克服训练数据成像条件的过拟合是有用的。</li></ul></blockquote><h2 id="7-2014分类挑战赛设置和结果"><a href="#7-2014分类挑战赛设置和结果" class="headerlink" title="7 2014分类挑战赛设置和结果"></a>7 2014分类挑战赛设置和结果</h2><p> ILSVRC2014年的分类挑战涉及到将图像分类为图像集层次结构中的1000个叶节点类别中的一个 。大约有120万张图像用于训练，5万张用于验证，10万张图像用于测试。每个图像与一个真实类别相关联，性能测量基于最高得分分类器预测。两个数字通常报道：top-1准确率， 比较真实类别与第一个预测类，和top-5错误率， 比较真实类别与前5预测类：一个图像被认为是正确分 类如果准确率前5，不管其排名。该挑战使用了排名前5名的错误率来进行排名。</p><p>我们参与了这个挑战，没有使用任何用于培训的外部数据。除了本文中提到的训练技术外，我 们在测试过程中还采用了一组技术来获得更高的性能，我们将在下面详细阐述。</p><ol><li><p>我们独立训练了同一个GoogLeNet模型的7个版本 (包括一个更广泛的版本) ，并使用它 们进行了集成预测。这些模型使用相同的初始化 (即使使用相同的初始权重，主要是 由于疏忽) 和学习率策略进行训练，它们只在采样方法和它们看到输入图像的随机顺 序上有所不同。<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/InceV1-Table2.png" alt=""></p></li><li><p>在测试过程中，我们采用了比克里日夫斯基等人更激进的种植方法。 [9].具体来说，我 们将图像的大小调整为4个尺度，其中较短的维度 (高度或宽度) 分别为256、288、320和352，取这些调整大小的图像的左、中和右的正方形 (在肖像图像的情况下，我 们取上、中和下的正方形) 。对于每个正方形，我们取4个角和中心224<em>224的图像以及调整到224</em>224的图像，以及它们的镜像版本。这导致每张图像有4<em>3</em>6*2=144种图像。AndrewHoward[8]在前一年的记录中使用了类似的方法，我们通过经验验证了它的表现比提议的方案略差。我们注意到，这种激进的种植在实际应用中可能没有 必要，因为在合理数量的图像出现后，更多图像的效益变得边际 (我们将在后面展示 )。<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/InceV1-Table3.png" alt=""></p></li><li><p>在多个作物和所有单个分类器上取极大概率的平均值，以获得最终的预测。在我们的 实验中，我们分析了验证数据的替代方法，如对作物的最大汇集和对分类器进行平均 ，但它们导致的性能低于简单的平均。</p></li></ol><p>在本文的其余部分中，我们分析了有助于最终提交的整体性能的多个因素。</p><p> 我们在挑战中的最终提交在验证和测试数据上都获得了6.67%的前5名错误，在其他参与者中排 名第一。与2012年的超人方法相比，这是一个相对减少了56.5%，与前一年的最佳方法  (Clarifai)相比，这两个相对减少了约40%，这两种方法都使用外部数据来训练分类器。下表显 示了一些性能最好的方法的统计数据。</p><p>我们还分析和报告了多个测试选择的性能，通过改变模型的数量和作物的数量来预测一个图像 的下表。当我们使用一个模型时，我们选择了验证数据上错误率前1最低的模型。所有的数字都 报告在验证数据集上， 以不过度适合测试数据统计数据。</p><blockquote><h3 id="提升性能的技术"><a href="#提升性能的技术" class="headerlink" title="提升性能的技术"></a>提升性能的技术</h3><p><strong>训练策略：</strong> 训练使用了7个模型融合。每个模型使用相同的初始化方法甚至相同的初始值，相同的学习率策略，仅在图像采样和输入顺序有区别。</p><p><strong>裁剪策略：</strong> 图片分别缩放到四种尺寸：256、288、320、352，取左、中、右（或上、中、下）正方形，再取四角和中心以及缩放到224x224正方形与镜像。共4x3x6x2个裁剪区。（4种尺寸、3个范围、6=（5+1）、镜像翻倍）。</p><p>将softmax概率在多个区域和所有单个分类器上取平均，以获得最终预测。</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>上述配置的结果：第一名</p><p>另外几种配置的结构表明：裁剪越多越精准，但平日不适用</p></blockquote><h2 id="8-2014检测挑战赛设置和结果"><a href="#8-2014检测挑战赛设置和结果" class="headerlink" title="8 2014检测挑战赛设置和结果"></a>8 2014检测挑战赛设置和结果</h2><p>ILSVRC的检测任务是在200个可能的类中，围绕着图像中的对象生成边界框。如果检测到的对象 与真实类别匹配，并且它们的边界框至少重叠了50%(使用Jaccard索引)，则索数正确。外 来检测被视为假阳性并受到惩罚。与分类任务相反，每个图像都可能包含许多物体或没有，它们的规模可能从大到小不等。使用平均平均精度(mAP)报告结果。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/InceV1-Table4.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/InceV1-Table5.png" alt=""></p><p>GoogLeNet所采用的检测方法类似于[6]的R-CNN，但增加了初始模型作为区域分类器。此外，通 过结合选择性搜索[20]方法与多盒[5]预测，以提高更高的目标边界框召回率，改进了区域建议 步骤。为了减少假阳性的数量，超像素的大小增加了2*。这使得选择性搜索算法的建议框减少的一半 。我们重新添加了200个来自多盒[5]的区域提案，总共约占[6]使用的提案的60%，同时将覆盖 率从92%提高到93%。减少数量和增加覆盖率的提案数量的总体效果是，对单个模型情况的平均 平均精度提高了1%。最后，在对每个区域进行分类时，我们使用了6个convnet的集合，将结果 的准确率从40%提高到43.9%。请注意，与R-CNN相反， 由于缺乏时间，我们没有使用边界盒回归。</p><p>我们首先报告顶级检测结果，并显示自第一版检测任务以来的进展。与2013年的调查结果相比 ，准确率几乎翻了一番。表现最好的团队都使用卷积网络。我们在表4中报告了官方分数和每个 团队的常见策略：外部数据的使用、集成模型或上下文模型。外部数据通常是ILSVRC12分类数 据，用于预训练一个模型，然后根据检测数据进行细化。一些团队还提到了本地化数据的使用 。 由于大部分定位任务边界框不包含在检测数据集中，因此可以使用这些数据预训练通用边界 框回归器，其分类方法与预训练相同。GoogLeNet条目没有使用本地化数据进行预训练。</p><p>在表5中，我们只使用单个模型来比较结果。表现最好的模型是通过深度洞察，令人惊讶的是， 通过3个模型的集成只提高了0.3分，而GoogLeNet通过集成获得了显著更强的结果。</p><blockquote><h3 id="提升性能的技术-1"><a href="#提升性能的技术-1" class="headerlink" title="提升性能的技术"></a><strong>提升性能的技术</strong></h3><ul><li>GoogLeNet检测采用的方法类似于R-CNN，但用Inception模块作为区域分类器进行了增强。</li><li>为了更高的bbox recall（目标边界框召回率），通过结合Selective Search方法与多box预测方法改进了区域生成步骤。</li><li>为了减少false positive的数量，超像素的尺寸增加了2倍。这将Selective Search算法的区域生成减少了一半。</li><li>减少区域生成的数量，增加覆盖率的整体影响是对于单个模型的情况平均精度均值增加了1%</li><li>最后，等分类单个区域时，我们使用了6个GoogLeNets的组合。这导致准确率从40%提高到43.9%。注意，与R-CNN相反，由于缺少时间我们没有使用bbox回归。</li></ul><h3 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h3><p>上述配置的结果：第一名</p><p>单个模型跑出的结果：最优表现得模型是DeepInsight并且作者挽尊惊讶的表示，组合了三个网络也是只提升了0.3个点，相反GoogLeNet组合后获得了明显更强的结果。</p></blockquote><h2 id="9-结果"><a href="#9-结果" class="headerlink" title="9 结果"></a>9 结果</h2><p>我们的结果似乎产生了一个坚实的证据，即通过现成的密集构建块来近似预期的最优稀疏结构是改进计算机视觉神经网络的可行方法。这种方法的主要优点是与较浅和较不宽的网络相比， 在计算需求的适度增加下显著提高质量。还要注意，我们的检测工作具有竞争力，尽管我们既没有利用上下文，也没有执行边界框</p><p>回归和这一事实为初始架构的强度提供了进一步的证据。虽然期望通过更昂贵的相似深度和宽度的网络可以实现相似的结果质量，但我们的方法提供了坚实的证据，证明转向稀疏架构是可 行和有用的想法。这表明未来将在[2]的基础上以自动化的方式创建更稀疏和更精细的结构。</p><blockquote><p><strong>Q1：论文试图解决什么问题？</strong></p><p>为了在使用更深更宽的网络的同时减少过拟合、降低参数量、提高计算资源利用率，本文提出了一种结合了稀疏性和密集计算的网络结构（Inception），巧妙地使用了1ⅹ1卷积来实现数据降维，并且在这基础上搭建了GoogLeNet的Inception v1版本。</p><p><strong>Q2：这是否是一个新的问题？</strong></p><p>不是新的问题，在原来对神经网络更宽更深的探究下，提出了新的网络架构方法</p><p><strong>Q3：这篇文章要验证一个什么科学假设？</strong></p><p>在卷积构建块中寻找最优局部结构（Inception结构），利用它可以实现在增加网络的宽度和深度的同时减少参数，高性能计算。</p><p><strong>Q4：有哪些相关研究？如何归类？谁是这一课题在领域内值得关注的研究员？</strong></p><p>稀疏矩阵运算，数学方面。Hebbian原则]，生物神经方面</p><p><strong>Q5：论文中提到的解决方案之关键是什么？</strong></p><p>1.使用更宽更深的网络，提升网络性能；</p><p>2.用稀疏连接代替密集连接，减少计算资源需求；</p><p>3.既利用稀疏性又使用稠密计算（1ⅹ1卷积的应用），提高计算资源利用率；</p><p>4.引入了Inception,重复叠加，找到最优结构。</p><p><strong>Q6：论文中的实验是如何设计的？</strong></p><p>1.使用了一种结合了稀疏性和密集计算的网络结构（Inception），并不断重复。</p><p>2.利用1×1卷积，3×3卷积，5×5卷积，3×3最大池化搭建网络，一共7个模块，22个层。</p><p>3.使用了1ⅹ1卷积来实现数据降维。</p><p>4.添加两个辅助分类器帮助训练</p><p><strong>Q7：用于定量评估的数据集是什么？代码有没有开源？</strong></p><p>ImageNet2014，有开源</p><p><strong>Q8：论文中的实验及结果有没有很好地支持需要验证的科学假设？</strong></p><p>证明了，两次第一呢</p><p><strong>Q9：这篇论文到底有什么贡献？</strong></p><p>1.引入Inception结构（融合不同的特征信息）</p><p>2.使用1×1的卷积核进行降维以及映射处理</p><p>3.添加两个辅助分类器帮助训练</p><p>4.丢弃全连接层，使用平均池化层（大大减少模型参数）</p><p><strong>Q10：下一步呢？有什么工作可以继续深入？</strong></p><p>以自动方式创建稀疏和更精细的结构，以及将Inception体系结构的见解应用于其他领域。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>EVSegNet-paper</title>
      <link href="/EVSegNet-paper/"/>
      <url>/EVSegNet-paper/</url>
      
        <content type="html"><![CDATA[<h1 id="EV-SegNet-Semantic-Segmentation-for-Event-based-Cameras"><a href="#EV-SegNet-Semantic-Segmentation-for-Event-based-Cameras" class="headerlink" title="EV-SegNet: Semantic Segmentation for Event-based Cameras"></a>EV-SegNet: Semantic Segmentation for Event-based Cameras</h1><blockquote><h1 id="EV-SegNet-基于事件相机的语义分割"><a href="#EV-SegNet-基于事件相机的语义分割" class="headerlink" title="EV-SegNet:基于事件相机的语义分割"></a>EV-SegNet:基于事件相机的语义分割</h1><h2 id="29-November-2018"><a href="#29-November-2018" class="headerlink" title="29 November, 2018"></a>29 November, 2018</h2></blockquote><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>​    事件相机是非常有前景的传感器，相较于基于帧的相机，它们展现出了多个优势。基于深度学习的方法在视觉识别任务中领先于现有技术，并且潜在地可以利用这些相机的优势，但是为了有效地处理事件数据，仍然需要进行一些适配。本研究引入了使用此类数据进行语义分割的首个基线模型。我们基于最先进的技术构建了一个仅以事件信息作为输入的语义分割卷积神经网络。此外，我们提出了一种新的DVS数据表示方法，它在相关任务中的表现超过了之前使用的事件表示法。由于这个任务尚不存在标注好的数据集，我们提出了一种自动生成DDD17数据集部分序列的近似语义分割标签的方法，并且我们将这些标签与模型一起发布，并证明它们有效于训练仅使用DVS数据的模型。我们将使用DVS数据进行的语义分割结果与使用对应灰度图像的结果进行了比较，展示了两者是互补的，并且值得结合使用。</p><h2 id="补充材料"><a href="#补充材料" class="headerlink" title="补充材料"></a>补充材料</h2><p>​    我们的方法在测试序列中的表现视频可在此链接查看：<a href="https://youtu.be/YQXBjWUSiaA。我们已经发布了数据集和代码，链接为：https://github.com/Shathe/Ev-SegNet。">https://youtu.be/YQXBjWUSiaA。我们已经发布了数据集和代码，链接为：https://github.com/Shathe/Ev-SegNet。</a></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/EVS-Figure1.png" alt=""></p><blockquote><p><strong>图1所示</strong>。基于事件相机数据(中间)的两个语义分割示例(左)。语义分割是我们的CNN的预测，只提供事件数据。显示灰度图像(右)只是为了方便可视化。</p></blockquote><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h2><p>​    动态视觉传感器（DVS）作为一种充满前景的事件相机，能够注册捕获环境中的强度变化。与传统相机不同，这种传感器不是以固定的帧率获取图像。正如它们的名字所示，这些相机捕获事件并记录异步事件流。一个事件表明在特定时刻和特定像素点发生了强度变化（关于事件获取的更多细节，请参见第3.2节）。事件相机相较于更传统的相机提供了多重优势，1）主要是其极高的时间分辨率，能够在微秒内捕获多个事件；2）极高的动态范围，可以在困难的光照环境下捕获信息；3）低功耗和带宽要求。Maqueda等人[24]在他们的工作中展示了视觉识别任务如何从这些优势中受益，强调事件相机是天然的运动检测器，并自动过滤掉任何时间上冗余的信息。此外，他们展示了这些相机提供的信息不仅仅是减去连续的传统图像那么简单。</p><p>​    这些相机为许多计算机视觉应用提供了新的可能性和特性，可能会增强解决方案。然而，仍需要开发新的算法以充分利用它们的能力，尤其是在识别任务方面。针对图像数据的基于深度学习的最新成果尚未在事件相机上尝试过。该工作的主要原因之一是这些相机的输出：它们没有提供标准的图像，而且还没有一种明确被采纳的方法来表示事件流以供卷积神经网络（CNN）使用。另一个挑战是缺乏标记的训练数据，这是训练大多数识别模型的关键。我们的工作包含了简单但有效的新颖想法来应对这两大挑战。这些想法对于许多DVS应用可能有所帮助，但我们关注的是一个尚未用此传感器探索过的应用，即语义分割。</p><p>​    本研究提出了将事件相机的潜力与深度学习技术结合在一起，应用于语义分割这一挑战性任务上。语义分割直观上似乎更适合使用外观信息的模型。然而，我们展示了如何通过合适的模型和表示方法，事件相机为这一任务提供了非常有前途的结果。</p><p>​    图1展示了我们工作输出的两个视觉结果示例。我们的主要贡献是：</p><ul><li>据我们所知，首次使用DVS数据进行语义分割的结果。我们构建了一个基于Xception的CNN，将此数据作为输入。由于没有现成的基准测试，我们提出了如何为DDD17基于事件的数据集的某些序列生成近似的语义分割标签。模型和数据正在发布中。</li><li>对不同DVS数据表示在语义分割上的性能进行了比较（包括一种新提出的表示方法，被证明优于现有方法），并分析了与传统图像相比的优势和劣势。</li></ul><h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h2><h3 id="2-1-事件相机应用"><a href="#2-1-事件相机应用" class="headerlink" title="2.1 事件相机应用"></a>2.1 事件相机应用</h3><p>​    如前所述，事件相机在许多情况下相较于传统相机提供了有价值的优势。我们发现最近的研究在多项通常由传统视觉传感器解决的任务中证明了这些优势。这些工作中的大多数集中在3D重建[28, 19, 35, 34]和6自由度相机跟踪[29, 11]的努力。尽管基于RGB图像的3D重建和定位解决方案已经非常成熟，但现有算法不能以完全相同的方式应用于事件相机。上述工作提出了不同的适应性方法。</p><p>​    我们还发现了一些探索这些相机在其他任务中应用的新方法，如光流估计[12, 23, 36]或者更接近我们目标任务的物体检测和识别[26, 6, 20, 30]。关于这些识别工作中使用的数据，Orchard等人[26]和Lagorce等人[20]在小型数据集上执行了识别任务，主要检测字符和数字。最新的研究开始使用更具挑战性的（但稀缺的）真实场景录制，如在Sironi等人[30]中使用的N-CARS数据集，或我们在本研究中使用的DDD17数据集[2]，因为它包含了真实世界的城市场景。</p><p>​    这些方法大多有一个共同的第一步：将事件信息编码成类图像表示，以便于处理。</p><p>​    我们在第3节详细讨论了不同的先前工作事件表示（编码空间和时间信息）以及我们提出的表示（对时间信息有不同的编码方式）。</p><h3 id="2-2-语义分割"><a href="#2-2-语义分割" class="headerlink" title="2.2 语义分割"></a>2.2 语义分割</h3><p>​    语义分割是一个视觉识别问题，其目标是为图像中的每个像素分配一个语义标签。目前，该问题的最先进解决方案主要是基于深度学习的，它们大多提出了不同变体的编码器-解码器CNN架构[5, 4, 17, 16]。</p><p>​    现有的一些语义分割解决方案针对的是实例级语义分割，例如，MaskRCNN[15]，包括三个主要步骤：区域提议、二值分割和分类。其他解决方案，如DeepLabv3+[5]，则针对类级语义分割。DeepLabv3+是Xception[7]的全卷积扩展，Xception也是图像分类的最先进架构，并且是我们工作的基础架构。由朱等人[37]进行的图像分割综述提供了语义分割更传统解决方案的详细汇编，而加西亚等人[13]则讨论了基于深度学习的语义分割的更近期方法，涵盖了从新架构到常见数据集。</p><p>​    迄今为止讨论的工作展示了使用RGB图像进行语义分割的CNNs的有效性。与我们的工作更为接近的是，我们发现其他工作在使用标准RGB图像之外的额外输入数据模态进行语义分割任务时也表现出了优异的性能。例如，语义分割的一个常见的额外输入数据是深度信息。曹等人[3]和古普塔等人[14]是如何使用卷积神经网络将RGB图像与深度信息结合起来的两个很好的例子。同样，在机器人领域非常常见的传感器，激光雷达传感器，也被证明在执行语义分割时能提供有用的额外信息[32, 10]。其他工作展示了如何结合较少见的模态，如荧光信息[1]，或如何在多光谱图像[10]上执行语义分割。用于医学图像分析的语义分割任务[22]通常也会应用或适应为RGB图像设计的基于CNN的方法到不同的医学成像传感器，如MRI图像[18]和CT数据[8]。</p><p>​    我们的工作聚焦于一个不同的模态，即事件相机数据，这在之前的语义分割工作中尚未探索。我们基于RGB图像上语义分割性能最优的模型之一[5]，使用Xception设计[7]来构建一个用于事件图像上的语义分割的编码器-解码器架构。我们的实验显示，仅使用来自公开基准[2]的事件数据就能获得良好的语义分割结果，接近于在相同场景下标准图像所达到的结果。我们还证明了当将这一模态与标准相机结合使用时，可以为更准确地解决这一问题带来补充性的好处。</p><h2 id="3-从事件到语义分割"><a href="#3-从事件到语义分割" class="headerlink" title="3 从事件到语义分割"></a>3 从事件到语义分割</h2><p>​    在本节中，我们将讨论视觉识别任务中使用的不同事件表示，以便最终提出用于语义分割的事件数据的丰富编码。    </p><h3 id="3-1-事件数据"><a href="#3-1-事件数据" class="headerlink" title="3.1 事件数据"></a>3.1 事件数据</h3><p>​    事件相机捕捉每个像素的强度变化。事件相机的输出不是传统相机的三维图像(高度、宽度和通道)，而是事件流。事件表示强度信号的对数(超过既定阈值σ)的正或负变化:</p><script type="math/tex; mode=display">\log(I_{t+1}) - \log(I_t) \geq \sigma,</script><p>​    在两个连续时间戳捕获的强度为$I_{t+1}$和$I_t$。每个事件$(e_i)$由四个不同的组成部分定义：像素的两个坐标（$x_i$, $y_i$）是在哪里测量到变化，一个可能为正或负的极性（$p_i$），以及一个时间戳（$t_i$​）：</p><script type="math/tex; mode=display">e_i = \{x_i, y_i, p_i, t_i\}.</script><p>​    事件是异步的，并具有所描述的编码，该编码在构造上并不提供适合广泛使用的视觉识别任务技术（如CNNs）的良好输入。也许最直接的表示方法是一个$n \times 4$矩阵，其中$n$​是事件数量。但显然，这种表示没有编码事件之间的空间关系。已经提出了几种策略，这些策略成功地将这些信息编码成在不同应用中成功应用的密集表示。</p><h3 id="3-2-事件表示"><a href="#3-2-事件表示" class="headerlink" title="3.2 事件表示"></a>3.2 事件表示</h3><p>​    <strong>基本的事件位置密集编码。</strong> 最成功应用的事件数据表示方法是创建一个图像，其中包含几个通道，编码以下信息。它在每个位置（$ x_i, y_i $）存储在任何时间$t_i$内发生在那里的事件信息，其中$t_i$在一个设定的集成间隔大小$T$内。这种表示的变体已被许多先前的工作使用，显示在非常不同的应用中的出色性能：光流估计[36]，对象检测[6]，分类[20, 27, 30]和回归任务[24]。</p><p>​    早期的工作只使用一个通道来编码事件发生。Nguyen等人[25]存储了每个像素中发生的最后一个事件的信息，即，选择表示正事件，负事件或没有事件的对应值。一个重要的缺点是只保留了最后一个事件的信息。</p><p>​    在一个更完整的表示中，最近一个关于方向盘角度估计的工作，由Maqueda等人[24]进行，将正和负事件发生存储到两个不同的通道中。换句话说，这种表示（Hist）编码了在每个像素$(x_i, y_i) $发生的正和负事件的2D直方图，如下所述：</p><script type="math/tex; mode=display">Hist(x, y, p) = \sum_{i=1, t_i \in W}^{N} \delta(x_i, x) \delta(y_i, y) \delta(p_i, p)</script><p>​    在此，$\delta$ 是克罗内克$\delta$函数（如果变量相等，该函数值为1，否则为0），$W$ 是考虑汇总事件信息的时间窗口或间隔，$N$ 是在间隔$W$内发生的事件数量。因此，乘积 $\delta(x_i, x)\delta(y_i, y)\delta(p_i, p)$ 表示事件 $e_i$ 是否在其坐标 $x_i, y_i$ 与 $x, y$ 值和其极性$p_i$与$p$匹配。这种表示有两个通道，每个极性 $p$（正事件和负事件）一个通道。我们稍后描述的提议表示将利用这两个直方图通道。</p><p>​    请注意，到目前为止讨论的所有表示仅使用时间信息（时间戳$t_i$）来查看每个事件所属的时间间隔。</p><p>​    <strong>包括时间信息的密集编码。</strong>然而，时间信息，即每个事件的时间戳$t_i$，包含对识别任务有用的信息，已经显示将每个事件的这种非空间信息包含到类图像编码中是有用的。Lagorce等人[20]提出了一个2通道图像，每个极性一个通道，称为时间表面。他们存储在集成间隔$W$期间最近的事件时间戳相关的每个像素的信息。后来，Sironi等人[30]通过改变时间表面的定义来增强这个先前的表示。他们现在计算每个像素的值，结合了在$ W $内发生的所有事件的时间戳信息。</p><p>​    Zhu等人[36]最近提出的另一种方法介绍了一种更完整的表示，它包括来自Maqueda等人[24]的事件发生直方图的两个通道，以及包含时间信息的另外两个通道。这两个通道（Recent）分别存储在集成间隔期间在该位置发生的最近正事件或负事件的标准化时间戳的每个像素$(x_i, y_i$）。</p><script type="math/tex; mode=display">\text{Recent}(x, y, p) = \max_{t_i \in W} t_i \delta(x_i, x) \delta(y_i, y) \delta(p_i, p)</script><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/EVS-Figure2.png" alt=""></p><blockquote><p><strong>图2所示</strong>。在3.2节中解释了来自负极性事件($p$ =-1)的数据的不同1通道编码的可视化(在0到255灰度值之间)。在这些示例中，事件信息集成的时间间隔为50ms ($T$= 50ms)。灰度为参考值。</p></blockquote><p>​    所有这些最近的表示都将事件时间戳和直方图归一化为在间隔$W$内的相对值。</p><p>​    受到所有这些先前工作的启发，我们提出了一种替代表示，结合了迄今为止所展示的最佳想法：2通道的事件直方图来编码事件的空间分布，以及在集成时间间隔期间发生的所有时间戳的信息。</p><p>​    <strong>我们提出的表示。</strong>我们提出了一个6通道图像表示。前两个通道是正事件和负事件的直方图（公式3）。其余四个通道是一种简单但有效的方式，用于存储在间隔(W)期间发生的所有事件时间戳相关的信息。我们可以将其视为一种存储它们如何沿着(T)分布的方式，而不是仅选择其中的一个时间戳。我们建议存储在每个像素（$x_i, y_i$）发生的事件的标准化时间戳的平均值（(M)）和标准偏差（(S)），分别为正事件和负事件分别计算，如下所示：</p><script type="math/tex; mode=display">M(x, y, p) = \frac{1}{\text{Hist}(x, y, p)} \sum_{i=1, t_i \in W}^N t_i \delta(x_i, x) \delta(y_i, y) \delta(p_i, p)</script><script type="math/tex; mode=display">S(x, y, p) = \sqrt{\frac{\sum_{i=1, t_i \in W}^N (t_i \delta(x_i, x) \delta(y_i, y) \delta(p_i, p) - M(x, y, p))^2}{\text{Hist}(x, y, p) - 1}}.</script><p>​    然后，我们的表示由这六个二维通道组成：$Hist(x, y, -1)$，$Hist(x, y, +1)$，$M(x, y, -1)$，$M(x, y, +1)$，$S(x, y, -1)$，$S(x, y, +1)$。图2展示了这些通道的一些可视化效果。在事件表示图像中，像素越亮，编码的值越高，例如，白色意味着$Hist(x, y, -1)$中负事件的最高数量。</p><h3 id="3-3-事件数据的语义分割"><a href="#3-3-事件数据的语义分割" class="headerlink" title="3.3 事件数据的语义分割"></a>3.3 事件数据的语义分割</h3><p>​    已经证明CNN在密集的事件数据表示上工作得很好，这在前一节[24, 36]中有详细说明，因此我们探索了一个基于CNN的架构来学习一个不同的视觉任务，语义分割。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/EVS-Figure3.png" alt=""></p><blockquote><p><strong>图3。</strong>基于事件的摄像机的语义分割。我们使用我们基于Xception[7]的编码器-解码器架构处理不同的2D事件数据编码（更多信息，请参见第3.3节详细信息）。最佳彩色观看。</p></blockquote><p>​    语义分割通常被建模为每像素分类，因此语义分割模型的输出具有与输入相同的分辨率。如前所述，使用RGB数据和额外模态，已经有很多最近成功的基于CNN的方法来解决这个问题。我们建立了一个受当前语义分割CNN最先进技术启发的架构，稍微调整以使用事件数据编码。相关工作通常遵循编码器-解码器架构，正如我们所做的那样。作为编码器，我们使用众所周知的Xception模型[7]，它已被证明在分类[7]和语义分割任务[5]中胜过其他编码器。作为解码器，也是遵循最先进工作[4, 5]，我们构建了一个轻量级解码器，将繁重的计算集中在编码器上。我们的架构还包括了最成功的最新模型的特性，包括：使用跳过连接来帮助深度架构的优化[16, 17]以避免梯度消失问题，以及使用辅助损失[33]，这也改善了学习过程的收敛。图3显示了本工作中构建的架构图，其中多通道事件表示作为网络输入。</p><p>​    <p>与类似的架构一样，我们通过损失的反向传播进行训练优化，损失计算为所有每像素损失的总和，通过参数梯度。我们使用常见的soft-max交叉熵损失函数（\(L\)）进行计算，如公式(7)所述：其中 \(N\) 是标记像素的数量，\(M\)是类别的数量。 \(y_{c,j}\) 是像素 \(j\) 属于类别 \(c\) 的二进制指示器（真实情况）。\( \hat{y}_{c,j} \)是CNN预测的像素 \(j\) 属于类别 \(c\) 的概率。</p></p><script type="math/tex; mode=display">\mathcal{L} = -\frac{1}{N} \sum_{j=1}^N \sum_{c=1}^M y_{c,j} \ln (\hat{y}_{c,j})</script><p>​    由于N是标记像素的数量，M是类别的数量。$y<em>{c,j}$ 是像素j属于类别c（真实情况）的二进制指示符。$\hat{y}</em>{c,j}$​ 是CNN预测的像素j属于类别c的概率。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/EVS-Table1.png" alt=""></p><blockquote><p><strong>表1。</strong> 由DDD17数据集某些序列的几个区间组成的Ev-Seg数据摘要</p></blockquote><h2 id="4-Ev-Seg-事件分割数据"><a href="#4-Ev-Seg-事件分割数据" class="headerlink" title="4. Ev-Seg: 事件分割数据"></a>4. Ev-Seg: 事件分割数据</h2><p>​    Ev-Seg数据是DDD17数据集[2]的语义分割扩展。DDD17数据集包括了40个不同驾驶设置的序列。这些序列在不同的场景中被记录（例如，高速公路和城市场景）。该数据集提供了同步的灰度和基于事件的信息，但是，它没有提供语义分割标签。</p><p>​    我们的扩展包括生成的（自动生成的，非手动注释的）语义分割标签，作为该数据集大部分的真实标记使用。除了标签，为了便于复制和进一步的实验，我们还发布了选定的灰度图像子集和相应的事件数据，这些数据用三种不同的表示编码（Maqueda等人[24]，Zhu等人[36]和本文提出的新方法）。</p><p>​    <strong>生成标签。</strong>除了手动标记每个像素的语义分割真实情况的明显负担之外，如果我们考虑直接在基于事件的数据上执行这项任务，它甚至更具挑战性。我们只需要看看任何一个可用的事件表示（见图1），就会意识到如果没有灰度图像并排显示，人眼很难在那里区分出许多类别。其他研究已经表明，CNNs对于噪声训练[31]或近似标签[1]是稳健的，包括Chen等人[6]的工作，他们也成功地使用从灰度图生成的标签进行基于事件的数据中的对象检测。因此，我们提出使用相应的灰度图像生成一个近似的标签集来进行训练，我们证明这足以训练模型直接在基于事件的数据上进行分割。</p><p>​    为了生成这些近似的语义标签，我们执行了以下三个步骤。</p><p>​    首先，我们在众所周知的城市环境数据集Cityscapes[9]上训练了一个CNN进行语义分割，但使用的是灰度图像。用于此步骤的架构与3.3小节中描述的架构相同，它遵循了语义分割的最新技术组件。这个灰度分割模型经过70个周期的训练，学习率为1e-4。最终模型在Cityscapes验证数据上获得了83%的类别MIoU。这仍然与该数据集上使用RGB图像获得的顶尖结果（92%MIoU）有些距离，但对于我们的过程来说质量已经足够。</p><p>​    其次，使用这个灰度模型，我们获得了选定序列的所有灰度图像的语义分割（我们接下来详细说明哪些序列被使用以及为什么）。这些分割是我们将考虑用来训练我们基于事件的分割模型的标签。</p><p>​    最后，作为生成标签的最终后处理步骤，我们裁剪了所有图像的底部部分，即图像底部的60行，它总是包含汽车仪表板，并且只会在生成的标签中引入噪声。</p><p>​    <strong>DDD17序列子集选择。</strong>如前所述，我们没有为所有DDD17数据生成标签。我们接下来讨论我们遵循的原因和选择标准。<br>​    部分DDD17序列在通过CNN时没有给出很好的标签。这有几个原因。由于可用于训练基本灰度语义分割模型的数据域为城市域cityscape数据，因此我们只选择城市场景中的DDD17序列。此外，只有具有足够对比度(不太亮也不太暗)的图像才有可能提供良好的生成的地面真值。因此，我们只选择在白天拍摄的序列，没有过度曝光。在这些限制条件下，只有6个序列与它们近似匹配。因此，我们对应用了限制的每个序列中的间隔执行了更详细的手工注释(详细信息见表1)。</p><p>​    <strong>数据摘要。</strong>表1显示了Ev-Seg数据的内容的摘要。从前面详细选择的六个序列中，五个序列用作训练数据，一个序列用于测试。我们选择测试具有更均匀类分布的序列，即包含更多类别标签的序列它看起来不像人/行人标签</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/EVS-Figure4.png" alt=""></p><blockquote><p><strong>图4。</strong> 测试顺序示例。根据灰度图像（左）生成语义标签图像（右）通过在灰度版城市景观上训练的CNN。</p></blockquote><p>​    这些标签的类别与众所周知的相同Cityscapes数据集[9]（见表1），除了天空和建筑类别。尽管这两种猫的自负在Cityscapes数据集中得到了正确的学习，当使用对DDD17数据集执行推断时灰度图像，由于域偏移，这些类别没有正确生成。因此，在我们的实验中，这两个类别是一起学习的，就好像它们是同样的事情。城市景观之间的领域转换DDD17数据集也是生成城市景观类别而非类别。</p><p>​    图4显示了灰度图像和对应生成的属于我们DDD17数据集的扩展。我们可以看到，尽管标签并不像手动注释那样完美（以及如前所述，类，如建筑和天空不是仅使用灰度正确学习的），它们是非常准确和明确。</p><h2 id="5-实验验证"><a href="#5-实验验证" class="headerlink" title="5 实验验证"></a>5 实验验证</h2><h3 id="5-1-实验设置和度量"><a href="#5-1-实验设置和度量" class="headerlink" title="5.1 实验设置和度量"></a>5.1 实验设置和度量</h3><p>​    <strong>度量。</strong> 我们的工作涉及语义分割问题，即使用事件摄像机的每像素分类。因此，我们根据标准指标评估我们的结果分类和语义分割：准确性和并集上的平均交集（$MIoU$）。</p><p>​    在语义分割中，给定一个预测图像$\hat{y}$和一个真实图像$y$，它们的像素数为$N$，可以分类为$C$​个不同的类别，准确度指标，公式（8）计算如下：</p><script type="math/tex; mode=display">\text{Accuracy}(y, \hat{y}) = \frac{1}{N} \sum_{i=1}^{N} \delta(y_i, \hat{y}_i)</script><p>​    而MIoU则按类别计算为：</p><script type="math/tex; mode=display">\text{MIoU}(y, \hat{y}) = \frac{1}{C} \sum_{j=1}^{C} \frac{\sum_{i=1}^{N} \delta(y_{i,c}, 1) \delta(\hat{y}_{i,c}, 1)}{\max(1, \delta(y_{i,c}, 1) + \delta(\hat{y}_{i,c}, 1))}</script><p>​    其中$\delta$表示克罗内克$\delta$函数，$y<em>i$指示像素$i$所属的类别，$y</em>{i,c}$是一个布尔值，表示像素$i$是否属于某个特定类别$c$。</p><p>​    <strong>设置。</strong>我们使用第3.3节中解释的CNN和第4节详细描述的Ev-Seg数据进行实验。我们从头开始训练所有模型变体，使用：初始学习率为1e-4的Adam优化器和多项式学习率衰减时间表。我们使用8的批量大小进行了30K次迭代训练，在训练期间我们执行了几个数据增强步骤：裁剪，旋转（-15°, 15°），垂直和水平移动（-25%，25%）和水平翻转。关于事件信息编码，对于训练，我们始终使用50ms的积分时间间隔，这已被证明在这个数据集[24]上表现良好。</p><h3 id="5-2节-事件语义分割"><a href="#5-2节-事件语义分割" class="headerlink" title="5.2节 事件语义分割"></a>5.2节 事件语义分割</h3><p>​    输入表示比较。一个好的输入表示对于CNN来说非常重要，以便正确学习和利用输入信息。表2比较了使用不同输入表示训练的几个语义分割模型。前三行对应于基于事件的表示。我们比较了一个基本的事件位置密集编码，一个也包含时间信息的密集编码，以及我们提出的编码（参见第3.2节了解详细信息）。我们的事件编码在不同的指标和评估上都略微但一致地在语义分割任务上表现更好。图5显示了这些结果的一些视觉示例。</p><p>​    所有模型（相同的架构，只是用不同的输入训练）都已使用50ms的积分间隔编码的数据进行了训练，但我们也使用不同的间隔大小对它们进行了评估。这是一个有趣的评估，因为通过改变聚合事件信息的时间间隔，我们在某种程度上模拟了不同的相机移动速度。换句话说，50ms或10ms的间隔可能会以不同的速度编码完全相同的运动。这一点非常重要，因为在实际场景中，模型必须在不同的速度下表现良好。我们可以看到，所有模型在使用与训练期间使用的积分时间（50ms）不同的间隔大小（10ms，250ms）编码的测试数据上的表现仅略有下降，参见图6示例。模型在不同积分间隔上表现相似有两个主要解释：1）编码是标准化的，2）训练数据包含不同的相机速度。这两件事都有助于在不同的时间间隔或移动速度上更好地泛化。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/EVS-Table2.png" alt=""></p><blockquote><p><strong>表2.</strong>不同输入表示在测试Ev-Seg数据上的语义分割性能。使用时间间隔训练的模型(T)为50ms，但测试了不同的T值:50ms, 10ms和250ms。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/EVS-Figure5.png" alt=""></p><blockquote><p><strong>图5。</strong>基于Ev-Seg数据的多幅测试图像语义分割。仅使用事件数据的不同输入表示的结果;(b)至(d)，或使用灰度数据(e)和(f)。为了可视化目的，显示了灰度原始图像(a)和地面真值标签。模型训练和测试的时间间隔为50ms。最好以彩色观看。</p></blockquote><p>​    <strong>事件相机与传统相机。</strong>表2还包括了在底部两行使用相应的灰度图像进行语义分割任务的结果。</p><p>​    尽管传统相机捕获的纯外观信息比事件相机丰富，但事件相机提供的运动信息对于语义分割任务也非常有用。从图5(e), (f)中使用灰度数据的结果示例中，我们可以看到事件信息如何帮助更好地分割移动对象，例如行人（在这些示例中以红色表示）或精细化对象边界。传统相机在检测小物体方面存在困难，一般来说，在极端照明（明亮或黑暗）条件下识别任何物体都会受到影响，而事件相机在识别没有移动的物体（因为它们与相机移动速度相同或因为它们太远而无法欣赏到它们的移动）方面更有困难。</p><p>​    单独使用传统相机进行语义分割比单独使用基于事件的相机表现更好。然而，我们的结果表明，结合使用两者进行语义分割会得到更好的结果。这表明它们学习到了互补的信息。有趣的是，我们应该注意训练和评估可用的数据正是我们能够正确分割灰度图像的数据，因此对灰度图像比基于事件的数据稍微有利一些（即，评估集中没有包括夜间图像，因为没有这些的真实标签）。</p><p>​    从我们的实验中得出两个明显的互补情况：1）一方面，众所周知，事件相机的主要缺点之一是与相机静止的物体不会触发事件，即，是不可见的。图7显示了一个在人行横道等待的汽车的例子，我们看到虽然传统相机可以完美地看到整个场景，但事件相机几乎没有捕捉到任何信息；2）另一方面，事件相机能够在传统视觉传感器根本看不到场景物体的情况下捕捉到有意义的信息，例如，在困难的照明环境下。这是由于它们高动态范围的缘故，图8展示了一个例子，在这个例子中，无论是灰度模型还是基于事件的模型都没有被训练过。由于输入表示上的领域偏移较小，基于事件的模型表现得更好。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/EMS-Figure6.png" alt=""></p><blockquote><p><strong>图6。</strong>语义分割结果(下图)使用不同事件数据表示(顶部)的集成间隔大小(T)。仅在50ms上训练的模型得到的结果集成用我们建议的表示编码的事件信息。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/EVS-Figure7.png" alt=""></p><blockquote><p><strong>图7。</strong>静态se序列上的语义分割结果(下图)，即在十字路口等待的汽车。这是一个明显的广告事件相机的情况，由于缺乏事件信息。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/EVS-Figure8.png" alt=""></p><blockquote><p>图8。极端光照的语义分割(下)具有不同输入表示的条件(夜间)(顶部):灰度图像和我们的事件数据表示。相应的模型只在光照良好的日间样本上训练。这是对于传统摄像机来说，这是一个明显的对抗案例，因为缺乏灰度捕获中的信息。</p></blockquote><h2 id="6-结论与未来工作"><a href="#6-结论与未来工作" class="headerlink" title="6 结论与未来工作"></a>6 结论与未来工作</h2><p>​    这项工作包括使用事件相机信息进行语义分割的第一个结果。我们建立一个编码器-解码器架构，该架构仅能从事件相机数据中学习语义tic分割。因为<br>对于这个问题没有可用的基准，我们建议如何为基于DDD17事件的部分序列生成自动但近似的语义分段标记数据集。我们的评估显示了这种方法如何允许<br>有效的学习语义分割模型事件数据。为了给模型提供信息，我们还提出了一个新颖的事件相机数据表示，对两者都进行编码事件直方图及其时间分布。我们的se语义分割实验，比较了不同的代表表示，表明我们的方法允许有效的学习语义分割模型，我们的ap方法优于其他以前使用的事件表示，即使在不同的时间间隔进行评估。我们还要比较仅从事件实现的分割数据从传统图像分割，显示他们的优点，他们的缺点和com的好处结合这两个传感器的任务。为今后的工作，其中之一主要的挑战仍然是获得和产生更多的和更好的语义分割标签，通过替代做主要适应方法和/或事件相机模拟器(他们目前不提供这种标签)。</p><h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>作者要感谢NVIDIA公司。感谢您捐赠用于这项工作的Titan Xp GPU。这研究得到了西班牙政府项目DPI2015-69376-R和DPI2016-76676-RAEI/ federal - ue和阿拉贡地区政府(DGA)的部分资助T45 17 r /工程师协会)。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>rcnnfaster-paper</title>
      <link href="/rcnnfaster-paper/"/>
      <url>/rcnnfaster-paper/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>rcnnfast-paper</title>
      <link href="/rcnnfast-paper/"/>
      <url>/rcnnfast-paper/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>rcnn-paper</title>
      <link href="/rcnn-paper/"/>
      <url>/rcnn-paper/</url>
      
        <content type="html"><![CDATA[<h1 id="《Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation》2014"><a href="#《Rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation》2014" class="headerlink" title="《Rich feature hierarchies for accurate object detection and semantic segmentation》2014"></a>《<em>Rich feature hierarchies for accurate object detection and semantic segmentation</em>》2014</h1><blockquote><h1 id="【用于精确物体定位和语义分割的丰富特征层次结构】"><a href="#【用于精确物体定位和语义分割的丰富特征层次结构】" class="headerlink" title="【用于精确物体定位和语义分割的丰富特征层次结构】"></a>【用于精确物体定位和语义分割的丰富特征层次结构】</h1></blockquote><h2 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a>摘要：</h2><p>过去几年，在权威数据集PASCAL上，物体检测的效果已经达到一个稳定水平。效果最好的方法是融合了多种低维图像特征和高维上下文环境的复杂融合系统。在这篇论文里，我们提出了一种简单并且可扩展的检测算法，可以将mAP在VOC2012最好结果的基础上提高30%以上——达到了53.3%。我们的方法结合了两个关键的因素：</p><ol><li>在候选区域上自下而上使用大型卷积神经网络(CNNs)，用以定位和分割物体。</li><li>当带标签的训练数据不足时，先针对辅助任务进行有监督预训练，再进行特定任务的调优，就可以产生明显的性能提升。</li></ol><p>因为我们把region proposal和CNNs结合起来，所以该方法被称为R-CNN：Regions with CNN features。我们也把R-CNN效果跟OverFeat比较了下（OverFeat是最近提出的在与我们相似的CNN特征下采用滑动窗口进行目标检测的一种方法），结果发现RCNN在200类ILSVRC2013检测数据集上的性能明显优于OVerFeat。</p><blockquote><p>【Overfeat:改进了Alex-net，并用图像缩放和滑窗方法在test数据集上测试网络；提出了一种图像定位的方法；最后通过一个卷积网络来同时进行分类，定位和检测三个计算机视觉任务，并在ILSVRC2013中获得了很好的结果。】</p></blockquote><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h2><p>特征很重要。在过去十年，各类视觉识别任务基本都建立在对SIFT[29]和HOG[7]特征的使用。但如果我们关注一下PASCAL VOC对象检测[15]这个经典的视觉识别任务，就会发现，2010-2012年进展缓慢，取得的微小进步都是通过构建一些集成系统和采用一些成功方法的变种才达到的。</p><p>SIFT和HOG是块方向直方图(blockwise orientation histograms)，一种类似大脑初级皮层V1层复杂细胞的表示方法。但我们知道识别发生在多个下游阶段，（我们是先看到了一些特征，然后才意识到这是什么东西）也就是说对于视觉识别来说，更有价值的信息，是层次化的，多个阶段的特征。</p><p>Fukushima的“neocognitron，一种受生物学启发用于模式识别的层次化、移动不变性模型，算是这方面最早的尝试。然而neocognitron缺乏监督学习算法。Lecun等人的工作表明基于反向传播的随机梯度下降(SGD)对训练卷积神经网络（CNNs）非常有效，CNNs被认为是继承自neocognitron的一类模型。</p><p>CNNs在1990年代被广泛使用，但随即便因为SVM的崛起而淡出研究主流。2012年，Krizhevsky等人在ImageNet大规模视觉识别挑战赛(ILSVRC)上的出色表现重新燃起了世界对CNNs的兴趣（AlexNet）。他们的成功在于在120万的标签图像上使用了一个大型的CNN，并且对LeCUN的CNN进行了一些改造（比如ReLU和Dropout Regularization）。</p><p>这个ImangeNet的结果的重要性在ILSVRC2012 workshop上得到了热烈的讨论。提炼出来的核心问题是：ImageNet上的CNN分类结果在何种程度上能够应用到PASCAL VOC挑战的物体检测任务上？</p><p>我们通过连接图像分类和目标检测，回答了这个问题。本论文是第一个说明在PASCAL VOC的物体检测任务上CNN比基于简单类HOG特征的系统有大幅的性能提升。我们主要关注了两个问题：使用深度网络定位物体和在小规模的标注数据集上进行大型网络模型的训练。</p><p>与图像分类不同的是检测需要定位一个图像内的许多物体。一个方法是将框定位看做是回归问题。但Szegedy等人的工作说明这种策略并不work（在VOC2007上他们的mAP是30.5%，而我们的达到了58.5%）。</p><blockquote><p>【也就是说将定位问题单纯作为回归解决效果并不好】</p></blockquote><p>另一个可替代的方法是使用滑动窗口探测器，通过这种方法使用CNNs至少已经有20年的时间了，通常用于一些特定的种类如人脸，行人等。为了获得较高的空间分辨率，这些CNNs都采用了两个卷积层和两个池化层。我们本来也考虑过使用滑动窗口的方法，但是由于网络层次更深，输入图片有非常大的感受野（195×195）and 步长（32×32），这使得采用滑动窗口的方法充满挑战。</p><p>我们是通过操作”recognition using regions”范式，解决了CNN的定位问题。测试时，对这每张图片，产生了接近2000个与类别无关的region proposal，对每个CNN抽取了一个固定长度的特征向量，然后借助专门针对特定类别数据的线性SVM对每个区域进行分类。我们不考虑region的大小，使用放射图像变形的方法来对每个不同形状的region proposal产生一个固定长度的作为CNN输入的特征向量（也就是把不同大小的proposal放到同一个大小）。图1展示了我们方法的全貌并突出展示了一些实验结果。由于我们结合了Region proposals和CNNs，所以起名<strong><em>R-CNN：Regions with CNN features。</em></strong></p><p>检测中面对的第二个挑战是标签数据太少，现在可获得的数据远远不够用来训练一个大型卷积网络。传统方法多是采用无监督与训练，再进行有监督调优。本文的第二个核心贡献是在辅助数据集（ILSVRC）上进行有监督预训练，再在小数据集上针对特定问题进行调优。这是在训练数据稀少的情况下一个非常有效的训练大型卷积神经网络的方法。我们的实验中，针对检测的调优将mAP提高了8个百分点。调优后，我们的系统在VOC2010上达到了54%的mAP，远远超过高度优化的基于HOG的可变性部件模型（deformable part model，DPM）</p><blockquote><p>【DPM:多尺度形变部件模型，连续获得07-09的检测冠军，2010年其作者Felzenszwalb Pedro被VOC授予”终身成就奖”。DPM把物体看成了多个组成的部件（比如人脸的鼻子、嘴巴等），用部件间的关系来描述物体，这个特性非常符合自然界很多物体的非刚体特征。DPM可以看做是HOG+SVM的扩展，很好的继承了两者的优点，在人脸检测、行人检测等任务上取得了不错的效果，但是DPM相对复杂，检测速度也较慢，从而也出现了很多改进的方法。】</p></blockquote><p>我们的系统也很高效，都是小型矩阵向量相乘和贪婪NMS这些特定类别的计算。这个计算特性源自于特征在不同类别之间的共享（对于不同类别，CNNC提取到的特征是一样的），这比之前使用的区域特征少了两个数量级的维度。<br>HOG-like特征的一个优点是简单性：能够很容易明白提取到的特征是什么，那我们能可视化出CNNC提取到的特征吗？全连接层有超过5千4百万的参数值，这是关键吗？这些都不是，我们将CNN切断，会发现，移除掉其中94%的参数，精度只会下降一点点。相反，通过网络中的探测单元我们可以看到卷积层学习了一组丰富的特性。（图3）</p><p>分析我们方法的失败案例，对于进一步提高很有帮助，所以我们借助Hoiem等人的定位分析工具做实验结果的报告和分析。分析结果，我们发发现主要的错误是因为mislocalization,而使用了bounding box regression之后，可以有效的降低这个错误。<br>介绍技术细节之前，我们提醒大家由于R-CNN是在推荐区域上进行操作，所以可以很自然地扩展到语义分割任务上。只要很小的改动，我们就在PASCAL VOC语义分割任务上达到了很有竞争力的结果，在VOC2011测试集上平均语义分割精度达到了47.9%。</p><h2 id="2-用RCNN做物体检测"><a href="#2-用RCNN做物体检测" class="headerlink" title="2 用RCNN做物体检测"></a>2 用RCNN做物体检测</h2><p>我们的物体检测系统有三个模块构成。第一个，产生类别无关的region proposal。这些推荐定义了一个候选检测区域的集合；第二个是一个大型卷积神经网络，用于从每个区域抽取特定大小的特征向量；第三个是一个指定类别的线性SVM。本部分，将展示每个模块的设计，并介绍他们的测试阶段的用法，以及参数是如何学习的细节，最后给出在PASCAL VOC 2010-12和ILSVRC2013上的检测结果。</p><h3 id="2-1模块设计"><a href="#2-1模块设计" class="headerlink" title="2.1模块设计"></a>2.1模块设计</h3><h4 id="区域推荐（region-proposal）"><a href="#区域推荐（region-proposal）" class="headerlink" title="区域推荐（region proposal）"></a><strong>区域推荐（region proposal）</strong></h4><p>近来有很多研究都提出了产生类别无关区域推荐的方法。比如: objectness（物体性），selective search（选择性搜索），category-independent object proposals(类别无关物体推荐)，constrained parametric min-cuts（受限参最小剪切, CPMC)，multi-scal combinatorial grouping(多尺度联合分组)，以及Ciresan等人的方法，将CNN用在规律空间块裁剪上以检测有丝分裂细胞，也算是一种特殊的区域推荐类型。由于R-CNN对特定区域算法是不关心的，所以我们采用了选择性搜索以方便和前面的工作进行可控的比较。</p><h4 id="特征提取（Feature-extraction）"><a href="#特征提取（Feature-extraction）" class="headerlink" title="特征提取（Feature extraction）"></a><strong>特征提取（Feature extraction）</strong></h4><p>我们使用Krizhevsky等人所描述的CNN的一个Caffe实现版本对每个推荐区域抽取一个4096维度的特征向量把一个输入为277277大小的图片，通过五个卷积层和两个全连接层进行前向传播,最终得到一个4096-D的特征向量。读者可以参考AlexNet获得更多的网络架构细节。<br>为了计算region proposal的特征，我们首先要对图像进行转换，使得它符合CNNC的输入（架构中的CNNC只能接受固定大小：277277）。这个变换有很多办法，我们使用了最简单的一种。无论候选区域是什么尺寸和宽高比，我们都把候选框变形成想要的尺寸。具体的，变形之前，我们现在候选框周围加上16的padding,再进行各向异性缩放。 这种形变使得mAp提高了3到5个百分点。在补充材料中，作者对比了各向异性和各向同性缩放缩放方法。</p><blockquote><p>【关于图像的各项同性 各向异性可以参考我的另一篇文章，那里有具体比较说明<br>链接：<a href="https://blog.csdn.net/v1_vivian/article/details/80245397">R-CNN中的各项同性 各向异性</a>】。</p></blockquote><h3 id="2-2测试阶段的物体检测"><a href="#2-2测试阶段的物体检测" class="headerlink" title="2.2测试阶段的物体检测"></a>2.2测试阶段的物体检测</h3><p>然后给出一张图像中所有的打分区域，然后使用NMS（每个类别是独立进行的），拒绝掉一些和高分区域的IOU大于阈值的候选框。</p><h4 id="运行时的分析"><a href="#运行时的分析" class="headerlink" title="运行时的分析"></a><strong>运行时的分析</strong></h4><p>两个特性让检测变得很高效。首先，所有的CNN参数都是跨类别共享的。其次，通过CNN计算的特征向量相比其他通用方法（比如spatial pyramids with bag-of-visual-word encodings）维度是很低的。UVA检测系统的特征比我们的要多两个数量级(360k vs 4k)。</p><p>这种共享的结果就是计算推荐区域特征的耗时可以分摊到所有类别的头上（GPU：每张图13s，CPU：每张图53s）。唯一的和具体类别有关的计算是特征向量和SVM权重和点积，以及NMS。实践中，所有的点积都可以批量化成一个单独矩阵间运算。特征矩阵的典型大小是2000×4096，SVM权重的矩阵是4096xN，其中N是类别的数量。</p><p>分析表明R-CNN可以扩展到上千个类别，而不需要借用近似技术（如hashing）。及时有10万个类别，矩阵乘法在现代多核CPU上只需要10s而已。但这种高效不仅仅是因为使用了区域推荐和共享特征。由于较高维度的特征，UVA系统存储100k linear predictors需要134GB的内存，而我们只要1.5GB，比我们高了两个数量级。</p><p>有趣的是R-CCN和最近Dean等人使用DPMs和hashing做检测的工作相比，他们用了1万个干扰类， 每五分钟可以处理一张图片，在VOC2007上的mAP能达到16%。我们的方法1万个检测器由于没有做近似，可以在CPU上一分钟跑完，达到59%的mAP（3.2节）。</p><h3 id="2-3训练"><a href="#2-3训练" class="headerlink" title="2.3训练"></a>2.3训练</h3><h4 id="有监督的预训练"><a href="#有监督的预训练" class="headerlink" title="有监督的预训练"></a><strong>有监督的预训练</strong></h4><p>我们在大型辅助训练集ILSVRC2012分类数据集（没有约束框数据）上预训练了CNN。预训练采用了Caffe的CNN库。总体来说，我们的CNN十分接近krizhevsky等人的网络的性能，在ILSVRC2012分类验证集在top-1错误率上比他们高2.2%。差异主要来自于训练过程的简化。</p><h4 id="特定领域的参数调优"><a href="#特定领域的参数调优" class="headerlink" title="特定领域的参数调优"></a><strong>特定领域的参数调优</strong></h4><p>为了让我们的CNN适应新的任务（即检测任务）和新的领域（变形后的推荐窗口）。我们只使用变形后的推荐区域对CNN参数进行SGD训练。我们替换掉了ImageNet专用的1000-way分类层，换成了一个随机初始化的21-way分类层，（其中20是VOC的类别数，1代表背景）而卷积部分都没有改变。我们对待所有的推荐区域，如果其和真实标注的框的IoU&gt;= 0.5就认为是正例，否则就是负例。SGD开始的learning_rate为0.001（是初始化预训练时的十分之一），这使得调优得以有效进行而不会破坏初始化的成果。每轮SGD迭代，我们统一使用32个正例窗口（跨所有类别）和96个背景窗口，即每个mini-batch的大小是128。另外我们倾向于采样正例窗口，因为和背景相比他们很稀少。</p><h4 id="目标种类分类器"><a href="#目标种类分类器" class="headerlink" title="目标种类分类器"></a><strong>目标种类分类器</strong></h4><p>思考一下检测汽车的二分类器。很显然，一个图像区域紧紧包裹着一辆汽车应该就是正例。同样的，没有汽车的就是背景区域，也就是负例。较为不明确的是怎样标注哪些只和汽车部分重叠的区域。我们使用IoU重叠阈值来解决这个问题，低于这个阈值的就是负例。这个阈值我们选择了0.3，是在验证集上基于{0, 0.1, … 0.5}通过网格搜索得到的。我们发现认真选择这个阈值很重要。如果设置为0.5，可以降低mAP5个点，设置为0，就会降低4个点。正例就严格的是标注的框。</p><blockquote><p>【IOU&lt;0.3被作为负例，ground-truth是正例，其余的全部丢弃】</p></blockquote><p>一旦特征提取出来，并应用标签数据，我们优化了每个类的线性SVM。由于训练数据太大，难以装进内存，我们选择了标准的hard negative mining method</p><blockquote><p>【难负例挖掘算法，用途就是正负例数量不均衡，而负例分散代表性又不够的问题，hard negative就是每次把那些顽固的棘手的错误,再送回去继续练,练到你的成绩不再提升为止.这一个过程就叫做’<em>hard negative mining</em>’】</p></blockquote><p>高难负例挖掘算法收敛很快，实践中只要在所有图像上经过一轮训练，mAP就可以基本停止增加了。<br>附录B中，我们讨论了，为什么在fine-tunning和SVM训练这两个阶段，我们定义得正负样例是不同的。</p><blockquote><p>【fine-tunning阶段是由于CNN对小样本容易过拟合，需要大量训练数据，故对IoU限制宽松： IoU&gt;0.5的建议框为正样本，否则为负样本； SVM这种机制是由于其适用于小样本训练，故对样本IoU限制严格：Ground Truth为正样本，与Ground Truth相交IoU＜0.3的建议框为负样本。】</p></blockquote><p>我们也会讨论为什么训练一个分类器是必要的，而不只是简单地使用来自调优后的CNN的最终fc8层的输出。</p><blockquote><p>【为什么单独训练了一个SVM而不是直接用softmax，作者提到，刚开始时只是用了ImageNet预训练了CNN，并用提取的特征训练了SVMs，此时用正负样本标记方法就是前面所述的0.3,后来刚开始使用fine-tuning时，也使用了这个方法，但是发现结果很差，于是通过调试选择了0.5这个方法，作者认为这样可以加大样本的数量，从而避免过拟合。然而，IoU大于0.5就作为正样本会导致网络定位准确度的下降，故使用了SVM来做检测，全部使用ground-truth样本作为正样本，且使用非正样本的，且IoU大于0.3的“hard negatives”，提高了定位的准确度】。</p></blockquote><h3 id="2-4-在PASCAL-VOC-2010-12上的结果"><a href="#2-4-在PASCAL-VOC-2010-12上的结果" class="headerlink" title="2.4 在PASCAL VOC 2010-12上的结果"></a>2.4 在PASCAL VOC 2010-12上的结果</h3><p>按照PASCAL VOC的最佳实践步骤，我们在VOC2007的数据集上验证了我们所有的设计思想和参数处理，我们在VOC2012上训练和优化了SVMs，最终结果在VOC 2010-12的数据库，我们在评估服务器上提交了两个结果（一个是有bunding box regression，一个没有）。</p><p>表1展示了在VOC2010的结果，我们将自己的方法同四种先进基准方法作对比，其中包括SegDPM，这种方法将DPM检测子与语义分割系统相结合并且使用附加的inter-detector的环境和图片检测器。更加恰当的比较是同Uijling的UVA系统比较，因为我们的方法同样基于候选框算法。对于候选区域的分类，他们通过构建一个四层的金字塔，并且将之与SIFT模板结合，SIFT为扩展的OpponentSIFT和RGB-SIFT描述子，每一个向量被量化为4000-word的codebook。分类任务由一个交叉核的SVM承担，对比这种方法的多特征方法，非线性内核的SVM方法，我们在mAP达到一个更大的提升，从35.1%提升至53.7%，而且速度更快。我们的方法在VOC2011/2012测试集上达到了相似的检测效果mAP53.3%。</p><h2 id="3-可视化、消融、模型的错误"><a href="#3-可视化、消融、模型的错误" class="headerlink" title="3 可视化、消融、模型的错误"></a>3 可视化、消融、模型的错误</h2><h3 id="3-1-可视化学习到的特征"><a href="#3-1-可视化学习到的特征" class="headerlink" title="3.1 可视化学习到的特征"></a>3.1 可视化学习到的特征</h3><p>直接可视化第一层filters非常容易理解，它们主要捕获方向性边缘和对比色。难以理解的是后面的层。Zeiler and Fergus提出了一种可视化的很棒的反卷积办法。我们则使用了一种简单的非参数化方法，直接展示网络学到的东西。这个想法是单一输出网络中一个特定单元（特征），然后把它当做一个正确类别的物体检测器来使用。</p><p>方法是这样的，先计算所有抽取出来的推荐区域（大约1000万），计算每个区域所导致的对应单元的激活值，然后按激活值对这些区域进行排序，然后进行最大值抑制，最后展示分值最高的若干个区域。这个方法让被选中的单元在遇到他想激活的输入时“自己说话”。我们避免平均化是为了看到不同的视觉模式和深入观察单元计算出来的不变性。</p><p>我们可视化了第五层的池化层pool5，是卷积网络的最后一层，feature_map(卷积核和特征数的总称)的大小是6 x 6 x 256 = 9216维。忽略边界效应，每个pool5单元拥有195×195的感受野，输入是227×227。pool5中间的单元，几乎是一个全局视角，而边缘的单元有较小的带裁切的支持。<br>图4的每一行显示了对于一个pool5单元的最高16个激活区域情况，这个实例来自于VOC 2007上我们调优的CNN，这里只展示了256个单元中的6个（附录D包含更多）。我们看看这些单元都学到了什么。第二行，有一个单元看到狗和斑点的时候就会激活，第三行对应红斑点，还有人脸，当然还有一些抽象的模式，比如文字和带窗户的三角结构。这个网络似乎学到了一些类别调优相关的特征，这些特征都是形状、纹理、颜色和材质特性的分布式表示。而后续的fc6层则对这些丰富的特征建立大量的组合来表达各种不同的事物。</p><p>在RCNN之前，overfeat已经是用深度学习的方法做目标检测，但RCNN是第一个可以真正可以工业级应用的解决方案。可以说改变了目标检测领域的主要研究思路，紧随其后的系列文章：Fast-RCNN ，Faster-RCNN都沿袭R-CNN的思路。</p><p>再放一张经典的图，区分一下各类计算机视觉的任务：</p><p>简单来说，分类、定位和检测的区别如下：<br>classify：是什么？<br>localization：在哪里？是什么？（单个目标）<br>detection：在哪里？分别是什么？（多个目标）</p><p>论文翻译：<br>摘要：<br>过去几年，在权威数据集PASCAL上，物体检测的效果已经达到一个稳定水平。效果最好的方法是融合了多种低维图像特征和高维上下文环境的复杂融合系统。在这篇论文里，我们提出了一种简单并且可扩展的检测算法，可以将mAP在VOC2012最好结果的基础上提高30%以上——达到了53.3%。我们的方法结合了两个关键的因素：</p><p>在候选区域上自下而上使用大型卷积神经网络(CNNs)，用以定位和分割物体。<br>当带标签的训练数据不足时，先针对辅助任务进行有监督预训练，再进行特定任务的调优，就可以产生明显的性能提升。<br>因为我们把region proposal和CNNs结合起来，所以该方法被称为R-CNN：Regions with CNN features。我们也把R-CNN效果跟OverFeat比较了下（OverFeat是最近提出的在与我们相似的CNN特征下采用滑动窗口进行目标检测的一种方法），结果发现RCNN在200类ILSVRC2013检测数据集上的性能明显优于OVerFeat。</p><p>【Overfeat:改进了Alex-net，并用图像缩放和滑窗方法在test数据集上测试网络；提出了一种图像定位的方法；最后通过一个卷积网络来同时进行分类，定位和检测三个计算机视觉任务，并在ILSVRC2013中获得了很好的结果。】</p><p>介绍：<br>特征很重要。在过去十年，各类视觉识别任务基本都建立在对SIFT[29]和HOG[7]特征的使用。但如果我们关注一下PASCAL VOC对象检测[15]这个经典的视觉识别任务，就会发现，2010-2012年进展缓慢，取得的微小进步都是通过构建一些集成系统和采用一些成功方法的变种才达到的。<br>SIFT和HOG是块方向直方图(blockwise orientation histograms)，一种类似大脑初级皮层V1层复杂细胞的表示方法。但我们知道识别发生在多个下游阶段，（我们是先看到了一些特征，然后才意识到这是什么东西）也就是说对于视觉识别来说，更有价值的信息，是层次化的，多个阶段的特征。<br>Fukushima的“neocognitron，一种受生物学启发用于模式识别的层次化、移动不变性模型，算是这方面最早的尝试。然而neocognitron缺乏监督学习算法。Lecun等人的工作表明基于反向传播的随机梯度下降(SGD)对训练卷积神经网络（CNNs）非常有效，CNNs被认为是继承自neocognitron的一类模型。<br>CNNs在1990年代被广泛使用，但随即便因为SVM的崛起而淡出研究主流。2012年，Krizhevsky等人在ImageNet大规模视觉识别挑战赛(ILSVRC)上的出色表现重新燃起了世界对CNNs的兴趣（AlexNet）。他们的成功在于在120万的标签图像上使用了一个大型的CNN，并且对LeCUN的CNN进行了一些改造（比如ReLU和Dropout Regularization）。<br>这个ImangeNet的结果的重要性在ILSVRC2012 workshop上得到了热烈的讨论。提炼出来的核心问题是：ImageNet上的CNN分类结果在何种程度上能够应用到PASCAL VOC挑战的物体检测任务上？<br>我们通过连接图像分类和目标检测，回答了这个问题。本论文是第一个说明在PASCAL VOC的物体检测任务上CNN比基于简单类HOG特征的系统有大幅的性能提升。我们主要关注了两个问题：使用深度网络定位物体和在小规模的标注数据集上进行大型网络模型的训练。<br>与图像分类不同的是检测需要定位一个图像内的许多物体。一个方法是将框定位看做是回归问题。但Szegedy等人的工作说明这种策略并不work（在VOC2007上他们的mAP是30.5%，而我们的达到了58.5%）。</p><p>【也就是说将定位问题单纯作为回归解决效果并不好】</p><p>另一个可替代的方法是使用滑动窗口探测器，通过这种方法使用CNNs至少已经有20年的时间了，通常用于一些特定的种类如人脸，行人等。为了获得较高的空间分辨率，这些CNNs都采用了两个卷积层和两个池化层。我们本来也考虑过使用滑动窗口的方法，但是由于网络层次更深，输入图片有非常大的感受野（195×195）and 步长（32×32），这使得采用滑动窗口的方法充满挑战。<br>我们是通过操作”recognition using regions”范式，解决了CNN的定位问题。测试时，对这每张图片，产生了接近2000个与类别无关的region proposal，对每个CNN抽取了一个固定长度的特征向量，然后借助专门针对特定类别数据的线性SVM对每个区域进行分类。我们不考虑region的大小，使用放射图像变形的方法来对每个不同形状的region proposal产生一个固定长度的作为CNN输入的特征向量（也就是把不同大小的proposal放到同一个大小）。图1展示了我们方法的全貌并突出展示了一些实验结果。由于我们结合了Region proposals和CNNs，所以起名<strong><em>R-CNN：Regions with CNN features。</em></strong></p><p>检测中面对的第二个挑战是标签数据太少，现在可获得的数据远远不够用来训练一个大型卷积网络。传统方法多是采用无监督与训练，再进行有监督调优。本文的第二个核心贡献是在辅助数据集（ILSVRC）上进行有监督预训练，再在小数据集上针对特定问题进行调优。这是在训练数据稀少的情况下一个非常有效的训练大型卷积神经网络的方法。我们的实验中，针对检测的调优将mAP提高了8个百分点。调优后，我们的系统在VOC2010上达到了54%的mAP，远远超过高度优化的基于HOG的可变性部件模型（deformable part model，DPM）</p><p>【DPM:多尺度形变部件模型，连续获得07-09的检测冠军，2010年其作者Felzenszwalb Pedro被VOC授予”终身成就奖”。DPM把物体看成了多个组成的部件（比如人脸的鼻子、嘴巴等），用部件间的关系来描述物体，这个特性非常符合自然界很多物体的非刚体特征。DPM可以看做是HOG+SVM的扩展，很好的继承了两者的优点，在人脸检测、行人检测等任务上取得了不错的效果，但是DPM相对复杂，检测速度也较慢，从而也出现了很多改进的方法。】</p><p>我们的系统也很高效，都是小型矩阵向量相乘和贪婪NMS这些特定类别的计算。这个计算特性源自于特征在不同类别之间的共享（对于不同类别，CNNC提取到的特征是一样的），这比之前使用的区域特征少了两个数量级的维度。<br>HOG-like特征的一个优点是简单性：能够很容易明白提取到的特征是什么，那我们能可视化出CNNC提取到的特征吗？全连接层有超过5千4百万的参数值，这是关键吗？这些都不是，我们将CNN切断，会发现，移除掉其中94%的参数，精度只会下降一点点。相反，通过网络中的探测单元我们可以看到卷积层学习了一组丰富的特性。（图3）</p><p>分析我们方法的失败案例，对于进一步提高很有帮助，所以我们借助Hoiem等人的定位分析工具做实验结果的报告和分析。分析结果，我们发发现主要的错误是因为mislocalization,而使用了bounding box regression之后，可以有效的降低这个错误。<br>介绍技术细节之前，我们提醒大家由于R-CNN是在推荐区域上进行操作，所以可以很自然地扩展到语义分割任务上。只要很小的改动，我们就在PASCAL VOC语义分割任务上达到了很有竞争力的结果，在VOC2011测试集上平均语义分割精度达到了47.9%。</p><p>用RCNN做物体检测<br>我们的物体检测系统有三个模块构成。第一个，产生类别无关的region proposal。这些推荐定义了一个候选检测区域的集合；第二个是一个大型卷积神经网络，用于从每个区域抽取特定大小的特征向量；第三个是一个指定类别的线性SVM。本部分，将展示每个模块的设计，并介绍他们的测试阶段的用法，以及参数是如何学习的细节，最后给出在PASCAL VOC 2010-12和ILSVRC2013上的检测结果。</p><p>2.1模块设计<br>区域推荐（region proposal）<br>近来有很多研究都提出了产生类别无关区域推荐的方法。比如: objectness（物体性），selective search（选择性搜索），category-independent object proposals(类别无关物体推荐)，constrained parametric min-cuts（受限参最小剪切, CPMC)，multi-scal combinatorial grouping(多尺度联合分组)，以及Ciresan等人的方法，将CNN用在规律空间块裁剪上以检测有丝分裂细胞，也算是一种特殊的区域推荐类型。由于R-CNN对特定区域算法是不关心的，所以我们采用了选择性搜索以方便和前面的工作进行可控的比较。</p><p>特征提取（Feature extraction）<br>我们使用Krizhevsky等人所描述的CNN的一个Caffe实现版本对每个推荐区域抽取一个4096维度的特征向量把一个输入为277277大小的图片，通过五个卷积层和两个全连接层进行前向传播,最终得到一个4096-D的特征向量。读者可以参考AlexNet获得更多的网络架构细节。<br>为了计算region proposal的特征，我们首先要对图像进行转换，使得它符合CNNC的输入（架构中的CNNC只能接受固定大小：277277）。这个变换有很多办法，我们使用了最简单的一种。无论候选区域是什么尺寸和宽高比，我们都把候选框变形成想要的尺寸。具体的，变形之前，我们现在候选框周围加上16的padding,再进行各向异性缩放。 这种形变使得mAp提高了3到5个百分点。在补充材料中，作者对比了各向异性和各向同性缩放缩放方法。</p><p>【关于图像的各项同性 各向异性可以参考我的另一篇文章，那里有具体比较说明<br>链接：R-CNN中的各项同性 各向异性】。</p><p>2.2测试阶段的物体检测<br>测试阶段，在测试图像上使用selective search抽取2000个推荐区域（实验中，我们使用了选择性搜索的快速模式）。然后变形每一个推荐区域，再通过CNN前向传播计算出特征。然后我们使用对每个类别训练出的SVM给整个特征向量中的每个类别单独打分。</p><p>【对每一个框使用每个类别的SVM进行打分】</p><p>然后给出一张图像中所有的打分区域，然后使用NMS（每个类别是独立进行的），拒绝掉一些和高分区域的IOU大于阈值的候选框。</p><p>运行时的分析<br>两个特性让检测变得很高效。首先，所有的CNN参数都是跨类别共享的。其次，通过CNN计算的特征向量相比其他通用方法（比如spatial pyramids with bag-of-visual-word encodings）维度是很低的。UVA检测系统的特征比我们的要多两个数量级(360k vs 4k)。<br>这种共享的结果就是计算推荐区域特征的耗时可以分摊到所有类别的头上（GPU：每张图13s，CPU：每张图53s）。唯一的和具体类别有关的计算是特征向量和SVM权重和点积，以及NMS。实践中，所有的点积都可以批量化成一个单独矩阵间运算。特征矩阵的典型大小是2000×4096，SVM权重的矩阵是4096xN，其中N是类别的数量。<br>分析表明R-CNN可以扩展到上千个类别，而不需要借用近似技术（如hashing）。及时有10万个类别，矩阵乘法在现代多核CPU上只需要10s而已。但这种高效不仅仅是因为使用了区域推荐和共享特征。由于较高维度的特征，UVA系统存储100k linear predictors需要134GB的内存，而我们只要1.5GB，比我们高了两个数量级。<br>有趣的是R-CCN和最近Dean等人使用DPMs和hashing做检测的工作相比，他们用了1万个干扰类， 每五分钟可以处理一张图片，在VOC2007上的mAP能达到16%。我们的方法1万个检测器由于没有做近似，可以在CPU上一分钟跑完，达到59%的mAP（3.2节）。</p><p>2.3训练<br>有监督的预训练<br>我们在大型辅助训练集ILSVRC2012分类数据集（没有约束框数据）上预训练了CNN。预训练采用了Caffe的CNN库。总体来说，我们的CNN十分接近krizhevsky等人的网络的性能，在ILSVRC2012分类验证集在top-1错误率上比他们高2.2%。差异主要来自于训练过程的简化。</p><p>特定领域的参数调优<br>为了让我们的CNN适应新的任务（即检测任务）和新的领域（变形后的推荐窗口）。我们只使用变形后的推荐区域对CNN参数进行SGD训练。我们替换掉了ImageNet专用的1000-way分类层，换成了一个随机初始化的21-way分类层，（其中20是VOC的类别数，1代表背景）而卷积部分都没有改变。我们对待所有的推荐区域，如果其和真实标注的框的IoU&gt;= 0.5就认为是正例，否则就是负例。SGD开始的learning_rate为0.001（是初始化预训练时的十分之一），这使得调优得以有效进行而不会破坏初始化的成果。每轮SGD迭代，我们统一使用32个正例窗口（跨所有类别）和96个背景窗口，即每个mini-batch的大小是128。另外我们倾向于采样正例窗口，因为和背景相比他们很稀少。</p><p>目标种类分类器<br>思考一下检测汽车的二分类器。很显然，一个图像区域紧紧包裹着一辆汽车应该就是正例。同样的，没有汽车的就是背景区域，也就是负例。较为不明确的是怎样标注哪些只和汽车部分重叠的区域。我们使用IoU重叠阈值来解决这个问题，低于这个阈值的就是负例。这个阈值我们选择了0.3，是在验证集上基于{0, 0.1, … 0.5}通过网格搜索得到的。我们发现认真选择这个阈值很重要。如果设置为0.5，可以降低mAP5个点，设置为0，就会降低4个点。正例就严格的是标注的框。</p><p>【IOU&lt;0.3被作为负例，ground-truth是正例，其余的全部丢弃】</p><p>一旦特征提取出来，并应用标签数据，我们优化了每个类的线性SVM。由于训练数据太大，难以装进内存，我们选择了标准的hard negative mining method</p><p>【难负例挖掘算法，用途就是正负例数量不均衡，而负例分散代表性又不够的问题，hard negative就是每次把那些顽固的棘手的错误,再送回去继续练,练到你的成绩不再提升为止.这一个过程就叫做’hard negative mining’】</p><p>高难负例挖掘算法收敛很快，实践中只要在所有图像上经过一轮训练，mAP就可以基本停止增加了。<br>附录B中，我们讨论了，为什么在fine-tunning和SVM训练这两个阶段，我们定义得正负样例是不同的。</p><p>【fine-tunning阶段是由于CNN对小样本容易过拟合，需要大量训练数据，故对IoU限制宽松： IoU&gt;0.5的建议框为正样本，否则为负样本； SVM这种机制是由于其适用于小样本训练，故对样本IoU限制严格：Ground Truth为正样本，与Ground Truth相交IoU＜0.3的建议框为负样本。】</p><p>我们也会讨论为什么训练一个分类器是必要的，而不只是简单地使用来自调优后的CNN的最终fc8层的输出。</p><p>【为什么单独训练了一个SVM而不是直接用softmax，作者提到，刚开始时只是用了ImageNet预训练了CNN，并用提取的特征训练了SVMs，此时用正负样本标记方法就是前面所述的0.3,后来刚开始使用fine-tuning时，也使用了这个方法，但是发现结果很差，于是通过调试选择了0.5这个方法，作者认为这样可以加大样本的数量，从而避免过拟合。然而，IoU大于0.5就作为正样本会导致网络定位准确度的下降，故使用了SVM来做检测，全部使用ground-truth样本作为正样本，且使用非正样本的，且IoU大于0.3的“hard negatives”，提高了定位的准确度】。</p><p>2.4 在PASCAL VOC 2010-12上的结果<br>按照PASCAL VOC的最佳实践步骤，我们在VOC2007的数据集上验证了我们所有的设计思想和参数处理，我们在VOC2012上训练和优化了SVMs，最终结果在VOC 2010-12的数据库，我们在评估服务器上提交了两个结果（一个是有bunding box regression，一个没有）。<br>表1展示了在VOC2010的结果，我们将自己的方法同四种先进基准方法作对比，其中包括SegDPM，这种方法将DPM检测子与语义分割系统相结合并且使用附加的inter-detector的环境和图片检测器。更加恰当的比较是同Uijling的UVA系统比较，因为我们的方法同样基于候选框算法。对于候选区域的分类，他们通过构建一个四层的金字塔，并且将之与SIFT模板结合，SIFT为扩展的OpponentSIFT和RGB-SIFT描述子，每一个向量被量化为4000-word的codebook。分类任务由一个交叉核的SVM承担，对比这种方法的多特征方法，非线性内核的SVM方法，我们在mAP达到一个更大的提升，从35.1%提升至53.7%，而且速度更快。我们的方法在VOC2011/2012测试集上达到了相似的检测效果mAP53.3%。</p><p>可视化、消融、模型的错误<br>3.1 可视化学习到的特征<br>直接可视化第一层filters非常容易理解，它们主要捕获方向性边缘和对比色。难以理解的是后面的层。Zeiler and Fergus提出了一种可视化的很棒的反卷积办法。我们则使用了一种简单的非参数化方法，直接展示网络学到的东西。这个想法是单一输出网络中一个特定单元（特征），然后把它当做一个正确类别的物体检测器来使用。<br>方法是这样的，先计算所有抽取出来的推荐区域（大约1000万），计算每个区域所导致的对应单元的激活值，然后按激活值对这些区域进行排序，然后进行最大值抑制，最后展示分值最高的若干个区域。这个方法让被选中的单元在遇到他想激活的输入时“自己说话”。我们避免平均化是为了看到不同的视觉模式和深入观察单元计算出来的不变性。<br>我们可视化了第五层的池化层pool5，是卷积网络的最后一层，feature_map(卷积核和特征数的总称)的大小是6 x 6 x 256 = 9216维。忽略边界效应，每个pool5单元拥有195×195的感受野，输入是227×227。pool5中间的单元，几乎是一个全局视角，而边缘的单元有较小的带裁切的支持。<br>图4的每一行显示了对于一个pool5单元的最高16个激活区域情况，这个实例来自于VOC 2007上我们调优的CNN，这里只展示了256个单元中的6个（附录D包含更多）。我们看看这些单元都学到了什么。第二行，有一个单元看到狗和斑点的时候就会激活，第三行对应红斑点，还有人脸，当然还有一些抽象的模式，比如文字和带窗户的三角结构。这个网络似乎学到了一些类别调优相关的特征，这些特征都是形状、纹理、颜色和材质特性的分布式表示。而后续的fc6层则对这些丰富的特征建立大量的组合来表达各种不同的事物。</p><h3 id="3-2-消融研究-Ablation-studies"><a href="#3-2-消融研究-Ablation-studies" class="headerlink" title="3.2 消融研究(Ablation studies)"></a>3.2 消融研究(Ablation studies)</h3><blockquote><p>ablation study 就是为了研究模型中所提出的一些结构是否有效而设计的实验。如你提出了某某结构，但是要想确定这个结构是否有利于最终的效果，那就要将去掉该结构的网络与加上该结构的网络所得到的结果进行对比，这就是ablation study。也就是（控制变量法）</p></blockquote><h4 id="没有调优的各层性能。"><a href="#没有调优的各层性能。" class="headerlink" title="没有调优的各层性能。"></a><strong>没有调优的各层性能</strong>。</h4><p>为了理解哪一层对于检测的性能十分重要，我们分析了CNN最后三层的每一层在VOC2007上面的结果。Pool5在3.1中做过剪短的表述。最后两层下面来总结一下。<br>fc6是一个与pool5连接的全连接层。为了计算特征，它和pool5的feature map（reshape成一个9216维度的向量）做了一个4096×9216的矩阵乘法，并添加了一个bias向量。中间的向量是逐个组件的半波整流（component-wise half-wave rectified）【Relu（x&lt;- max(0,x)）】<br>fc7是网络的最后一层。跟fc6之间通过一个4096×4096的矩阵相乘。也是添加了bias向量和应用了ReLU。</p><p>我们先来看看没有调优的CNN在PASCAL上的表现，没有调优是指所有的CNN参数就是在ILSVRC2012上训练后的状态。分析每一层的性能显示来自fc7的特征泛化能力不如fc6的特征。这意味29%的CNN参数，也就是1680万的参数可以移除掉，而且不影响mAP。更多的惊喜是即使同时移除fc6和fc7，仅仅使用pool5的特征，只使用CNN参数的6%也能有非常好的结果。可见CNN的主要表达力来自于卷积层，而不是全连接层。这个发现提醒我们也许可以在计算一个任意尺寸的图片的稠密特征图（dense feature map）时使仅仅使用CNN的卷积层。这种表示可以直接在pool5的特征上进行滑动窗口检测的实验。</p><h4 id="调优后的各层性能。"><a href="#调优后的各层性能。" class="headerlink" title="调优后的各层性能。"></a>调优后的各层性能。</h4><p>我们来看看调优后在VOC2007上的结果表现。提升非常明显，mAP提升了8个百分点，达到了54.2%。fc6和fc7的提升明显优于pool5，这说明pool5从ImageNet学习的特征通用性很强，在它之上层的大部分提升主要是在学习领域相关的非线性分类器。</p><h4 id="对比其他特征学习方法。"><a href="#对比其他特征学习方法。" class="headerlink" title="对比其他特征学习方法。"></a><strong>对比其他特征学习方法。</strong></h4><p>相当少的特征学习方法应用与VOC数据集。我们找到的两个最近的方法都是基于固定探测模型。为了参照的需要，我们也将基于基本HOG的DFM方法的结果加入比较</p><p>第一个DPM的特征学习方法，DPM ST,将HOG中加入略图表征的概率直方图。直观的，一个略图就是通过图片中心轮廓的狭小分布。略图表征概率通过一个被训练出来的分类35*35像素路径为一个150略图表征的的随机森林方法计算</p><p>第二个方法，DPM HSC，将HOG特征替换成一个稀疏编码的直方图。为了计算HSC，在每个像素上使用一个学习到的1007 * 7 像素（灰度空间）原子求解稀疏码激活，（原文是atoms,应该是这么翻译吧）由此产生的激活以三种方式（全波和半波）整流，空间池化，l2标准化，然后进行幂运算。</p><p>所有的RCNN变种算法都要强于这三个 DPM方法（表2， 8-10行），包括两种特征学习的方法与最新版本的DPM方法比较，我们的mAP要多大约20个百分点，61%的相对提升。略图表征与HOG现结合的方法比单纯HOG的性能高出2.5%，而HSC的方法相对于HOG提升四个百分点（当内在的与他们自己的DPM基准比价，全都是用的非公共DPM执行，这低于开源版本）。这些方法分别达到了29.1%和34.3%。</p><h3 id="3-3-检测错误分析"><a href="#3-3-检测错误分析" class="headerlink" title="3.3 检测错误分析"></a>3.3 检测错误分析</h3><p>为了揭示出我们方法的错误之处， 我们使用Hoiem提出的优秀的检测分析工具，来理解调参是怎样改变他们，并且观察相对于DPM方法，我们的错误形式。这个分析方法全部的介绍超出了本篇文章的范围，我们建议读者查阅文献21来了解更加详细的介绍（例如“normalized AP”的介绍），由于这些分析是不太有关联性，所以我们放在图4和图5的题注中讨论。</p><h3 id="3-4-Bounding-box回归"><a href="#3-4-Bounding-box回归" class="headerlink" title="3.4 Bounding-box回归"></a>3.4 Bounding-box回归</h3><p>基于错误分析，我们使用了一种简单的方法减小定位误差。受到DPM[17]中使用的约束框回归训练启发，我们训练了一个线性回归模型在给定一个选择区域的pool5特征时去预测一个新的检测窗口。详细的细节参考附录C。表1、表2和图4的结果说明这个简单的方法，修复了大量的错位检测，提升了3-4个百分点。</p><blockquote><p>Bounding-Box的具体内容可以参考我的另一篇文章，那里写的比较详细。<br>链接：<a href="https://blog.csdn.net/v1_vivian/article/details/80292569">边框回归：BoundingBox-Regression</a></p></blockquote><h2 id="4-语义分割"><a href="#4-语义分割" class="headerlink" title="4 语义分割"></a>4 语义分割</h2><p>区域分类是语义分割的标准技术，这使得我们很容易将R-CNN应用到PASCAL VOC分割任务的挑战。为了和当前主流的语义分割系统（称为O2P，second-order pooling[4]）做对比，我们使用了一个开源的框架。O2P使用CPMC针对每张图片产生150个区域推荐，并预测每个区域的品质，对于每个类别，进行支撑向量回归（support vector regression，SVR）。他们的方法很高效，主要得益于CPMC区域的品质和多特征类型的强大二阶池化（second-second pooling，SIFT和LBP的增强变种）。我们也注意到Farabet等人[16]将CNN用作多尺度逐像素分类器，在几个高密度场景标注数据集（不包括PASCAL）上取得了不错的成绩。<br>我们学习[2,4]，将Hariharan等人提供的额外标注信息补充到PASCAL分割训练集中。设计选择和超参数都在VOC 2011验证集上进行交叉验证。最后的测试结果只执行了一次。</p><h3 id="用于分割的CNN特征。"><a href="#用于分割的CNN特征。" class="headerlink" title="用于分割的CNN特征。"></a><strong>用于分割的CNN特征</strong>。</h3><p>为了计算CPMC区域上的特征，我们执行了三个策略，每个策略都先将矩形窗口变形到227×227大小。第一个策略完全忽略区域的形状(full ignore)，直接在变形后的窗口上计算CNN特征，就和我们检测时做的一样。但是，这些特征忽略了区域的非矩形形状。两个区域也许包含相似的约束框却几乎没有重叠。因此，第二个策略(fg，foreground)只计算前景遮罩（foreground mask）的CNN特征，我们将所有的背景像素替换成平均输入，这样减除平均值后他们就会变成0。第三个策略(full+fg)，简单的并联全部（full）特征和前景（fg）特征；我们的实验验证了他们的互补性。</p><h3 id="在VOC-2011上的结果。"><a href="#在VOC-2011上的结果。" class="headerlink" title="在VOC 2011上的结果。"></a><strong>在VOC 2011上的结果。</strong></h3><p>表3显示了与O2P相比较的VOC 2011验证集的结果（每个类别的计算结果见补充材料）。在每个特征计算策略中，FC6总是优于FC7，下面就针对fc6进行讨论。fg策略略优于full，表明掩蔽区域形状提供了更强的信号，匹配我们的直觉。然而，full+fg的平均精度为47.9%，比fg优4.2%（也稍优于O2P），这表明即使提供了FG特征，由full特征提供的上下文也是有很多信息。值得注意的是，训练20个SVR，在我们的full+fg特征在单核上需要1小时，而在O2P特征则需要10个小时。</p><p>在表4中,我们给出了VOC 2011测试集上的结果。比较我们的最佳执行方法，fc6（full+fg），对抗两个强大的baselines。我们的方法在21个类别中的11个达到最高的分割精度，最高的总体分割精度为47.9%，平均跨类别（但可能与O2P结果在任何合理的误差范围内）。通过微调可能会取得更好的成绩。</p><h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h2><p>最近几年，物体检测陷入停滞，表现最好的检测系统是复杂的将多低层级的图像特征与高层级的物体检测器环境与场景识别相结合。本文提出了一种简单并且可扩展的物体检测方法，达到了VOC 2012数据集相对之前最好性能的30%的提升。</p><p>我们取得这个性能主要通过两个方面：第一是应用了自底向上的候选框训练的高容量的卷积神经网络进行定位和分割物体。另外一个是使用在标签数据匮乏的情况下训练大规模神经网络的一个方法。我们展示了在有监督的情况下使用丰富的数据集（图片分类）预训练一个网络作为辅助性的工作是很有效的，然后采用稀少数据（检测）去调优定位任务的网络。我们猜测“有监督的预训练+特定领域的调优”这一范式对于数据稀少的视觉问题是很有效的。</p><p>最后,我们注意到能得到这些结果，将计算机视觉中经典的工具和深度学习(自底向上的区域候选框和卷积神经网络）组合是非常重要的。而不是违背科学探索的主线，这两个部分是自然而且必然的结合。</p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>yolov7-paper</title>
      <link href="/yolov7-paper/"/>
      <url>/yolov7-paper/</url>
      
        <content type="html"><![CDATA[<p>这个博客上传不上去自定义域名,新建一个新的也不太行。</p><h1 id="YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors"><a href="#YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors" class="headerlink" title="YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors"></a><strong>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</strong></h1><h2 id="Abstract—摘要"><a href="#Abstract—摘要" class="headerlink" title="Abstract—摘要"></a><strong>Abstract—摘要</strong></h2><p>YOLOv7在5FPS到 160 FPS 范围内的速度和准确度都超过了所有已知的物体检测器，YOLOv7 在 5 FPS 到 160 FPS 范围内的速度和准确度都超过了所有已知的目标检测器，并且在 GPU V100 上 30 FPS 或更高的所有已知实时目标检测器中具有最高的准确度 56.8% AP。YOLOv7-E6 目标检测器（56 FPS V100，55.9% AP）比基于transformer-based的检测器 SWINL Cascade-Mask R-CNN（9.2 FPS A100，53.9% AP）的速度和准确度分别高出 509% 和 2%，以及基于卷积的检测器 ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2% AP) 速度提高 551%，准确率提高 0.7%，以及 YOLOv7 的表现优于：YOLOR、YOLOX、Scaled-YOLOv4、YOLOv5、DETR、Deformable DETR  , DINO-5scale-R50, ViT-Adapter-B 和许多其他物体探测器在速度和准确度上。 此外，我们只在 MS COCO 数据集上从头开始训练 YOLOv7，而不使用任何其他数据集或预训练的权重。  源码发布在： GitHub - WongKinYiu/yolov7: Implementation of paper - YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</p><blockquote><h4 id="YOLOv7的成就"><a href="#YOLOv7的成就" class="headerlink" title="YOLOv7的成就"></a>YOLOv7的成就</h4><p>在5FPS到160FPS的范围内，在速度和精度上都超过了所有已知的物体检测器，在GPU V100上以30 FPS或更高的速度在所有已知 的实时物体检测器中具有最高的精度56.8%AP</p><h4 id="YOLOv7-E6在速度和精度上优于"><a href="#YOLOv7-E6在速度和精度上优于" class="headerlink" title="YOLOv7-E6在速度和精度上优于"></a>YOLOv7-E6在速度和精度上优于</h4><ul><li>基于transformer的检测器SWINL Cascade-Mask R-CNN</li><li>基于卷积的检测器ConvNeXt XL级联掩码R-CNN</li></ul><h4 id="YOLOv7优于"><a href="#YOLOv7优于" class="headerlink" title="YOLOv7优于"></a>YOLOv7优于</h4><p>YOLOR、YOLOX、Scaled-YOLOv4、YOLOv5、DETR、可变形DETR、DINO-5scale-R50、ViT-Adapter-B和许多其他物体检测器的速度和精度。</p><p><strong>训练方面：</strong>作者只在COCO数据集上从0开始训练YOLOv7，而不使用任何其他数据集或预先训练的权重。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Figure1.png" alt=""></p></blockquote><h2 id="一、-Introduction——简介"><a href="#一、-Introduction——简介" class="headerlink" title="一、 Introduction——简介"></a>一、 Introduction——简介</h2><p>实时对象检测是计算机视觉中非常重要的主题，因为它通常是计算机视觉系统中的必要组件。 例如，多目标跟踪[94, 93]，自动驾驶[40, 18]，机器人[35, 58]，医学图像分析[34, 46]等。执行实时目标检测的计算设备通常是一些移动CPU或GPU，以及各大厂商开发的各种神经处理单元（NPU）。 比如苹果神经引擎（Apple）、神经计算棒（Intel）、Jetson AI边缘设备（Nvidia）、边缘TPU（谷歌）、神经处理引擎（Qualcomm）、AI处理单元（MediaTek）、 和 AI SoC（Kneron）都是 NPU。 上面提到的一些边缘设备专注于加速不同的操作，例如普通卷积、深度卷积或 MLP 操作。 在本文中，我们提出的实时目标检测器主要希望它能够同时支持移动 GPU 和从边缘到云端的 GPU 设备。</p><p>近年来，实时目标检测器仍在针对不同的边缘设备进行开发。例如，开发MCUNet [49, 48] 和 NanoDet [54] 的运营专注于生产低功耗单芯片并提高边缘 CPU 的推理速度。 至于 YOLOX [21] 和 YOLOR [81] 等方法，他们专注于提高各种 GPU 的推理速度。 最近，实时目标检测器的发展集中在高效架构的设计上。 至于可以在 CPU [54, 88, 84, 83] 上使用的实时目标检测器，他们的设计主要基于 MobileNet [28, 66, 27]、ShuffleNet [92, 55] 或 GhostNet [25]  . 另一个主流的实时目标检测器是为 GPU [81, 21, 97] 开发的，它们大多使用 ResNet [26]、DarkNet [63] 或 DLA [87]，然后使用 CSPNet [80] 策略来优化架构。 本文提出的方法的发展方向与当前主流的实时目标检测器不同。 除了架构优化之外，我们提出的方法将专注于训练过程的优化。 我们的重点将放在一些优化的模块和优化方法上，它们可能会增加训练成本以提高目标检测的准确性，但不会增加推理成本。我们将提出的模块和优化方法称为可训练的bag-of-freebies。</p><p>最近，模型重新参数化[13,12,29]和动态标签分配[20,17,42]已成为网络训练和目标检测的重要课题。 主要是在上述新概念提出之后，物体检测器的训练演变出了很多新的问题。 在本文中，我们将介绍我们发现的一些新问题，并设计解决这些问题的有效方法。对于模型重参数化，我们用梯度传播路径的概念分析了适用于不同网络层的模型重参数化策略，并提出了有计划的重参数化模型。 此外，当我们发现使用动态标签分配技术时，具有多个输出层的模型的训练会产生新的问题。即：“如何为不同分支的输出分配动态目标？” 针对这个问题，我们提出了一种新的标签分配方法，称为coarse-to-fine引导式标签分配</p><p>本文的贡献总结如下：（1）我们设计了几种可训练的bag-of-freebies方法，使得实时目标检测可以在不增加推理成本的情况下大大提高检测精度；(2) 对于目标检测方法的演进，我们发现了两个新问题，即重新参数化的模块如何替换原始模块，以及动态标签分配策略如何处理分配给不同输出层的问题。 此外，我们还提出了解决这些问题所带来的困难的方法；(3) 我们提出了实时目标检测器的“扩展”和“复合缩放”方法，可以有效地利用参数和计算；  (4) 我们提出的方法可以有效减少最先进实时目标检测器约40%的参数和50%的计算量，并具有更快的推理速度和更高的检测精度。</p><blockquote><h3 id="本文主要工作"><a href="#本文主要工作" class="headerlink" title="本文主要工作"></a>本文主要工作</h3><ul><li>提出了一个实时对象检测器，主要是希望它能够从边缘到云端同时支持移动GPU和GPU设备</li><li>优化了架构，专注于优化训练过程。重点放在优化模块和优化方法上，称为可训练的<strong>Bag of freebies</strong></li></ul><blockquote><p>其实关于<strong>Bag of freebies</strong>和<strong>Bag of specials</strong>我们在YOLOv4就见过，现在来回顾一下：</p><p>Bag of freebies：字面意思就是“免费赠品”。在这里就是指用一些比较有用的训练技巧来训练模型，  <strong>只会改变训练策略或只会增加训练成本(不增加推理成本)的方法。</strong>从而使得模型获得更好的准确率但<strong>不增加模型的复杂度</strong>，也就<strong>不会增加推理的计算量。</strong><br>Bag of specials：指一些插件模块(plugin modules)和后处理方法(post-processing methods)，  <strong>它们只稍微增加了推理成本，但可以极大的提高目标检测的准确度。</strong>  一般来说，这些插件用来提高一个模型中的特定属性。比如<strong>增加感受野( SPP、  ASPP、   RFB)，引入注意力机制(spatial attention、channel attention)，提高特征整合的能力(FPN、ASFF、BiFPN)。</strong></p></blockquote><ul><li>对于模型重参数化问题，本文使用梯度传播路径的概念分析了<strong>适用于不同网络中的层的模型重参数化策略，并提出了有计划的重参数化模型。</strong></li><li>对于动态标签分配问题，本文提出了<strong>一种新的标签分配方法，称为由粗到细引导标签分配</strong></li></ul><h4 id="本文主要贡献"><a href="#本文主要贡献" class="headerlink" title="本文主要贡献"></a>本文主要贡献</h4><p>(1)提出了几种可用于训练的方法，这些方法<strong>仅仅会增加训练上的负担用于提升model性能，而不会增加推理负担</strong></p><p>(2)对于目标检测方法的发展，作者发现了两个新问题： </p><ul><li><strong>①重参数化模块如何替换原始模块</strong></li><li><strong>②动态标签分配策略如何处理对不同输出层的分配</strong></li></ul></blockquote><p>不过本文提出了解决这俩问题的方法</p><p>(3)作者针对目标检测可以更有效的利用参数和计算问题，提出了<strong>“扩展”(extend)和“**</strong>复合缩放”(compound scaling)**</p><p>(4)提出的方法可以有效的减少40%参数量和50%计算量，高精度高速度</p><h2 id="二、Related-work—相关工作"><a href="#二、Related-work—相关工作" class="headerlink" title="二、Related work—相关工作"></a>二、Related work—相关工作</h2><h3 id="2-1-Real-time-object-detectors—实时物体检测器"><a href="#2-1-Real-time-object-detectors—实时物体检测器" class="headerlink" title="2.1 Real-time object detectors—实时物体检测器"></a>2.1 Real-time object detectors—实时物体检测器</h3><p>目前最先进的实时目标检测器主要基于 YOLO [61, 62, 63] 和 FCOS [76, 77]，分别是 [3, 79, 81, 21, 54, 85, 23]  . 能够成为最先进的实时目标检测器通常需要以下特性：（1）更快更强的网络架构；  (2) 更有效的特征整合方法[22, 97, 37, 74, 59, 30, 9, 45]； (3) 更准确的检测方法 [76, 77, 69];  (4) 更稳健的损失函数 [96, 64, 6, 56, 95, 57]；  (5) 一种更有效的标签分配方法 [99, 20, 17, 82, 42]；  (6) 更有效的训练方法。 在本文中，我们不打算探索需要额外数据或大型模型的自我监督学习或知识蒸馏方法。 相反，我们将针对与上述 (4)、(5) 和 (6) 相关的最先进方法衍生的问题设计新的可训练bag-of-freebies方法。</p><blockquote><p>先进的网络应该具有以下特性：</p><p>(1)更快更有效的<strong>网络</strong></p><p>(2)更有效的<strong>特征集成方法</strong></p><p>(3)更准确的<strong>检测方法</strong></p><p>(4)更有鲁棒性的<strong>损失函数</strong></p><p>(5)更有效的<strong>标签匹配方法</strong></p><p>(6)更有效的<strong>训练方法</strong></p><p>本文中主要针对(4)、(5)、(6)。</p></blockquote><h3 id="2-2-Model-re-parameterization—模型重新参数化"><a href="#2-2-Model-re-parameterization—模型重新参数化" class="headerlink" title="2.2 Model re-parameterization—模型重新参数化"></a>2.2 Model re-parameterization—模型重新参数化</h3><p>模型重新参数化技术 [71、31、75、19、33、11、4、24、13、12、10、29、14、78] 在推理阶段将多个计算模块合并为一个。 模型重参数化技术可以看作是一种集成技术，我们可以将其分为两类，即模块级集成和模型级集成。 模型级别的重新参数化有两种常见的做法来获得最终的推理模型。 一种是用不同的训练数据训练多个相同的模型，然后对多个训练模型的权重进行平均。 另一种是对不同迭代次数的模型权重进行加权平均。 模块级重新参数化是最近比较热门的研究问题。这种方法在训练时将一个模块拆分为多个相同或不同的模块分支，在推理时将多个分支模块整合为一个完全等效的模块。然而，并非所有提出的重新参数化模块都可以完美地应用于不同的架构。 考虑到这一点，我们开发了新的重新参数化模块，并为各种架构设计了相关的应用策略。</p><blockquote><h4 id="模型重新参数化的介绍"><a href="#模型重新参数化的介绍" class="headerlink" title="模型重新参数化的介绍"></a>模型重新参数化的介绍</h4><p>在<strong>训练</strong>时<strong>将一个模块拆分为多个相同或不同的模块分支</strong>；在<strong>推理</strong>时<strong>将多个分支模块整合为一个完全等效的模块</strong>。   </p><p>有两种方法：即模块级集成和模型级集成</p><h4 id="获得最终推理模型的两种方法"><a href="#获得最终推理模型的两种方法" class="headerlink" title="获得最终推理模型的两种方法"></a>获得最终推理模型的两种方法</h4><p>(1)用不同的训练数据训练多个相同的模型，然后对多个训练模型的权重进行平均</p><p>(2)对不同迭代次数的模型权重进行加权平均</p><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul><li>训练时，采用多分支的网络使模型获取更好的特征表达</li><li>推理时，将并行融合成串行，从而降低计算量和参数量，提升速度(融合后理论上和融合前识别效果一样，实际基本都是稍微降低一点点)</li></ul><h4 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h4><p>并不是所有提出的重新参数化模块都可以完美地应用于不同的架构。</p></blockquote><h3 id="2-3-Model-scaling—模型缩放"><a href="#2-3-Model-scaling—模型缩放" class="headerlink" title="2.3 Model scaling—模型缩放"></a>2.3 Model scaling—模型缩放</h3><p>模型缩放 [72, 60, 74, 73, 15, 16, 2, 51] 是一种放大或缩小已设计模型并使其适合不同计算设备的方法。 模型缩放方法通常使用不同的缩放因子，例如分辨率（输入图像的大小）、深度（层数）、宽度（通道数）和阶段（特征金字塔的数量），以实现良好的权衡 -off 表示网络参数的数量、计算量、推理速度和准确性。 网络架构搜索（NAS）是常用的模型缩放方法之一。  NAS 可以自动从搜索空间中搜索到合适的缩放因子，而无需定义过于复杂的规则。  NAS 的缺点是需要非常昂贵的计算来完成对模型缩放因子的搜索。 在[15]中，研究人员分析了缩放因子与参数数量和操作量之间的关系，试图直接估计一些规则，从而获得模型缩放所需的缩放因子。 查阅文献，我们发现几乎所有模型缩放方法都独立分析单个缩放因子，甚至复合缩放类别中的方法也独立优化缩放因子。 这样做的原因是因为大多数流行的 NAS 架构处理的比例因子不是很相关。 我们观察到，所有基于连接的模型，例如 DenseNet [32] 或 VoVNet [39]，都会在缩放此类模型的深度时改变某些层的输入宽度。 由于提出的架构是基于串联的我们必须为此模型设计一种新的复合缩放方法。</p><blockquote><h4 id="常采用的缩放因子"><a href="#常采用的缩放因子" class="headerlink" title="常采用的缩放因子"></a>常采用的缩放因子</h4><ul><li>分辨率(输入图像的大小)</li><li>深度(层数)<br>宽度(通道数)</li><li>阶段(特征金字塔的数量)</li></ul><h4 id="常用方法：NAS"><a href="#常用方法：NAS" class="headerlink" title="常用方法：NAS"></a>常用方法：NAS</h4><p><strong>介绍：</strong>NAS即模型搜索，其主要思路就是不需要人为去设计特定的网络，而是让模型自己去选择</p><p><strong>注意问题：</strong>    </p><pre><code>(1)怎么定义候选空间</code></pre><p>(2)加速训练   </p><p><strong>不足：</strong>消耗大量的时间和资源</p></blockquote><h2 id="三、Architecture—网络结构"><a href="#三、Architecture—网络结构" class="headerlink" title="三、Architecture—网络结构"></a>三、Architecture—网络结构</h2><h3 id="3-1-Extended-efficient-layer-aggregation-networks—扩展的高效层聚合网络"><a href="#3-1-Extended-efficient-layer-aggregation-networks—扩展的高效层聚合网络" class="headerlink" title="3.1 Extended efficient layer aggregation networks—扩展的高效层聚合网络"></a><strong>3.1 Extended efficient layer aggregation networks—扩展的高效层聚合网络</strong></h3><p>在大多数关于设计高效架构的文献中，主要考虑因素不超过参数的数量、计算量和计算密度。Ma 等人还从内存访问成本的特点出发，分析了输入/输出通道比、架构的分支数量以及 element-wise 操作对网络推理速度的影响。多尔阿尔等人在执行模型缩放时还考虑了激活，即更多地考虑卷积层输出张量中的元素数量。 图 2（b）中 CSPVoVNet [79] 的设计是 VoVNet [39] 的变体。 除了考虑上述基本设计问题外，CSPVoVNet [79] 的架构还分析了梯度路径，以使不同层的权重能够学习更多不同的特征。上述梯度分析方法使推理更快、更准确。 图 2 (c) 中的 ELAN [1] 考虑了以下设计策略——“如何设计一个高效的网络？”。 他们得出了一个结论：通过控制最短最长的梯度路径，更深的网络可以有效地学习和收敛。 在本文中，我们提出了基于 ELAN 的扩展 ELAN（E-ELAN），其主要架构如图 2（d）所示。</p><p>无论梯度路径长度和大规模 ELAN 中计算块的堆叠数量如何，它都达到了稳定状态。 如果无限堆叠更多的计算块，可能会破坏这种稳定状态，参数利用率会降低。 提出的E-ELAN使用expand、shuffle、merge cardinality来实现在不破坏原有梯度路径的情况下不断增强网络学习能力的能力。在架构方面，E-ELAN 只改变了计算块的架构，而过渡层的架构完全没有改变。 我们的策略是使用组卷积来扩展计算块的通道和基数。 我们将对计算层的所有计算块应用相同的组参数和通道乘数。 然后，每个计算块计算出的特征图会根据设置的组参数g被打乱成g个组，然后将它们连接在一起。 此时，每组特征图的通道数将与原始架构中的通道数相同。 最后，我们添加 g 组特征图来执行合并基数。 除了保持原有的 ELAN 设计架构，E-ELAN 还可以引导不同组的计算块学习更多样化的特征。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Figure2.png" alt=""></p><blockquote><h4 id="（a）VoVNet"><a href="#（a）VoVNet" class="headerlink" title="（a）VoVNet"></a><strong>（a）VoVNet</strong></h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-VoVNet.png" alt=""></p><p><strong>VoVNet</strong>是一个基于连接的模型，由OSA组成，将 DenseNet 改进的更高效，区别于常见的plain结构和残差结构。</p><p>这种结构<strong>不仅继承了 DenseNet 的多感受野表示多种特征的优点</strong>，也解决了<strong>密集连接效率低下的问题</strong>。</p><h4 id="（b）CSPVoVNet"><a href="#（b）CSPVoVNet" class="headerlink" title="（b）CSPVoVNet"></a><strong>（b）CSPVoVNet</strong></h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-CSPVoVNet.png" alt=""></p><p><strong>(b)是(a)的CSP变体</strong>，CSPVoVNet除了考虑到参数量、计算量、计算密度、ShuffleNet v2提出的内存访问成本(输入输出通道比，架构分支数量，element-wise等)，还分析了梯度路径，可以让不同层的权重学习到更具有区分性的特征。</p><h4 id="（c）ELAN"><a href="#（c）ELAN" class="headerlink" title="（c）ELAN"></a><strong>（c）ELAN</strong></h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-ELAN.png" alt=""></p><p>(c)就<strong>“如何设计一个高效的网络”</strong>得出结论：<strong>通过控制最短最长的梯度路径，更深的网络可以更有效地学习和收敛。</strong></p><blockquote><p><strong>梯度路径设计的优点与缺点</strong></p><p>  <strong>梯度路径设计策略总共有3个优点：</strong></p><ol><li><strong>可以有效地使用网络参数</strong> ，在这一部分中提出通过调整梯度传播路径，不同计算单元的权重可以学习各种信息，从而实现更高的参数利用效率</li><li><p><strong>具有稳定的模型学习能力，</strong>由于梯度路径设计策略直接确定并传播信息以更新权重到每个计算单元，因此所设计的架构可以避免训练期间的退化</p><ol><li><strong>具有高效的推理速度</strong>，梯度路径设计策略使得参数利用非常有效，因此网络可以在不增加额外复杂架构的情况下实现更高的精度。<br>由于上述原因，所设计的网络在架构上可以更轻、更简单。</li></ol><p><strong>梯度路径设计策略有1个缺点：</strong></p></li><li><p>当梯度更新路径<strong>不是网络的简单反向前馈路径</strong>时，编程的难度将大大增加。</p></li></ol></blockquote><h4 id="d-E-ELAN"><a href="#d-E-ELAN" class="headerlink" title="(d) E-ELAN"></a>(d) E-ELAN</h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-E-ELAN.png" alt=""></p><p>(d)是作者提出的 <strong>Extended-ELAN (E-ELAN)</strong> ，E-ELAN 使用了 <strong>expand、shuffle、merge cardinality</strong>来实现对 ELAN 网络的增强。</p><p>在架构方面，E-ELAN <strong>只改变了计算块(computation blocks)的架构，而过渡层(transition layer)的架构完全没有改变</strong>。 </p><p><strong>YOLOv7 的策略是使用组卷积来扩展计算块的通道和基数</strong>。</p><p><strong>实现方法</strong></p><ul><li>首先，使用<strong>组卷积</strong>来增大<strong>通道</strong>和<strong>计算块的基数</strong>  (所有计算块使用的组参数及通道乘数都相同)</li><li>接着，将<strong>计算块得到的特征图 shuffle 到 g 个组</strong>，然后 <strong>concat</strong>，这样一来每个组中的特征图通道数和初始结构的通道数是相同的</li><li>最后，将 g 个组的特征都相加</li></ul></blockquote><h3 id="3-2-Model-scaling-for-concatenation-based-models—-基于连接的模型的模型缩放"><a href="#3-2-Model-scaling-for-concatenation-based-models—-基于连接的模型的模型缩放" class="headerlink" title="3.2 Model scaling for concatenation-based models— 基于连接的模型的模型缩放"></a>3.2 Model scaling for concatenation-based models— 基于连接的模型的模型缩放</h3><p>模型缩放的主要目的是调整模型的一些属性，生成不同尺度的模型，以满足不同推理速度的需求。 例如，EfficientNet [72] 的缩放模型考虑了宽度、深度和分辨率。至于 scaled-YOLOv4 [79]，其缩放模型是调整阶段数。 在 [15] 中，Dollar等人。分析了香草卷积和组卷积在进行宽度和深度缩放时对参数量和计算量的影响，并以此设计了相应的模型缩放方法。<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Figure3.png" alt=""></p><p> 以上方法主要用在PlainNet或ResNet等架构中。 这些架构在进行扩容或缩容时，每一层的入度和出度都不会发生变化，因此我们可以独立分析每个缩放因子对参数量和计算量的影响。 然而，如果将这些方法应用于基于连接的架构，我们会发现当对深度进行放大或缩小时，紧接在基于连接的计算块之后的平移层的入度将减小或 增加，如图3（a）和（b）所示。</p><p>从上述现象可以推断，对于基于串联的模型，我们不能单独分析不同的缩放因子，而必须一起考虑。 以scaling-up depth为例，这样的动作会导致transition layer的输入通道和输出通道的比例发生变化，这可能会导致模型的硬件使用率下降。 因此，我们必须为基于级联的模型提出相应的复合模型缩放方法。 当我们缩放一个计算块的深度因子时，我们还必须计算该块的输出通道的变化。 然后，我们将对过渡层进行等量变化的宽度因子缩放，结果如图3（c）所示。 我们提出的复合缩放方法可以保持模型在初始设计时的特性并保持最佳结构。</p><blockquote><h4 id="模型缩放的主要目的"><a href="#模型缩放的主要目的" class="headerlink" title="模型缩放的主要目的"></a>模型缩放的主要目的</h4><p>模型缩放的主要目的是<strong>调整模型的某些属性，并生成不同比例的模型</strong>，以满足不同推理速度的需要【像V5和YOLOX】</p><h4 id="问题引入"><a href="#问题引入" class="headerlink" title="问题引入"></a>问题引入</h4><p>每个比例因子的参数和计算是可以独立分析的。然而，如果将这些方法应用于基于连接的模型架构，我们会发现<strong>当对深度进行放大或缩小时，后续网络层的输入width发生变化，使后续层的输入channel和输出channel的ratio发生变化，紧接在基于连接的计算块之 后的转化层(translation layer)的入度将减小或增加</strong>，从而导致模型的硬件使用率下降如图(a)-&gt;(b)的过程。<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Figure4.png" alt=""></p><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p>作者对于连接模型，提出了一种<strong>复合模型方法</strong>，在考虑计算模块深度因子缩放的同时也考虑过渡层宽度因子做同等量的变化</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Figure5.png" alt=""></p><p>当对连接结构的网络进行尺度缩放时，只缩放<strong>计算块</strong>的深度，转换层的其余部分只进行宽度的缩放</p></blockquote><h2 id="四、Trainable-bag-of-freebies一可训练的bag-of-freebies"><a href="#四、Trainable-bag-of-freebies一可训练的bag-of-freebies" class="headerlink" title="四、Trainable bag-of-freebies一可训练的bag-of-freebies"></a>四、Trainable bag-of-freebies一可训练的bag-of-freebies</h2><h3 id="4-1-Planned-re-parameterized-convolution—卷积重参化"><a href="#4-1-Planned-re-parameterized-convolution—卷积重参化" class="headerlink" title="4.1 Planned re-parameterized convolution—卷积重参化"></a>4.1 Planned re-parameterized convolution—卷积重参化</h3><p>虽然 RepConv [13] 在 VGG [68] 上取得了优异的性能，但当我们直接将其应用于 ResNet [26] 和 DenseNet [32] 等架构时，其准确率会显着降低。 我们使用梯度流传播路径来分析重新参数化的卷积应该如何与不同的网络相结合。我们还相应地设计了计划的重新参数化卷积。</p><p>RepConv 实际上将 3×3 卷积、1×1 卷积和恒等连接组合在一个卷积层中。 在分析了 RepConv 和不同架构的组合和对应性能后，我们发现 RepConv 中的恒等连接破坏了 ResNet 中的残差和 DenseNet 中的连接，为不同的特征图提供了更多的梯度多样性。 基于以上原因，我们使用无身份连接的 RepConv (RepConvN) 来设计计划重参数化卷积的架构。 在我们的想法中，当一个带有残差或连接的卷积层被重新参数化的卷积代替时，应该没有恒等连接。 图 4 显示了我们设计的用于 PlainNet 和 ResNet 的“计划重新参数化卷积”的示例。 至于基于残差模型和基于连接模型的完整计划的重新参数化卷积实验，将在消融研究部分上进行介绍。</p><blockquote><h4 id="回顾RepVGG"><a href="#回顾RepVGG" class="headerlink" title="回顾RepVGG"></a>回顾RepVGG</h4><p>RepVGG是采用了VGG风格进行搭建的，采用了重参数化技术，因此叫RepVGG。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Figure22.png" alt=""></p><p><strong>A)是ResNet结构</strong>，最上面的部分的identity采用了1×1卷积，而RepVGG大体结构与ResNet相似</p><p><strong>(B)图是RepVGG训练时结构</strong>，借鉴了ResNet的residual block的结构，具体包括3×3、1×1、shortcut三个分支。</p><ul><li><strong>当输入输出的维度不一致</strong>，即stride=2时，则只有3×3、1×1两个分支</li><li><strong>当输入输出的维度一致时</strong>，在后三层卷积中不仅有1×1的identity connection，还有一个无卷积的直接进行特征融合的identity connection</li></ul><p><strong>(C)图是RepVGG测试时结构</strong>，会把这些连接全部去掉，就变成了一个单一的VGG结构，这种操作也被称为训练与预测的解耦合</p><h4 id="问题引入-1"><a href="#问题引入-1" class="headerlink" title="问题引入"></a>问题引入</h4><p>RepConv中带有的 identity connection破坏了ResNet中的残差和DenseNet中的concatenation，而我们知道残差和concatenation为不同的特征图提供了更多的梯度多样性，这样一来会导致精度下降。所以当一个带有残差或concatenation的卷积层被重参数的卷积所取代时，应该不存在 identity connection</p><h4 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h4><p>基于此，作者使用无identity connection的RepConv (RepConvN)来设计规划重参化卷积结构，如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Figure44.png" alt=""></p></blockquote><h3 id="4-2-Coarse-for-auxiliary-and-fine-for-lead-loss—辅助训练模块"><a href="#4-2-Coarse-for-auxiliary-and-fine-for-lead-loss—辅助训练模块" class="headerlink" title="4.2 Coarse for auxiliary and fine for lead loss—辅助训练模块"></a>4.2 Coarse for auxiliary and fine for lead loss—辅助训练模块</h3><p>我的个人理解就是：</p><p><strong>Lead head</strong>是负责<strong>最终输出</strong>的 head ；<strong>Auxiliary head</strong>是负责<strong>辅助训练</strong>的 head</p><p>那么这个标题就是在说负责辅助训练的head用粗标签，负责最终输出的head用细标签</p><h4 id="4-2-1-Deep-supervision—深度监督"><a href="#4-2-1-Deep-supervision—深度监督" class="headerlink" title="4.2.1 Deep supervision—深度监督"></a>4.2.1 Deep supervision—深度监督</h4><p>深度监督 [38] 是一种经常用于训练深度网络的技术。 它的主要概念是在网络的中间层添加额外的辅助头，以辅助损失为指导的浅层网络权重。 即使对于 ResNet [26] 和 DenseNet [32] 等通常收敛良好的架构，深度监督 [70, 98, 67, 47, 82, 65, 86, 50] 仍然可以显着提高模型在许多任务上的性能 . 图 5 (a) 和 (b) 分别显示了“没有”和“有”深度监督的目标检测器架构。 在本文中，我们将负责最终输出的head称为lead head，用于辅助训练的head称为辅助head。<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Figure55.png" alt=""></p><blockquote><p>深度监督是一种常用于训练深度网络的技术。其主要概念是在网络的中间层增加额外的Auxiliary head，以及以Auxiliary损失为导向的浅层网络权值。</p><p>下图(A)是无深度监督，  (B)是有深度监督</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-%E7%9B%91%E7%9D%A3.png" alt=""></p><h4 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h4><p>将负责<strong>最终输出的head</strong>为<strong>Lead head</strong>，将用于<strong>辅助训练的head</strong>称为<strong>Auxiliary head</strong>。</p></blockquote><h4 id="4-2-2-Label-assigner—-标签分配器"><a href="#4-2-2-Label-assigner—-标签分配器" class="headerlink" title="4.2.2 Label assigner— 标签分配器"></a>4.2.2 <strong>Label assigner— 标签分配器</strong></h4><p>接下来我们要讨论标签分配的问题。 过去，在深度网络的训练中，标签分配通常直接指的是ground truth，并根据给定的规则生成hard  label。 然而，近年来，如果我们以物体检测为例，研究人员往往会利用网络预测输出的质量和分布，然后与ground truth一起考虑使用一些计算和优化方法来生成可靠的soft label [61、8、36、99、91、44、43、90、20、17、42]。例如，YOLO [61] 使用预测边界框回归和地面实况的 IoU 作为 objectness 的soft label。 在本文中，我们将网络预测结果与基本事实一起考虑，然后分配soft label的机制称为“标签分配器”。</p><p>无论auxiliary head or lead head的情况如何，都需要对目标目标进行深度监督。 在soft label分配器相关技术的过程中，我们无意中发现了一个新的衍生问题，即“如何将soft label分配给auxiliary head and lead head？” 据我们所知，到目前为止，相关文献还没有探讨过这个问题。 目前最流行的方法的结果如图5（c）所示，就是将辅助head和lead head分离出来，然后利用各自的预测结果和ground truth进行标签分配。 本文提出的方法是一种新的标签分配方法，通过前导头预测来指导辅助头和前导头。换句话说，我们使用引导头预测作为指导来生成从coarse-to-fine的分层标签，这些标签分别用于辅助头部和引导头学习。 提出的两种深度监督标签分配策略分别如图 5 (d) 和 (e) 所示。</p><blockquote><h4 id="以前方法"><a href="#以前方法" class="headerlink" title="以前方法"></a>以前方法</h4><p> 过去，在深度网络的训练中，标签分配通常直接参考GT，并根据给定的规则生成hard label</p><h4 id="本文方法-1"><a href="#本文方法-1" class="headerlink" title="本文方法"></a>本文方法</h4><p> 作者提出一个“label assigner(标签分配器)” 机制，该机制将网络预测结果与GT一起考虑，然后分配soft label</p><blockquote><h4 id="关于hard-label和soft-label："><a href="#关于hard-label和soft-label：" class="headerlink" title="关于hard label和soft label："></a>关于hard label和soft label：</h4><ul><li>hard label：有些论文中也称为hard target ，其实这也是借鉴了知识蒸馏的思想，hard字面意思上就可以看出比较强硬，是就是，不是就是不是，标签形式：(1,2,3…)或(0,1,0,0…)【举个栗子：要么是猫要么是狗】</li><li>soft label：soft是以概率的形式来表示。可理解为对标签的平滑也即软化，比如像[0.6,0.4]，【举个栗子：有60%的概率是猫，  40%的概率是狗】，就好像不会给你非常明确的回答。</li></ul><p>现在比较流行的结构中，经常会将网络输出的数据分布通过一定的优化方法等与GT进行匹配生成soft label（其实我们熟悉的经过softmax的或者sigmod输出就是一种soft label）。</p></blockquote><h4 id="问题引入-2"><a href="#问题引入-2" class="headerlink" title="问题引入"></a>问题引入</h4><p>如何为Auxiliary head和Lead head分配soft label？</p><h4 id="目前方法"><a href="#目前方法" class="headerlink" title="目前方法"></a>目前方法</h4><p>如(c)所示，这是独立标签匹配结构，将Auxiliary head和Lead head分离，然后使用它们自己的预测结果和真实标签来进行标签分配。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-IndepA.png" alt=""></p></blockquote><h4 id="4-2-3-Lead-head-guided-label-assigner—-先导头导向标签分配器"><a href="#4-2-3-Lead-head-guided-label-assigner—-先导头导向标签分配器" class="headerlink" title="4.2.3 Lead head guided label assigner— 先导头导向标签分配器"></a>4.2.3 Lead head guided label assigner— 先导头导向标签分配器</h4><p><strong>(Lead head guided label assigner) lead head导向标签分配器</strong> 主要基于lead head的预测结果和GT来计算，并通过优化过程生成soft标签。这组soft标签将用作辅助head和lead head的训练。这样做的原因是因为lead head具有相对较强的学习能力，因此由其生成的soft标签应更能代表源数据和目标之间的分布和相关性。此外，我们可以将这种学习视为一种广义残差学习。通过让较浅的辅助head直接学习lead head已经学习的信息，lead head将更能够专注于学习尚未学习的剩余(residual )信息。</p><blockquote><h4 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h4><p>  主要基于<strong>Lead head的预测结果和GT</strong>来计算，并通过优化过程生成<strong>soft label</strong> 。这组<strong>soft label</strong> 将用作Auxiliary head和Lead head的训练。</p><h4 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h4><ul><li>Lead head 的学习能力更强一些，所以从它当中得到的soft label在数据集的数据概率中更具有代表性。</li><li>Lead head 能够只学习 Auxiliary head 没有学习到的特征。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-LeadGuA.png" alt=""></p></blockquote><h4 id="4-2-4-Coarse-to-fine-lead-head-guided-label-assigner—-由粗到精的-lead-head-指导标签分配器"><a href="#4-2-4-Coarse-to-fine-lead-head-guided-label-assigner—-由粗到精的-lead-head-指导标签分配器" class="headerlink" title="4.2.4 Coarse-to-fine lead head guided label assigner— 由粗到精的 lead head 指导标签分配器"></a>4.2.4 Coarse-to-fine lead head guided label assigner— 由粗到精的 lead head 指导标签分配器</h4><p>(Coarse-to-fine lead head guided label assigner)由粗至细的lead head引导标签分配器 ，也使用lead head的预测结果和GT生成soft标签。然而，在此过程中，生成了两组不同的soft标签，即粗标签和细标签，其中细标签与lead head引导标签分配器生成的soft标签相同，并且通过放松正样本分配过程的约束，允许更多grid被视为正目标来生成粗标签。其原因是辅助head的学习能力不如lead head强，为了避免丢失需要学习的信息，将重点优化目标检测任务中辅助head的召回。对于lead head的输出，可以从高召回率结果中过滤出高精度结果作为最终输出。但是必须注意，如果粗标签的附加权重与细标签很接近，它可能在最终预测时产生不良先验。因此为了使这些超粗正网格具有较小的影响，在解码器中设置了限制，使得超粗正grid不能完美地产生soft标签。上述机制允许在学习过程中动态调整细标签和粗标签的重要性，并且使细标签的优化上界总是高于粗标签。</p><blockquote><h4 id="具体方法-1"><a href="#具体方法-1" class="headerlink" title="具体方法"></a>具体方法</h4><p>在这个过程中生成了两组不同的soft label，即粗标签和细标签</p><ul><li>细标签与Lead head 在标签分配器上生成的soft label相同</li><li>粗标签是通过放宽认定positive target的条件生成的，也就是允许更多的grids作为positive target</li></ul><blockquote><p>粗标签和细标签究竟是什么？<br>首先是细标签，网络最终输出的三个head是Lead head，会将这部分的预测结果与ground truth生成soft label，网络会觉得这个soft label得到是数据分布更接近真实的数据分布，训练得到的内容更加“细致”，<br>再来说说粗标签，Auxiliary head由于是从中间网络部分得到的，它的预测效果肯定是没有深层网络Lead head提取到的数据或者特征更细致，所以Auxiliary head部分的内容是比较“粗糙”的，在训练过程中，会将Lead head与ground truth的soft label当成一个全新的ground truth，然后与Auxiliary head之间建立损失函数，说白了，就是让辅助head的预测结果也“近似”为Lead head<br>—————————以上引用@爱吃肉的鹏 的解读，感谢大佬！</p></blockquote><h4 id="原因-1"><a href="#原因-1" class="headerlink" title="原因"></a>原因</h4><p>Auxiliary head 的学习能力没有Lead head 那么强大，为了避免信息丢失，需要接收更多的信息来学习，在目标检测任务中， Auxiliary head 需要有很高的 recall。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Coarse.png" alt=""></p><p><strong>【注意】</strong>如果粗label非常接近与细label，可能会产生不好的结果</p><p><strong>【解决方法】</strong> 为了让超粗正grids(Auxiliary head 的正标签)影响小一些，在 decoder 中设置了限制条件，所以那些超粗正grids不能产生 soft label，这样就能在训练的过程中，让grids自动的调节粗标签和细标签的重要程度。</p></blockquote><h3 id="4-3-Other-trainable-bag-of-freebies—其他的可训练“工具”"><a href="#4-3-Other-trainable-bag-of-freebies—其他的可训练“工具”" class="headerlink" title="4.3 Other trainable bag-of-freebies—其他的可训练“工具”"></a>4.3 Other trainable bag-of-freebies—其他的可训练“工具”</h3><p>本节中，我们将列出一些可训练的免费赠品包。 这些免费赠品是我们在训练中使用的一些技巧，但最初的概念并不是我们提出的。 这些freebies的训练细节将在附录中详细说明，包括（1）conv-bn-activation topology中的Batch normalization：这部分主要将batch normalization layer直接连接到convolutional layer。这样做的目的是在推理阶段将批归一化的均值和方差整合到卷积层的偏差和权重中。  (2) YOLOR[81]中的隐式知识与卷积特征图相结合的加法和乘法方式：YOLOR中的隐式知识可以在推理阶段通过预计算简化为向量。 该向量可以与前一个或后一个卷积层的偏差和权重相结合。  (3) EMA 模型：EMA 是一种在 mean teacher [75] 中使用的技术，在我们的系统中，我们使用 EMA 模型纯粹作为最终的推理模式</p><blockquote><p>(1) conv-bn-activation 结构组合中的批量归一化</p><p>(2)  YOLOR中的隐式知识与卷积特征图以及加法和乘法方式相结合</p><p>(3)  EMA模型</p><p>这个详细解读大家看上面翻译就好~</p></blockquote><h2 id="五、Experiments—实验"><a href="#五、Experiments—实验" class="headerlink" title="五、Experiments—实验"></a>五、Experiments—实验</h2><h3 id="5-1-Experimental-setup—实验步骤"><a href="#5-1-Experimental-setup—实验步骤" class="headerlink" title="5.1 Experimental setup—实验步骤"></a>5.1 Experimental setup—实验步骤</h3><p>我们使用 Microsoft COCO 数据集进行实验并验证我们的对象检测方法。 我们所有的实验都没有使用预训练模型。 也就是说，所有模型都是从头开始训练的。 在开发过程中，我们使用train 2017 set进行训练，然后使用val 2017 set进行验证和选择超参数。 最后，我们展示了对象检测在 2017 年测试集上的性能，并将其与最先进的对象检测算法进行了比较。 详细的训练参数设置在附录中描述。</p><p>我们设计了边缘 GPU、普通 GPU 和云 GPU 的基本模型，它们分别称为 YOLOv7tiny、YOLOv7 和 YOLOv7-W6。 同时，我们还针对不同的业务需求，使用基础模型进行模型缩放，得到不同类型的模型。 对于YOLOv7，我们对颈部进行stack scaling，并使用提出的复合缩放方法对整个模型的深度和宽度进行缩放，并以此获得YOLOv7-X。 对于 YOLOv7-W6，我们使用新提出的复合缩放方法得到 YOLOv7-E6 和 YOLOv7-D6。 此外，我们将提议的 EELAN 用于 YOLOv7-E6，从而完成了 YOLOv7E6E。 由于 YOLOv7-tiny 是一个面向边缘 GPU 的架构，它会使用leaky ReLU 作为激活函数。 至于其他模型，我们使用 SiLU 作为激活函数。 我们将在附录中详细描述每个模型的比例因子。</p><blockquote><ul><li>数据集： COCO 数据集</li><li>预训练模型： 无，从0开始训练</li><li><p>不同GPU和对应模型：</p><ul><li><p>边缘GPU：YOLOv7-tiny</p></li><li><p>普通GPU：YOLOv7</p></li><li><p>云GPU的基本模型： YOLOv7-W6</p></li></ul></li><li><p>激活函数：</p><ul><li><p>YOLOv7 tiny： leaky ReLU</p></li><li><p>其他模型：SiLU</p></li></ul></li></ul></blockquote><h3 id="5-2-Baselines—基线网络"><a href="#5-2-Baselines—基线网络" class="headerlink" title="5.2 Baselines—基线网络"></a>5.2 Baselines—基线网络</h3><p>我们选择以前版本的 YOLO [3, 79] 和最先进的目标检测器 YOLOR [81] 作为我们的基线。 表 1 显示了我们提出的 YOLOv7 模型与使用相同设置训练的基线的比较。</p><p>从结果我们看到，如果与 YOLOv4 相比，YOLOv7 的参数减少了 75%，计算量减少了 36%，AP 提高了 1.5%。 如果与 state-of-theart YOLOR-CSP 相比，YOLOv7 的参数减少了 43%，计算量减少了 15%，AP 提高了 0.4%。 在tiny模型的性能上，与YOLOv4-tiny-31相比，YOLOv7tiny参数数量减少了39%，计算量减少了49%，但AP保持不变。在云 GPU 模型上，我们的模型仍然可以有更高的 AP，同时减少 19% 的参数数量和 33% 的计算量。</p><blockquote><p>选择以前版本的YOLO[YOLOv4 ，Scaled-YOLOv4]和最先进的目标检测器YOLOR作为基线。表1展示了本文提出的YOLOv7模型与使 用相同设置训练的基线模型的比较。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Table1.png" alt=""></p><p>表1：基线目标探测器的比较</p><p>结论：具体数值就不重复了，通过对比我们可以看出参数量和计算量都有减少，精确度提高了。</p></blockquote><h3 id="5-3-Comparison-with-state-of-the-arts—与其他流行网络的对比"><a href="#5-3-Comparison-with-state-of-the-arts—与其他流行网络的对比" class="headerlink" title="5.3 Comparison with state-of-the-arts—与其他流行网络的对比"></a>5.3 Comparison with state-of-the-arts—与其他流行网络的对比</h3><p>我们将所提出的方法与用于通用 GPU 和移动 GPU 的最先进的目标检测器进行比较，结果如表 2 所示。从表 2 的结果中，我们知道所提出的方法具有最佳的速度-准确度权衡 - 全面关闭。 如果我们将 YOLOv7-tiny-SiLU 与 YOLOv5-N (r6.1) 进行比较，我们的方法在 AP 上的速度提高了 127 fps，准确率提高了 10.7%。 此外，YOLOv7 161 fps 的帧率有 51.4% AP，而相同 AP 的 PPYOLOE-L 只有 78 fps 的帧率。 在参数使用方面，YOLOv7 比 PPYOLOE-L 少 41%。 如果我们将推理速度为 114 fps 的 YOLOv7-X 与推理速度为 99 fps 的 YOLOv5-L (r6.1) 进行比较，YOLOv7-X 可以将 AP 提高 3.9%。 如果将 YOLOv7X 与类似规模的 YOLOv5-X (r6.1) 进行比较，YOLOv7-X 的推理速度快了 31 fps。 此外，在参数量和计算量方面，YOLOv7-X 相比 YOLOv5-X（r6.1）减少了 22% 的参数和 8% 的计算量，但 AP 提升了 2.2%。</p><p>如果我们使用输入分辨率 1280 比较 YOLOv7 和 YOLOR，YOLOv7-W6 的推理速度比 YOLOR-P6 快 8 fps，检测率也提高了 1% AP。 至于YOLOv7-E6和YOLOv5-X6（r6.1）的对比，前者AP增益比后者高0.9%，参数少45%，计算量少63%，推理速度提升47%。YOLOv7-D6 的推理速度与 YOLOR-E6 接近，但 AP 提高了 0.8%。  YOLOv7-E6E 的推理速度与 YOLOR-D6 接近，但 AP 提高了 0.3%。</p><blockquote><p>本文将提出的方法与通用GPU和移动GPU的最先进的对象检测器进行了比较，结果如表2所示：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Table2.png" alt=""></p><p>结论：反正是打败了它们，目前我们最牛就对咯</p></blockquote><h3 id="5-4-Ablation-study—消融研究"><a href="#5-4-Ablation-study—消融研究" class="headerlink" title="5.4 Ablation study—消融研究"></a>5.4 Ablation study—消融研究</h3><h4 id="5-4-1-Proposed-compound-scaling-method—提出的复合-scaling-方法"><a href="#5-4-1-Proposed-compound-scaling-method—提出的复合-scaling-方法" class="headerlink" title="5.4.1 Proposed compound scaling method—提出的复合 scaling 方法"></a>5.4.1 Proposed compound scaling method—提出的复合 scaling 方法</h4><p>显示了使用不同模型缩放策略进行缩放时获得的结果。 其中，我们提出的复合缩放方法是将计算块的深度放大1.5倍，将过渡块的宽度放大1.25倍。 如果我们的方法与仅按比例放大宽度的方法相比，我们的方法可以以更少的参数和计算量将 AP 提高 0.5%。 如果我们的方法与只放大深度的方法相比，我们的方法只需增加2.9%的参数数量和1.2%的计算量，就可以提高0.2%的AP。</p><blockquote><p>表3显示了使用不同模型缩放策略进行缩放时获得的结果：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Table3.png" alt=""></p><p>结论：复合缩放策略可以更有效地利用参数和计算。</p></blockquote><h4 id="5-4-2-Proposed-planned-re-parameterized-model—提出的-planned-re-parameterized-模型"><a href="#5-4-2-Proposed-planned-re-parameterized-model—提出的-planned-re-parameterized-模型" class="headerlink" title="5.4.2 Proposed planned re-parameterized model—提出的 planned re-parameterized 模型"></a>5.4.2 Proposed planned re-parameterized model—提出的 planned re-parameterized 模型</h4><p>为了验证我们提出的平面重参数化模型的普遍性，我们分别将其用于基于连接的模型和基于残差的模型进行验证。 我们选择用于验证的基于连接的模型和基于残差的模型分别是 3-stacked ELAN 和 CSPDarknet。</p><p>在基于 concatenation 的模型实验中，我们将 3-stacked ELAN 中不同位置的 3×3 卷积层替换为 RepConv，详细配置如图 6 所示。从表 4 所示的结果可以看出，所有更高 AP 值存在于我们提出的计划重新参数化模型中。</p><p>在处理基于残差的模型的实验中，由于原始dark block没有 3 × 3 con卷积块符合我们的设计策略，我们额外设计了一个反向dark block进行实验，其架构如图 7 所示。由于具有dark block和反向dark block的 CSPDarknet 具有完全相同的参数和操作量，所以它是 公平比较。 表 5 所示的实验结果充分证实了所提出的计划重新参数化模型在基于残差的模型上同样有效。 我们发现 RepCSPResNet [85] 的设计也符合我们的设计模式</p><blockquote><h4 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h4><p>为了验证提出的 planned re-parameterized 模型的通用性，将其分别用于基于连接的模型和基于残差的模型进行验证。</p><h4 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h4><p>三层ELAN和CSPDarknet</p><p>将3层堆叠ELAN中不同位置的3 × 3卷积层替换为RepConv，详细配置如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-F6T4.png" alt=""></p><p><strong>结论：</strong>从表4所示的结果中，看到所有较高的AP值都出现在提出的Planned RepConcatenation 模型上。</p><p>在处理基于残差模型的实验中，由于原始dark block没有3×3的卷积块，作者另外设计了一种反向dark block，其体系结构如图7所示：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-F7T5.png" alt=""></p><p><strong>结论：</strong>表5所示的实验结果证实了所提出的重参化策略对于residual的模型依旧有效。 RepCSPResNet的设计也符合本文的设计模式</p></blockquote><h4 id="5-4-3-Proposed-assistant-loss-for-auxiliary-head—提出的-auxiliary-head-的-assistant-loss"><a href="#5-4-3-Proposed-assistant-loss-for-auxiliary-head—提出的-auxiliary-head-的-assistant-loss" class="headerlink" title="5.4.3 Proposed assistant loss for auxiliary head—提出的 auxiliary head 的 assistant loss"></a>5.4.3 Proposed assistant loss for auxiliary head—提出的 auxiliary head 的 assistant loss</h4><p>在辅助头部实验的辅助损失中，我们比较了引导头部和辅助头部方法的一般独立标签分配，并且我们还比较了两种提出的引导引导标签分配方法。 我们在表 6 中展示了所有比较结果。从表 6 中列出的结果可以看出，任何增加辅助损失的模型都可以显着提高整体性能。 此外，我们提出的引导式标签分配策略比 AP、AP50 和 AP75 中的一般独立标签分配策略具有更好的性能。 至于我们提出的粗略辅助和精细引导标签分配策略，它在所有情况下都会产生最佳结果。 在图 8 中，我们展示了在辅助头和前导头上通过不同方法预测的对象图。 从图 8 中我们发现，如果辅助头部学习引导引导soft label，它确实会帮助引导头部从一致目标中提取残差信息。</p><p>由于提出的 YOLOv7 使用多个金字塔来联合预测目标检测结果，我们可以直接将辅助头连接到中间层的金字塔进行训练。 这种训练可以弥补下一层金字塔预测中可能丢失的信息。由于上述原因，我们在提议的 E-ELAN 架构中设计了部分辅助头。 我们的方法是在合并基数之前在其中一组特征图之后连接辅助头，这种连接可以使新生成的一组特征图的权重不会被辅助损失直接更新。 我们的设计允许每个铅头金字塔仍然从不同大小的物体中获取信息。 表 8 显示了使用两种不同方法获得的结果，即粗到细导联方法和部分粗到细导联方法。 显然，部分由粗到细的导引法具有更好的辅助效果。</p><blockquote><p>作者比较了lead head和auxiliary head的独立标签分配策略，同时也比较了所提出的引导型标签分配方法，在表6中显示了所有的比较结果：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/YOLOV7-Table6.png" alt=""></p><p><strong>结论：</strong>从表6中列出的结果可以看出，任何增加辅助损失的模型都可以显著提高整体性能。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Figure8.png" alt=""></p><p><strong>结论：</strong>如果auxiliary head学习先导引导soft label，它确实会帮助lead head从一致目标中提取残差信息。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Table7.png" alt=""></p><p><strong>结论：</strong> 从表中的数字来看，通过距离目标中心的大小来约束目标的上界可以获得更好的性能。<br><strong>方法：</strong> E-ELAN 架构中设计了 partial auxiliary head 。在合并基数（ cardinality ）之前，在一组特征图后连接 auxiliary head ，这种连接可以使新生成的特征图集的权值不直接通过辅助损失来更新<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Table8.png" alt=""></p><p><strong>结论：</strong> partial auxiliary head方法具有更好的辅助效果。</p></blockquote><h2 id="六、Conclusions—结论"><a href="#六、Conclusions—结论" class="headerlink" title="六、Conclusions—结论"></a>六、Conclusions—结论</h2><p>在本文中，我们提出了一种新的实时目标检测器架构和相应的模型缩放方法。 此外，我们发现目标检测方法的发展过程产生了新的研究课题。 在研究过程中，我们发现了重新参数化模块的替换问题和动态标签分配的分配问题。 为了解决这个问题，我们提出了可训练的bag-of-freebies方法来提高目标检测的准确性。 基于上述，我们开发了 YOLOv7 系列目标检测系统，该系统获得了 state-of-the-art 的结果。</p><blockquote><p>本文提出了一种新的实时检测器，解决了重参化模块的替换问题和动态标签的分配问题。</p><h4 id="主要工作："><a href="#主要工作：" class="headerlink" title="主要工作："></a>主要工作：</h4><p><strong>（ 1 ）高效聚合网络架构：</strong> YOLOV7 对 ELAN 进行扩展，提出的一个新的网络架构 E-ELAN ，以高效为主<br><strong>（ 2 ）重参数化卷积：</strong> YOLOV7 将模型重参数化引入到网络架构中，重参数化这一思想最早出现于 REPVGG 中<br><strong>（ 3 ）辅助头检测：</strong> YOLOv7 中将 head 部分的浅层特征提取出来作为 Auxiliary head ，深层特征也就是网络的最终输出作为 Lead head<br><strong>（ 4 ）基于连接的模型缩放：</strong> 作者对于连接模型提出了一种复合模型方法，当对连接结构的网络进行尺度缩放时，只缩放计算块的深度，转换层的其余部分只进行宽度的缩放<br><strong>（ 5 ）动态标签分配策略：</strong> Lead head 导向标签分配方法和由粗到精的 lead head 指导标签分配方法</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>yolov6-paper</title>
      <link href="/yolov6-paper/"/>
      <url>/yolov6-paper/</url>
      
        <content type="html"><![CDATA[<h1 id="YOLOv6-A-Single-Stage-Object-Detection-Framework-for-IndustrialApplications"><a href="#YOLOv6-A-Single-Stage-Object-Detection-Framework-for-IndustrialApplications" class="headerlink" title="YOLOv6: A Single-Stage Object Detection Framework for IndustrialApplications"></a>YOLOv6: A Single-Stage Object Detection Framework for IndustrialApplications</h1><h2 id="Abstract—摘要"><a href="#Abstract—摘要" class="headerlink" title="Abstract—摘要"></a>Abstract—摘要</h2><p>多年来，YOLO系列一直是高效物体检测的事实上的工业级标准。YOLO社区以压倒性的优势丰富了它在众多硬件平台和丰富场景中的应用。在这份技术报告中，我们努力把它的极限推到一个新的水平，以坚定不移的心态向行业应用迈进。考虑到现实环境中对速度和准确性的不同要求，我们广泛地研究了来自工业界或学术界的最新的物体检测进展。具体来说，我们大量吸收了最近的网络设计、训练策略、测试技术、量化和优化方法的思想。在此基础上，我们整合了我们的想法和实践，建立了一套不同规模的可部署的网络，以适应多样化的用例。在YOLO作者的慷慨许可下，我们将其命名为YOLOv6。我们也表示热烈欢迎用户和贡献者的进一步改进。对于性能的表现，我们的YOLOv6-N在COCO数据集上达到了35.9%的AP，在NVIDIA Tesla T4 GPU上的吞吐量为1234 FPS。YOLOv6-S以495 FPS的速度达到了43.5%的AP，超过了其他相同规模的主流检测器（YOLOv5-S、YOLOX-S和PPYOLOE-S）。我们的量化版本YOLOv6-S甚至在869 FPS时带来了新的最先进的43.3%AP。此外，YOLOv6-M/L也比其他具有类似推理速度的检测器取得了更好的准确性表现（即49.5%/52.3%）。我们仔细进行了实验来验证每个组件的有效性。</p><blockquote><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a><strong>背景</strong></h4><p>YOLO系列如今已广泛应用在工业方面</p><h4 id="本文主要工作"><a href="#本文主要工作" class="headerlink" title="本文主要工作"></a><strong>本文主要工作</strong></h4><p>YOLOv6大量地吸收了最近的网络设计、训练策略、测试技术、量化和优化方法的想法。（也就是说没有吸睛创新点，主要做的也是缝合和堆砌）</p><h4 id="最终效果"><a href="#最终效果" class="headerlink" title="最终效果"></a><strong>最终效果</strong></h4><ul><li>精度更高：YOLOv6-N在COCO数据集上达到了35.9%的AP，在NVIDIA Tesla T4 GPU上的吞吐量为1234 FPS。</li><li>速度更快：YOLOv6-S以495 FPS的速度达到了43.5%的AP，超过了其他相同规模的主流检测器(YOLOv5-S、YOLOX-S和 PPYOLOE-S)。</li><li>量化版本也有提高：量化版本YOLOv6-S甚至在869 FPS时带来了新的最先进的43.3%AP。</li><li>其余版本：YOLOv6-M/L也比其他具有类似推理速度的检测器取得了更好的准确性表现(即49.5%/52.3%)。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Figure1.png" alt=""></p></blockquote><h2 id="一、Introduction——介绍"><a href="#一、Introduction——介绍" class="headerlink" title="一、Introduction——介绍"></a>一、Introduction——介绍</h2><p>YOLO系列一直是工业应用中最受欢迎的检测框架，因为它在速度和精度之间有很好的平衡。YOLO系列的开创性工作是YOLOv1-3[32-34]，它开辟了one-stage检测器的新道路，并在后来进行了大幅改进。YOLOv4[1]将检测框架重组为几个独立的部分（backbone, neck and head），并在当时验证了bag-of-freebies和bag-of-specials，设计了一个适合在单个GPU上训练的框架。目前，YOLOv5[10]、YOLOX[7]、PPY-OLOE[44]和YOLOv7[42]都是可以部署的高效检测器的竞争对象。不同尺寸的模型通常是通过缩放技术获得的。</p><p>在这份报告中，我们从经验上观察到几个重要的因素，促使我们重新装修YOLO框架：（1）来自RepVGG[3]的重新参数化是一种优越的技术，在检测中还没有得到很好的利用。我们还注意到，RepVGG块的简单模型扩展变得不切实际，为此我们认为小网络和大网络之间网络设计的优雅一致性是不必要的。对于小型网络来说，普通的单路径架构是一个较好的选择，但对于大型模型来说，参数的指数增长和单路径架构的计算成本使其不可行；（2）基于重参数化的检测器的量化也需要细致的处理，否则在训练和推理过程中，由于其异质配置导致的性能下降将是难以处理的。(3) 以前的工作[7, 10, 42, 44]往往不太注意部署，其延迟通常是在V100这样的高成本机器上进行比较。当涉及到真正的服务环境时，存在着硬件差距。通常情况下，像Tesla T4这样的低功耗GPU成本较低，并提供相当好的推理性能。(4) 考虑到架构上的差异，先进的特定领域策略，如标签分配和损失函数设计，需要进一步验证；(5) 对于部署，我们可以容忍训练策略的调整，以提高准确率性能，但不增加推理成本，如知识提炼。</p><p>考虑到上述意见，我们带来了YOLOv6的诞生，它在准确性和速度方面完成了迄今为止的最佳权衡。我们在图1中展示了YOLOv6与其他同行在类似规模下的比较。为了在不降低性能的情况下提高推理速度，我们研究了最先进的量化方法，包括训练后量化（PTQ）和量化感知训练（QAT），并将其纳入YOLOv6，以实现部署就绪的目标。网络的目标。</p><blockquote><h4 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a><strong>背景</strong></h4><p>(1)来自RepVGG的重参数化是一种优越的技术，在检测(已有的yolo版本)中尚未得到很好的利用。同时，作者认为小型网络和大型网络的设计不一样，对大型网络来说，对RepVGG块进行简单地模型缩放不切实际</p><p>(2)基于重参数化的检测器的量化需要细致的处理</p><p>(3)以前的工作往往不太关注部署，其延迟通常在 V100 等高成本机器上进行比较。在实际服务环境方面存在硬件差距</p><p>(4)标签分配和损失函数设计，需要进一步验证考虑架构的差异</p><p>(5)对于部署，训练过程可以使用知识蒸馏等策略，但不增加推理成本</p><h4 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a><strong>贡献</strong></h4><p>(1)为不同场景应用定制不同规模的模型，小模型以普通的单路径主干为特征，而大模型建立在高效的多分支块上    </p><p>(2)加入了自蒸馏策略，同时执行了分类任务和回归任务</p><p>(3)融合了各种先进tricks，如：标签分配检测技术、损失函数和数据增强技术</p><p>(4)在重新优化器和通道蒸馏的帮助下改革了定量检测方案，得到了一个更好的探测器</p></blockquote><h2 id="二、Method——方法"><a href="#二、Method——方法" class="headerlink" title="二、Method——方法"></a>二、Method——方法</h2><p>YOLOv6的重新设计由以下部分组成，网络设计、标签分配、损失函数、数据增强、适合工业界的改进，以及量化和部署：</p><ul><li>网络设计：Backbone：与其他主流架构相比，我们发现RepVGG[3]骨干网络在推理速度相近的情况下，在小型网络中具备更多的特征表示能力，而由于参数和计算成本的爆炸性增长，它很难被扩展以获得更大的模型。在这方面，我们把RepBlock[3]作为我们小型网络的构建模块。对于大型模型，我们修改了一个更有效的CSP[43]块，名为CSPStackRep块。Neck：YOLOv6的颈部采用了YOLOv4和YOLOv5之后的PAN拓扑结构[24]。我们用RepBlocks或CSPStackRep Blocks来增强颈部，以实现Rep-PAN。Head：我们简化了解耦头，使其更加有效，称为高效解耦头。</li><li>标签的分配：我们通过大量的实验评估了最近在YOLOv6上的标签分配策略[5, 7, 18, 48, 51]的进展，结果表明TAL[5]更加有效和 训练友好。<br>损失函数：主流anchor-free目标检测器的损失函数包含分类损失、box回归损失和object损失。对于每个损失，我们用所有可用的技术进行了系统的实验，最后选择VariFocal损失[50]作为我们的分类损失，SIoU[8]/GIoU[35]损失作为我们的回归损失。</li><li>适合工业界的改进：我们引入了更多常见的做法和技巧来提高性能，包括自我蒸馏和更多的训练epoch。对于自蒸馏，分类和回归都分别由教师模型进行监督。回归的蒸馏是由于DFL[20]而实现的。此外，软标签和硬标签的信息比例通过余弦衰减动态下降，这有助于学生在训练过程中的不同阶段选择性地获取知识。此外，我们还遇到了在评估时不添加额外的灰边而导致性能受损的问题，对此我们提供了一些补救措施。我们提供了一些补救措施。</li><li>量化和部署：为了解决基于重新参数化的模型量化时的性能下降问题，我们用RepOptimizer[2]训练YOLOv6，以获得PTQ友好的权重。我们进一步采用QAT与信道精馏[36]和图优化来追求极致的性能。我们的量化YOLOv6-S以42.3%的AP和869 FPS的吞吐量（批次大小=32）达到了一个新的技术水平。</li></ul><h3 id="2-1-Network-Design—-网络设计"><a href="#2-1-Network-Design—-网络设计" class="headerlink" title="2.1 Network Design— 网络设计"></a>2.1 Network Design— 网络设计</h3><p>一个单阶段目标检测器一般由以下部分组成：backbone、neck和head。主干部分主要决定了特征表示能力，同时，由于它承载了很大一部分计算成本，所以它的设计对推理效率有关键影响。neck用于将低层次的物理特征与高层次的语义特征聚合在一起，然后在各个层次建立金字塔特征图。头部由几个卷积层组成，它根据neck集合的多层次特征预测最终的检测结果。从结构上看，它可以分为 anchor-based 和 anchor-free的，或者说是参数耦合头和参数解耦头。</p><p>在YOLOv6中，基于硬件友好型网络设计的原则[3]，我们提出了两个可扩展的可重新参数化的骨干和颈部，以适应不同规模的模型，以及一个高效的混合通道策略的解耦头。</p><blockquote><p>单阶段物体探测器的组成：</p><ul><li><strong>主干网络：</strong>主要决定了特征表示能力 </li><li><strong>颈部：</strong>用于将低级的物理特征与高级的语义特征进行聚合，然后在所有层次上建立金字塔形特征映射</li><li><strong>头部：</strong>由多个卷积层组成，并根据颈部组装的多级特征预测最终检测结果</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Figure2.png" alt=""></p></blockquote><h4 id="2-1-1-Backbone—主干网络"><a href="#2-1-1-Backbone—主干网络" class="headerlink" title="2.1.1 Backbone—主干网络"></a>2.1.1 Backbone—主干网络</h4><p>如上所述，骨干网络的设计对检测模型的有效性和效率有很大影响。以前的研究表明，多分支网络[13, 14, 38, 39]往往能比单路径网络[15, 37]取得更好的分类性能，但往往伴随着并行性的降低，导致推理延迟的增加。相反，像VGG[37]这样的普通单路径网络具有高并行性和较少内存占用的优势，导致更高的推理效率。最近在RepVGG[3]中，提出了一种结构上的重新参数化方法，将训练时的多分支拓扑结构与推理时的普通结构解耦，以实现更好的速度-准确度权衡。<br>受上述工作的启发, 我们设计了一个高效的可重参数化骨干网, 命名为EfficientRep. 对于小型模型, 骨干网的主要组成部分是训练阶段的Rep-Block, 如图3(a)所示. 在推理阶段，每个RepBlock被转换为具有ReLU激活函数的3×3卷积层（表示为RepConv），如图3（b）所示。通常情况下，3×3卷积在主流的GPU和CPU上被高度优化，它享有更高的计算密度。因此，EfficientRep Backbone充分地利用了硬件的计算能力，使推理的延迟大大降低，同时提高了表示能力。</p><p>然而，我们注意到，随着模型容量的进一步扩大，单路径简单网络的计算成本和参数数量呈指数级增长。为了在计算负担和准确性之间实现更好的权衡，我们修改了一个CSPStackRep块来构建大中型网络的主干。如图3（c）所示，CSPStackRep Block由三个1×1的卷积层和一个由两个RepVGG块[3]或RepConv（分别在训练或推理时）组成的堆栈子块与一个剩余连接组成。此外，还采用了跨阶段部分（CSP）连接来提高性能，而没有过多的计算成本。与CSPRepResStage[45]相比，它有一个更简洁的前景，并考虑了准确性和速度之间的平衡。</p><blockquote><h4 id="背景-2"><a href="#背景-2" class="headerlink" title="背景"></a><strong>背景</strong></h4><p>RepVGG 主干在小型网络中具有更强的特征表示能力，但是随着参数和计算成本的爆炸式增长， RepVGG 在大模型中难以获得较高的性能</p><h4 id="改进工作"><a href="#改进工作" class="headerlink" title="改进工作"></a><strong>改进工作</strong></h4><ul><li>设计了一个高效的可重新参数化的骨干，称为<strong>EffificientRep</strong></li><li>在小模型<strong>(n/t/s)</strong>中，使用<strong>RepBlock</strong></li><li>在大模型<strong>(m/l)</strong> 中，使用<strong>CSPStackRep Blocks</strong>  </li></ul><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a><strong>方法</strong></h4><p>（1）将 Backbone 中 stride=2 的普通 Conv 层替换成了 <strong>stride=2 的RepConv层</strong></p><p>（2）将原始的 CSP-Block 都重新设计为 <strong>RepBlock</strong>，其中 <strong>RepBlock 的第一个RepConv会做channel</strong> <strong>维度的变换和对齐</strong> </p><p>（3）将原始的 SPPF 优化设计为更加高效的<strong>SimSPPF</strong></p><p>下图为 EfficientRep Backbone 具体设计结构图</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-EfficientRep.png" alt=""></p><p>下图是网络在不同情况下的结构</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Figure3.png" alt=""></p></blockquote><p>图<strong>(a)</strong>：表示训练的时候，RepVGG block接一个ReLU</p><p>图<strong>(b)</strong>：表示推理的时候，RepVGG块被替换成了RepConv</p><p>图<strong>(c)</strong>：表示CSPStackRep块的结构   ( 3 个 1x1 conv + 2 个 RepVGG(训练) / RepConv(推理) + 1 个Relu。)</p><h4 id="2-1-2-Neck—颈部"><a href="#2-1-2-Neck—颈部" class="headerlink" title="2.1.2 Neck—颈部"></a><strong>2.1.2 Neck—颈部</strong></h4><p>在实践中，多尺度的特征整合已被证明是目标检测的一个关键和有效的部分[9, 21, 24, 40]。我们采用YOLOv4[1]和YOLOv5[10]中修改的PAN拓扑结构[24]作为我们检测颈部的基础。此外，我们用RepBlock（用于小模型）或CSPStackRep Block（用于大模型）取代YOLOv5中使用的CSPBlock，并相应调整宽度和深度。YOLOv6的颈部被表示为Rep-PAN。</p><blockquote><p>参考YOLOv4和v5用的PAN，结合Backbone里的RepBlock或者CSPStackRep，提出了一个Rep-PAN</p><h4 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a><strong>方法</strong></h4><p>Rep-PAN 基于 PAN拓扑方式，用 RepBlock 替换了 YOLOv5 中使用的 CSP-Block，对整体 Neck 中的算子进行了调整</p><h4 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h4><p>在硬件上达到高效推理的同时，保持较好的多尺度特征融合能力</p><p>Rep-PAN 基于PAN拓扑方式，用RepBlock替换了YOLOv5中使用的CSP-Block，同时对整体Neck中的算子进行了调整，目的是在硬件上达到高效推理的同时，保持较好的多尺度特征融合能力</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-RepPAN.png" alt=""></p></blockquote><h3 id="2-1-3-Head—头部"><a href="#2-1-3-Head—头部" class="headerlink" title="2.1.3 Head—头部"></a>2.1.3 Head—头部</h3><h4 id="Efficient-decoupled-head—高效的解耦头"><a href="#Efficient-decoupled-head—高效的解耦头" class="headerlink" title="Efficient decoupled head—高效的解耦头"></a>Efficient decoupled head—高效的解耦头</h4><p>高效的解耦头 YOLOv5的检测头是一个耦合头，其参数在分类和定位分支之间共享，而其在FCOS[41]和YOLOX[7]中的同类产品则将这两个分支解耦，并且在每个分支中引入额外的两个3×3卷积层以提高性能。在YOLOv6中，我们采用混合通道策略来建立一个更有效的解耦头。具体来说，我们将中间的3×3卷积层的数量减少到只有一个。头部的宽度由骨干和颈部的宽度乘数共同缩放。这些修改进一步降低了计算成本，以实现更低的推理延迟。</p><blockquote><h4 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h4><p>（1）将中间的3x3卷积层的数量减少为<strong>1</strong></p><p>（2）Head 的尺度和 backbone 及 neck <strong>同大同小</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-head.png" alt=""></p><h4 id="目的-1"><a href="#目的-1" class="headerlink" title="目的"></a>目的</h4><p>进一步降低了计算成本，以实现更低的推断延迟。</p></blockquote><h4 id="Anchor-free"><a href="#Anchor-free" class="headerlink" title="Anchor-free"></a>Anchor-free</h4><p><strong>Anchor-free</strong>检测器因其更好的泛化能力和解码预测结果的简单性而脱颖而出。其后期处理的时间成本大大降低。有两种类型的Anchor-free检测器：基于锚点[7, 41]和基于关键点[16, 46, 53]。在YOLOv6中，我们采用了基于锚点的范式，其回归分支实际上预测了从锚点到box四边的距离。</p><blockquote><h4 id="不用Anchor-based的原因"><a href="#不用Anchor-based的原因" class="headerlink" title="不用Anchor-based的原因"></a><strong>不用Anchor-based的原因</strong></h4><p>由于 Anchor-based检测器需要在训练之前进行聚类分析以确定最佳 Anchor 集合，这会一定程度提高检测器的复杂度  在一些边缘端的应用中，需要在硬件之间搬运大量检测结果的步骤，也会带来额外的延时</p><h4 id="Anchor-free优点"><a href="#Anchor-free优点" class="headerlink" title="Anchor-free优点"></a>Anchor-free优点</h4><p>Anchor-free方案<strong>不需要预设参数，同时后处理耗时短</strong></p><p><strong>Anchor-free方案有两种</strong></p><ul><li><strong>point-base(FCOS)</strong> ——YOLOv6使用的</li><li>keypoint-based  ( CornerNet)</li></ul></blockquote><h3 id="2-2-Label-Assignment—标签分配"><a href="#2-2-Label-Assignment—标签分配" class="headerlink" title="2.2 Label Assignment—标签分配"></a>2.2 Label Assignment—标签分配</h3><h4 id="SimOTA"><a href="#SimOTA" class="headerlink" title="SimOTA"></a>SimOTA</h4><p>SimOTA OTA[6]认为物体检测中的标签分配是一个最优传输问题。它从全局的角度为每个ground-truth目标定义了正/负的训练样本。SimOTA[7]是OTA[6]的简化版本，它减少了额外的超参数并保持了性能。在YOLOv6的早期版本中，SimOTA被用作标签分配方法。然而，在实践中，我们发现引入SimOTA会减慢训练过程。而且，陷入不稳定的训练也并不罕见。因此，我们渴望有一个替代SimOTA的方法。</p><blockquote><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a><strong>介绍</strong></h4><p>OTA 将目标检测中的标签分配视为最佳传输问题。它从<strong>全局角度为每个真实对象定义了正/负训练样本。</strong></p><p>SimOTA 是 OTA 的简化版本，它<strong>减少了额外的超参数</strong>并保持了性能。</p><h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><p>  ①计算成对预测框与真值框代价，由分类及回归loss构成</p><p>  ②计算真值框与前k个预测框IoU，其和为Dynamic k；因此对于不同真值框，其Dynamic k存在差异</p><p>  ③最后选择代价最小的前Dynamic k个预测框作为正样本</p><h4 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h4><p>SimOTA 会拉慢训练速度，容易导致训练不稳定</p></blockquote><h4 id="Task-alignment-learning—任务对齐学习"><a href="#Task-alignment-learning—任务对齐学习" class="headerlink" title="Task alignment learning—任务对齐学习"></a>Task alignment learning—任务对齐学习</h4><p>任务对齐学习 任务对齐学习（TAL）最早是在TOOD[5]中提出的，其中设计了一个统一的分类分数和预测框质量的指标。IoU被这个度量所取代，用于分配对象标签。在一定程度上，任务（分类和箱体回归）不一致的问题得到了缓解。<br>TOOD的另一个主要贡献是关于任务对齐的头（T-head）。T-head堆叠卷积层以建立交互式特征，在此基础上使用任务对齐预测器（TAP）。PP-YOLOE[45]改进了T-head，用轻量级的ESE注意力取代了T-head中的层注意力，形成ET-head。然而，我们发现ET-head在我们的模型中会降低推理速度，而且没有准确性的提高。</p><p>因此，我们保留了Efficient解耦头的设计。此外，我们观察到TAL比SimOTA能带来更多的性能提升，并且能稳定训练。因此，我们在YOLOv6中采用TAL作为默认的标签分配策略。</p><blockquote><h4 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h4><p>TAL 是在 TOOD 中被提出的，其中设计了一个【分类得分和定位框质量的统一度量标准】，使用该度量结果代替 IoU 来帮助</p><p>分配标签，有助于解决任务不对齐的问题，且更稳定，效果更好</p><h4 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h4><p> ①在各个特征层计算gt与预测框IoU及与分类得分乘积作为score，进行分类检测任务对齐</p><p> ②对于每个gt选择top-k个最大的score对应bbox</p><p> ③选取bbox所使用anchor的中心落在gt内的为正样本</p><p> ④若一个anchor box对应多个gt，则选择gt与预测框IoU最大那个预测框对应anchor负责该gt</p><h4 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h4><p>  TAL比SimOTA带来更多的性能改善，稳定训练</p></blockquote><hr><h3 id="2-3-Loss-Functions—损失函数"><a href="#2-3-Loss-Functions—损失函数" class="headerlink" title="2.3 Loss Functions—损失函数"></a>2.3 Loss Functions—损失函数</h3><h4 id="2-3-1-Classification-Loss—分类损失"><a href="#2-3-1-Classification-Loss—分类损失" class="headerlink" title="2.3.1 Classification Loss—分类损失"></a>2.3.1 Classification Loss—分类损失</h4><p>提高分类器的性能是优化检测器的一个关键部分。Focal Loss[22]修改了传统的交叉熵损失，以解决正负样本之间或难易样本之间类别不平衡的问题。为了解决训练和推理之间质量估计和分类的不一致使用问题，质量Focal Loss（QFL）[20]进一步扩展了Focal Loss，对分类分数和分类监督的定位质量进行联合表示。而VariFocal Loss（VFL）[50]则源于Focal Loss[22]，但它对正负样本的处理是不对称的。通过考虑正负样本的不同重要程度，它平衡了来自两个样本的学习信号。Poly Loss[17]将常用的分类损失分解为一系列的加权多项式基数。它在不同的任务和数据集上调整多项式系数，通过实验证明它比交叉熵损失和Focal Loss更好。</p><p>我们在YOLOv6上评估了所有这些高级分类损失，最终采用了VFL[50]。</p><blockquote><ul><li><strong>VariFocal Loss(VFL) ：</strong>提出了非对称的加权操作。</li><li>针对正负样本有不平衡的问题和正样本中不等权的问题，来发现更多有价值的正样本。因此选择 VariFocal Loss 作为分类损失。</li></ul></blockquote><h4 id="2-3-2-Box-Regression-Loss—-回归框损失"><a href="#2-3-2-Box-Regression-Loss—-回归框损失" class="headerlink" title="2.3.2 Box Regression Loss— 回归框损失"></a><strong>2.3.2 Box Regression Loss— 回归框损失</strong></h4><p>回归损失提供了精确定位box边界的重要学习信号。L1损失是早期工作中最初的回归损失。逐渐地，各种精心设计的回归损失涌现出来，如IoU系列损失[8, 11, 35, 47, 52, 52] 和概率损失[20]。</p><p><strong>IoU-系列损失</strong> IoU损失[47]将预测框的四个边界作为一个整体单位进行回归。它被证明是有效的，因为它与评价指标一致。IoU有很多变体，如GIoU[35]、DIoU[52]、CIoU[52]、α-IoU[11]和SIoU[8]等，形成相关损失函数。在这项工作中，我们用GIoU、CIoU和SIoU进行实验。而SIoU被应用于YOLOv6-N和YOLOv6-T，而其他的则使用GIoU。</p><p><strong>概率损失</strong> Distribution Focal Loss（DFL）[20]将连续分布的box位置简化为离散的概率分布。它考虑了数据的模糊性和不确定性，而没有引入任何其他强的先验因素，这有助于提高box的定位精度，特别是当ground-truth boxes模糊时。在DFL的基础上，DFLv2[19]开发了一个轻量级的子网络，以利用分布统计和实际定位质量之间的密切关联，这进一步提高了检测性能。然而，DFL通常比一般的目标框回归多输出17倍的回归值，导致了大量的开销。额外的计算成本大大阻碍了小模型的训练。而DFLv2由于有了额外的子网络，进一步增加了计算负担。在我们的实验中，DFLv2在我们的模型上带来了与DFL相似的 在我们的模型上，DFLv2带来的性能增益与DFL相似。因此，我们 只在YOLOv6-M/L中采用DFL。实验细节可以在可在第3.3.3节中找到。</p><blockquote><h4 id="IoU-series-Loss-loU系列损失"><a href="#IoU-series-Loss-loU系列损失" class="headerlink" title="IoU-series Loss-loU系列损失"></a><strong>IoU-series Loss-loU系列损失</strong></h4><p>SIoU Loss在小模型上提升明显， GIoU Loss在大模型上提升明显，因此选择SIoU  (for n/t/s)  /GIoU (for m/l) 损失作为回归损失。</p><h4 id="Probability-Loss-概率损失"><a href="#Probability-Loss-概率损失" class="headerlink" title="Probability Loss-概率损失"></a>Probability Loss-概率损失</h4><p>Distribution Focal Loss (DFL) 和 Distribution Focal Loss (DFL) v2可以带来一定的性能提升，但是对效率影响较大，因此弃用。</p></blockquote><h4 id="2-3-3-Object-Loss—-目标损失"><a href="#2-3-3-Object-Loss—-目标损失" class="headerlink" title="2.3.3 Object Loss— 目标损失"></a>2.3.3 Object Loss— 目标损失</h4><p>物体损失最早是在FCOS[41]中提出的，用于降低低质量bounding boxes的得分，以便在后期处理中过滤掉它们。它也被用于YOLOX[7]，以加速收敛并提高网络精度。作为一个像FCOS和YOLOX一样的anchor-free框架，我们已经在YOLOv6中尝试了object loss。不太幸运的是，它并没有带来很多好的效果。详细情况在第3节中给出。</p><blockquote><p>Object loss 首次提出是在 FCOS 中，用于降低 low-quality bbox 的得分，利于在 NMS 中过滤掉， YOLOX 中使用了该 loss 来加 速收敛并提升准确性，但 YOLOv6 中使用同样的方法后并无收益。</p></blockquote><h3 id="2-4-Industry-handy-improvements—工业界处理改进"><a href="#2-4-Industry-handy-improvements—工业界处理改进" class="headerlink" title="2.4. Industry-handy improvements—工业界处理改进"></a>2.4. Industry-handy improvements—工业界处理改进</h3><h4 id="2-4-1-More-training-epochs—更多的训练epoch"><a href="#2-4-1-More-training-epochs—更多的训练epoch" class="headerlink" title="2.4.1 More training epochs—更多的训练epoch"></a>2.4.1 More training epochs—更多的训练epoch</h4><p>经验结果表明，随着训练时间的增加，检测器的性能也在不断进步。我们将训练时间从300 epochs延长到400 epochs，以达到一个更好的收敛性。</p><blockquote><p>将训练从300个epochs延长到400个epochs，以达到更好的收敛性。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table12.png" alt=""></p><p>YOLOv6-N、T、S在较长时期的训练中，使AP分别提高了0.4%、0.6%和0.5%。</p></blockquote><h4 id="2-4-2-Self-distillation—-自蒸馏"><a href="#2-4-2-Self-distillation—-自蒸馏" class="headerlink" title="2.4.2 Self-distillation— 自蒸馏"></a>2.4.2 Self-distillation— 自蒸馏</h4><p>为了进一步提高模型的准确性，同时不引入太多的额外计算成本，我们应用经典的知识蒸馏技术，使教师和学生的预测之间的KL-散度最小。我们把老师限定为学生本身，但进行了预训练，因此我们称之为自我蒸馏。请注意，KL-散度通常被用来衡量数据分布之间的差异。然而，在目标检测中有两个子任务，其中只有分类任务可以直接利用基于KL-散度的知识提炼。由于DFL损失[20]的存在，我们也可以在box回归上执行它。知识提炼损失 可以被表述为：</p><p>L KD = KL(p cls t ||p s ) + KL(p t ||p s ),</p><p>其中p cls t和p s分别是教师模型和学生模型的班级预测，相应地p reg t和p reg是盒式回归预测。现在，总体损失s函数被表述为：</p><p>L total = L det + αL KD ,</p><p>其中L det是用预测和标签计算的检测损失。引入超参数α是为了平衡两种损失。在训练的早期阶段，教师的软标签更容易学习。随着训练的继续，学生的表现将与教师相匹配，因此硬标签将更多地帮助学生。在此基础上，我们对α应用余弦权重衰减来动态调整来自硬标签和来自教师的软标签的信息。我们进行了详细的实验来验证YOLOv6的自我蒸馏的效果，这将在第3节讨论。</p><blockquote><h4 id="背景-3"><a href="#背景-3" class="headerlink" title="背景"></a>背景</h4><p>为了进一步提高模型的准确性，同时不引入太多的额外计算成本，我们应用经典的知识蒸馏技术，使教师和学生的预测之间的KL-散度最小。</p><h4 id="方法-3"><a href="#方法-3" class="headerlink" title="方法"></a>方法</h4><p>作者限制教师模型与学生模型网络结构相同，但经过预训练，因此称为自蒸馏。</p><p>归因于DFL损失，回归分支也可使用知识蒸馏，损失函数如式1所示：</p><script type="math/tex; mode=display">L_{KD} = KL(p_t^{cls} || p_s^{cls}) + KL(p_t^{reg} || p_s^{reg}),</script><h4 id="效果-1"><a href="#效果-1" class="headerlink" title="效果"></a>效果</h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table13.png" alt=""></p><ul><li>仅在分类分支上应用自蒸馏可以提高<strong>0.4%</strong>的AP</li><li>在预测框回归任务上执行自蒸馏可以提高<strong>0.3%</strong>的AP</li><li>权重衰减的引入自蒸馏使模型可以提高<strong>0.6%</strong>的AP</li></ul></blockquote><h4 id="2-4-3-Gray-border-of-images—-图像灰色边界"><a href="#2-4-3-Gray-border-of-images—-图像灰色边界" class="headerlink" title="2.4.3 Gray border of images— 图像灰色边界"></a>2.4.3 Gray border of images— 图像灰色边界</h4><p>我们注意到，在评价YOLOv5[10]和YOLOv7[42]的实现中的模型性能时，每个图像周围都有一个半截灰色的边框。虽然没有增加有用的信息，但它有助于检测图像边缘附近的物体。这个技巧也适用于YOLOv6。然而，额外的灰色像素显然会降低推理速度。没有灰色边界，YOLOv6的性能就会下降，这也是[10, 42]的情况。我们推测，这个问题与Mosaic增强中的灰边填充有关[1, 10]。为了验证，我们进行了在最后一个epoch中关闭马赛克增强的实验[7]（又称淡化策略）。在这方面，我们改变了灰色边框的面积，并将带有灰色边框的图像直接调整为目标图像的大小。结合这两种策略，我们的模型可以在不降低推理速度的情况下保持甚至提高性能。推理速度。</p><blockquote><h4 id="背景-4"><a href="#背景-4" class="headerlink" title="背景"></a>背景</h4><p>半截灰色的边框它有助于检测图像边缘附近的物体</p><h4 id="方法-4"><a href="#方法-4" class="headerlink" title="方法"></a>方法</h4><p>（1）在最后一个epoch中关闭mosaic增强的实验(又称淡化策略)</p><p>（2）改变了灰色边框的面积，并将带有灰色边框的图像直接调整为目标图像的大小</p><h4 id="效果-2"><a href="#效果-2" class="headerlink" title="效果"></a>效果</h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-effection.png" alt=""></p><p>YOLOv6-N/S/M的最终性能更准确，最终图像尺寸从672减少到640。</p></blockquote><h3 id="2-5-Quantization-and-Deployment—量化和部署"><a href="#2-5-Quantization-and-Deployment—量化和部署" class="headerlink" title="2.5. Quantization and Deployment—量化和部署"></a>2.5. Quantization and Deployment—量化和部署</h3><h4 id="2-5-1-Reparameterizing-Optimizer—重新参数化"><a href="#2-5-1-Reparameterizing-Optimizer—重新参数化" class="headerlink" title="2.5.1 Reparameterizing Optimizer—重新参数化"></a>2.5.1 Reparameterizing Optimizer—重新参数化</h4><p>RepOptimizer[2]在每个优化步骤中提出梯度重新参数化。该技术也能很好地解决了基于再参数化的模型的量化问题。因此，我们以这种方式重建了YOLOv6的重新参数化块，并使用重新优化器对其进行训练，以获得对PTQ友好的权值。特征图的分布很窄（如图4，B.1），这大大有利于量化过程，结果见第3.5.1节。<br>（1）RepOptimizer 提出了在每次训练的时候进行梯度重参数化，该方法能够较好的解决基于重参数化的模型。               </p><p>（2）YOLOv6 中就使用了 RepOptimizer 用于获得 PTQ-friendly 的权重，其特征的分布是非常狭窄的，能够有利于量化。<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Figure4.png" alt=""></p><h4 id="2-5-2-Sensitivity-Analysis—敏感度"><a href="#2-5-2-Sensitivity-Analysis—敏感度" class="headerlink" title="2.5.2 Sensitivity Analysis—敏感度"></a>2.5.2 Sensitivity Analysis—敏感度</h4><p>我们通过将量化敏感操作部分转换为浮点计算，进一步提高了PTQ的性能。为了获得灵敏度分布，我们常用了几个指标，即均方误差（MSE）、信噪比（SNR）和余弦相似度。通常，为了进行比较，可以选择输出特征映射（在激活某一层之后）来计算有量化和没有量化的这些度量。作为一种替代方法，它也可以通过开关特定层[29]的量化来计算验证AP。</p><p> 我们在使用重新优化器训练的YOLOv6-S模型上计算所有这些指标，并选择前6个敏感层，以浮动形式运行。敏感性分析的完整图表见B.2。</p><blockquote><p>通过将量化敏感操作部分转换为浮点计算，进一步提高了 PTQ 性能为了得到敏感性分布，作者使用了 mean-square error (MSE), signal-noise ratio (SNR) 和 cosine similarity 。</p></blockquote><h4 id="2-5-3-Quantization-aware-Training-with-Channel-wise-Distillation—基于通道蒸馏的量化感知训练"><a href="#2-5-3-Quantization-aware-Training-with-Channel-wise-Distillation—基于通道蒸馏的量化感知训练" class="headerlink" title="2.5.3 Quantization-aware Training with Channel-wise Distillation—基于通道蒸馏的量化感知训练"></a>2.5.3 Quantization-aware Training with Channel-wise Distillation—基于通道蒸馏的量化感知训练</h4><p> 在PTQ不足的情况下，我们建议涉及量化感知训练（QAT）来提高量化性能。为了解决在训练和推理过程中假量化器的不一致性问题，有必要在重新优化器上建立QAT。此外，在YOLOv6框架内采用了通道蒸馏[36]（后来称为CW蒸馏），如图5所示。这也是一种自蒸馏的方法，其中教师网络是在fp32精度上的学生模型。参见第3.5.1节中的实验。</p><blockquote><p>为防止PTQ不足，作者引入QAT (训练中量化)，保证训练推理一致，作者同样使用RepOptimizer，此外使用channel-wise蒸馏，如图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-PTQ.png" alt=""></p></blockquote><h2 id="三、Experiments—实验"><a href="#三、Experiments—实验" class="headerlink" title="三、Experiments—实验"></a>三、Experiments—实验</h2><h3 id="3-1-Implementation-Details—实施细节"><a href="#3-1-Implementation-Details—实施细节" class="headerlink" title="3.1 Implementation Details—实施细节"></a>3.1 Implementation Details—实施细节</h3><p>我们使用与YOLOv5 [10]相同的优化器和学习时间表，i.即具有动量和学习率余弦衰减的随机梯度下降（SGD）。还利用了预热、分组权重衰减策略和指数移动平均（EMA）。我们采用了两个强数据增强（Mosaic [1，10]和Mixup [49]）[1，7，10]。超参数设置的完整列表可以在我们发布的代码中找到。我们在COCO 2017 [23]训练集上训练我们的模型，并在COCO 2017验证集上评估准确性。我们所有的模型都在8个NVIDIA A100 GPU上进行训练，速度性能在配备TensorRT版本7的NVIDIA Tesla T4 GPU上进行测量。2除非另有说明。还有速度使用其他TensorRT版本或其他设备测量的性能在附录A中演示。</p><blockquote><p>(1)采用了和YOLOv5相同的优化算法和学习机制设置(包括SGD、学习率、预热、分组权重衰减策略和EMA、还有两个数据增强)</p><p>(2)在COCO 2017训练集上训练模型，并在COCO 2017验证集上评估准确性</p><p>(3)所有的模型都在8个NVIDIA A100 GPU上进行训练，速度性能在配备TensorRT版本7的NVIDIA Tesla T4 GPU上进行测量</p></blockquote><h3 id="3-2-Comparisons—对照实验"><a href="#3-2-Comparisons—对照实验" class="headerlink" title="3.2 Comparisons—对照实验"></a>3.2 Comparisons—对照实验</h3><p>考虑到这项工作的目标是为工业应用构建网络，我们主要关注部署后所有模型的速度性能，包括吞吐量（批量大小为1或32的FPS）和GPU延迟，而不是FLOPs或参数数量。我们将YOLOv 6与YOLO系列的其他最先进的探测器进行了比较，包括YOLOv 5 [10]，YOLOX [7]，PPYOLOE [45]和YOLOv 7 [42]。请注意，我们使用TensorRT在相同的Tesla T4 GPU上测试了所有官方型号的FP 16精度的速度性能[28]。YOLOv 7-Tiny的性能根据其开源代码和输入大小为416和640的权重进行重新评估。结果示于表1和图2中。1.与YOLOv 5-N/YOLOv 7-Tiny（输入大小=416）相比，我们的YOLOv 6-N显著提高了7。9%/2.6%。在吞吐量和延迟方面，它还具有最佳的速度性能。与YOLOX-S/PPYOLOE-S相比，YOLOv 6-S可使AP提高3.0%/0.4%，速度更快。我们将YOLOv 5-S和YOLOv 7-Tiny（输入大小=640）与YOLOv 6-T进行比较，我们的方法是2。精度提高9%，批处理大小为1时，速度提高73/25 FPS。YOLOv 6-M的性能比YOLOv 5-M高4倍。2%的AP，在相同的速度下，实现了2.在更高的速度下，AP比YOLOX-M/PPYOLOE-M高7%/0.6%。此外，它比YOLOv 5-L更准确，更快。YOLOv 6-L为2。在相同的延迟限制下，比YOLOX-L/PPYLOE-L准确8%/1.1%。我们还通过用ReLU替换SiLU（表示为YOLOv 6-L-ReLU）来提供YOLOv 6-L的更快版本。达到51。7%AP，延迟为8.8 ms，在精度和速度上均优于YOLOX-L/PPYOLOE-L/YOLOv 7。</p><blockquote><h4 id="和SOTA对比"><a href="#和SOTA对比" class="headerlink" title="和SOTA对比"></a>和SOTA对比</h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-vssota.png" alt=""></p><p>(1)相比 YOLOv5-N/YOLOv7-Tiny (input size=416) ，YOLOv6-N 分别提升了 7.9% 和 2.6% ，也达到了最高的速度</p><p>(2)相比 YOLOX-S/PPYOLOE-S, YOLOv6-S 分别提升了 3.0% 和 0.4%</p><p>(3)相比 YOLOv5-S 和 YOLOv7-Tiny (input size=640) ，YOLOv6-M 在同等速度的情况下高了 4.2% AP</p><p>(4)相比 YOLOX-M/PPYOLOE-M ，YOLOv6-M 更快，且分别高了 2.7% 和 0.6% AP</p><p>(5)相比 YOLOX-L/PPYOLOE-L/YOLOv7 ，YOLOv6-L-Relu 达到了 51.7% AP，超越了前面几个方法</p></blockquote><h3 id="3-3-Ablation-Study—消融实验"><a href="#3-3-Ablation-Study—消融实验" class="headerlink" title="3.3 Ablation Study—消融实验"></a>3.3 Ablation Study—消融实验</h3><h4 id="3-3-1-Network—-网络"><a href="#3-3-1-Network—-网络" class="headerlink" title="3.3.1 Network— 网络"></a><strong>3.3.1 Network— 网络</strong></h4><p><strong>Backbone and neck—骨干网络和颈部</strong></p><p>主干和颈部我们探讨了单路径结构和多分支结构对主干和颈部的影响，以及CSPStackRep Block的信道系数（表示为CC）。本部分描述的所有模型均采用TAL作为标签分配策略，VFL作为分类损失，GIoU和DFL作为回归损失。结果示于表2中。我们发现，在不同规模的模型的最佳网络结构应该拿出不同的解决方案。对于YOLOv 6-N，单路径结构在精度和速度方面优于多分支结构。</p><p>由于相对较低的存储器占用和较高的并行度，因此运行得更快。对于YOLOv 6-S，两种块样式带来相似的性能。当涉及到较大的模型，多分支结构实现更好的性能，在准确性和速度。并且我们最终选择多分支，其中对于YOLOv 6-M具有2/3的信道系数，并且对于YOLOv 6-L具有1/2的信道系数。此外，我们研究了颈部的宽度和深度对YOLOv 6-L的影响。表3中的结果显示细长颈部执行0.2%，比宽浅颈在相同的速度。</p><blockquote><p>作者比较backbone及neck中不同block及CSPStackRep Block中channel系数影响</p><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>不同网络结构适用不同策略</p></blockquote><p><strong>Combinations of convolutional layers and activation functions—卷积层和激活函数组合</strong></p><p>YOLO系列采用了广泛的激活函数，ReLU [27]，LReLU [25]，Swish [31]，SiLU [4]，Mish [26]等。在这些激活函数中，SiLU是使用最多的。一般来说，SiLU具有更好的准确性，并且不会导致太多额外的计算成本。然而，当涉及到工业应用时，特别是部署具有TensorRT [28]加速的模型时，ReLU由于融合到卷积中而具有更大的速度优势。此外，我们进一步验证了RepConv/普通卷积（表示为Conv）和ReLU/SiLU/LReLU在不同规模的网络中的组合的有效性，以实现更好的权衡。如表4所示，具有SiLU的Conv在准确性方面表现最好，而RepConv和ReLU的组合实现了更好的折衷。我们建议用户在延迟敏感的应用程序中采用ReLU的RepConv。我们选择使用RepConv/ReLU组合</p><blockquote><p>YOLO系列中常用激活函数有ReLU、LReLU、Swish、SiLU、Mish等，  SiLU精度最高且最常用，但是部署与TensorRT 加速 的模型时无法与卷积层融合，  ReLU更具有速度优势</p><p>进一步验证了RepConv/普通卷积(记为Conv)和ReLU/SiLU/LReLU组合在不同大小的网络中的有效性<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table3.png" alt=""></p><h4 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h4><p>（1）Conv+SiLU性能最佳，但RepConv+ReLU达到性能与速度均衡</p><p>（2）在YOLOv6-N/T/S/M中使用RepConv/ReLU组合来获得更高的推理速度</p><p>（3）在大型模型YOLOv6-L中使用Conv/SiLU组合来加速训练和提高性能。</p></blockquote><p><strong>Miscellaneous design—其余设计</strong></p><p>我们还对第2节中提到的其他网络部分进行了一系列消融。1基于YOLOv 6-N。我们选择YOLOv 5-N作为基线，并逐步添加其他组件。结果示于表5中。首先，对于解耦头（表示为DH），我们的模型是1。精确度提高4%，时间成本增加5%。其次，我们验证了无锚范式比基于锚的范式快51%，因为它的3倍少的预定义锚，这导致输出的维数更少。此外，表示为EB+RN的骨架（EfficientRep骨架）和颈部（Rep-PAN颈部）的统一修饰带来3。6%的AP改进，运行速度提高21%。最后，优化的解耦头（混合通道，HC）带来0。2%AP和6.FPS的准确性和速度分别提高了8%。</p><blockquote><h4 id="操作及结论"><a href="#操作及结论" class="headerlink" title="操作及结论"></a>操作及结论</h4><p><strong>DH:</strong> 以YOLOv5-N为基线，验证YOLOv6-N中不同部件影响，使用解耦头(DH)性能提升1.4%，耗时增加5%</p><p><strong>AF:</strong> Anchor-free方案耗时降低51%</p><p><strong>EB+RN:</strong> 主干网络EfficientRep +颈部Rep-PAN 使得性能提升3.6%，耗时降低21%</p><p><strong>HC:</strong> Head中混合通道策略，使得性能提升0.2%，耗时降低6.8%</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table5.png" alt=""></p><h4 id="3-3-2-Label-Assignment—标签分配"><a href="#3-3-2-Label-Assignment—标签分配" class="headerlink" title="3.3.2 Label Assignment—标签分配"></a>3.3.2 Label Assignment—标签分配</h4><p>在表6中，我们分析了主流标签分配策略的有效性。在YOLOv6N上进行实验。如预期的，我们观察到SimOTA和TAL是最好的两个策略与ATSS相比，SimOTA可以提高AP 2.0%，TAL带来0。AP比SimOTA高5%。考虑到TAL的稳定训练和更好的准确性能，我们采用TAL作为我们的标签分配策略。此外，TOOD [5]的实现采用ATSS [51]作为早期训练时期的预热标签分配策略。我们还保留了热身策略，并对其进行了进一步的探索。详细信息如表7所示，我们可以发现，在没有预热或通过其他策略预热的情况下（即：例如，SimOTA）也可以实现类似的性能。</p><blockquote><p>对比可知，  <strong>SimOTA</strong>和<strong>TAL</strong>是最好的两种策略。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table6.png" alt=""></p><h4 id="结论-2"><a href="#结论-2" class="headerlink" title="结论"></a>结论</h4><p>考虑到TAL的稳定训练和更好的准确性性能，作者采用TAL作为我们的标签分配策略。</p><p>进一步探索warm-up策略：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table7.png" alt=""></p><h4 id="结论-3"><a href="#结论-3" class="headerlink" title="结论"></a>结论</h4><p>如果没有warm-up或通过其他策略(即SimOTA)进行热身，也可以达到类似的性能。</p></blockquote><h4 id="3-3-3-Loss-functions—损失函数"><a href="#3-3-3-Loss-functions—损失函数" class="headerlink" title="3.3.3 Loss functions—损失函数"></a>3.3.3 Loss functions—损失函数</h4><p><strong>Classification Loss—分类损失</strong></p><p><strong>分类损失：</strong>我们在YOLOv 6-N/S/M上实验了Focal Loss [22]、Poly loss [17]、QFL [20]和VFL [50]。从表8中可以看出，VFL带来0。与局灶性丢失相比，YOLOv 6-N/S/M的AP改善分别为2%/0.3%/0.1%。我们选择VFL作为分类损失函数。</p><blockquote><p>作者对不同分类损失函数进行验证</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table8.png" alt=""></p><h4 id="结论-4"><a href="#结论-4" class="headerlink" title="结论"></a>结论</h4><p>选择VFL作为分类损失函数</p></blockquote><h4 id="Regression-Loss—-回归损失"><a href="#Regression-Loss—-回归损失" class="headerlink" title="Regression Loss— 回归损失"></a>Regression Loss— 回归损失</h4><p>在YOLOv 6-N/S/M上对回归损失IoU序列和概率损失函数进行了实验。YOLOv 6 N/S/M采用了最新的IoU系列损耗。表9中的实验结果显示，对于YOLOv 6-N和YOLOv 6-T，SIoU损失优于其他损失，而CIoU损失在YOLOv 6-M上表现更好。对于概率损失，如表10所列，引入DFL可以获得0。YOLOv 6-N/S/M的性能增益分别为2%/0.1%/0.2%。然而，对于小模型，推理速度受到很大影响。因此，仅在YOLOv 6-M/L中引入DFL。</p><blockquote><p>作者在YOLOv6-N/S/M上实验了IoU系列损失和概率损失函数</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table910.png" alt=""></p><h4 id="结论-5"><a href="#结论-5" class="headerlink" title="结论"></a>结论</h4><p>（1）关于<strong>IoU</strong>系列损失：  YOLOv6-N及YOLOv6-T使用SIoU损失，其余使用GIoU损失</p><p>（2）关于概率损失：  YOLOv6-M/L使用DFL，其余未使用</p></blockquote><p><strong>Object Loss- 目标损失</strong></p><p>如表11所示，还用YOLOv 6实验物体损失。从表11中，我们可以看到对象丢失对YOLOv 6-N/S/M网络有负面影响，其中最大减少为1。YOLOv 6-N上的1% AP。负增益可能来自目标分支与TAL中的其他两个分支之间的冲突。具体来说，在训练阶段，预测框和地面实况框之间的IoU以及分类得分用于联合构建度量作为分配标签的标准。但是，引入的对象分支将要对齐的任务数量从两个扩展到三个，这显然增加了难度。基于实验结果和该分析，然后在YOLOv6中丢弃对象丢失。</p><blockquote><p>使用YOLOv6也进行了Object Loss 实验</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table11.png" alt=""></p><h4 id="结论-6"><a href="#结论-6" class="headerlink" title="结论"></a>结论</h4><p>YOLOv6-N/S/M中目标损失都降低了效果；作者选择丢弃</p><p><strong>原因：</strong>负增益可能来自于TAL中对象分支和其他两个分支之间的冲突，TAL中将IoU与分类联合作为，额外引入一分支导</p><p>致两分支对齐变为三分支，增加对齐难度</p></blockquote><h3 id="3-4-Industry-handy-improvements—工业的便利改进"><a href="#3-4-Industry-handy-improvements—工业的便利改进" class="headerlink" title="3.4 Industry-handy improvements—工业的便利改进"></a>3.4 Industry-handy improvements—工业的便利改进</h3><p>（这一部分内容请看第2部分，就不再重复讲咯~）</p><h3 id="3-5-Quantization-Results—量化结果"><a href="#3-5-Quantization-Results—量化结果" class="headerlink" title="3.5. Quantization Results—量化结果"></a>3.5. Quantization Results—量化结果</h3><h4 id="3-5-1-PTQ"><a href="#3-5-1-PTQ" class="headerlink" title="3.5.1 PTQ"></a>3.5.1 PTQ</h4><p>当使用RepOptimizer训练模型时，平均性能得到了显著提高，请参见表15。RepOptimizer通常更快，几乎相同</p><blockquote><h3 id="使用RepOptimizer训练模型"><a href="#使用RepOptimizer训练模型" class="headerlink" title="使用RepOptimizer训练模型"></a>使用RepOptimizer训练模型</h3><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table15.png" alt=""></p><p><strong>结论：</strong> 表明RepOptimizer带来性能大幅改进</p></blockquote><h4 id="3-5-2-QAT"><a href="#3-5-2-QAT" class="headerlink" title="3.5.2 QAT"></a>3.5.2 QAT</h4><p>对于v1.0，我们将伪量化器应用于从第2节获得的非敏感层。5.2执行量化感知训练并将其称为部分QAT。我们将结果与表16中的完整QAT。部分QAT导致更好的准确性，但吞吐量略有降低。由于v2中的量化敏感层的去除。0版本，我们直接在使用RepOptimizer训练的YOLOv 6-S上使用完整的QAT。我们通过图优化消除插入量化器，以获得更高的精度和更快的速度。我们在表17中比较来自PaddleSlim [30]的基于蒸馏的量化结果。请注意，我们的YOLOv 6-S的量化版本是最快和最准确的，也请参见图1.</p><blockquote><p>对于v1.0，作者将假量化器应用于非敏感层，进行量化感知训练，并称之为部分QAT。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table16.png" alt=""></p><h4 id="结论-7"><a href="#结论-7" class="headerlink" title="结论"></a>结论</h4><p>Partial QAT (只对敏感层进行量化)比full QAT性能更佳，但耗时略增加</p><p>在v2.0版本中删除了量化敏感层，作者直接在使用RepOptimizer训练的YOLOv6-S上使用全QAT。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table17.png" alt=""></p><h4 id="结论-8"><a href="#结论-8" class="headerlink" title="结论"></a>结论</h4><p>表明作者量化的YOLOv6-S速度快性能佳，其余检测器使用PaddleSlim中基于蒸馏量化方法。</p></blockquote><h2 id="四、-Conclusion—结论"><a href="#四、-Conclusion—结论" class="headerlink" title="四、 Conclusion—结论"></a>四、 Conclusion—结论</h2><p>简而言之，考虑到持续的工业需求，我们提出了YOLOv6的当前形式，仔细研究了迄今为止物体探测器组件的所有进步，同时灌输了我们的思想和实践。其结果在精度和速度上都超过了其他可用的实时检测器。为了方便工业部署，我们还为YOLOv6提供了定制的量化方法，使其成为开箱即用的快速检测器。我们衷心感谢学术界和工业界的杰出想法和努力。未来，我们将继续扩大该项目，以满足更高的标准和更苛刻的场景。</p><blockquote><p>YOLOv6在精度和速度上都超过了其他可用的目标检测器。为了方便工业部署，作者还为YOLOv6提供了定制的量化方法，使其成为开箱 即用的快速检测器。</p><h4 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h4><p>（1）RepVGG提出的结构重参数化方法表现良好，但在此之前没有检测模型使用。作者认为RepVGG的block缩放不合理，小模型和大模型</p><p>（2）没必要保持相似网络结构；小模型使用单路径架构，大模型就不适合在单路径上堆参数量。</p><p>（3）使用重参数化的方法后，检测器的量化也需要重新考虑，否则因为训练和推理时的结构不同，性能可能会退化。</p><p>（4）前期工作很少关注部署。前期工作中，推理是在V100等高配机器完成的，但实际使用时往往用T4等低功耗推理gpu，作者更关注后者</p><p>的性能。</p><p>（5）针对网络结构的变化，重新考虑标签分配和损失函数。</p><p>（6）对于部署，可以调整训练策略，在不增加推理成本的情况下提升性能，如使用知识蒸馏。</p><h4 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h4><p> （1）在不同的工业落地场景下，设计了不同的模型，兼顾精度与速度。其中，小模型为单分支，大模型为多分支。    </p><p> （2）在分类和回归任务上都使用自蒸馏策略，动态调整教师模型和标签，便于学生模型的训练。</p><p> （3）分析了各种标签分配、损失函数和数据增强技术，选择合适的策略进一步提升性能。</p><p> （4） 基于RepOptimizer优化器和通道蒸馏，对量化方式做了改进。</p><h4 id="未来完善"><a href="#未来完善" class="headerlink" title="未来完善"></a>未来完善</h4><ol><li><p>完善 YOLOv6 全系列模型，持续提升检测性能。</p></li><li><p>在多种硬件平台上，设计硬件友好的模型。</p></li><li><p>支持 ARM 平台部署以及量化蒸馏等全链条适配。</p></li><li><p>横向拓展和引入关联技术，如半监督、自监督学习等等。</p></li><li><p>探索 YOLOv6 在更多的未知业务场景上的泛化性能。</p></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>yolov4-paper</title>
      <link href="/yolov4-paper/"/>
      <url>/yolov4-paper/</url>
      
        <content type="html"><![CDATA[<h1 id="《YOLOv4：Optimal-Speed-and-Accuracy-of-Object-Detection》"><a href="#《YOLOv4：Optimal-Speed-and-Accuracy-of-Object-Detection》" class="headerlink" title="《YOLOv4：Optimal Speed and Accuracy of Object Detection》"></a>《YOLOv4：Optimal Speed and Accuracy of Object Detection》</h1><h2 id="Abstract—摘要"><a href="#Abstract—摘要" class="headerlink" title="Abstract—摘要"></a><strong>Abstract—摘要</strong></h2><p>大量的特征据说可以提高卷积神经网络(CNN)的精度。需要在大数据集上对这些特征的组合进行实际测试，并对结果进行理论证明。有些特性只适用于某些模型，只适用于某些问题，或仅适用于小规模数据集；而一些特性，如批处理标准化和残差连接，适用于大多数模型、任务和数据集。我们假设这些普遍特征包括加权残差连接(WRC)、跨阶段部分连接(CSP)、交叉小批归一化(CmBN)、自我对抗训练(SAT)和Mish激活。我们使用新功能：WRC，CSP，CmBN，SAT，Mish激活，Mosaic数据增强、CmBN，DropBlock正则化和CIoU损失，并结合其中一些实现最先进的结果：43.5%AP，(65.7%AP50)的实时速度∼65FPS Tesla V100。源代码是在<a href="https://github.com/AlexeyAB/darknet.。">https://github.com/AlexeyAB/darknet.。</a> </p><blockquote><h4 id="提高CNN准确性的方法"><a href="#提高CNN准确性的方法" class="headerlink" title="提高CNN准确性的方法"></a><strong>提高CNN准确性的方法</strong></h4><p>（1）<strong>专用特性：</strong> 一些特征只针对某一模型，某一问题，或仅为小规模数据集</p><p>（2）<strong>通用特性：</strong> 一些特性，如批处理规范化和残差连接，则适用于大多数模型、任务和数据集。这些通用特性包括加权剩余连接(WRC)、跨阶段部分连接(CSP)、跨小批标准化(CmBN)、自反训练(SAT)和Mish 激活函数。</p><h4 id="YOLOv4使用的技巧"><a href="#YOLOv4使用的技巧" class="headerlink" title="YOLOv4使用的技巧"></a>YOLOv4使用的技巧</h4><p><strong>使用新特性：</strong>WRC、CSP、CmBN、SAT、Mish 激活函数、Mosaic数据增强、CmBN、DropBlock正则化、CIoU损失，结合这些技巧实现先进的结果。</p><h4 id="实现结果"><a href="#实现结果" class="headerlink" title="实现结果"></a>实现结果</h4><p>在Tesla V100上，MS COCO数据集以65 FPS的实时速度达到43.5 % AP ( 65.7 % AP50 )。</p></blockquote><h2 id="一、-Introduction—简介"><a href="#一、-Introduction—简介" class="headerlink" title="一、 Introduction—简介"></a><strong>一、 Introduction—简介</strong></h2><p>大多数基于cnn的对象检测器基本上只适用于推荐系统。例如，通过城市摄像机搜索免费停车位是由慢速精确的模型执行的，而汽车碰撞警告与快速不准确的模型有关。为了提高实时目标检测器的精度，不仅可以将它们用于提示生成推荐系统，还可以用于独立的流程管理和减少人工输入。在传统图形处理单元(GPU)上的实时对象检测器操作允许它们以可承受的价格大规模使用。最精确的现代神经网络不能实时运行，需要大量的gpu来进行大的小批量训练。我们通过创建一个在普通的GPU上实时运行的CNN来解决这些问题，而其训练只需要一个普通的GPU。<br>这项工作的主要目标是在生产系统中设计一个目标检测器的快速运行速度，并优化并行计算，而不是低计算体积理论指标(BFLOP)。我们希望所设计的对象能够方便地训练和使用。例如，任何使用普通的GPU进行训练和测试的人都可以实现实时、高质量和令人信服的目标检测结果，如图1所示的YOLOv4结果所示。我们的贡献总结如下：</p><p>1.我们开发了一个高效而强大的目标检测模型。它使每个人都可以使用一个1080 Ti或2080 Ti GPU来训练一个超快和准确的目标探测器。</p><p>2.我们验证了state-of-the-art Bag-of Freebies and Bag-of-Specials对目标检测的影响。</p><p>3.我们修改了最先进的方法，使其更有效，更适合于单一的GPU训练，包括CBN[89]，PAN[49]，SAM[85]等。</p><blockquote><h4 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h4><p>（1）<strong>改进性能：</strong> 大多数基于CNN的目标检测器主要只适用于推荐系统，因此需要提高实时目标探测器的准确性。</p><p>（2）<strong>单GPU训练：</strong> 最精确的现代神经网络不能实时运行，需要大量的GPU来进行大规模的小批量训练。我们通过创建一个在常规GPU上实时运行的CNN来解决这些问题，而训练只需要一个常规GPU。</p><h4 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h4><p>设计生产系统中目标检测器的快速运行速度，优化并行计算，而不是低计算量理论指标 （BFLOP）。</p><h4 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h4><p>（1）开发了一个高效、强大的目标检测模型。使用单个1080 Ti或2080 Ti GPU就能训练一个超级快速和精确的目标探测器。</p><p>（2）验证了在检测器训练过程中，最先进的Bag-of-Freebies 和Bag-of-Specials对目标检测方法的影响。</p><p>（3）修改了最先进的方法，使其更有效，更适合于单GPU训练。</p><blockquote><p><strong>Q： Bag-of-Freebies 和Bag-of-Specials</strong></p><p>Bag-of-Freebies： 指不会显著影响模型测试速度和模型复杂度的技巧，主要就是数据增强操作、标签软化等外在训练方法，即不需要改变网络模型。</p><p>Bag-of-Specials： 是用最新最先进的方法（网络模块）来魔改检测模型。只增加少量推理成本但能显著提高对象检测精度的插件模块和后处理方法，一般来说，这些插件模块都是为了增强模型中的某些属性，如扩大感受野、引入注意力机制或加强特征整合能力等，而后处理是筛选模型预测结果的一种方法。</p></blockquote></blockquote><h2 id="二、Related-work—相关工作"><a href="#二、Related-work—相关工作" class="headerlink" title="二、Related work—相关工作"></a><strong>二、Related work—相关工作</strong></h2><h3 id="2-1-Object-detection-models—目标检测模型"><a href="#2-1-Object-detection-models—目标检测模型" class="headerlink" title="2.1 Object detection models—目标检测模型"></a>2.1 Object detection models—目标检测模型</h3><p>现代探测器通常由两部分组成，一个是在ImageNet上预先训练的主干，另一个是用于预测物体的类和边界框的头部。对于那些运行在GPU平台上的检测器，它们的主干可以是VGG[68]、ResNet[26]、ResNeXt[86]或DenseNet[30]。对于那些运行在CPU平台上的检测器，它们的主干可以是SqueezeNet [31]、MobileNet[28,66,27,74]或ShufflfleNet[97,53]。对于头部部分，通常可分为一级目标探测器和两级目标探测器两类。最具代表性的两级目标探测器是R-CNN[19]系列，包括Fast R-CNN[18]、Faster R-CNN[64]、R-FCN[9]和Libra R-CNN[58].也可以使一个两级目标检测器成为一个无锚点的目标检测器，如反应点[87]。对于单级目标探测器，最具代表性的模型是YOLO[61,62,63]、SSD[50]和RetinaNet[45]。近年来，无锚的单级目标探测器已经发展起来。这类检测器有CenterNet [13]、CornerNet [37,38]、FCOS[78]等。近年来开发的目标探测器经常在主干和头部之间插入一些层，这些层通常用于收集不同阶段的特征图。我们可以称之为物体探测器的颈部。通常，颈部由几条自下向上的路径和几条自上向下的路径组成。配备这种机制的网络包括特征金字塔网络(FPN)[44]、路径聚合网络(PAN)[49]、BiFPN[77]和NAS-FPN[17]。</p><p>除了上述模型外，一些研究人员还强调了直接构建一个新的主干(DetNet[43]，DetNAS[7])或一个新的整体模型(SpineNet[12]，HitDetector[20])用于目标检测。 </p><p>综上所述，一个普通的物体探测器由以下几个部分组成：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4%E7%89%A9%E4%BD%93%E6%8E%A2%E6%B5%8B%E5%99%A8.png" alt=""></p><blockquote><h4 id="现代目标检测器组成"><a href="#现代目标检测器组成" class="headerlink" title="现代目标检测器组成"></a>现代目标检测器组成</h4><p><strong>（1）主干backbone：</strong> 在ImageNet上预先训练的网络用来特征提取。</p><ul><li>在<strong>GPU</strong>平台上运行的检测器，主干可以是VGG、ResNet、ResNeXt或DenseNet。</li><li>在<strong>CPU</strong>平台上运行的检测器，主干可以是SqueezeNet、MobileNet或ShuffleNet。</li></ul><p><strong>（2）头部head：</strong> 对图像特征进行预测，生成边界框和并预测类别。通常分为两类即单阶段目标检测器和两阶段目标检测器。</p><ul><li><strong>two stage：</strong> R-CNN系列，包括fast R-CNN、faster R-CNN、R-FCN和Libra R-CNN。</li><li><strong>one stage：</strong> 最具代表性的模型有YOLO、SSD和RetinaNet。</li></ul><p><strong>（3）颈部neck：</strong> 近年来发展起来的目标检测器常常在主干和头部之间插入一系列混合和组合图像特征的网络层，并将图像特征传递到预测层。称之为目标检测器的颈部neck。</p><p>通常，一个颈部neck由几个自下而上的路径和几个自上而下的路径组成。具有该机制的网络包括特征金字塔网络(FPN)、路径汇聚网络(PAN)、BiFPN和NAS-FPN。</p><p>综上所述，一个普通的物体探测器是由“特征输入、骨干网络、颈部和头部”四部分组成的：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-%E7%89%A9%E4%BD%93%E6%8E%A2%E6%B5%8B%E5%99%A8%E7%BB%84%E6%88%90.png" alt=""></p></blockquote><h3 id="2-2-Bag-of-freebies"><a href="#2-2-Bag-of-freebies" class="headerlink" title="2.2 Bag of freebies"></a>2.2 Bag of freebies</h3><p>通常，一个传统的目标检测器是离线训练的。因此，研究者总是喜欢利用这一优势，开发出更好的训练方法，使目标检测器在不增加推理成本的情况下获得更好的精度。我们把这些只会改变培训策略或只增加培训成本的方法称为“bag of freebies”。目标检测方法经常采用的、满足bag of freebies.定义的是数据增强。数据增强的目的是为了增加输入图像的可变性，从而使所设计的目标检测模型对在不同环境下获得的图像具有更高的鲁棒性。例如，光度畸变和几何畸变是两种常用的数据增强方法，它们肯定有利于目标检测任务。在处理光度失真时，我们会调整图像的亮度、对比度、色调、饱和度和噪声。对于几何失真，我们添加了随机缩放、裁剪、翻转和旋转。 </p><p>上述数据增强方法都是像素级调整，并保留调整区域中的所有原始像素信息。此外，一些从事数据增强工作的研究人员将其重点放在了模拟对象遮挡问题上。它们在图像分类和目标检测方面取得了良好的效果。例如，随机擦除[100]和CutOut[11]可以随机选择图像中的矩形区域，并填充一个随机的或互补的零值。对于hide-and-seek[69]和grid mask[6]，它们随机或均匀地选择一个图像中的多个矩形区域，并将它们替换为所有的零。如果将类似的概念应用于特征映射，则会有DropOut[71]、Drop连接[80]和DropBlock[16]方法。此外，一些研究者提出了使用多个图像一起进行数据增强的方法。例如，MixUp[92]使用两幅图像用不同的系数比进行乘法和叠加，然后用这些叠加的比率来调整标签。 </p><p>CutMix[91]是将裁剪后的图像覆盖到其他图像的矩形区域，并根据混合区域的大小调整标签。除上述方法外，还采用了样式转移GAN[15]进行数据增强，这种使用可以有效地减少CNN学习到的纹理偏差。</p><p>与上面提出的各种方法不同，其他一些bag of freebies都致力于解决数据集中的语义分布可能存在偏差的问题。在处理语义分布偏差问题时，一个非常重要的问题是不同类之间存在数据不平衡的问题，这个问题通常通过两级对象检测器中的硬负例挖掘[72]或在线硬例挖掘[67]来解决。但该示例挖掘方法不适用于单级对象检测器，因为这种检测器属于密集预测体系结构。因此，Lin等人[45]提出了焦点损失来解决不同类之间存在的数据不平衡问题。另一个非常重要的问题是，很难表达不同类别之间的关联程度与单一热硬表示之间的关系。这种表示方案经常用于执行标记。[73]中提出的标签平滑方法是将硬标签转换为软标签进行训练，使模型的鲁棒性更强。为了获得更好的软标签，Islam等人引入了知识精馏的概念来设计标签细化网络</p><p>最后bag of freebies是边界盒(BBox)回归的目标函数。传统的对象检测器通常使用均方误差(MSE)直接对BBox的中心点坐标和高度和宽度进行回归，{,w、h}或左上角点和右下角点。对于基于锚的方法，是估计相应的偏移量，例如和,然而，直接估计BBox中每个点的坐标值是要将这些点作为自变量来处理，但实际上并没有考虑对象本身的完整性。为了更好地处理这一问题，一些研究人员最近提出了IoU损失[90]，它考虑了预测的BBox区域和地面真实BBox区域的覆盖范围。IoU损失计算过程将通过使用地面真相执行IoU，触发BBox的四个坐标点的计算，然后将生成的结果连接到一个整个代码中。由于IoU是一种尺度不变表示，它可以解决传统方法计算{x、y、w、h}的l1或l2损失时，损失会随着尺度的增加而增加的问题。最近，一些研究人员继续改善IoU的损失。例如，GIoU损失[65]除了包括覆盖区域外，还包括物体的形状和方向。他们提出找到能够同时覆盖预测的BBox和地面真实BBox的最小面积的BBox，并使用该BBox作为分母来代替IoU损失中最初使用的分母。对于DIoU损失[99]，它另外考虑了物体中心的距离，而CIoU损失[99]则同时考虑了重叠面积、中心点之间的距离和高宽比。CIoU在BBox回归问题上可以获得更好的收敛速度和精度。</p><blockquote><h4 id="BoF方法一：数据增强"><a href="#BoF方法一：数据增强" class="headerlink" title="BoF方法一：数据增强"></a>BoF方法一：数据增强</h4><p><strong>（1）像素级调整</strong></p><p>①光度失真： brightness(亮度)、contrast(对比度)、hue(色度)、saturation(饱和度)、noise(噪声)</p><p>②几何失真： scaling(缩放尺寸)、cropping(裁剪)、flipping(翻转)、rotating(旋转)</p><p><strong>（2）模拟目标遮挡</strong></p><p>①erase(擦除)、CutOut(剪切)： 随机选择图像的矩形区域，并填充随机或互补的零值</p><p>②hide-and-seek和grid mask： 随机或均匀地选择图像中的多个矩形区域，并将它们替换为全零</p><p>③将上述方式作用于特征图上： DropOut、DropConnect、DropBlock</p><p><strong>（3）将多张图像组合在一起</strong></p><p>①MixUp： 使用两个图像以不同的系数比率相乘后叠加，利用叠加比率调整标签</p><p>②CutMix： 将裁剪的图像覆盖到其他图像的矩形区域，并根据混合区域大小调整标签</p><p><strong>（4）使用style transfer GAN进行数据扩充，有效减少CNN学习到的纹理偏差。</strong></p><h4 id="BoF方法二：解决数据集中语义分布偏差问题"><a href="#BoF方法二：解决数据集中语义分布偏差问题" class="headerlink" title="BoF方法二：解决数据集中语义分布偏差问题"></a>BoF方法二：解决数据集中语义分布偏差问题</h4><p><strong>①两阶段对象检测器：</strong> 使用硬反例挖掘或在线硬例挖掘来解决。不适用于单级目标检测。</p><p><strong>②单阶段目标检测器：</strong> focal损来处理各个类之间存在的数据不平衡问题。</p><h4 id="BoF方法三：边界框-BBox-回归的目标函数"><a href="#BoF方法三：边界框-BBox-回归的目标函数" class="headerlink" title="BoF方法三：边界框(BBox)回归的目标函数"></a>BoF方法三：边界框(BBox)回归的目标函数</h4><p><strong>①IoU损失：</strong> 将预测BBox区域的区域和真实BBox区域考虑在内。由于IoU是尺度不变的表示，它可以解决传统方法在计算{x, y, w, h}的l1或l2损耗时，损耗会随着尺度的增大而增大的问题。</p><p><strong>②GIoU loss：</strong> 除了覆盖区域外，还包括了物体的形状和方向。他们提出寻找能够同时覆盖预测BBox和地面真实BBox的最小面积BBox，并以此BBox作为分母来代替IoU损失中原来使用的分母。</p><p><strong>③DIoU loss：</strong> 它额外考虑了物体中心的距离。</p><p><strong>④CIoU loss ：</strong> 同时考虑了重叠区域、中心点之间的距离和纵横比。对于BBox回归问题，CIoU具有更好的收敛速度和精度。</p></blockquote><h3 id="2-3-Bag-of-specials"><a href="#2-3-Bag-of-specials" class="headerlink" title="2.3 Bag of specials"></a>2.3 Bag of specials</h3><p>对于那些只增加少量推理成本但又能显著提高目标检测精度的插件模块和后处理方法，我们称它们为“bag of specials”。一般来说，这些插件模块是用于增强模型中的某些属性，如扩大接受域、引入注意机制或增强特征整合能力等，而后处理是筛选模型预测结果的一种方法。 </p><p>可用于增强感受野的常见模块是SPP[25]、ASPP[5]和RFB[47]。SPP模块起源于空间金字塔匹配(SPM)[39]，SPMs的原始方法是将特征映射分割成几个d×d相等的块，其中d可以是{1,2,3，…}，从而形成空间金字塔，然后提取bag-of-word特征。SPP将SPM集成到CNN中，使用最大池化操作，而不是bag-of-word操作。由于He等人[25]提出的SPP模块将输出一维特征向量，因此在全卷积网络(FCN)中应用是不可行的。因此，在YOLOv3[63]的设计中，Redmon和Farhadi将SPP模块改进为核大小为k×k，其中k={1,5,9,13}，步幅等于1。在这种设计下，相对较大的最大池有效地增加了主干特征的接受域。 在添加改进版本的SPP模块后，YOLOv3-608在MS COCO目标检测任务上将AP50升级了2.7%，额外计算0.5%。ASPP[5]模块与改进的SPP模块在操作上的差异主要是从原始的k×k核大小，步幅最大池化等于1到多个3×3核大小，扩张比等于k，步幅等于1。RFB模块采用k×k核的多个扩张卷积，扩张比等于k，步幅等于1，以获得比ASPP更全面的空间覆盖。RFB[47]只需要花费7%的额外推理时间，就可以使MS COCO上的SSD的AP50增加5.7%。</p><p>目标检测中常用的注意模块主要分为通道式注意和点态注意，这两种注意模型的代表分别是Squeeze-and-Excitation (SE)[29]和空间注意模块(SAM)[85]。虽然SE模块可以提高ResNet50的力量在ImageNet图像分类任务1%top-1精度的只增加2%计算，但在GPU通常将使推理时间增加约10%，所以它更适合用于移动设备。但对于SAM，它只需要额外支付0.1%的计算量，就可以将ResNet50-SE提高到ImageNet图像分类任务的0.5%的top-1精度。最重要的是，它根本不影响GPU上的推理速度。</p><p>在特征集成方面，早期的实践是使用skip connection[51]或hyper-column[22]将低级物理特征与高级语义特征进行集成。随着FPN等多尺度预测方法越来越流行，人们提出了许多整合不同特征金字塔的轻量级模块。这类模块包括SFAM[98]、ASFF[48]和BiFPN[77]。SFAM的主要思想是利用SE模块在多尺度连接的特征图上执行信道级重加权。对于ASFF，它使用softmax作为点级重新加权，然后添加不同尺度的特征图。在BiFPN中，提出了多输入加权残差连接来进行尺度水平重加权，然后添加不同尺度的特征图。</p><p>在深度学习的研究中，一些人将重点放在寻找良好的激活函数上。一个好的激活函数可以使梯度更有效地传播，同时也不会造成太多的额外计算成本。2010年，Nair和Hinton[56]提出ReLU来实质上解决传统的tanh和s型激活函数中经常遇到的梯度消失问题。随后，提出了LReLU[54]、PReLU[24]、ReLU6[28]、尺度指数线性单位(SELU)[35]、Swish[59]、hard-Swish[27]、Mish[55]等，它们也被用于解决梯度消失问题。LReLU和PReLU的主要目的是解决当输出小于零时，ReLU的梯度为零的问题。对于ReLU6和hard-swish，它们是专门为量化网络设计的。对于神经网络的自归一化，提出了SELU激活函数来满足该目标。需要注意的一点是，Swish和Mish都是连续可区分的激活函数。 </p><p>在基于深度学习的对象检测中常用的后处理方法是NMS，它可以用于过滤那些预测同一对象不好的预测框，并且只保留响应率较高的候框。NMS试图改进的方法与优化目标函数的方法是一致的。NMS提出的原始方法不考虑上下文信息，因此Girshick等[19]在R-CNN中添加分类置信分数作为参考，根据置信分数的顺序，按照高到低的顺序进行greedy NMS。对于soft NMS[1]，它考虑了对象的遮挡在greedy NMS中可能导致置信度分数下降的问题。DIoU NMS[99]开发者的思维方式是在soft NMS的基础上，将中心点距离的信息添加到BBox的筛选过程中。值得一提的是，由于上述所有的后处理方法都没有直接涉及到所捕获的图像特征，因此在后续的无锚定方法的开发中，不再需要后处理。</p><blockquote><h4 id="BoS方法一：插件模块之增强感受野"><a href="#BoS方法一：插件模块之增强感受野" class="headerlink" title="BoS方法一：插件模块之增强感受野"></a>BoS方法一：插件模块之增强感受野</h4><p><strong>①改进的SPP模块</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-SPP.png" alt=""></p><p><strong>②ASPP模块</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-ASPP.png" alt=""></p><p><strong>③RFB模块</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-RFB.png" alt=""></p><h4 id="BoS方法二：插件模块之注意力机制"><a href="#BoS方法二：插件模块之注意力机制" class="headerlink" title="BoS方法二：插件模块之注意力机制"></a>BoS方法二：插件模块之注意力机制</h4><p><strong>①channel-wise注意力：</strong> 代表是Squeeze-and-Excitation挤压激励模块(SE)。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-SE.png" alt=""></p><p><strong>②point-wise注意力：</strong> 代表是Spatial Attention Module空间注意模块(SAM)。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-SAM.png" alt=""></p><h4 id="BoS方法三：插件模块之特征融合"><a href="#BoS方法三：插件模块之特征融合" class="headerlink" title="BoS方法三：插件模块之特征融合"></a>BoS方法三：插件模块之特征融合</h4><p><strong>①SFAM：</strong> 主要思想是利用SE模块在多尺度的拼接特征图上进行信道级重加权。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-SFAM.png" alt=""></p><p><strong>②ASFF：</strong> 使用softmax对多尺度拼接特征图在点维度进行加权。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-ASFF.png" alt=""></p><p><strong>③BiFPN：</strong> 提出了多输入加权剩余连接来执行按比例的水平重加权，然后添加不同比例的特征图。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-BiFPN.png" alt=""></p><h4 id="BoS方法四：激活函数"><a href="#BoS方法四：激活函数" class="headerlink" title="BoS方法四：激活函数"></a>BoS方法四：激活函数</h4><p><strong>①LReLU和PReLU：</strong> 主要目的是解决输出小于0时ReLU的梯度为零的问题。</p><p><strong>②ReLU6和hard-Swish：</strong> 专门为量化网络设计的。</p><p><strong>③SELU：</strong> 针对神经网络的自归一化问题。</p><p><strong>④Swish和Mish：</strong> 都是连续可微的激活函数。</p><h4 id="BoS方法五：后处理"><a href="#BoS方法五：后处理" class="headerlink" title="BoS方法五：后处理"></a>BoS方法五：后处理</h4><p><strong>①NMS：</strong> 目标检测中常用的后处理方法是NMS, NMS可以对预测较差的bbox进行过滤，只保留响应较高的候选bbox。NMS试图改进的方法与优化目标函数的方法是一致的。NMS提出的原始方法没有考虑上下文信息，所以在R-CNN中加入了分类的置信分作为参考，按照置信分的顺序，从高到低依次进行贪心NMS。</p><p><strong>②soft NMS：</strong> 考虑了对象的遮挡可能导致带IoU分数的贪婪NMS的信心分数下降的问题。</p><p><strong>③DIoU NMS：</strong> 在soft NMS的基础上，将中心点距离信息添加到BBox筛选过程中。值得一提的是，由于以上的后处理方法都没有直接引用捕获的图像特征，因此在后续的无锚方法开发中不再需要后处理。</p></blockquote><h2 id="三、Methodology—方法"><a href="#三、Methodology—方法" class="headerlink" title="三、Methodology—方法"></a><strong>三、Methodology—方法</strong></h2><h3 id="3-1-Selection-of-architecture—架构选择"><a href="#3-1-Selection-of-architecture—架构选择" class="headerlink" title="3.1 Selection of architecture—架构选择"></a>3.1 Selection of architecture—架构选择</h3><p>我们的目标是在输入网络分辨率、卷积层数、参数数（滤波器大小2<em>滤波器</em>通道/组）和层输出数（滤波器）之间找到最优的平衡。例如，我们的大量研究表明，在ILSVRC2012(ImageNet)数据集[10]上，CSPResNext50比CSPDarknet53要好得多。然而，相反地，在检测MS COCO数据集[46]上的对象方面，CSPDarknet53比CSPResNext50更好。</p><p>下一个目标是选择额外的块来增加感受野，以及从不同检测器级别的参数聚合的最佳方法：例如FPN、PAN、ASFF、BiFPN。 </p><p>对于分类最优的参考模型对于探测器来说并不总是最优的。与分类器相比，该探测器需要以下条件：</p><ul><li>更高的输入网络大小（分辨率）</li><li>用于检测多个小大小的物体更多的层</li><li>更高的接受域以覆盖增加的输入网络大小更多的参数</li><li>模型更大的能力来检测单一图像中多个不同大小的物体</li></ul><p>假设来说，我们可以假设应该选择一个具有更大的接受场大小（具有更多的卷积层3×3）和更多的参数的模型作为主干。表1显示了CSPResNeXt50、CSPDarknet53和efficientnetB3的信息。CSPResNext50只包含16个卷积层3×3、一个425×425感受野和20.6 M参数，而CSPDarknet53包含29个卷积层3×3、一个725×725感受野和27.6 M参数。这一理论证明，加上我们进行的大量实验，表明CSPDarknet53神经网络是两者作为探测器主干的最佳模型。 </p><p>不同大小的感受野的影响总结如下：</p><ul><li>到对象大小，允许查看整个对象到网络大小</li><li>允许查看对象周围的上下文</li><li>增加图像点和最终激活之间的连接数量 </li></ul><p>我们在CSPDarknet53上添加了SPP块，因为它显著地增加了接受域，分离出了最重要的上下文特征，并且几乎不会导致降低网络运行速度。我们使用PANet作为来自不同检测器级别的不同主干级别的参数聚合的方法，而不是在YOLOv3中使用的FPN。</p><p>最后，我们选择CSPDarknet53主干、SPP附加模块、PANet路径聚合颈和YOLOv3（基于锚点）的头作为YOLOv4的体系结构。</p><p>未来，我们计划显著扩展检测器的f Bag of Freebies(BoF)的内容，理论上可以解决一些问题，提高检测器的精度，并以实验方式依次检查每个特征的影响。<br>我们不使用Cross-GPU批处理归一化(CGBN或SyncBN)或昂贵的专用设备。这允许任何人都可以在传统的图形处理器上再现我们最先进的结果，例如GTX 1080Ti或RTX 2080Ti。 </p><blockquote><h4 id="架构选择目标"><a href="#架构选择目标" class="headerlink" title="架构选择目标"></a>架构选择目标</h4><p><strong>目标一：在输入网络分辨率、卷积层数、参数数(filter size2×filters × channel / groups)和层输出数(filters)之间找到最优平衡。</strong></p><p>检测器需要满足以下条件：</p><p><strong>①更高的输入网络大小(分辨率)：</strong> 用于检测多个小型对象</p><p><strong>②更多的层：</strong> 一个更高的接受域，以覆盖增加的输入网络的大小</p><p><strong>③更多的参数：</strong> 模型有更强大的能力，以检测单个图像中的多个不同大小的对象。</p><p><strong>目标二：选择额外的块来增加感受野</strong></p><p>不同大小的感受野的影响总结如下：</p><p><strong>①对象大小：</strong> 允许查看整个对象</p><p><strong>②网络大小：</strong> 允许查看对象周围的上下文</p><p><strong>③超过网络大小：</strong> 增加图像点和最终激活之间的连接数</p><p><strong>目标三：选择不同的主干层对不同的检测器层(如FPN、PAN、ASFF、BiFPN)进行参数聚合的最佳方法。</strong></p><h4 id="YOLOv4架构"><a href="#YOLOv4架构" class="headerlink" title="YOLOv4架构"></a>YOLOv4架构</h4><p><strong>（1）CSPDarknet53主干（backbone）：</strong> 作者实验对比了CSPResNext50、CSPDarknet53和EfficientNet-B3。从理论与实验角度表明：CSPDarkNet53更适合作为检测模型的Backbone。（还是自家的网络结构好用）</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-backbone.png" alt=""></p><blockquote><p>CSP介绍：</p><p>CSP是可以增强CNN学习能力的新型backbone，论文发表2019年11月份</p><p><strong>主要技巧：</strong>CSPNet将底层的特征映射分为两部分，一部分经过密集块和过渡层，另一部分与传输的特征映射结合到下一阶段。</p></blockquote><p><strong>（2）SPP附加模块增加感受野：</strong> 在CSPDarknet53上添加了SPP块，SPP来源于何恺明大佬的SPP Net因为它显著增加了接受域，分离出了最重要的上下文特性，并且几乎不会降低网络运行速度。</p><p><strong>（3）PANet路径聚合（neck）：</strong> PANet主要是特征融合的改进，使用PANet作为不同检测层的不同主干层的参数聚合方法。而不是YOLOv3中使用的FPN。</p><p><strong>（4）基于锚的YOLOv3头部（head）：</strong> 因为是anchor-base方法，因此分类、回归分支没有改变。</p><p><strong>总结：</strong> YOLOv4模型 = CSPDarkNet53 + SPP + PANet(path-aggregation neck) + YOLOv3-head</p></blockquote><h3 id="3-2-Selection-of-BoF-and-BoS—BoF和BoS的选择"><a href="#3-2-Selection-of-BoF-and-BoS—BoF和BoS的选择" class="headerlink" title="3.2 Selection of BoF and BoS—BoF和BoS的选择"></a>3.2 Selection of BoF and BoS—BoF和BoS的选择</h3><p>为了改进目标检测训练，CNN通常使用以下：</p><ul><li>激活：ReLU, leaky-ReLU, parametric-ReLU,ReLU6, SELU, Swish, or Mish</li><li>边界盒回归损失：MSE，IoU、GIoU、CIoU、DIoU</li><li>数据增强：CutOut, MixUp, CutMix</li><li>正则化方法：DropOut, DropPath，Spatial DropOut [79], or DropBlock</li><li>规范化的网络激活（通过均值和方差）：批标准化(BN)[32]，Cross-GPU Batch Normalization(CGBN或SyncBN)[93]，Filter Response Normalization(FRN)[70]，或交叉迭代批标准化(CBN)[89]</li><li>Skip-connections：Residual connections，加权Residual connections、多输入加权Residual connections或Cross stage partial连接(CSP) </li></ul><p>对于训练激活函数，由于PReLU和SELU更难训练，而且ReLU6是专门为量化网络设计的，因此我们将上述激活函数从候选列表中删除。在需求化方法上，发表DropBlock的人将其方法与其他方法进行了详细的比较，其正则化方法获得了很大的成功。因此，我们毫不犹豫地选择了DropBlock作为我们的正则化方法。至于归一化方法的选择，由于我们关注于只使用一个GPU的训练策略，因此不考虑syncBN。 </p><blockquote><p>为了提高目标检测训练，CNN通常使用以上提到的方法</p><ul><li><p><strong>（1）激活函数：</strong> 由于PReLU和SELU更难训练，我们选择专门为量化网络设计的ReLU6</p></li><li><p><strong>（2）正则化：</strong> 我们选择DropBlock</p></li><li><strong>（3）归一化：</strong> 由于是单GPU，所以没有考虑syncBN</li></ul></blockquote><h3 id="3-3-Additional-improvements—进一步改进"><a href="#3-3-Additional-improvements—进一步改进" class="headerlink" title="3.3 Additional improvements—进一步改进"></a>3.3 Additional improvements—进一步改进</h3><p>为了使设计的探测器更适合训练单GPU上，我们做了额外的设计和改进如下：</p><p> 我们引入了一种新的数据增强Mosic，和自我对抗训练（SAT）<br>我们选择最优超参数而应用遗传算法<br>我们修改一些现有方法使设计适合有效的训练和检测，modifified SAM，modifified PAN，和交叉小批归一化(CmBN) </p><p>Mosaic代表了一种新的数据增强方法，它混合了4个训练图像。因此，混合了4种不同的上下文，而CutMix只混合了2个输入图像。这允许检测其正常上下文之外的对象。此外，批归一化计算每一层上4个不同图像的激活统计信息。这大大减少了对大型小批量处理的需求</p><p>自对抗训练(SAT)也代表了一种新的数据增强技术，可以在2个向前向后的阶段运行。在第一阶段，神经网络改变了原始图像，而不是网络的权值。通过这种方式，神经网络对自己进行敌对性攻击，改变原始图像，以制造出图像上没有想要的目标的欺骗。在第二阶段，神经网络被训练以正常的方式检测修改后的图像上的对象。 </p><blockquote><p><strong>（1）新的数据增强Mosic和自我对抗训练（SAT）</strong><br>①Mosaic： Mosaic代表了一种新的数据增强方法，它混合了4幅训练图像。基于现有数据极大的丰富了样本的多样性，极大程度降低了模型对于多样性学习的难度。</p><p><strong>②自对抗训练（SAT）：</strong></p><p>在第一阶段，神经网络改变原始图像而不是网络权值。通过这种方式，神经网络对自己执行一种对抗性攻击，改变原始图像，以制造图像上没有期望对象的假象。<br>在第二阶段，神经网络以正常的方式对这个修改后的图像进行检测。<br><strong>（2）应用遗传算法选择最优超参数</strong><br><strong>（3）修改现有的方法，使设计适合于有效的训练和检测</strong><br><strong>①修改的SAM：</strong> 将SAM从空间上的注意修改为点态注意</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-SAMpro.png" alt=""></p><p><strong>②修改PAN：</strong> 将PAN的快捷连接替换为shortcut 连接</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-PANpro.png" alt=""></p><p><strong>③交叉小批量标准化(CmBN)：</strong> CmBN表示CBN修改后的版本，如图所示，只在单个批内的小批之间收集统计信息。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-CmBNpro.png" alt=""></p></blockquote><h3 id="3-4-YOLOv4"><a href="#3-4-YOLOv4" class="headerlink" title="3.4 YOLOv4"></a>3.4 YOLOv4</h3><p> 在本节中，我们将详细说明YOLOv4的细节。</p><p>YOLOv4 consists of :<br>• Backbone: CSPDarknet53 [ 81 ]<br>• Neck: SPP [ 25 ], PAN [ 49 ]<br>• Head: YOLOv3 [ 63 ]</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-bag.png" alt=""></p><blockquote><h4 id="YOLOv4包括"><a href="#YOLOv4包括" class="headerlink" title="YOLOv4包括"></a>YOLOv4包括</h4><ul><li><strong>主干(backbone)：</strong> CSPDarknet53</li><li><strong>颈部(neck)：</strong> SPP ， PAN</li><li><strong>头(head)：</strong> YOLOv3</li></ul><p>YOLO v4使用</p><ul><li>Bag of Freebies 外在引入技巧： CutMix和Mosaic数据增强，DropBlock正则化，类标签平滑</li><li>Bag of Specials 网络改进技巧： Mish激活、跨级部分连接(CSP)、多输入加权剩余连接(MiWRC)</li><li>Bag of Freebies 外在检测器引入技巧： CIoU loss, CmBN, DropBlock正则化，Mosaic数据增强，自对抗训练，消除网格敏感性，为一个真值使用多个锚，余弦退火调度，最优超参数，随机训练形状</li><li>Bag of Specials检测器网络改进技巧： Mish激活、SPP-block、SAM-block、PAN路径聚合块、DIoU-NMS</li></ul></blockquote><h2 id="四、Experiments—实验"><a href="#四、Experiments—实验" class="headerlink" title="四、Experiments—实验"></a><strong>四、Experiments—实验</strong></h2><h3 id="4-1-Experimental-setup—实验设置"><a href="#4-1-Experimental-setup—实验设置" class="headerlink" title="4.1 Experimental setup—实验设置"></a>4.1 Experimental setup—实验设置</h3><p>在ImageNet图像分类实验中，默认的超参数如下：训练步骤为8000000；批大小和小批量大小分别为128和32；采用多项式衰减学习率调度策略，初始学习率为0.1；预热步骤为1000；动量衰减和权重衰减分别设置为0.9和0.005。我们所有的BoS实验都使用与默认设置相同的超参数，在BoF实验中，我们额外添加了50%的训练步骤。在BoF实验中，我们验证了MixUp、CutMix、Mosaic、模糊数据增强和标签平滑正则化方法。在BoS实验中，我们比较了LReLU、Swish和Mish激活功能的影响。所有实验均采用1080 Ti或2080TiGPU进行训练。 </p><p>在MS COCO目标检测实验中，默认的超参数如下：训练步长为500,500；采用步长衰减学习率调度策略，初始学习率为0.01，在400000步和450000步时分别乘以0.1倍；动量和权重衰减分别设置为0.9和0.0005。所有架构都使用一个GPU来执行批处理大小为64的多规模训练，而小批处理大小为8或4，这取决于架构和GPU内存限制。除在超参数搜索实验中使用遗传算法外，所有其他实验均使用默认设置。遗传算法使用YOLOv3-SPP对GIoU损失进行训练，并搜索300个时元的最小值5k集。我们采用搜索学习率0.00261，动量0.949，IoU阈值分配地面真值0.213，遗传算法实验采用损失归一化器0.07。我们验证了大量的BoF，包括网格灵敏度消除、Mosaic数据增强、IoU阈值、遗传算法、类标签平滑、交叉小批归一化、自对抗训练、余弦退火调度器、动态小批大小、dropblock、优化锚点、不同类型的IoU损失。我们还在各种BoS上进行了实验，包括Mish、SPP、SAM、RFB、BiFPN和高斯YOLO[8]。对于所有的实验，我们只使用一个GPU来进行训练，因此不使用优化多个GPU的像syncBN这样的技术。</p><blockquote><p>（1）在ImageNet图像分类实验中，默认超参数为：</p><ul><li><strong>训练步骤：</strong> 8,000,000</li><li><strong>批大小和小批大小分别：</strong> 128和32</li><li><strong>初始学习率：</strong> 0.1</li><li><strong>warm-up步长：</strong> 1000</li><li><strong>动量衰减：</strong> 0.9</li><li><strong>权重衰减：</strong> 0.005</li></ul><p>（2）在MS COCO对象检测实验中，默认的超参数为：</p><ul><li><strong>训练步骤：</strong> 500500</li><li><strong>初始学习率：</strong> 0.01</li><li><strong>warm-up步长：</strong> 在400,000步和450,000步分别乘以因子0.1</li><li><strong>动量衰减：</strong> 0.9</li><li><strong>权重衰减：</strong> 0.0005</li><li><strong>GPU数量：</strong> 1个</li><li><strong>批处理大小：</strong> 64</li></ul></blockquote><h3 id="4-2-Influence-of-different-features-on-Classifier-training—不同特征对分类器训练的影响"><a href="#4-2-Influence-of-different-features-on-Classifier-training—不同特征对分类器训练的影响" class="headerlink" title="4.2 Influence of different features on Classifier training—不同特征对分类器训练的影响"></a>4.2 Influence of different features on Classifier training—不同特征对分类器训练的影响</h3><p>首先，我们研究了不同特征对分类器训练的影响；具体来说，类标签平滑的影响，不同数据增强技术的影响，bilateral blurring, MixUp, CutMix and Mosaic，如Fugure7所示，以及不同激活的影响，如Leaky-relu（默认）、Swish和Mish。 </p><p>在我们的实验中，如表2所示，通过引入：CutMix和Mosaic数据增强、类标签平滑和Mish激活等特征，提高了分类器的精度。因此，我们用于分类器训练的BoF backbone(Bag of Freebies)包括以下内容：CutMix和Mosaic数据增强和类标签平滑。此外，我们使用Mish激活作为补充。 </p><blockquote><p>研究了不同特征对分类器训练的影响：类标签平滑的影响，不同数据增强技术的影响，不同的激活的影响。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-augmentation.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-Table23.png" alt=""></p><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>（1）通过引入特征如：CutMix和Mosaic数据增强、类标签平滑、Mish激活等，可以提高分类器的准确率。</p><p>（2）CutMix和Mosaic数据增强和类标签平滑可用于分类器训练的BoF backbone，此外，还可以使用Mish激活作为补充选项。</p></blockquote><h3 id="4-3-Influence-of-different-features-on-Detector-training—不同特征对检测器训练的影响"><a href="#4-3-Influence-of-different-features-on-Detector-training—不同特征对检测器训练的影响" class="headerlink" title="4.3 Influence of different features on Detector training—不同特征对检测器训练的影响"></a><strong>4.3 Influence of different features on Detector training—不同特征对检测器训练的影响</strong></h3><p> 进一步研究了不同的Bag-of Freebies(BoF-detector)对探测器训练精度的影响，如表4所示。通过研究在不影响FPS的情况下提高检测器精度的不同特征，我们显著地扩展了BoF列表： </p><p> S：消除网格灵敏度的公式 其中cx和cy总是整数，在YOLOv3中用于计算对象坐标，因此，对于接近cx或cx+1值的bx值，需要极高的tx绝对值。我们通过将s型矩阵乘以一个超过1.0的因子来解决这个问题，从而消除了对象无法检测到的网格的影响。</p><ul><li>M：Mosaic data-在训练期间使用4张图像的马赛克，而不是单一的图像 </li><li>IT：IoU阈值-使用多个锚作为单一地面真实IoU(truth, anchor) &gt;IoU阈值</li><li>GA：Genetic algorithms-在前10%的时间段内使用遗传算法选择最优超参数</li><li>LS:类标签平滑-使用类标签平滑的s型符号激活 </li><li>CBN：CmBN-使用交叉小批标准化来收集整个批内的统计信息，而不是在单个小批内收集统计数据</li><li>CA:余弦退火调度器-改变正弦波训练过程中的学习速率</li><li>DM：动态小批量大小-在小分辨率训练中，通过使用随机训练形状自动增加小批量大小</li><li>OA：优化的锚-使用优化的锚与512x512网络分辨率进行训练</li><li>GIoU，CIoU，DIoU，MSE-使用不同的损失算法进行边界框回归 </li></ul><p>进一步研究了不同的Bag-of-Specials (bos-检测器)对检测器训练精度的影响，包括PAN、RFB、SAM、高斯YOLO(G)和ASFF，如表5所示。在我们的实验中，检测器在使用SPP、PAN和SAM时性能最好。</p><blockquote><p>进一步的研究关注不同Bag-of-Freebies免费包 (BoF-detector)对检测器训练精度的影响，通过研究在不影响FPS（帧率：每秒传输的帧数）的情况下提高检测器精度的不同特征，我们显著扩展了BoF列表：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-Table4.png" alt=""></p><p>表4：Bag-of-Freebies 的消融研究。( CSPResNeXt50 - PANet - SPP , 512 × 512)。 粗体黑色表示有效</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-Table5.png" alt=""></p><h4 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h4><p>当使用SPP、PAN和SAM时，检测器的性能最佳。</p></blockquote><h3 id="4-4-Influence-of-different-backbones-and-pre-trained-weightings-on-Detector-training—不同的backbone和预先训练权重对检测器训练的影响"><a href="#4-4-Influence-of-different-backbones-and-pre-trained-weightings-on-Detector-training—不同的backbone和预先训练权重对检测器训练的影响" class="headerlink" title="4.4 Influence of different backbones and pre- trained weightings on Detector training—不同的backbone和预先训练权重对检测器训练的影响"></a>4.4 Influence of different backbones and pre- trained weightings on Detector training—不同的backbone和预先训练权重对检测器训练的影响</h3><p>进一步研究了不同主干模型对检测器精度的影响，如表6所示。我们注意到，具有最佳分类精度特征的模型在检测器精度方面并不总是最好的。</p><p>首先，虽然使用不同特征训练的CSPResNeXt-50模型的分类精度高于CSPDarknet53模型，但CSPDarknet53模型在目标检测方面具有更高的精度。</p><p>其次，使用CSPResF和Mish进行50分类器训练可以提高分类精度，但进一步应用这些预先训练的权重用于检测器训练会降低检测器的精度。然而，在CSPDarknet53分类器训练中使用BoF和Mish可以提高分类器和使用该分类器预训练的加权的检测器的准确性。最终的结果是，主干CSPDarknet53比CSPResNeXt50更适合用于检测器。</p><p>我们观察到，CSPDarknet53模型由于各种改进，显示出更大的能力来提高探测器的精度。</p><blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-Table6.png" alt=""></p><p>表6：使用不同的分类器预训练权重进行检测器训练(所有其他训练参数在所有模型中都是相似的)。</p><h4 id="结论-2"><a href="#结论-2" class="headerlink" title="结论"></a>结论</h4><ul><li>具有最佳分类精度的模型在检测器精度方面并不总是最佳的。</li><li>骨干CSPDarknet53比CSPResNeXt50更适合于检测器。</li><li>由于各种改进，CSPDarknet53模型展示了更大的能力来提高检测器的精度。</li></ul></blockquote><h3 id="4-5-Influence-of-different-mini-batch-size-on-Detec-tor-training—不同的小批尺寸对检测器培训的影响"><a href="#4-5-Influence-of-different-mini-batch-size-on-Detec-tor-training—不同的小批尺寸对检测器培训的影响" class="headerlink" title="4.5 Influence of different mini-batch size on Detec- tor training—不同的小批尺寸对检测器培训的影响"></a>4.5 Influence of different mini-batch size on Detec- tor training—不同的小批尺寸对检测器培训的影响</h3><p>最后，我们分析了用不同的小批量训练的模型得到的结果，结果如表7所示。从表7所示的结果中，我们发现在添加BoF和BoS训练策略后，小批量大小对检测器的性能几乎没有影响。这一结果表明，在引入BoF和BoS后，不再需要使用昂贵的gpu进行训练。换句话说，任何人都只能使用一个普通的GPU来训练一个优秀的探测器。</p><blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-Table7.png" alt=""></p><p>表7：使用不同的 mini-batch size 进行检测器训练。</p><h4 id="结论-3"><a href="#结论-3" class="headerlink" title="结论"></a>结论</h4><ul><li>加入BoF和BoS训练策略后，小批量大小对检测器的性能几乎没有影响。</li><li>minibatch越大越好，CSPDarknet53对minibatch不敏感，利于单卡训练。</li><li>在引入BoF和BoS之后，不再需要使用昂贵的GPU进行训练。</li></ul></blockquote><h2 id="五、Results—结果"><a href="#五、Results—结果" class="headerlink" title="五、Results—结果"></a>五、Results—结果</h2><p>与其他最先进的对象检测器所获得的结果的比较如图8所示。我们的YOLOv4位于Pareto最优性曲线上，在速度和精度方面都优于最快和最精确的探测器。 </p><p>由于不同的方法使用不同架构的gpu进行推理时间验证，我们在通常采用的Maxwell、Pascal和Volta架构的gpu上操作YOLOv4，并将它们与其他最先进的方法进行比较。表8列出了使用MaxwellGPU的帧率比较结果，它可以是GTX TitanX（Maxwell）或TeslaM40GPU。表9列出了使用PascalGPU的帧率比较结果，可以是TitanX(Pascal)、TitanXp、GTX 1080Ti或特斯拉P100GPU。如表10所述，它列出了使用VoltaGPU的帧率比较结果，可以是Titan Volta或Tesla V100GPU。 </p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-Figure8.png" alt=""></p><p>图8 不同物体探测器的速度和精度比较。(一些文章只针对其中一个GPU : Maxwell / Pascal / Volta ，阐述了它们探测器的FPS)</p><p><strong>结论</strong></p><ul><li>得到的结果与其他最先进的物体探测器的比较如图8所示。我们的YOLOv4位于Pareto最优曲线上，无论是速度还是精度都优于最快最准确的检测器。</li><li>由于不同的方法使用不同架构的gpu进行推理时间验证，我们在Maxwell架构、Pascal架构和Volta架构常用的gpu上运行YOLOv4，并与其他最先进的方法进行比较。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>ET</title>
      <link href="/ET/"/>
      <url>/ET/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>os</title>
      <link href="/os/"/>
      <url>/os/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>yolov1-code</title>
      <link href="/yolov1-code/"/>
      <url>/yolov1-code/</url>
      
        <content type="html"><![CDATA[<h2 id=""><a href="#" class="headerlink" title="#"></a>#</h2><h2 id="writetxt-py"><a href="#writetxt-py" class="headerlink" title="writetxt.py"></a>writetxt.py</h2><h3 id="解析-XML-文件"><a href="#解析-XML-文件" class="headerlink" title="解析 XML 文件"></a>解析 XML 文件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析 XML 文件</span></span><br><span class="line">tree = ET.parse(<span class="string">&#x27;example.xml&#x27;</span>)  <span class="comment"># 假设这个 XML 文件的名称是 &#x27;example.xml&#x27;</span></span><br><span class="line">root = tree.getroot()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历 XML 文件中的 &#x27;object&#x27; 元素</span></span><br><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> root.findall(<span class="string">&#x27;object&#x27;</span>):</span><br><span class="line">    <span class="comment"># 获取对象的类别名称</span></span><br><span class="line">    name = obj.find(<span class="string">&#x27;name&#x27;</span>).text</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取对象的边界框坐标</span></span><br><span class="line">    bndbox = obj.find(<span class="string">&#x27;bndbox&#x27;</span>)</span><br><span class="line">    xmin = <span class="built_in">int</span>(bndbox.find(<span class="string">&#x27;xmin&#x27;</span>).text)</span><br><span class="line">    ymin = <span class="built_in">int</span>(bndbox.find(<span class="string">&#x27;ymin&#x27;</span>).text)</span><br><span class="line">    xmax = <span class="built_in">int</span>(bndbox.find(<span class="string">&#x27;xmax&#x27;</span>).text)</span><br><span class="line">    ymax = <span class="built_in">int</span>(bndbox.find(<span class="string">&#x27;ymax&#x27;</span>).text)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Object: <span class="subst">&#123;name&#125;</span>, Bounding Box: [<span class="subst">&#123;xmin&#125;</span>, <span class="subst">&#123;ymin&#125;</span>, <span class="subst">&#123;xmax&#125;</span>, <span class="subst">&#123;ymax&#125;</span>]&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="root-findall解释"><a href="#root-findall解释" class="headerlink" title="root.findall解释"></a>root.findall解释</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印出所有部门和所有员工</span></span><br><span class="line">    <span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模拟的 XML 内容</span></span><br><span class="line">    xml_content = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    &lt;company&gt;</span></span><br><span class="line"><span class="string">        &lt;department name=&quot;Development&quot;&gt;</span></span><br><span class="line"><span class="string">            &lt;employee&gt;John&lt;/employee&gt;</span></span><br><span class="line"><span class="string">            &lt;employee&gt;Alice&lt;/employee&gt;</span></span><br><span class="line"><span class="string">        &lt;/department&gt;</span></span><br><span class="line"><span class="string">        &lt;department name=&quot;HR&quot;&gt;</span></span><br><span class="line"><span class="string">            &lt;employee&gt;Bob&lt;/employee&gt;</span></span><br><span class="line"><span class="string">            &lt;employee&gt;Eve&lt;/employee&gt;</span></span><br><span class="line"><span class="string">        &lt;/department&gt;</span></span><br><span class="line"><span class="string">    &lt;/company&gt;</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从字符串加载 XML</span></span><br><span class="line">    tree = ET.ElementTree(ET.fromstring(xml_content))</span><br><span class="line">    root = tree.getroot()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印所有部门</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;部门列表:&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> dept <span class="keyword">in</span> root.findall(<span class="string">&#x27;department&#x27;</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;- <span class="subst">&#123;dept.attrib[<span class="string">&#x27;name&#x27;</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印所有员工</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n员工列表:&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> emp <span class="keyword">in</span> root.findall(<span class="string">&#x27;.//employee&#x27;</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;- <span class="subst">&#123;emp.text&#125;</span>&quot;</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">.//employee : xml语言中，.表示当前层级company，//表示s</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">部门列表:</span><br><span class="line">- Development</span><br><span class="line">- HR</span><br><span class="line"></span><br><span class="line">员工列表:</span><br><span class="line">- John</span><br><span class="line">- Alice</span><br><span class="line">- Bob</span><br><span class="line">- Eve</span><br></pre></td></tr></table></figure><h3 id="XML-文件内容解释"><a href="#XML-文件内容解释" class="headerlink" title="XML 文件内容解释"></a>XML 文件内容解释</h3><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">1. `&lt;annotation&gt;`: 这是根元素，表示整个文件是一个注释文档。</span><br><span class="line">2. `&lt;folder&gt;VOC2012&lt;/folder&gt;`: 图片所在的文件夹名称。</span><br><span class="line">3. `&lt;filename&gt;2007_000027.jpg&lt;/filename&gt;`: 被标注的图片的文件名。</span><br><span class="line">4. `&lt;source&gt;`: 描述图片的来源信息。</span><br><span class="line">    - `&lt;database&gt;The VOC2007 Database&lt;/database&gt;`: 图片来自的数据库。</span><br><span class="line">    - `&lt;annotation&gt;PASCAL VOC2007&lt;/annotation&gt;`: 使用的标注信息。</span><br><span class="line">    - `&lt;image&gt;flickr&lt;/image&gt;`: 图片来源（这里是Flickr）。</span><br><span class="line">5. `&lt;size&gt;`: 图片的尺寸。</span><br><span class="line">    - `&lt;width&gt;486&lt;/width&gt;`: 图片的宽度。</span><br><span class="line">    - `&lt;height&gt;500&lt;/height&gt;`: 图片的高度。</span><br><span class="line">    - `&lt;depth&gt;3&lt;/depth&gt;`: 图片的颜色通道数（这里是3，即 RGB）。</span><br><span class="line">6. `&lt;segmented&gt;0&lt;/segmented&gt;`: 表明图片是否被分割过，0表示没有分割。</span><br><span class="line">7. `&lt;object&gt;`: 标注的对象。</span><br><span class="line">    - `&lt;name&gt;person&lt;/name&gt;`: 对象的类别名称，这里是 &quot;person&quot;。</span><br><span class="line">    - `&lt;pose&gt;Unspecified&lt;/pose&gt;`: 对象的姿态，这里没有特别指定。</span><br><span class="line">    - `&lt;truncated&gt;0&lt;/truncated&gt;`: 表明对象是否被截断，0表示没有被截断。</span><br><span class="line">    - `&lt;difficult&gt;0&lt;/difficult&gt;`: 表明对象是否难以识别，0表示容易识别。</span><br><span class="line">    - `&lt;bndbox&gt;`: 对象的边界框坐标。</span><br><span class="line">        - `&lt;xmin&gt;174&lt;/xmin&gt;`: 边界框左上角的 x 坐标。</span><br><span class="line">        - `&lt;ymin&gt;101&lt;/ymin&gt;`: 边界框左上角的 y 坐标。</span><br><span class="line">        - `&lt;xmax&gt;349&lt;/xmax&gt;`: 边界框右下角的 x 坐标。</span><br><span class="line">        - `&lt;ymax&gt;351&lt;/ymax&gt;`: 边界框右下角的 y 坐标。</span><br><span class="line">    - `&lt;part&gt;`: 描述对象的部分（如果有）。（专注于分割任务）</span><br><span class="line">        - 每个 `&lt;part&gt;` 元素包含部分的名称（如 &quot;head&quot;、&quot;hand&quot;、&quot;foot&quot;）和该部分的边界框坐标。</span><br></pre></td></tr></table></figure><h3 id="打印图片坐标"><a href="#打印图片坐标" class="headerlink" title="打印图片坐标"></a>打印图片坐标</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图像的路径</span></span><br><span class="line">image_path = <span class="string">r&#x27;D:\Desktop\QNJS\Model\Yolo\Yolov1\YOLOV1-pytorch\VOCdevkit\JPEGImages\2007_000027.jpg&#x27;</span>  <span class="comment"># 替换为你的图像文件路径</span></span><br><span class="line">image = cv2.imread(image_path)</span><br><span class="line"><span class="keyword">if</span> image <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;无法加载图像，请检查你的路径。&quot;</span>)</span><br><span class="line">    exit()</span><br><span class="line"></span><br><span class="line">height, width = image.shape[:<span class="number">2</span>]  <span class="comment"># 高、宽、通道</span></span><br><span class="line">right_bottom = (width - <span class="number">1</span>, height - <span class="number">1</span>)  <span class="comment"># x是宽 y是高</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;图像右下角的坐标是: <span class="subst">&#123;right_bottom&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="在图像上画正确框"><a href="#在图像上画正确框" class="headerlink" title="在图像上画正确框"></a>在图像上画正确框</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图像的路径</span></span><br><span class="line">image_path = <span class="string">&#x27;path/to/your/image.jpg&#x27;</span>  <span class="comment"># 替换为你的图像文件路径</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 边界框坐标</span></span><br><span class="line">bbox = [<span class="number">174</span>, <span class="number">101</span>, <span class="number">349</span>, <span class="number">351</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取图像</span></span><br><span class="line">image = cv2.imread(image_path)</span><br><span class="line"><span class="keyword">if</span> image <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Error: 无法加载图像。请检查你的路径。&quot;</span>)</span><br><span class="line">    exit()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画框</span></span><br><span class="line"><span class="comment"># bbox 的格式是 [xmin, ymin, xmax, ymax]</span></span><br><span class="line">cv2.rectangle(image, (bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]), (bbox[<span class="number">2</span>], bbox[<span class="number">3</span>]), (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图像</span></span><br><span class="line">cv2.imshow(<span class="string">&quot;Image with Bounding Box&quot;</span>, image)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)  <span class="comment"># 等待按键</span></span><br><span class="line">cv2.destroyAllWindows()  <span class="comment"># 关闭所有打开的窗口</span></span><br></pre></td></tr></table></figure><h3 id="字符串转为整数"><a href="#字符串转为整数" class="headerlink" title="字符串转为整数"></a>字符串转为整数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">obj_struct[<span class="string">&#x27;bbox&#x27;</span>] = [<span class="built_in">int</span>(<span class="built_in">float</span>(bbox.find(<span class="string">&#x27;xmin&#x27;</span>).text)),</span><br><span class="line">                      <span class="built_in">int</span>(<span class="built_in">float</span>(bbox.find(<span class="string">&#x27;ymin&#x27;</span>).text)),</span><br><span class="line">                      <span class="built_in">int</span>(<span class="built_in">float</span>(bbox.find(<span class="string">&#x27;xmax&#x27;</span>).text)),</span><br><span class="line">                      <span class="built_in">int</span>(<span class="built_in">float</span>(bbox.find(<span class="string">&#x27;ymax&#x27;</span>).text))]</span><br><span class="line"><span class="comment"># 直接int(&#x27;107.2&#x27;)会报错，所以必须先把 浮点数的字符串 转为 浮点数 后转为 整数</span></span><br></pre></td></tr></table></figure><h2 id="new-resnet-py"><a href="#new-resnet-py" class="headerlink" title="new_resnet.py"></a>new_resnet.py</h2>]]></content>
      
      
      <categories>
          
          <category> Model </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>requirement</title>
      <link href="/requirement/"/>
      <url>/requirement/</url>
      
        <content type="html"><![CDATA[<h2 id="环境打包指南"><a href="#环境打包指南" class="headerlink" title="环境打包指南"></a>环境打包指南</h2><h3 id="1-生成requirements-txt"><a href="#1-生成requirements-txt" class="headerlink" title="1 生成requirements.txt"></a>1 生成requirements.txt</h3><h4 id="1-1-生成环境包"><a href="#1-1-生成环境包" class="headerlink" title="1.1 生成环境包"></a>1.1 生成环境包</h4><p>一个包含你项目当前环境中所有已安装包的列表</p><p>最好切换到项目的根目录，这样会在当前位置生成txt文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip freeze &gt; requirements.txt</span><br></pre></td></tr></table></figure><h4 id="1-2-生成项目包"><a href="#1-2-生成项目包" class="headerlink" title="1.2 生成项目包"></a>1.2 生成项目包</h4><p>一个包含你项目代码中所有已安装包的列表</p><p>最好切换到项目的根目录，这样会在当前位置生成txt文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pipreqs ./ --encoding=utf-8</span><br></pre></td></tr></table></figure><h3 id="2-删除txt外的多余包"><a href="#2-删除txt外的多余包" class="headerlink" title="2 删除txt外的多余包"></a>2 删除txt外的多余包</h3><p>会卸载当前环境下所有不在 <code>requirements.txt</code> 文件中的包。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip-sync requirements.txt</span><br></pre></td></tr></table></figure><h3 id="3-查看当前环境下的安装包"><a href="#3-查看当前环境下的安装包" class="headerlink" title="3 查看当前环境下的安装包"></a>3 查看当前环境下的安装包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip list</span><br></pre></td></tr></table></figure><h3 id="4-下载txt内的要求包"><a href="#4-下载txt内的要求包" class="headerlink" title="4 下载txt内的要求包"></a>4 下载txt内的要求包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><h3 id="5-查看依赖包"><a href="#5-查看依赖包" class="headerlink" title="5 查看依赖包"></a>5 查看依赖包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip show `xxx`</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>datasets</title>
      <link href="/datasets/"/>
      <url>/datasets/</url>
      
        <content type="html"><![CDATA[<h1 id="PASCAL-VOC2012"><a href="#PASCAL-VOC2012" class="headerlink" title="PASCAL VOC2012"></a>PASCAL VOC2012</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>PASCALVOC挑战赛 (The PASCAL Visual object Classes ) 是一个世界级的<strong>计算机视觉挑战赛</strong>，PASCAL全称: Pattern Analysis, StaticalModeling and Computational Learning，是一个由欧盟资助的网络组织PASCALVOC挑战赛主要包括以下几类: <strong>图像分类</strong>(object classification)，<strong>目标检测</strong>(Object Detection)，<strong>目标分割</strong>(Object Segmentation)，<strong>动作识别</strong>(Action Classification)等</p><p>注意：2012年的test数据集不公开。故可以使用VOC2012的trainval进行训练，VOC2007的test进行测试。</p><h2 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h2><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">VOC2012</span><br><span class="line">├── Annotations             所有的图像标注信息（XML文件```  </span><br><span class="line">├── ImageSets         </span><br><span class="line">│   ├── Action              人的行为动作图像信息</span><br><span class="line">│   ├── Layout              人的各个部位图像信息</span><br><span class="line">│   ├── Main                目标检测分类图像信息```</span><br><span class="line">│   │   ├── train.txt       训练集 5717```</span><br><span class="line">│   │   ├── val.txt         验证集 5823```</span><br><span class="line">│   │   └── trainval.txt    训练集+验证集 11540```</span><br><span class="line">│   └── Segmentation        目标分割图像信息</span><br><span class="line">├── JPEGImages              所有图像文件```</span><br><span class="line">├── SegmentationClass       图像分割png图（基于分类）</span><br><span class="line">└── SegmentationObject      图像分割png图（基于目标）</span><br></pre></td></tr></table></figure><h2 id="XML格式"><a href="#XML格式" class="headerlink" title="XML格式"></a>XML格式</h2><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&lt;annotation&gt;                                 标注</span><br><span class="line">  &lt;folder&gt;JPEGImages&lt;/folder&gt;                   文件夹名</span><br><span class="line">  &lt;filename&gt;2007_000027.jpg&lt;/filename&gt;          文件名</span><br><span class="line">  &lt;source&gt;                                      来源</span><br><span class="line">    &lt;database&gt;The VOC2007 Database&lt;/database&gt;       数据库</span><br><span class="line">    &lt;annotation&gt;PASCAL VOC2007&lt;/annotation&gt;         标注</span><br><span class="line">    &lt;image&gt;flickr&lt;/image&gt;                           图像来源</span><br><span class="line">  &lt;/source&gt;                                     来源结束</span><br><span class="line">  &lt;size&gt;                                        大小</span><br><span class="line">    &lt;width&gt;486&lt;/width&gt;                              宽486</span><br><span class="line">    &lt;height&gt;500&lt;/height&gt;                            高500</span><br><span class="line">    &lt;depth&gt;3&lt;/depth&gt;                                通道3</span><br><span class="line">  &lt;/size&gt;                                       大小结束</span><br><span class="line">  &lt;segmented&gt;0&lt;/segmented&gt;                      分割</span><br><span class="line">  &lt;object&gt;                                      物体</span><br><span class="line">    &lt;name&gt;person&lt;/name&gt;                              名称</span><br><span class="line">    &lt;pose&gt;Unspecified&lt;/pose&gt;                         姿势确定</span><br><span class="line">    &lt;truncated&gt;0&lt;/truncated&gt;                        是否截断</span><br><span class="line">    &lt;difficult&gt;0&lt;/difficult&gt;                        训练难易</span><br><span class="line">    &lt;bndbox&gt;                                        边界框</span><br><span class="line">      &lt;xmin&gt;174&lt;/xmin&gt;                                  左上角坐标x</span><br><span class="line">      &lt;ymin&gt;101&lt;/ymin&gt;                                  左上角坐标y</span><br><span class="line">      &lt;xmax&gt;349&lt;/xmax&gt;                                  右下角坐标x</span><br><span class="line">      &lt;ymax&gt;351&lt;/ymax&gt;                                  右下角坐标y</span><br><span class="line">    &lt;/bndbox&gt;                                           边界框结束</span><br><span class="line">    &lt;part&gt;                                          部位</span><br><span class="line">      &lt;name&gt;head&lt;/name&gt;                                 部位名称</span><br><span class="line">      &lt;bndbox&gt;                                          边界框</span><br><span class="line">        &lt;xmin&gt;169&lt;/xmin&gt;                                    左上角坐标x</span><br><span class="line">        &lt;ymin&gt;104&lt;/ymin&gt;                                    左上角坐标y</span><br><span class="line">        &lt;xmax&gt;209&lt;/xmax&gt;                                    右下角坐标x</span><br><span class="line">        &lt;ymax&gt;146&lt;/ymax&gt;                                    右下角坐标y</span><br><span class="line">      &lt;/bndbox&gt;                                         边界框结束</span><br><span class="line">    &lt;/part&gt;                                         部位结束</span><br><span class="line">  &lt;/object&gt;                                     物体结束</span><br><span class="line">&lt;/annotation&gt;                                标注结束</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="自建数据集"><a href="#自建数据集" class="headerlink" title="自建数据集"></a>自建数据集</h2><p>将图像和XML标注文件分别存于两个文件夹，并应用算法从中生成训练集和测试集。</p><h1 id="Cifar-10"><a href="#Cifar-10" class="headerlink" title="Cifar-10"></a>Cifar-10</h1><p>CIFAR-10是一个包含10个类别，共60,000张32x32像素的彩色图像数据集。</p><p>每个类别有6,000张图片，总共分为5个训练批次和1个测试批次，每批10,000张图片。</p><p>测试集由每类随机抽取的1,000张图片组成，共10,000张。</p><p>剩余50,000张用于训练，每类各5,000张。训练数据被打乱并均匀分配到5个批次中，导致各批次中各类图片数量可能不等。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Cifar10-class.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_dataloader</span>(<span class="params">batch_size</span>):</span><br><span class="line">    data_transform = &#123;</span><br><span class="line">        <span class="string">&quot;train&quot;</span>: transforms.Compose([transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">                                     transforms.RandomHorizontalFlip(),</span><br><span class="line">                                     transforms.ToTensor(),</span><br><span class="line">                                     transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])]),</span><br><span class="line">        <span class="string">&quot;val&quot;</span>: transforms.Compose([transforms.Resize(<span class="number">256</span>),</span><br><span class="line">                                   transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">                                   transforms.ToTensor(),</span><br><span class="line">                                   transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])])&#125;</span><br><span class="line">    train_dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./C10_dataset&#x27;</span>, train=<span class="literal">True</span>, transform=data_transform[<span class="string">&quot;train&quot;</span>], download=<span class="literal">True</span>)</span><br><span class="line">    test_dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./C10_dataset&#x27;</span>, train=<span class="literal">False</span>, transform=data_transform[<span class="string">&quot;val&quot;</span>], download=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;训练数据集长度: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(train_dataset)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;测试数据集长度: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(test_dataset)))</span><br><span class="line">    <span class="comment"># DataLoader创建数据集</span></span><br><span class="line">    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> train_dataloader,test_dataloader</span><br></pre></td></tr></table></figure><h1 id="Fashion-MNIST"><a href="#Fashion-MNIST" class="headerlink" title="Fashion-MNIST"></a>Fashion-MNIST</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Fashion-MNIST 是一个替代传统 MNIST 手写数字识别数据集的图像数据集。它由 Zalando（一家德国的时尚科技公司）提供，目的是为了提供一个更加具有挑战性的基准数据集，用于在机器学习算法上进行训练和测试。</p><h2 id="图片详情"><a href="#图片详情" class="headerlink" title="图片详情"></a>图片详情</h2><ul><li><strong>图片大小</strong>: 每张图片都是 28x28 像素的灰度图。</li><li><strong>图片数量</strong>: 一共有 70,000 张图片，其中 60,000 张用于训练，10,000 张用于测试。</li><li><strong>图片形式</strong>: 每张图片都被存储为一个 784（28x28）长度的一维数组，数组的每个元素代表一个像素点的灰度值。</li></ul><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><p>Fashion-MNIST 数据集包含 10 个类别，每个类别由一种特定类型的时尚商品组成。这些类别分别是：</p><ol><li>T-shirt/top（T恤/上衣）</li><li>Trouser（裤子）</li><li>Pullover（套衫）</li><li>Dress（连衣裙）</li><li>Coat（外套）</li><li>Sandal（凉鞋）</li><li>Shirt（衬衫）</li><li>Sneaker（运动鞋）</li><li>Bag（包）</li><li>Ankle boot（踝靴）</li></ol><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mnist_train = torchvision.datasets.FashionMNIST(root=<span class="string">&#x27;~/Datasets/FashionMNIST&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br><span class="line">mnist_test = torchvision.datasets.FashionMNIST(root=<span class="string">&#x27;~/Datasets/FashionMNIST&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transforms.ToTensor())</span><br></pre></td></tr></table></figure><h1 id="MNIST-数据集"><a href="#MNIST-数据集" class="headerlink" title="MNIST 数据集"></a>MNIST 数据集</h1><h2 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h2><p>MNIST 是一个手写数字数据集，广泛用于机器学习和计算机视觉的入门训练。它包含了从0到9的手写数字图片。</p><h2 id="图片详情-1"><a href="#图片详情-1" class="headerlink" title="图片详情"></a>图片详情</h2><ul><li><strong>图片大小</strong>: 每张图片都是28x28像素的灰度图。</li><li><strong>图片数量</strong>: 总共有70,000张图片，其中60,000张用于训练，10,000张用于测试。</li><li><strong>图片形式</strong>: 每张图片都被存储为一个28x28的矩阵，每个元素代表一个像素的灰度值。</li></ul><h2 id="分类-1"><a href="#分类-1" class="headerlink" title="分类"></a>分类</h2><p>数据集包含10个类别，分别对应数字0到9。</p><h2 id="数据导入（PyTorch）"><a href="#数据导入（PyTorch）" class="headerlink" title="数据导入（PyTorch）"></a>数据导入（PyTorch）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">0.5</span>,))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h1 id="SVHN-数据集"><a href="#SVHN-数据集" class="headerlink" title="SVHN 数据集"></a>SVHN 数据集</h1><h2 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h2><p>SVHN（Street View House Numbers）是一个真实世界的图像数据集，用于数字识别。它包含来自谷歌街景图像的门牌号码。</p><h2 id="图片详情-2"><a href="#图片详情-2" class="headerlink" title="图片详情"></a>图片详情</h2><ul><li><strong>图片大小</strong>: 图片大小不一，每张图片中包含多个数字。</li><li><strong>图片数量</strong>: 包含超过600,000张数字图像。</li><li><strong>图片形式</strong>: 彩色图片，每张图片的格式为RGB。</li></ul><h2 id="分类-2"><a href="#分类-2" class="headerlink" title="分类"></a>分类</h2><p>数据集包含10个类别，分别对应数字0到9。</p><h2 id="数据导入（PyTorch）-1"><a href="#数据导入（PyTorch）-1" class="headerlink" title="数据导入（PyTorch）"></a>数据导入（PyTorch）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], std=[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_dataset = datasets.SVHN(root=<span class="string">&#x27;./data&#x27;</span>, split=<span class="string">&#x27;train&#x27;</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">test_dataset = datasets.SVHN(root=<span class="string">&#x27;./data&#x27;</span>, split=<span class="string">&#x27;test&#x27;</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>cv2</title>
      <link href="/cv2/"/>
      <url>/cv2/</url>
      
        <content type="html"><![CDATA[<h3 id="OpenCV-基本操作笔记"><a href="#OpenCV-基本操作笔记" class="headerlink" title="OpenCV 基本操作笔记"></a>OpenCV 基本操作笔记</h3><p><strong>1. 导入 OpenCV 库</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br></pre></td></tr></table></figure></p><p><strong>2. 读取图像</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image = cv.imread(<span class="string">&#x27;path_to_image.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure></p><p>读取结果为‘BGR’且‘HWC’</p><p><strong>3. 显示图像</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cv.imshow(<span class="string">&#x27;Window Name&#x27;</span>, image)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)  <span class="comment"># 等待按键事件</span></span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure></p><p><strong>4. 转换为灰度图像</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)</span><br></pre></td></tr></table></figure></p><p><strong>5. 保存图像</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv.imwrite(<span class="string">&#x27;path_to_save.jpg&#x27;</span>, image)</span><br></pre></td></tr></table></figure></p><p><strong>6. 调整图像大小</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">resized_image = cv.resize(image, (width, height))</span><br></pre></td></tr></table></figure></p><p><strong>7. 图像裁剪</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cropped_image = image[y_start:y_end, x_start:x_end]</span><br></pre></td></tr></table></figure></p><p><strong>8. 图像旋转</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取图像尺寸</span></span><br><span class="line">(h, w) = image.shape[:<span class="number">2</span>]</span><br><span class="line"><span class="comment"># 计算旋转中心</span></span><br><span class="line">center = (w // <span class="number">2</span>, h // <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 旋转矩阵</span></span><br><span class="line">M = cv.getRotationMatrix2D(center, angle, <span class="number">1.0</span>)  <span class="comment"># angle 是旋转角度</span></span><br><span class="line">rotated_image = cv.warpAffine(image, M, (w, h))</span><br></pre></td></tr></table></figure></p><p><strong>9. 图像翻转</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flipped_image = cv.flip(image, flipCode)  <span class="comment"># flipCode: 0 垂直翻转，1 水平翻转</span></span><br></pre></td></tr></table></figure></p><p><strong>10. 边缘检测</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">edges = cv.Canny(image, threshold1, threshold2)</span><br></pre></td></tr></table></figure></p><p><strong>11. 图像模糊（滤波）</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">blurred_image = cv.GaussianBlur(image, (kernel_width, kernel_height), sigmaX)</span><br></pre></td></tr></table></figure></p><p><strong>12. 图像二值化</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">_, binary_image = cv.threshold(gray_image, thresh, maxval, cv.THRESH_BINARY)</span><br></pre></td></tr></table></figure></p><p><strong>13. 图像腐蚀与膨胀</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 腐蚀</span></span><br><span class="line">eroded = cv.erode(image, kernel, iterations=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 膨胀</span></span><br><span class="line">dilated = cv.dilate(image, kernel, iterations=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><p><strong>14. 图像轮廓检测</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">contours, _ = cv.findContours(binary_image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)</span><br><span class="line"><span class="comment"># 绘制轮廓</span></span><br><span class="line">cv.drawContours(image, contours, -<span class="number">1</span>, (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), <span class="number">3</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pandas</title>
      <link href="/PANDAS/"/>
      <url>/PANDAS/</url>
      
        <content type="html"><![CDATA[<p>当然可以。以下是 Pandas 库中 <code>pd.read_csv</code>、<code>DataFrame.head</code> 和 <code>DataFrame.iloc</code> 方法的基本用法总结：</p><h3 id="1-pd-read-csv"><a href="#1-pd-read-csv" class="headerlink" title="1. pd.read_csv"></a>1. <code>pd.read_csv</code></h3><p>用于从 CSV 文件中读取数据并创建 DataFrame。</p><p><strong>基本用法</strong>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(filepath_or_buffer, sep=<span class="string">&#x27;,&#x27;</span>, header=<span class="number">0</span>, names=<span class="literal">None</span>, index_col=<span class="literal">None</span>, usecols=<span class="literal">None</span>, ...)</span><br></pre></td></tr></table></figure></p><ul><li><code>filepath_or_buffer</code>: CSV 文件的路径或类文件对象。</li><li><code>sep</code> 或 <code>delimiter</code>: 用于分割数据的字符，默认为逗号 <code>,</code>。</li><li><code>header</code>: 指定作为列名的行号，默认第一行为列名，如果没有列名则设置为 <code>None</code>。</li><li><code>names</code>: 显式指定列名的列表。</li><li><code>index_col</code>: 用作行索引的列编号或列名。</li><li><code>usecols</code>: 读取指定的列，接受列号或列名的列表。</li><li>其他参数可用于进一步定制读取过程，如数据类型转换、日期解析等。</li></ul><h3 id="2-DataFrame-head"><a href="#2-DataFrame-head" class="headerlink" title="2. DataFrame.head"></a>2. <code>DataFrame.head</code></h3><p>用于查看 DataFrame 的前几行。</p><p><strong>基本用法</strong>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.head(n=<span class="number">5</span>)</span><br></pre></td></tr></table></figure></p><ul><li><code>n</code>: 指定要显示的行数，默认为 5。</li></ul><h3 id="3-DataFrame-iloc"><a href="#3-DataFrame-iloc" class="headerlink" title="3. DataFrame.iloc"></a>3. <code>DataFrame.iloc</code></h3><p>用于基于索引位置选择数据。</p><p><strong>基本用法</strong>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.iloc[row_index, column_index]</span><br></pre></td></tr></table></figure></p><ul><li><code>row_index</code>: 行的位置索引（整数或整数列表）。</li><li><code>column_index</code>: 列的位置索引（整数或整数列表）。</li></ul><p><code>iloc</code> 用于按位置选择数据，不论数据的索引是怎样的。它是基于整数的位置选择方法，所以接受的是整数或整数的切片对象。</p><p><strong>示例</strong>:</p><ul><li>选择第一行：<code>df.iloc[0]</code></li><li>选择前三行和前两列：<code>df.iloc[0:3, 0:2]</code></li></ul><p>这些方法是 Pandas 库中非常重要的数据操作基础，广泛应用于数据处理和分析中。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>plt</title>
      <link href="/PLT/"/>
      <url>/PLT/</url>
      
        <content type="html"><![CDATA[<h3 id="Matplotlib-基础"><a href="#Matplotlib-基础" class="headerlink" title="Matplotlib 基础"></a>Matplotlib 基础</h3><p><strong>1. 导入库</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure></p><p><strong>2. 基本绘图</strong><br>绘制简单的线图。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">y = [<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">25</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><strong>3. 添加标题和轴标签</strong><br>给图形添加标题和 X 轴、Y 轴标签。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, y)</span><br><span class="line">plt.title(<span class="string">&quot;Sample Plot&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;X Axis&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Y Axis&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><strong>4. 多条线绘制和图例</strong><br>在同一幅图上绘制多条线，并添加图例。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y2 = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(x, y, label=<span class="string">&quot;Line 1&quot;</span>)</span><br><span class="line">plt.plot(x, y2, label=<span class="string">&quot;Line 2&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><strong>5. 散点图</strong><br>绘制散点图。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><strong>6. 条形图</strong><br>绘制条形图。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.bar(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><strong>7. 直方图</strong><br>绘制直方图。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">data = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, <span class="number">1000</span>)</span><br><span class="line">plt.hist(data, bins=<span class="number">30</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><strong>8. 子图</strong><br>创建包含多个子图的图形。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 1行2列</span></span><br><span class="line">ax[<span class="number">0</span>].plot(x, y)</span><br><span class="line">ax[<span class="number">1</span>].scatter(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><strong>9. 自定义样式</strong><br>自定义线条颜色、类型和标记。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, y, color=<span class="string">&#x27;red&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><strong>10. 保存图形</strong><br>将图形保存为文件。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, y)</span><br><span class="line">plt.savefig(<span class="string">&quot;plot.png&quot;</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Image-basic</title>
      <link href="/Image-basic/"/>
      <url>/Image-basic/</url>
      
        <content type="html"><![CDATA[<h2 id="JPG-转-Tensor"><a href="#JPG-转-Tensor" class="headerlink" title="JPG 转 Tensor"></a>JPG 转 Tensor</h2><p>这里是详细说明 JPG 图像通过 OpenCV 的 <code>imread</code> 读取，然后转换为 PyTorch 张量的整个流程，包括数据格式和通道的变化：</p><h3 id="1-JPG-图像通过-OpenCV-imread-读取："><a href="#1-JPG-图像通过-OpenCV-imread-读取：" class="headerlink" title="1. JPG 图像通过 OpenCV imread 读取："></a>1. <strong>JPG 图像通过 OpenCV <code>imread</code> 读取</strong>：</h3><ul><li>JPG 图像是一种常用的压缩图像格式。</li><li>当使用 OpenCV 的 <code>imread</code> 函数读取 JPG 图像时，图像被加载为一个 NumPy 数组。</li><li>此数组的形状是 <code>(Height, Width, Channels)</code>，其中 Height 和 Width 分别是图像的高度和宽度（以像素为单位），Channels 是颜色通道的数量。</li><li>对于彩色图像，OpenCV 默认以 BGR（蓝、绿、红）格式读取，所以 Channels = 3。</li><li>数组中的每个元素是一个 0 到 255 之间的整数，代表相应像素的颜色强度。</li></ul><h3 id="2-转换为-PyTorch-张量："><a href="#2-转换为-PyTorch-张量：" class="headerlink" title="2. 转换为 PyTorch 张量："></a>2. <strong>转换为 PyTorch 张量</strong>：</h3><ul><li>PyTorch 通常处理的是张量（Tensor）格式的数据。</li><li>当将 OpenCV 读取的图像转换为 PyTorch 张量时，数据格式会从 NumPy 数组变为 PyTorch 张量。</li><li>这个转换过程通常包括改变通道的顺序和数据类型。</li><li>PyTorch 中的张量通常使用 <code>(Channels, Height, Width)</code> 的顺序，这与 OpenCV 的 <code>(Height, Width, Channels)</code> 不同。</li><li>因此，通常会对图像数据进行转置操作，比如 <code>np.transpose(image, (2, 0, 1))</code>，以将其从 <code>(H, W, C)</code> 转换为 <code>(C, H, W)</code>。</li><li>此外，如果原始图像是以 BGR 格式读取的，可能还需要将其转换为 RGB 格式，因为 PyTorch 通常使用 RGB 格式。</li></ul><h3 id="3-通道的变化："><a href="#3-通道的变化：" class="headerlink" title="3. 通道的变化："></a>3. <strong>通道的变化</strong>：</h3><ul><li>在 OpenCV 中，彩色图像默认以 BGR 格式读取，且尺寸顺序为‘HWC’</li><li>在 PyTorch 中，彩色图像通常表示为 RGB 格式，且尺寸顺序为‘CHW’</li><li>因此，在转换过程中需要将 BGR 格式转换为 RGB 格式，这通常通过重新排列通道来实现。</li></ul><h3 id="4-整个流程的示例代码如下："><a href="#4-整个流程的示例代码如下：" class="headerlink" title="4. 整个流程的示例代码如下："></a>4. <strong>整个流程的示例代码如下</strong>：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用OpenCV读取图像</span></span><br><span class="line">image = cv2.imread(<span class="string">&#x27;path_to_image.jpg&#x27;</span>)  <span class="comment"># 图像为 (H, W, C) 和 BGR 格式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将BGR转换为RGB</span></span><br><span class="line">image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将图像从 (H, W, C) 转换为 (C, H, W)</span></span><br><span class="line">image = np.transpose(image, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将NumPy数组转换为PyTorch张量</span></span><br><span class="line">tensor = torch.from_numpy(image)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor 现在是 (C, H, W) 格式的张量</span></span><br></pre></td></tr></table></figure><p>这样，JPG 图像就被转换成了 PyTorch 可以直接处理的张量格式。在这个张量中，图像数据的每个元素都代表对应像素点在特定通道上的颜色强度。</p><h2 id="灰度图像和黑白图像"><a href="#灰度图像和黑白图像" class="headerlink" title="灰度图像和黑白图像"></a>灰度图像和黑白图像</h2><p>灰度图像和黑白图像是两种常见的图像类型，它们在图像处理和计算机视觉中有着广泛的应用。下面是对它们的介绍以及它们的数据特征：</p><h3 id="1-灰度图像："><a href="#1-灰度图像：" class="headerlink" title="1. 灰度图像："></a>1. <strong>灰度图像</strong>：</h3><ul><li>灰度图像是指只有灰度级别的图像，没有彩色信息。在灰度图像中，每个像素只有一个亮度值，通常表示为 0（黑色）到 255（白色）之间的整数。</li><li>在灰度图像中，像素值较低（接近0）的区域看起来更黑，像素值较高（接近255）的区域看起来更亮。</li><li>灰度图像的数据结构较为简单，每个像素只有一个通道（通常是单通道），这使得处理速度更快，占用的存储空间也更小。</li><li>灰度图像常用于图像处理中的各种算法，如边缘检测、图像分割等，因为去除了色彩信息，可以更专注于图像的结构和形状。</li></ul><h3 id="2-黑白图像（二值图像）："><a href="#2-黑白图像（二值图像）：" class="headerlink" title="2. 黑白图像（二值图像）："></a>2. <strong>黑白图像（二值图像）</strong>：</h3><ul><li>黑白图像，也称为二值图像，是一种特殊的灰度图像，每个像素只有两种可能的值：0（黑色）或 255（白色）。</li><li>在二值图像中，图像通常是高对比度的，只显示黑色和白色，没有灰度中间值。</li><li>二值图像常用于阈值处理、文档扫描、某些类型的图像分析（如形态学操作）等应用。</li><li>生成二值图像的一种常见方法是应用阈值处理，将灰度图像中的每个像素转换为黑色或白色，这个过程称为二值化。</li></ul><p>在数据表示上，灰度图像通常是一个二维数组（Height x Width），其中每个元素代表一个像素的亮度值。而二值图像同样是二维数组，但每个元素只有两种可能的值（0或255）。在图像处理库中，如OpenCV或Pillow，通常提供了将彩色图像转换为灰度图像或二值图像的功能。</p><h2 id="读取图像文件"><a href="#读取图像文件" class="headerlink" title="读取图像文件"></a>读取图像文件</h2><h3 id="1-Scikit-image-io-imread"><a href="#1-Scikit-image-io-imread" class="headerlink" title="1. Scikit-image (io.imread)"></a>1. Scikit-image (<code>io.imread</code>)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line">image = io.imread(image_path)</span><br></pre></td></tr></table></figure><ul><li><strong>特点</strong>：默认以 RGB 格式读取图像。适用于科学计算和图像分析。</li><li><strong>颜色格式</strong>：RGB。</li><li><strong>适用场景</strong>：图像处理、计算机视觉研究和分析。</li></ul><h3 id="2-OpenCV-cv2-imread"><a href="#2-OpenCV-cv2-imread" class="headerlink" title="2. OpenCV (cv2.imread)"></a>2. OpenCV (<code>cv2.imread</code>)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">image = cv2.imread(image_path)</span><br></pre></td></tr></table></figure><ul><li><strong>特点</strong>：默认以 BGR 格式读取图像。提供了丰富的图像处理和计算机视觉功能。</li><li><strong>颜色格式</strong>：BGR。</li><li><strong>适用场景</strong>：实时图像处理、计算机视觉和机器学习应用。</li></ul><h3 id="3-PIL-Pillow-Image-open"><a href="#3-PIL-Pillow-Image-open" class="headerlink" title="3. PIL / Pillow (Image.open)"></a>3. PIL / Pillow (<code>Image.open</code>)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image = Image.<span class="built_in">open</span>(image_path)</span><br></pre></td></tr></table></figure><ul><li><strong>特点</strong>：以 PIL 图像对象形式读取图像，需要转换为 numpy 数组才能用于像素级操作。</li><li><strong>颜色格式</strong>：RGB（或原图像的格式）。</li><li><strong>适用场景</strong>：基本图像处理，与其他Python库集成。</li></ul><h3 id="4-Matplotlib-plt-imread"><a href="#4-Matplotlib-plt-imread" class="headerlink" title="4. Matplotlib (plt.imread)"></a>4. Matplotlib (<code>plt.imread</code>)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">image = plt.imread(image_path)</span><br></pre></td></tr></table></figure><ul><li><strong>特点</strong>：以 numpy 数组形式读取图像，通常用于图像展示和绘图。</li><li><strong>颜色格式</strong>：RGB。</li><li><strong>适用场景</strong>：图像展示和绘制图表。</li></ul><h3 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h3><ul><li><strong>颜色格式</strong>：OpenCV 默认以 BGR 格式读取，而其他库通常以 RGB 格式读取。</li><li><strong>数据类型</strong>：OpenCV 和 Matplotlib 返回 numpy 数组，PIL 返回一个图像对象，scikit-image 也返回 numpy 数组。</li><li><strong>用途</strong>：OpenCV 更适合复杂的图像处理和计算机视觉任务。PIL 简单易用，适合基本图像处理。Matplotlib 主要用于绘图和图像展示，scikit-image 适合科学研究中的图像分析。</li></ul><h2 id="show-用于显示图像"><a href="#show-用于显示图像" class="headerlink" title="show 用于显示图像"></a><code>show</code> 用于显示图像</h2><h3 id="1-OpenCV-cv-imshow"><a href="#1-OpenCV-cv-imshow" class="headerlink" title="1. OpenCV (cv.imshow)"></a>1. OpenCV (<code>cv.imshow</code>)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">image = cv2.imread(image_path)</span><br><span class="line">cv2.imshow(<span class="string">&#x27;Window Name&#x27;</span>, image)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure><ul><li><strong>特点</strong>：直接使用 OpenCV 窗口显示图像。</li><li><strong>用途</strong>：实时图像处理和计算机视觉应用。</li><li><strong>注意</strong>：需要使用 <code>cv2.waitKey()</code> 来等待键盘事件，<code>cv2.destroyAllWindows()</code> 关闭所有 OpenCV 创建的窗口。</li></ul><h3 id="2-PIL-Pillow-Image-show"><a href="#2-PIL-Pillow-Image-show" class="headerlink" title="2. PIL / Pillow (Image.show)"></a>2. PIL / Pillow (<code>Image.show</code>)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image = Image.<span class="built_in">open</span>(image_path)</span><br><span class="line">image.show()</span><br></pre></td></tr></table></figure><ul><li><strong>特点</strong>：在默认图片查看器中打开图像。</li><li><strong>用途</strong>：基本图像处理和快速预览。</li><li><strong>注意</strong>：这种方式会临时保存图像到磁盘，然后使用系统默认的图像查看器打开。</li></ul><h3 id="3-Matplotlib-plt-imshow-和-plt-show"><a href="#3-Matplotlib-plt-imshow-和-plt-show" class="headerlink" title="3. Matplotlib (plt.imshow 和 plt.show)"></a>3. Matplotlib (<code>plt.imshow</code> 和 <code>plt.show</code>)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">image = plt.imread(image_path)</span><br><span class="line">plt.imshow(image)</span><br><span class="line">plt.axis(<span class="string">&#x27;off&#x27;</span>)  <span class="comment"># 可选，关闭坐标轴</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><ul><li><strong>特点</strong>：使用 Matplotlib 绘图窗口显示图像。</li><li><strong>用途</strong>：数据分析和绘图，结合图表和图像展示。</li><li><strong>注意</strong>：可以与 Matplotlib 的其他绘图功能结合，如添加标题、坐标轴标签等。</li></ul><h3 id="4-Scikit-image-io-imshow"><a href="#4-Scikit-image-io-imshow" class="headerlink" title="4. Scikit-image (io.imshow)"></a>4. Scikit-image (<code>io.imshow</code>)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line">image = io.imread(image_path)</span><br><span class="line">io.imshow(image)</span><br><span class="line">io.show()</span><br></pre></td></tr></table></figure><ul><li><strong>特点</strong>：使用 Matplotlib 显示图像（如果安装了 Matplotlib）。</li><li><strong>用途</strong>：科学计算和图像分析。</li><li><strong>注意</strong>：实际显示依赖于安装的后端，通常是 Matplotlib。</li></ul><h3 id="比较-1"><a href="#比较-1" class="headerlink" title="比较"></a>比较</h3><ul><li><strong>OpenCV</strong> 的 <code>imshow</code> 方法适合于实时图像处理和计算机视觉应用，允许快速更新和操作窗口。</li><li><strong>PIL/Pillow</strong> 的 <code>show</code> 方法适合于快速预览图像，但依赖于外部程序。</li><li><strong>Matplotlib</strong> 的 <code>imshow</code> 结合 <code>show</code> 方法适用于数据分析和图像展示，允许高度定制化的图表和图像展示。</li><li><strong>Scikit-image</strong> 的 <code>imshow</code> 方法提供了一个与 Matplotlib 集成的简单接口，适用于科学研究中的图像展示。</li></ul>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PIL-pillow</title>
      <link href="/PIL-pillow/"/>
      <url>/PIL-pillow/</url>
      
        <content type="html"><![CDATA[<p>PIL（Python Imaging Library）是一个强大的图像处理库，用于打开、操作和保存许多不同格式的图像文件。Pillow 是 PIL 的一个活跃的分支版本，因此它通常被称为 “Pillow”。以下是一些使用 Pillow (PIL) 的基本操作：</p><ol><li><p><strong>打开和显示图像</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&#x27;path_to_image.jpg&#x27;</span>)  <span class="comment"># 打开图像文件</span></span><br><span class="line">image.show()  <span class="comment"># 显示图像</span></span><br></pre></td></tr></table></figure></li><li><p><strong>图像转换</strong>：</p><ul><li>转换图像格式（例如，将 RGB 转换为灰度）：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gray_image = image.convert(<span class="string">&#x27;L&#x27;</span>)  <span class="comment"># 转换为灰度图像</span></span><br></pre></td></tr></table></figure></li><li>改变图像大小：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">resized_image = image.resize((width, height))  <span class="comment"># 更改图像大小</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>裁剪图像</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cropped_image = image.crop((left, top, right, bottom))  <span class="comment"># 裁剪图像</span></span><br></pre></td></tr></table></figure></li><li><p><strong>保存图像</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image.save(<span class="string">&#x27;path_to_save_image.jpg&#x27;</span>)  <span class="comment"># 保存图像到文件</span></span><br></pre></td></tr></table></figure></li><li><p><strong>图像旋转和翻转</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rotated_image = image.rotate(<span class="number">90</span>)  <span class="comment"># 旋转图像90度</span></span><br><span class="line">flipped_image = image.transpose(Image.FLIP_LEFT_RIGHT)  <span class="comment"># 水平翻转图像</span></span><br></pre></td></tr></table></figure></li><li><p><strong>绘图和添加文本</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> ImageDraw, ImageFont</span><br><span class="line">draw = ImageDraw.Draw(image)</span><br><span class="line">font = ImageFont.load_default()  <span class="comment"># 加载默认字体</span></span><br><span class="line">draw.text((x, y), <span class="string">&quot;Hello&quot;</span>, (r, g, b), font=font)  <span class="comment"># 在图像上添加文本</span></span><br></pre></td></tr></table></figure></li></ol><p>Pillow 提供了丰富的功能来处理和分析图像数据，它是许多图像处理和计算机视觉项目的基础。由于其简单易用的接口，它在 Python 图像处理领域中非常受欢迎。</p>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch-17</title>
      <link href="/Pytorch-17/"/>
      <url>/Pytorch-17/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>   目前应已对基本的分类网络架构非常了解：AlexNet，VGG16、NiN、 GoogLeNet以及ResNet。现在开始进入我们计算机视觉应用，我们要学习：<strong>将学过的架构应用于计算机视觉问题</strong>、<strong>将解决过程打包成项目</strong></p><p>   要考虑的东西：数据、算力 、训练过程、训练技巧（之前Pytorch-11的demo还是太浅显了）</p><p>   经过这一堂课，将能够<strong>自由调用任意数据</strong>、<strong>任意卷积架构</strong>，<strong>实现较为恰当的训练</strong>、<strong>最终落地成自己的深度视觉案例</strong>。</p><p>   最基本地来说，如果只考虑对图像的内容进行判断情况，我们至少也有<strong>识别（recognition）</strong>、<strong>检测（detection）</strong>、<strong>分割 （segmentation）</strong>三种最基本的任务。</p><div class="table-container"><table><thead><tr><th>任务类型</th><th>描述</th><th>数据特点</th><th>应用场景</th></tr></thead><tbody><tr><td>图像识别</td><td>对图像中的单一对象进行属性判断</td><td>数据集规整简单，物体轮廓完整，拍摄清晰</td><td>例如机场人脸识别</td></tr><tr><td>检测任务</td><td>对图像中的多个或单个对象进行定位和属性判断</td><td>图像复杂，使用边界框定位对象</td><td>如道路车辆识别、景区人群监控</td></tr><tr><td>分割任务</td><td>对图像中的多个或单个对象每个像素进行分类，确定精确边界</td><td>高细节，多类别标签，像素级判断</td><td>美颜相机、换脸特效等</td></tr></tbody></table></div><p>   因此，在图像数据集的读取过程中，你可能发现一个图像数据集中会带有很多个<strong>指向不同任务的标签</strong>、 甚至很多个<strong>指向不同任务的训练集</strong>。</p><div class="table-container"><table><thead><tr><th>数据集</th><th>描述</th><th>参数</th><th>标签类型</th><th>应用</th></tr></thead><tbody><tr><td>CelebA</td><td>人脸数据集，用于人脸相关的计算机视觉任务</td><td><code>target_type</code></td><td><code>attr</code>（属性）、<code>identity</code>（个体）、<code>bbox</code>（边界框）、<code>landmarks</code>（特征）</td><td>人脸识别、检测任务</td></tr><tr><td>Cityscapes</td><td>城市街道场景数据集，用于图像分割任务</td><td><code>mode</code>、<code>target_type</code></td><td><code>fine</code>/<code>coarse</code>（模式）、<code>instance</code>（实例分割）、<code>semantic</code>（语义分割）、<code>polygon</code>（多边形分割）、<code>color</code>（颜色分割）</td><td>图像分割任务</td></tr></tbody></table></div><p>   我们不能使用同样的代码加载不同的数据 集。</p><p>   几乎所有数据集都被要求只能使用于学术场景，许多数据集需 要注册、申请才能够使用，许多数据集甚至完全不开源。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将.jpg转为tensor的函数</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 函数：读取JPG图片文件，使用OpenCV，然后将其转换为PyTorch张量</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">image_to_tensor</span>(<span class="params">image_path</span>):</span><br><span class="line">    <span class="comment"># 使用OpenCV读取图片</span></span><br><span class="line">    image = cv2.imread(image_path)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将图片从BGR转换为RGB</span></span><br><span class="line">    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将图片数据转换为浮点数，并缩放到[0, 1]区间</span></span><br><span class="line">    image = image.astype(np.float32) / <span class="number">255.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 调整数据的布局从(H, W, C)变为(C, H, W)</span></span><br><span class="line">    image = np.transpose(image, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将numpy数组转换为torch张量</span></span><br><span class="line">    tensor = torch.from_numpy(image)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果需要添加批次维度，可以取消以下代码的注释</span></span><br><span class="line">    <span class="comment"># tensor = tensor.unsqueeze(0)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line">read_image_to_tensor(<span class="string">r&#x27;D:\Desktop\QNJS\Dataset\[][][]zzzzzzip\datasets4\picturestotensor\Train\female\000059.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将tensor转为图片的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tensor_to_image</span>(<span class="params">tensor</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Convert a torch tensor into a numpy.ndarray for visualization.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 检查张量是否有批次维度</span></span><br><span class="line">    <span class="keyword">if</span> tensor.ndim == <span class="number">4</span> <span class="keyword">and</span> tensor.shape[<span class="number">0</span>] == <span class="number">1</span>:  <span class="comment"># 形状为[1, C, H, W]</span></span><br><span class="line">        tensor = tensor.squeeze(<span class="number">0</span>)  <span class="comment"># 去除批次维度</span></span><br><span class="line">    <span class="comment"># 将张量的值范围从[0, 1]转换为[0, 255]</span></span><br><span class="line">    tensor = tensor.mul(<span class="number">255</span>).byte()</span><br><span class="line">    <span class="comment"># 转置张量的维度为[H, W, C]</span></span><br><span class="line">    tensor = tensor.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).numpy()</span><br><span class="line">    <span class="comment"># 使用matplotlib展示图像</span></span><br><span class="line">    plt.imshow(tensor)</span><br><span class="line">    <span class="comment"># 关闭坐标轴</span></span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    <span class="comment"># 显示图像</span></span><br><span class="line">    plt.show()</span><br><span class="line">tensor_to_image(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机五张将tensor转为图片的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotsample</span>(<span class="params">data</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">5</span>,figsize=(<span class="number">10</span>,<span class="number">10</span>)) <span class="comment">#建立子图</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        num = random.randint(<span class="number">0</span>,<span class="built_in">len</span>(data)-<span class="number">1</span>) <span class="comment">#首先选取随机数，随机选取五次</span></span><br><span class="line">        <span class="comment">#抽取数据中对应的图像对象，make_grid函数可将任意格式的图像的通道数升为3，而不改变图像原始的数据</span></span><br><span class="line">        <span class="comment">#而展示图像用的imshow函数最常见的输入格式也是3通道</span></span><br><span class="line">        npimg = torchvision.utils.make_grid(data[num][<span class="number">0</span>]).numpy()</span><br><span class="line">        nplabel = data[num][<span class="number">1</span>] <span class="comment">#提取标签</span></span><br><span class="line">        <span class="comment">#将图像由(3, weight, height)转化为(weight, height, 3)，并放入imshow函数中读取</span></span><br><span class="line">        axs[i].imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">        axs[i].set_title(nplabel) <span class="comment">#给每个子图加上标签</span></span><br><span class="line">        axs[i].axis(<span class="string">&quot;off&quot;</span>) <span class="comment">#消除每个子图的坐标轴</span></span><br><span class="line">        </span><br><span class="line">plotsample(data)</span><br></pre></td></tr></table></figure><h2 id="一、数据"><a href="#一、数据" class="headerlink" title="一、数据"></a>一、数据</h2><h3 id="1-认识经典数据"><a href="#1-认识经典数据" class="headerlink" title="1 认识经典数据"></a>1 认识经典数据</h3><h4 id="1-1入门数据：MNIST、其他数字与字母识别"><a href="#1-1入门数据：MNIST、其他数字与字母识别" class="headerlink" title="1.1入门数据：MNIST、其他数字与字母识别"></a>1.1入门数据：MNIST、其他数字与字母识别</h4><p>最适合用于教学和实验、几乎对所有的电脑都无负担的MNIST一族。</p><p>许多研究论文采用MNIST或Fashion-MNIST数据集作为基准测试，因为它们是计算机视觉领域最初广泛使用的数据集之一。如果你开发了一种新的网络架构，并认为其达到了最先进（State of the Art, SOTA）水平，但在MNIST或Fashion-MNIST上的性能未能达到95%的准确率，那么这个架构的学习能力可能还不足。因此，MNIST常被用来作为性能评估的基准线。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line">mnist = torchvision.datasets.FashionMNIST(root=<span class="string">&#x27;.\\&#x27;</span></span><br><span class="line">                                         ,train=<span class="literal">False</span></span><br><span class="line">                                         ,download=<span class="literal">True</span></span><br><span class="line">                                         ,transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">fmnist = torchvision.datasets.FashionMNIST(root =<span class="string">&#x27;I：\F盘 + 代码\F盘datasets&#x27;</span></span><br><span class="line">                                         ,train =<span class="literal">True</span> <span class="comment">#根据类的不同，参数可能发生变化</span></span><br><span class="line">                                         ,download =<span class="literal">False</span> <span class="comment">#未下载则设置为True</span></span><br><span class="line">                                         ,transform =transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">svhn = torchvision.datasets.SVHN(root =<span class="string">&#x27;I：\F盘 + 代码\F盘datasets\SVHN&#x27;</span></span><br><span class="line">                                 ,split =<span class="string">&quot;train&quot;</span></span><br><span class="line">                                 ,download = <span class="literal">False</span></span><br><span class="line">                                 ,transform = transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">omnist = torchvision.datasets.Omniglot(root =<span class="string">&#x27;I：\F盘 + 代码\F盘datasets&#x27;</span></span><br><span class="line">                                       ,background = <span class="literal">True</span></span><br><span class="line">                                       ,download = <span class="literal">False</span></span><br><span class="line">                                       ,transform = transforms.ToTensor())</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th>数据集名称</th><th>数据描述</th></tr></thead><tbody><tr><td>FashionMNIST</td><td>衣物用品数据集</td></tr><tr><td>MNIST</td><td>手写数字数据集（白底黑字）</td></tr><tr><td>KuzushijiMNIST</td><td>日语手写字符识别数据集，包含48个平假名字符和一个平假名的模糊版本</td></tr><tr><td>QMNIST</td><td>与MNIST高度相似的手写数字数据集</td></tr><tr><td>EMNIST</td><td>与MNIST高度相似，在MNIST的基础上拓展的手写数字数据集</td></tr><tr><td>Omniglot</td><td>全语种手写字母数据集，包含来自50个不同字母的1623个不同的手写字符， 专用于“一次性学习”（字符）</td></tr><tr><td>USPS</td><td>另一个体系的手写数字数据集，常用来与MNIST对比（黑底白字）</td></tr><tr><td>SVHN</td><td>实际街景数字数据集（Street View House Number），是数字识别和位置测量中重要的一个数据集。有源自街景的彩色街道门牌号，但在PyTorch中内置的是32x32的彩色数据集。注意，使用本数据集需要SciPy库的支持。</td></tr></tbody></table></div><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E5%85%A5%E9%97%A8%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%95%B0%E6%8D%AE.png" alt=""></p><ul><li>导入数据</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line">mnist = torchvision.datasets.FashionMNIST(root=<span class="string">&#x27;.\\&#x27;</span></span><br><span class="line">                                         ,train=<span class="literal">False</span></span><br><span class="line">                                         ,download=<span class="literal">False</span></span><br><span class="line">                                         ,transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">fminst = torchvision.datasets.FashionMNIST(root=<span class="string">r&#x27;D:\Desktop\QNJS\Dataset\FashionMNIST&#x27;</span></span><br><span class="line">                                           ,train=<span class="literal">False</span></span><br><span class="line">                                           ,download=<span class="literal">False</span></span><br><span class="line">                                           ,transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">svhn = torchvision.datasets.SVHN(root =<span class="string">&#x27;I：\F盘 + 代码\F盘datasets\SVHN&#x27;</span></span><br><span class="line">                                 ,split =<span class="string">&quot;train&quot;</span></span><br><span class="line">                                 ,download = <span class="literal">False</span></span><br><span class="line">                                 ,transform = transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">omnist = torchvision.datasets.Omniglot(root =<span class="string">&#x27;I：\F盘 + 代码\F盘datasets&#x27;</span></span><br><span class="line">                                       ,background = <span class="literal">True</span></span><br><span class="line">                                       ,download = <span class="literal">False</span></span><br><span class="line">                                       ,transform = transforms.ToTensor())</span><br><span class="line"><span class="built_in">print</span>(fminst)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">只能看到样本量</span></span><br><span class="line"><span class="string">Dataset FashionMNIST</span></span><br><span class="line"><span class="string">    Number of datapoints: 10000</span></span><br><span class="line"><span class="string">    Root location: D:\Desktop\QNJS\Dataset\FashionMNIST</span></span><br><span class="line"><span class="string">    Split: Test</span></span><br><span class="line"><span class="string">    StandardTransform</span></span><br><span class="line"><span class="string">Transform: ToTensor()</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><ul><li>查看更多特征<ul><li>使用.data的方式查看特征，.targets的方式查看标签，不是所有的数据集都支持这两种方法！<ul><li>保险方法是：索引的方式调用单个样本</li></ul></li><li>查看样本量：len(fminst)、fminst</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 样本尺寸</span></span><br><span class="line"><span class="built_in">print</span>(fminst[<span class="number">0</span>][<span class="number">0</span>].shape)</span><br><span class="line"><span class="comment"># 样本量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(fminst))</span><br><span class="line"><span class="built_in">print</span>(fminst)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 报错概率最低的查看方法</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> fminst:</span><br><span class="line">    <span class="built_in">print</span>(x.shape,y)</span><br><span class="line">    <span class="keyword">break</span>  <span class="comment"># 打印其中一个</span></span><br></pre></td></tr></table></figure><ul><li>可视化</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#可视化</span></span><br><span class="line"><span class="comment">#实际上，在读图时如果不加ToTensor的预处理，很可能直接读出PIL文件</span></span><br><span class="line"><span class="comment">#PIL可以直接可视化</span></span><br><span class="line">fmnist = torchvision.datasets.FashionMNIST(root =<span class="string">&#x27;I：\F盘 + 代码\F盘datasets&#x27;</span></span><br><span class="line">                                         ,train =<span class="literal">True</span> <span class="comment">#根据类的不同，参数可能发生变化</span></span><br><span class="line">                                         ,download =<span class="literal">False</span> <span class="comment">#未下载则设置为True</span></span><br><span class="line">                                        <span class="comment"># ,transform = transforms.ToTensor()</span></span><br><span class="line">                                         )</span><br><span class="line">fminst[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># (&lt;PIL.Image.Image image mode=L size=28x28 at 0x268283D32E0&gt;, 9)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotsample</span>(<span class="params">data</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">5</span>,figsize=(<span class="number">10</span>,<span class="number">10</span>)) <span class="comment">#建立子图</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        num = random.randint(<span class="number">0</span>,<span class="built_in">len</span>(data)-<span class="number">1</span>) <span class="comment">#首先选取随机数，随机选取五次</span></span><br><span class="line">        <span class="comment">#抽取数据中对应的图像对象，make_grid函数可将任意格式的图像的通道数升为3，而不改变图像原始的数据</span></span><br><span class="line">        <span class="comment">#而展示图像用的imshow函数最常见的输入格式也是3通道</span></span><br><span class="line">        npimg = torchvision.utils.make_grid(data[num][<span class="number">0</span>]).numpy()</span><br><span class="line">        nplabel = data[num][<span class="number">1</span>] <span class="comment">#提取标签</span></span><br><span class="line">        <span class="comment">#将图像由(3, weight, height)转化为(weight, height, 3)，并放入imshow函数中读取</span></span><br><span class="line">        axs[i].imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">        axs[i].set_title(nplabel) <span class="comment">#给每个子图加上标签</span></span><br><span class="line">        axs[i].axis(<span class="string">&quot;off&quot;</span>) <span class="comment">#消除每个子图的坐标轴</span></span><br><span class="line"></span><br><span class="line">plotsample(fminst)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 随机打印五张图片</span></span><br></pre></td></tr></table></figure><h4 id="1-2-竞赛数据集"><a href="#1-2-竞赛数据集" class="headerlink" title="1.2 竞赛数据集"></a>1.2 竞赛数据集</h4><div class="table-container"><table><thead><tr><th>数据集名称</th><th>数据描述</th></tr></thead><tbody><tr><td>ImageNet2012</td><td>ImageNet大规模视觉挑战赛（ILSVRC）在2012年所使用的比赛数据。1000分类 通用数据集，涵盖动植物、人类、生活用品、食物、景色、交通工具等类别。任 何互联网大厂在深度学习模型上线之前必用的测试数据集。2012年版本数据集由 于版权原因已在全网下架，现只分享给拥有官方学术头衔的机构或个人（这意味 着，你在申请数据时必须使用xxx@xxx.edu的电子邮件地址，任何免费的、非学 术机构性质的电子邮件地址都不能获得数据申请）。也因此，PyTorch中的 torchvision.datasets.ImageNet类已失效。</td></tr><tr><td>ImageNet2019</td><td>ILSVRC在2017年之后就取消了识别任务，并将比赛转到Kaggle上举办，现在 ImageNet2019版本可在Kaggle上免费下载，主要用于检测任务。</td></tr><tr><td>PASCAL VOCSegmentation<br>PASCAL VOCDetection</td><td>模式分析，统计建模和计算学习大赛（Pattern Analysis, Statistical Modelling and Computation Learning，PSACAL），视觉对象分类（Visual Object Classes）数据集。与ImageNet类型相似，覆盖动植物、人类、生活用品、食 物、交通、等类别，但数据量相对较小。在PyTorch中被分为 VOCSegementation与VOCDetection两个类，分别支持分割和检测任务，虽然带 有类别标签但基本不支持识别任务。同时，PyTorch支持从2007到2012年的5个 版本的下载，通常我们都使用最新版本。</td></tr><tr><td>CocoCaptions<br>CocoDetection</td><td>微软Microsoft Common Objects in Context数据集，是大规模场景理解挑战赛 （Large-scale Scene Understanding challenge，LSUN）中的核心数据，主要 覆盖复杂的日常场景，是继ImageNet之后，最受关注的物体检测、语义分割、图 像理解（Captions）方面的数据集，也是唯一关注图像理解的大规模挑战赛。其中，用于图像理解的部分是2015年之前的数据集，用于检测和分割的则是2017及之后的版本。需要安装COCO API才可以调用。PyTorch没有提供用于分割的 API，但是我们依然可以自己下载数据集用于分割。</td></tr><tr><td>LSUN</td><td>城市景观、城市建筑、人文风光数据集，包含10种场景、20种对象的大型数据 集。其中，场景图像被用于LSUN大规模场景理解挑战赛。20分类包含包含交通 工具、动物、人类等图像，可用于识别任务。在我们导入数据时，我们会选择 LSUN中的某一个场景或一个对象进行训练，因此个人基于LSUN进行的识别任务 是2分类的。</td></tr></tbody></table></div><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E7%AB%9E%E8%B5%9B%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E4%BF%A1%E6%81%AF.png" alt=""></p><blockquote><p>LSUN是lmdb格式的数据，必要时需要pip install lmdb</p></blockquote><p>   下面以LSUN为例子进行说明</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data_val = torchvision.datasets.LSUN(root=<span class="string">r&#x27;D:\Desktop\QNJS\Dataset\lsun-master\lsun-master\data&#x27;</span>,</span><br><span class="line">                                     classes=[<span class="string">&#x27;classroom_val&#x27;</span>,<span class="string">&#x27;church_outdoor_val&#x27;</span>],</span><br><span class="line">                                     transform=transforms.ToTensor(),</span><br><span class="line">                                     )</span><br><span class="line">image_tensor = data_val[<span class="number">0</span>][<span class="number">0</span>] <span class="comment"># 数据有600个，0-299为classroom，300-600为church_outdoor</span></span><br><span class="line"><span class="comment"># 第一个类别标签为0，第二个类别标签为1</span></span><br></pre></td></tr></table></figure><h4 id="1-3-景物、人脸、通用、其他"><a href="#1-3-景物、人脸、通用、其他" class="headerlink" title="1.3 景物、人脸、通用、其他"></a>1.3 景物、人脸、通用、其他</h4><div class="table-container"><table><thead><tr><th>数据集名称</th><th>简要描述</th></tr></thead><tbody><tr><td>CelebA</td><td>全球名人(单人)人脸数据集，可能是全球最大的人脸数据集之一，图片质量很高， 拥有丰富的标签。虽然PyTorch官方说明中允许下载，但实际上下载链接已失效， 需自行下载。且由于CelebA下载后的文件是7z压缩格式，类datasets.CelebA无法读取，因此下载后的图像无法使用CelebA类来进行导入。JPG可以抽样。</td></tr><tr><td>CIFAR10</td><td>10分类通用数据集，涵盖10类动物与交通工具，可能是除了MNIST系列之外，最常用于教学/基准线的图像识别数据集</td></tr><tr><td>CIFAR100</td><td>100分类通用数据集，涵盖动植物、人类、食物、交通工具。CIFAR100是在 CIFAR10的基础上拓展出来的，每个图像样本都含有20个粗粒度标签和100个细粒 度标签，通常我们使用细粒度标签进行分类。</td></tr><tr><td>STL10</td><td>从ImageNet数据集中抽样的、专用于无监督/半监督深度学习的数据集。</td></tr><tr><td>SBD</td><td>2011版本VOC的衍生数据集，全称为语义边界数据集（Semantic Boundaries Dataset and Benchmark），专门用于语义轮廓识别。</td></tr><tr><td>Cityscapes  Cityscapes extra</td><td>城市街景数据集，包含50个城市、不同季节、不同时段的街景图片和视频。可用于 密集语义分割和车辆、人群的实例分割。拓展数据集可用于检测任务、以及雨幕、 大雾的语义分割以及全景分割。需注册才能够进行下载，仅分享给拥有官方学术头衔的机构。</td></tr><tr><td>Places365</td><td>目前为止最大规模的景观数据集，由MIT支持，涵盖城市景观、自然风光、室内室 外各种场景。</td></tr></tbody></table></div><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E9%80%9A%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86%E5%86%85%E5%AE%B9%E7%9B%B8%E5%85%B3.png" alt=""></p><ul><li><p>能把CelebA运行起来，以及可以在Palace365做一些预测即可。</p></li><li><p>下面以CIFAR10/100为例，进行说明。</p></li><li><p>注意目前只有MINST和CIFAR可以.data/.targets/.classes。</p></li><li><p>SBU和SBD数据也可以使用torchvision.datasets进行读取</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#import torch, torchvision</span></span><br><span class="line"><span class="comment">#import torchvision.transforms as transforms</span></span><br><span class="line"><span class="comment">#import matplotlib.pyplot as plt</span></span><br><span class="line"><span class="comment">#import numpy as np</span></span><br><span class="line"></span><br><span class="line">data = torchvision.datasets.CIFAR10(root = <span class="string">r&quot;D:\Desktop\QNJS\Dataset\cifar&quot;</span></span><br><span class="line">                                   ,train=<span class="literal">True</span></span><br><span class="line">                                   ,download=<span class="literal">False</span></span><br><span class="line">                                   ,transform = transforms.ToTensor()</span><br><span class="line">                                   )</span><br><span class="line"></span><br><span class="line">data[<span class="number">0</span>][<span class="number">0</span>].shape  <span class="comment"># torch.Size([3, 32, 32])</span></span><br><span class="line">data.classes  <span class="comment"># classes&#x27;s list</span></span><br><span class="line">np.unique(data.targets)  <span class="comment"># array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br></pre></td></tr></table></figure><h4 id="1-4-总结"><a href="#1-4-总结" class="headerlink" title="1.4 总结"></a>1.4 总结</h4><p>   以上数据均可通过torchvision.datasets进行导入，相对友好的教学数据集是LSUN和CIFAR。</p><p>   对于自己的数据集，我们可以通过下面的形式进行导入。</p><h3 id="2-使用自己的数据、图片创造数据集"><a href="#2-使用自己的数据、图片创造数据集" class="headerlink" title="2 使用自己的数据、图片创造数据集"></a>2 使用自己的数据、图片创造数据集</h3><h4 id="2-1-从图像png-jpg到四维tensor"><a href="#2-1-从图像png-jpg到四维tensor" class="headerlink" title="2.1 从图像png/jpg到四维tensor"></a>2.1 从图像png/jpg到四维tensor</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = torchvision.datasets.ImageFolder(root=<span class="string">&quot;XXXX&quot;</span>                     </span><br><span class="line">                            ,transform=torchvision.transforms.ToTensor()</span><br><span class="line">                                       )</span><br></pre></td></tr></table></figure><p>   可以接受 .jpg、.jpeg、.png、.ppm、.bmp、.pgm、.tif、.tiff、.webp这9种不同的图片格式作为输入，并且还能够通过文件夹的分类自动识别标签。</p><p>   在Opencv库里有imread函数，可以将图片转为像素矩阵，后我们可将其转为张量，故图片转为张量的形式并不难。</p><p>   重点是怎么把标签读取进来，并将图片和标签打包成一个元组。</p><p>   在如下的文件夹结构中，这样的层次关系如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">F:/JPG-Datasets</span><br><span class="line">│</span><br><span class="line">├── train</span><br><span class="line">│   ├── Female</span><br><span class="line">│   │   ├── image1.jpg</span><br><span class="line">│   │   ├── image2.jpg</span><br><span class="line">│   │   └── ...</span><br><span class="line">│   └── Male</span><br><span class="line">│       ├── image1.jpg</span><br><span class="line">│       ├── image2.jpg</span><br><span class="line">│       └── ...</span><br><span class="line">│</span><br><span class="line">└── <span class="built_in">test</span></span><br><span class="line">    ├── Female</span><br><span class="line">    │   ├── image1.jpg</span><br><span class="line">    │   ├── image2.jpg</span><br><span class="line">    │   └── ...</span><br><span class="line">    └── Male</span><br><span class="line">        ├── image1.jpg</span><br><span class="line">        ├── image2.jpg</span><br><span class="line">        └── ...</span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">train_dataset = torchvision.datasets.ImageFolder(root=r<span class="string">&quot;D:\Desktop\QNJS\Dataset\[][][]zzzzzzip\datasets4\picturestotensor\Train&quot;</span>,</span><br><span class="line">                                        transform=torchvision.transforms.ToTensor()</span><br><span class="line">                                       )</span><br><span class="line"><span class="built_in">print</span>(train_dataset)</span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">Dataset ImageFolder</span></span><br><span class="line"><span class="string">    Number of datapoints: 60</span></span><br><span class="line"><span class="string">    Root location: D:\Desktop\QNJS\Dataset\[][][]zzzzzzip\datasets4\picturestotensor\Train</span></span><br><span class="line"><span class="string">    StandardTransform</span></span><br><span class="line"><span class="string">Transform: ToTensor()</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基本操作</span></span><br><span class="line">train_dataset.classes</span><br><span class="line">train_dataset.data</span><br><span class="line">train_dataset.targets</span><br><span class="line">np.unique(train_dataset.targets)</span><br><span class="line"></span><br><span class="line">train_dataset.imgs <span class="comment">#查看具体的图像地址,一般不会用到</span></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">[(&#x27;F:\\datasets4\\picturestotensor\\Train\\female\\000001.jpg&#x27;, 0),</span></span><br><span class="line"><span class="string"> (&#x27;F:\\datasets4\\picturestotensor\\Train\\female\\000002.jpg&#x27;, 0),</span></span><br><span class="line"><span class="string"> ...</span></span><br><span class="line"><span class="string"> (&#x27;F:\\datasets4\\picturestotensor\\Train\\male\\Pet-Peacock.png&#x27;, 1)]</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>​    当图像标签类别较少时，可以轻松按类别整理图像。但标签多或数据量大时，手动分类变得费时。幸运的是，许多大型图像数据集，如LSUN，已预先按标签分类。</p><p>​    使用ImageFolder读取后的数据是无法轻易更改的，比如标签。大多会选择更加灵活的方式：<strong>自己写一个读取数据用的类</strong>。</p><hr><p>​    CLASS <code>torch.utils.data.Dataset</code>的存在地位相当于是<code>nn.Module</code>，在PyTorch中，许多torchvision.datasets中读数据的类，都继承自 Dataset。</p><p>​    如果一个读取数据的类继承自Dataset，那它读取出的数据一定是可以通过<strong>索引</strong>的方式进行调用和查看的。</p><p>​    Dataset中规定，如果一个子类要继承Dataset，则必须在子类中定义 <code>__getitem__()</code>方法。这个方法中的代码必须满足三个功能：1）读取单个图片并转化为张量。 2）读取该图片对应的标签。 3）将该图片的张量与对应标签打包成一个样本并输出。</p><p>​    定义好后。Dataset类中包含自动循环 <code>__getitem__()</code> 并拼接其输出结果的功能。该子类的输出就一定是打包好的整个数据集。</p><blockquote><p>下面以celebA数据集的子集为例，完成读取类别识别的任务</p><ul><li>celebAsubset<ul><li>Anno<ul><li>identity_CelebA_1000.txt</li><li>list_attr_celeba_1000</li></ul></li><li>Eval<ul><li>list_eval_partition.txt</li></ul></li><li>Img<ul><li>img_align_celeba_png.7z</li><li>Img_celeba.7z</li></ul></li><li>README</li></ul></li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">imgpath = <span class="string">r&quot;D:\Desktop\QNJS\Dataset\[][][]zzzzzzip\datasets4\picturestotensor\celebAsubset\Img\Img_celeba.7z\img_celeba&quot;</span></span><br><span class="line">csvpath = <span class="string">r&quot;D:\Desktop\QNJS\Dataset\[][][]zzzzzzip\datasets4\picturestotensor\celebAsubset\Anno\identity_CelebA_1000.txt&quot;</span></span><br><span class="line"></span><br><span class="line">identity = pd.read_csv(csvpath</span><br><span class="line">                       ,sep = <span class="string">&quot; &quot;</span></span><br><span class="line">                       ,header = <span class="literal">None</span>)   <span class="comment"># 不要把txt的第一行当做列名</span></span><br><span class="line"></span><br><span class="line">idx = <span class="number">0</span></span><br><span class="line">imgdir = os.path.join(imgpath,identity.iloc[idx,<span class="number">0</span>])  <span class="comment"># 图像目录</span></span><br><span class="line">image = io.imread(imgdir) <span class="comment"># 提取出的，索引为idx的图像的像素值矩阵</span></span><br><span class="line">label = identity.iloc[idx,<span class="number">1</span>]</span><br><span class="line">sample = (torch.tensor(image),<span class="built_in">int</span>(label))</span><br><span class="line"></span><br><span class="line"><span class="comment">##----基于此完成了：txt与jpg文件下，打包读取一张图片形成元组----##</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    自定义数据集，用于读取celebA数据集中的个体识别（identity recognition）数据的标签和图像</span></span><br><span class="line"><span class="string">    图像格式为jpg</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,csv_file, root_dir, transform = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        参数说明：</span></span><br><span class="line"><span class="string">            csv_file (字符串): 标签csv/txt的具体地址</span></span><br><span class="line"><span class="string">            root_dir (string): 所有图片所在的根目录</span></span><br><span class="line"><span class="string">            transform (callable, optional): 选填，需要对样本进行的预处理</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()  <span class="comment"># 选填</span></span><br><span class="line">        self.identity = pd.read_csv(csv_file,sep=<span class="string">&quot; &quot;</span>,header=<span class="literal">None</span>)</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment">#展示数据中总共有多少个样本</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.identity)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__info__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;CustomData&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\t Number of samples: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(self.identity)))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\t Number of classes: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(np.unique(self.identity.iloc[:,<span class="number">1</span>]))))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\t root_dir: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.root_dir))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,idx</span>):</span><br><span class="line">        <span class="comment">#保证idx不是一个tensor,官方Pytorch推荐</span></span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(idx):</span><br><span class="line">            idx = idx.tolist()</span><br><span class="line">        </span><br><span class="line">        imgdir = os.path.join(self.root_dir,self.identity.iloc[idx,<span class="number">0</span>]) <span class="comment">#图像目录</span></span><br><span class="line">        image = io.imread(imgdir) <span class="comment">#提取出的，索引为idx的图像的像素值矩阵</span></span><br><span class="line">        label = self.identity.iloc[idx,<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.transform != <span class="literal">None</span>:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line">            </span><br><span class="line">        sample = (image,label)</span><br><span class="line">        <span class="keyword">return</span> sample</span><br><span class="line">    </span><br><span class="line">data = CustomDataset(csvpath,imgpath)</span><br><span class="line">data[<span class="number">20</span>]</span><br><span class="line">data.__len__()</span><br><span class="line">data.__info__()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> data:</span><br><span class="line">    <span class="built_in">print</span>(x.shape)</span><br><span class="line">    <span class="built_in">print</span>(y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><blockquote><p>下面读取属性识别的标签</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">imgpath = <span class="string">r&quot;D:\Desktop\QNJS\Dataset\[][][]zzzzzzip\datasets4\picturestotensor\celebAsubset\Img\Img_celeba.7z\img_celeba&quot;</span></span><br><span class="line">csvpath = <span class="string">r&quot;D:\Desktop\QNJS\Dataset\[][][]zzzzzzip\datasets4\picturestotensor\celebAsubset\Anno\list_attr_celeba_1000.txt&quot;</span></span><br><span class="line"></span><br><span class="line">attr_ = pd.read_csv(csvpath, header=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">attr_ = pd.DataFrame(attr_.iloc[<span class="number">1</span>:, <span class="number">0</span>].<span class="built_in">str</span>.split().tolist(),</span><br><span class="line">                     columns=attr_.iloc[<span class="number">0</span>, <span class="number">0</span>].split())  <span class="comment"># 规整</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 开始代码------#</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDataset_attr</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    自定义数据集，用于读取celebA数据集中的属性识别（attribute recognition）数据的标签和图像</span></span><br><span class="line"><span class="string">    图像格式为jpg</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, csv_file, root_dir, labelname, transform=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        参数说明：</span></span><br><span class="line"><span class="string">            csv_file (字符串): 标签csv/txt的具体地址</span></span><br><span class="line"><span class="string">            root_dir (string): 所有图片所在的根目录</span></span><br><span class="line"><span class="string">            transform (callable, optional): 选填，需要对样本进行的预处理</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.attr_ = pd.read_csv(csvpath, header=<span class="literal">None</span>)</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.labelname = labelname</span><br><span class="line">        self.transform = transform</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 展示数据中总共有多少个样本</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.attr_)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__info__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;CustomData&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\t Number of samples: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(self.attr_) - <span class="number">1</span>))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\t Number of classes: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(pd.read_csv(csvpath, header=<span class="literal">None</span>).iloc[<span class="number">0</span>,<span class="number">0</span>].split())))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\t root_dir: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.root_dir))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):  <span class="comment"># 索引要求只有一个参数</span></span><br><span class="line">        <span class="comment"># 保证idx不是一个tensor</span></span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(idx):</span><br><span class="line">            idx = idx.tolist()</span><br><span class="line"></span><br><span class="line">        self.attr_ = pd.DataFrame(self.attr_.iloc[<span class="number">1</span>:, <span class="number">0</span>].<span class="built_in">str</span>.split().tolist(),</span><br><span class="line">                                  columns=self.attr_.iloc[<span class="number">0</span>, <span class="number">0</span>].split())</span><br><span class="line"></span><br><span class="line">        imgdic = os.path.join(self.root_dir, self.attr_.iloc[idx, <span class="number">0</span>])  <span class="comment"># 图像目录</span></span><br><span class="line">        image = io.imread(imgdic)  <span class="comment"># 提取出的，索引为idx的图像的像素值矩阵</span></span><br><span class="line">        label = <span class="built_in">int</span>(self.attr_.loc[idx, self.labelname])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform != <span class="literal">None</span>:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line"></span><br><span class="line">        sample = (image, label)</span><br><span class="line">        <span class="keyword">return</span> sample</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">labelname = <span class="string">&quot;Attractive&quot;</span></span><br><span class="line">data = CustomDataset_attr(csvpath, imgpath, labelname)</span><br><span class="line"></span><br><span class="line">data.__info__()</span><br></pre></td></tr></table></figure><h4 id="2-2-从二维表（csv-txt）到四维tensor"><a href="#2-2-从二维表（csv-txt）到四维tensor" class="headerlink" title="2.2 从二维表（csv/txt）到四维tensor"></a>2.2 从二维表（csv/txt）到四维tensor</h4><p>略</p><h4 id="2-3-从mat-pt-lmdb到四维tensor"><a href="#2-3-从mat-pt-lmdb到四维tensor" class="headerlink" title="2.3 从mat/pt/lmdb到四维tensor"></a>2.3 从mat/pt/lmdb到四维tensor</h4><p>略</p><h3 id="3-图片数据的基本预处理与数据增强"><a href="#3-图片数据的基本预处理与数据增强" class="headerlink" title="3 图片数据的基本预处理与数据增强"></a>3 图片数据的基本预处理与数据增强</h3><h4 id="3-1-数据预处理"><a href="#3-1-数据预处理" class="headerlink" title="3.1 数据预处理"></a>3.1 数据预处理</h4><p>​    当我们将所有有效数据导入后，我们至少需要确保：</p><ul><li>1）全部样本的尺寸是一致的（同时，全部样本的通道数是一致的）</li><li>2）图像最终以Tensor形式被输入卷积网络</li><li>3）图像被恰当地归一化</li></ul><div class="table-container"><table><thead><tr><th>类</th><th>说明</th></tr></thead><tbody><tr><td>Compose</td><td>transforms专用的，类似于nn.Sequential的打包功能，可以将数个transforms下 的类打包，形成类似于管道的结构来统一执行。</td></tr><tr><td>Resize</td><td>尺寸调整。需要输入最终希望得到的图像尺寸。注意区别于使用裁剪缩小尺寸或使 用填充放大尺寸。</td></tr><tr><td>CenterCrop</td><td>中心裁剪。需要输入最终希望得到的图像尺寸。</td></tr><tr><td>Normalize</td><td>归一化 (Tensor Only)。对每张图像的每个通道进行归一化，每个通道上的每个像 素会减去该通道像素值的均值，并除以该通道像素值的方差。</td></tr><tr><td>ToTensor</td><td>(PIL Only)将任意图像转变为Tensor</td></tr></tbody></table></div><blockquote><p> 前两项是为了CNN能够运行，第三项是为了训练变得更快速</p></blockquote><p>​    图像预处理时，通常将图像调整到28x28或224x224。若尺寸接近，用裁剪；尺寸差距大，先Resize再裁剪至目标尺寸（行业惯例）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要列表，专业用法</span></span><br><span class="line">transform1 = transforms.Compose([transforms.Resize(<span class="number">256</span>)</span><br><span class="line">                              ,transforms.CenterCrop(<span class="number">224</span>)])</span><br><span class="line"><span class="comment"># 等价于，不需要列表</span></span><br><span class="line">transform2 = nn.Sequential(transforms.Resize(<span class="number">256</span>)</span><br><span class="line">                        ,transforms.CenterCrop(<span class="number">224</span>))</span><br></pre></td></tr></table></figure><p>​    在归一化图像前，先将图像转为Tensor，然后按通道进行标准化。因此类 transforms.Normalize() 往往是在[0,1]区间上执行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([transforms.ToTensor()                             ,transforms.Normalize(<span class="number">0.5</span>,<span class="number">0.5</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment">#1) 常见且通用的做法，该写法只适用于三通道图像</span></span><br><span class="line">transforms.Normalize(mean=[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], <span class="comment">#代表三个通道上需要减去的值分别是0.5</span></span><br><span class="line">                     std=[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>]) <span class="comment">#代表三个通道上需要除以的值分别是0.5</span></span><br><span class="line"><span class="comment">#在保证数据范围在[0,1]的前提下，使用这个值可以令数据范围拓展到[-1,1] </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#也可写作：</span></span><br><span class="line">transforms.Normalize(<span class="number">0.5</span>,<span class="number">0.5</span>)</span><br><span class="line"><span class="comment">#这种写法中，Normalize类会根据通道数进行相应的计算，任意通道数的图像都可以使用,故推荐写这种写法</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#注意区分，这种写法只能用于单通道（灰度）图像</span></span><br><span class="line">transforms.Normalize([<span class="number">0.5</span>],[<span class="number">0.5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#2) ImageNet数据集上的均值和方差，可被用于任意实物照片分类</span></span><br><span class="line">transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                     std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#3) MNIST数据集上的均值和方差，可被用于MNIST系列（数字字母之类）</span></span><br><span class="line">transforms.Normalize((<span class="number">0.1307</span>), (<span class="number">0.3081</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#你也可以根据自己的数据集和自己希望实现的数值范围，来计算放入Normalize的值</span></span><br><span class="line"><span class="comment">#跑一两个epoch，选择起点最高的组作为归一化的参数</span></span><br></pre></td></tr></table></figure><p>​    <strong>对图像而言，必须完成的预处理就只有尺寸调整和归一化而已。</strong></p><h4 id="3-2-数据增强"><a href="#3-2-数据增强" class="headerlink" title="3.2 数据增强"></a>3.2 数据增强</h4><p>​    数据增强通过修改和重组现有数据来增加数据量，可以缓解数据不足问题，提高模型的鲁棒性和泛化能力，减少过拟合。</p><p>​    PyTorch的<code>torchvision.transforms</code>提供了多种数据增强方法，主要分为<strong>尺寸变化</strong>、<strong>像素值变化</strong>、<strong>视角变化</strong>和<strong>其他变化</strong>。</p><ul><li>尺寸变化</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-%E5%B0%BA%E5%AF%B8%E5%8F%98%E5%8C%96.png" alt=""></p><ul><li>像素变化</li></ul><p>​    能调整亮度、对比度、饱和度、色相等色彩属性。比较少用。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-%E5%83%8F%E7%B4%A0%E5%8F%98%E5%8C%96.png" alt=""></p><ul><li>变形/视角变化。都比较常用。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-%E5%8F%98%E5%BD%A2%E8%A7%86%E8%A7%92%E5%8F%98%E5%8C%96.png" alt=""></p><ul><li>其他（转换格式、数据处理流程打包）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-%E5%85%B6%E4%BB%96.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">256</span>),                    <span class="comment"># 将图像大小调整为 256x256</span></span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>),                <span class="comment"># 从图像中心裁剪出 224x224 的区域 ，也可以换为随机裁剪</span></span><br><span class="line">    transforms.ColorJitter(brightness=<span class="number">0.5</span>,     <span class="comment"># 随机调整亮度</span></span><br><span class="line">                           contrast=<span class="number">0.5</span>,       <span class="comment"># 随机调整对比度</span></span><br><span class="line">                           saturation=<span class="number">0.5</span>,     <span class="comment"># 随机调整饱和度</span></span><br><span class="line">                           hue=<span class="number">0.05</span>),          <span class="comment"># 随机调整色调</span></span><br><span class="line">    transforms.RandomHorizontalFlip(),         <span class="comment"># 以 50% 的概率水平翻转图像</span></span><br><span class="line">    transforms.RandomVerticalFlip(),           <span class="comment"># 以 50% 的概率垂直翻转图像</span></span><br><span class="line">    transforms.RandomRotation(<span class="number">30</span>),             <span class="comment"># 随机旋转图像±30度</span></span><br><span class="line">    transforms.RandomGrayscale(p=<span class="number">0.1</span>),         <span class="comment"># 以 10% 的概率将图像转换为灰度图</span></span><br><span class="line">    transforms.ToTensor(),                     <span class="comment"># 将 PIL 图像或 NumPy ndarray 转换为张量</span></span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],  <span class="comment"># 标准化图像</span></span><br><span class="line">                         std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment">#对于景色数据，水平翻转和随机裁剪都可能会比较有利</span></span><br><span class="line"><span class="comment">#因为建筑可能位于图像的任何地方，而根据尝试，水平翻转后的图像也能够被一眼看出是什么景色</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>​    这些方法<strong>在每次迭代训练时会随机应用变换，从而每次向模型呈现略有不同的数据版本</strong>。这种随机性实际上扩展了数据集的多样性，提升了模型的泛化能力，这就是深度学习框架中数据增强的巧妙设计。———“数据增强”</p><p>​    在PyTorch中，自定义数据集类（继承自<code>Dataset</code>）通常在<code>__getitem__()</code>方法中实现<code>transform</code>操作，而不是在<code>__init__()</code>中，这意味着原始数据被存储在内存中且不会立即进行变换。当通过<code>__getitem__()</code>调用数据时，从原始数据中复制出的样本会被<code>transform</code>处理，而内存中的原始数据保持不变。这种设计既节约内存（只存储一份原始数据），又提高效率（仅对需要的样本进行处理）。数据增强实际上是在从DataLoader中提取数据进行训练时才执行的。</p><p>​    在没有数据增强(<code>transform</code>)的情况下，每个<code>epoch</code>中数据批次不同，但整体数据一致，只是样本训练顺序变化，不增加新样本。引入<code>transform</code>后，即使原始数据相同，每次调用时随机变换（如裁剪、旋转、翻转）产生“新样本”，使每个<code>epoch</code>的数据独特，实质上增加了数据量，达到数据增强效果。因此，使用<code>transform</code>时，模型实际上不会接触到原始未处理的数据。</p><p>​    数据增强虽提高模型的鲁棒性和泛化能力，但也带来一些缺点：首先，由于数据中的随机性，模型收敛速度变慢，迭代周期增长；其次，数据增强的随机性无法通过随机数种子完全控制，导致模型结果可能有一定不稳定性。因此，使用数据增强的模型通常只能给出一个结果范围，而论文中报告的往往是这个范围的上限。</p><h2 id="二-训练与算法"><a href="#二-训练与算法" class="headerlink" title="二 训练与算法"></a>二 训练与算法</h2><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E5%AE%8C%E6%95%B4%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B.png" alt=""></p><h3 id="1-更强大的优化算法"><a href="#1-更强大的优化算法" class="headerlink" title="1 更强大的优化算法"></a>1 更强大的优化算法</h3><h3 id="1-1-AdaGrad"><a href="#1-1-AdaGrad" class="headerlink" title="1.1 AdaGrad"></a>1.1 AdaGrad</h3><p>​    之前的随机梯度下降或小批量随机梯度下降 中，我们所使用的权重迭代公式如下所示：</p><script type="math/tex; mode=display">w_{(t+1)} = w_{(t)} - \eta \frac{\partial L(w)}{\partial w}</script><p>​    <strong>Glorot条件</strong>中所声明的：只有所有权重上的梯度值都比较接近时，模型才能具有较好的学习能力，整体 损失才能够被优化到较好的局部最小值上。</p><p>​    使用每个维度上权重梯度的大小来<strong>自适应</strong>调整学习率，从而避免一个学习率难以适应所有维度的问题。</p><p>​    定义了gt用于表达<strong>第t次迭代时</strong>所有权重需要使用的梯度：</p><script type="math/tex; mode=display">g_t \equiv \frac{\partial L}{\partial w} = \frac{1}{n} \sum_{i=1}^{n} \nabla_w L(x^{(i)}, y^{(i)}, w)</script><p>​    我们使用梯度值的平方和作为学习率的分母来控制步长的大小（这是一种非常激进的策略，梯度的平方和可能是一个非常大的数）</p><script type="math/tex; mode=display">G_t = \sum_{\tau=1}^{t} g_{\tau}^2== \sum_{\tau=1}^{t} g_{\tau}g_{\tau}^T</script><script type="math/tex; mode=display">G_0 = g_0^2\\G_1 = G_0 + g_1^2\\G_2 = G_1 + g_2^2\\\ldots\\G_t = G_{t-1} + g_t^2</script><p>​    权重的具体迭代公式为：</p><script type="math/tex; mode=display">w_t = w_{t-1} - \frac{\eta}{\sqrt{G_t + \epsilon}} * g_t</script><script type="math/tex; mode=display">= w_{t-1} - \frac{\eta}{\sqrt{G_t + \epsilon}} * \frac{\partial L}{\partial w}</script><p>​    说明如下，其中t为迭代次数，m个w，n个样本</p><script type="math/tex; mode=display">\mathbf{g}_t = \begin{bmatrix}g_t(1) \\g_t(2) \\\vdots \\g_t(m)\end{bmatrix}=\begin{bmatrix}\frac{1}{n} \sum_{i=1}^{n} \nabla_w L(x^{(i)}, y^{(i)}, w_t(1)) \\\frac{1}{n} \sum_{i=1}^{n} \nabla_w L(x^{(i)}, y^{(i)}, w_t(2)) \\\vdots \\\frac{1}{n} \sum_{i=1}^{n} \nabla_w L(x^{(i)}, y^{(i)}, w_t(m))\end{bmatrix}</script><p>​    最终的迭代公式为：</p><script type="math/tex; mode=display">\begin{bmatrix}\theta^{(1)}_{t+1} \\\theta^{(2)}_{t+1} \\\vdots \\\theta^{(m)}_{t+1}\end{bmatrix}=\begin{bmatrix}\theta^{(1)}_t \\\theta^{(2)}_t \\\vdots \\\theta^{(m)}_t\end{bmatrix}-\eta\left(\begin{bmatrix}\epsilon & 0 & \cdots & 0 \\0 & \epsilon & \cdots & 0 \\\vdots & \vdots & \ddots & \vdots \\0 & 0 & \cdots & \epsilon\end{bmatrix}+\begin{bmatrix}G^{(1,1)}_t & 0 & \cdots & 0 \\0 & G^{(2,2)}_t & \cdots & 0 \\\vdots & \vdots & \ddots & \vdots \\0 & 0 & \cdots & G^{(m,m)}_t\end{bmatrix}\right)^{-\frac{1}{2}}\begin{bmatrix}g^{(1)}_t \\g^{(2)}_t \\\vdots \\g^{(m)}_t\end{bmatrix}.</script><p>​    我们在一 开始为AdaGrad设置较大的学习率为0.01，并在一开始进行较快的迭代。</p><p>​    通过学习率来修正梯度平稳性使得AdaGrad表现出非常适合于稀疏矩阵的特性。因为在AdaGrad的基础理论中，经过适当训练，AdaGrad会对高频特征设置较低的学习率，对低频特征设置较高的学习率。</p><p>​    此AdaGrad迭 代后期的学习率非常小，而非常容易出现梯度消失现象，故会出现损失值上无法下降的情况。</p><p>​    <code>torch.optim.Adagrad (params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0,eps=1e-10)</code></p><p>​    除非我们的数据是稀疏矩阵，否则我们较少会用到AadGrad，但是AdaGrad的思想却是我们常用的优化算 法的基础。</p><h3 id="1-2-RMSprop"><a href="#1-2-RMSprop" class="headerlink" title="1.2 RMSprop"></a>1.2 RMSprop</h3><p>​    为了解决 AdaGrad所面临的、随着迭代学习率会变得太小的问题，RMSprop算法由此提出。</p><script type="math/tex; mode=display">g_t = \frac{\partial L}{\partial w}\\G_t = \alpha G_{t-1} + (1 - \alpha)g_t^2\\\Delta_t = B\Delta_{t-1} - \frac{\eta}{\sqrt{G_t + \epsilon}} * g_t\\w_t = w_{t-1} + \Delta_t</script><p>​    <code>torch.optim.RMSprop (params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)</code></p><p>​    RMSprop存在可能让学习率增大的情况，违反了优化算法中的“正定性”（positive definiteness）</p><h3 id="1-3-Adam"><a href="#1-3-Adam" class="headerlink" title="1.3 Adam"></a>1.3 Adam</h3><p>​    Adam算法俗称是Adagrad和Momentum的结合，此结合就是RMSprop，故Adam实际上也是RMSprop的优化</p><script type="math/tex; mode=display">g_t = \frac{\partial L}{\partial w}\\\hat{V}_t = \frac{V_t}{1 - \beta_1} = \frac{\beta_1 V_{t-1} + (1 - \beta_1)g_t}{1 - \beta_1} = \frac{\beta_1}{1 - \beta_1} V_{t-1} + g_t\\\hat{G}_t = \frac{G_t}{1 - \beta_2} = \frac{\beta_2 G_{t-1} + (1 - \beta_2)g_t^2}{1 - \beta_2} = \frac{\beta_2}{1 - \beta_2} G_{t-1} + g_t^2\\w_t = w_{t-1} - \frac{\eta}{\sqrt{\hat{G}_t + \epsilon}} * \hat{V}_t</script><script type="math/tex; mode=display"></script><p>​    <code>CLASS torch.optim.Adam (params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)</code></p><p>​    Adam也会出现违反优化算法中的“正定性”（positive definiteness）的情况，故Adam的迭代流程修改为如下流程,称为AMSGrad：</p><script type="math/tex; mode=display">\hat{G}_t = \max(\hat{G}_{t-1}, \hat{G}_t)</script><h3 id="1-4-权重衰减"><a href="#1-4-权重衰减" class="headerlink" title="1.4 权重衰减"></a>1.4 权重衰减</h3><p>​    参数：weight_decay：</p><script type="math/tex; mode=display">w_t = (1 - \lambda)w_{t-1} - \eta * g_t</script><p>​    但需要注意的是，这个性质对于任何除了普通梯度下降之外的算法都不适用，比如，不适用于 AdaGrad，不适用于RMSprop，也不适用于Adam以及Adam的变体</p><script type="math/tex; mode=display">g_t = \frac{\partial L}{\partial w} + \lambda w\\w_t = w_{t-1} - \eta \left( \frac{\hat{V}_t}{\sqrt{\hat{G}_t + \epsilon}} + \lambda w_{t-1} \right)</script><p>​    一般不进行权重衰减。</p><h3 id="2-从经典架构出发"><a href="#2-从经典架构出发" class="headerlink" title="2 从经典架构出发"></a>2 从经典架构出发</h3><h3 id="2-1-调用经典架构"><a href="#2-1-调用经典架构" class="headerlink" title="2.1 调用经典架构"></a>2.1 调用经典架构</h3><p>​    对每种类型的架构，models中都包含了至少一个实现架构本身的父类 （呈现为“驼峰式”命名）以及一个包含预训练功能的子类（全部小写）<br>​    在实际使用模型时，我们几乎总是直接调用小写的类来进行使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models <span class="keyword">as</span> m</span><br><span class="line"></span><br><span class="line">resnet18_ = m.resnet18()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(resnet18_)</span><br><span class="line"><span class="built_in">print</span>(resnet18_.conv1)</span><br><span class="line"><span class="built_in">print</span>(resnet18_.layer1[<span class="number">0</span>].conv1)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models <span class="keyword">as</span> m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vgg16_ = m.vgg16()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vgg16_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;--------------&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(vgg16_.features[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">in_channels = <span class="number">1</span></span><br><span class="line">out_num = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">vgg16_.features[<span class="number">0</span>] = nn.Conv2d(in_channels, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">vgg16_.classifier[<span class="number">6</span>] = nn.Linear(in_features=<span class="number">4096</span>, out_features=out_num, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;--------------&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(vgg16_)</span><br></pre></td></tr></table></figure><p>​    大部分时候完全套用经典架构都不能满足我们建模的需求，因此我们需要基于经典架构构建我们自己的架构。</p><h3 id="2-2-基于经典架构自建架构"><a href="#2-2-基于经典架构自建架构" class="headerlink" title="2.2 基于经典架构自建架构"></a>2.2 基于经典架构自建架构</h3><p>​    我们可能有许多不同的建立架构的思路。最常见的方式是按照VGG的方式对网络进行加深，另一种则是使用经典网络中的块（例如残差网络中经典的残差单元、GoogLeNet中的inception等结构）来加深网络。</p><p>​    经验证明，在inception和残差单元之前增加一些普通卷积层会有好处。</p><p>​    缺点是网络的架构和具体的层无法在代码中清晰地 显示出来，同时层与层内部的层次结构也不一致。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models <span class="keyword">as</span> m</span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"></span><br><span class="line"><span class="comment"># 先查看网络结构，挑选我们要使用的部分</span></span><br><span class="line">vgg16_bn_ = m.vgg16_bn()  <span class="comment"># 初始化的参数</span></span><br><span class="line"></span><br><span class="line">vgg16_bn_.features[<span class="number">7</span>:<span class="number">14</span>]</span><br><span class="line">resnet18_ = m.resnet18()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet1</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 第一个卷积层自己写，以保证输入数据在尺寸、通道数上都正确</span></span><br><span class="line">        self.conv1 =nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">              , vgg16_bn_.features[<span class="number">1</span>:<span class="number">3</span>]) <span class="comment"># B R</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 后续的架构直接从经典架构中选</span></span><br><span class="line">        <span class="comment"># 对尺寸很小的数据集而言，我们的深度本来就不深，因此可以试着在特征图数量上有所增加</span></span><br><span class="line">        self.block2 = vgg16_bn_.features[<span class="number">7</span>:<span class="number">14</span>]  <span class="comment"># 卷 卷 最大池化</span></span><br><span class="line">        self.block3 = resnet18_.layer3  <span class="comment"># 残差块</span></span><br><span class="line"></span><br><span class="line">        self.avgpool = resnet18_.avgpool</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(in_features=<span class="number">256</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.block3(self.block2(x))</span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">256</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">data = torch.ones(<span class="number">10</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">net = MyNet1()</span><br><span class="line"><span class="built_in">print</span>(net(data).size())</span><br><span class="line"></span><br><span class="line">summary(net,input_size=(<span class="number">10</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicConv2d</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_, out_=<span class="number">10</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv = nn.Sequential(nn.Conv2d(in_, out_, **kwargs)</span><br><span class="line">                                  , nn.BatchNorm2d(out_)</span><br><span class="line">                                  , nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                  )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels=<span class="number">1</span>, out_features=<span class="number">10</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.block1 = nn.Sequential(BasicConv2d(in_=in_channels, out_=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">                                    , BasicConv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">                                    , nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">                                    , nn.Dropout2d(<span class="number">0.25</span>))</span><br><span class="line">        self.block2 = nn.Sequential(BasicConv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">                                    , BasicConv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">                                    , BasicConv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">                                    , nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">                                    , nn.Dropout2d(<span class="number">0.25</span>))</span><br><span class="line"></span><br><span class="line">        self.classifier_ = nn.Sequential(</span><br><span class="line">              nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">256</span>)</span><br><span class="line">            , nn.BatchNorm1d(<span class="number">256</span>)  <span class="comment"># 此时数据已是二维，因此需要BatchNorm1d</span></span><br><span class="line">            , nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">            , nn.Linear(<span class="number">256</span>, out_features)</span><br><span class="line">            , nn.LogSoftmax(<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.block2(self.block1(x))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>)</span><br><span class="line">        output = self.classifier_(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">data = torch.ones(<span class="number">10</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">net2 = MyNet2()</span><br><span class="line"><span class="built_in">print</span>(net2(data).size())</span><br><span class="line"><span class="comment"># 查看自己构建的网络架构和参数量</span></span><br><span class="line">summary(net2, input_size=(<span class="number">10</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br></pre></td></tr></table></figure><h3 id="2-3-模型的预训练-迁移学习"><a href="#2-3-模型的预训练-迁移学习" class="headerlink" title="2.3 模型的预训练/迁移学习"></a>2.3 模型的预训练/迁移学习</h3><p>预训练在实际使用中能够提供帮助的原因主要是：</p><ol><li><p><strong>层级化的信息提取</strong>：在一个神经网络中，不同层级的层提取的信息层次不同。越靠近输入层的部分通常负责提取更加基础和广泛的特征（例如，形状、边缘等），而越接近输出层的部分则提取与特定任务更紧密相关的高级特征。</p></li><li><p><strong>共享常识性知识</strong>：预训练模型的浅层部分可以被视为一种常识性知识的提取器，这些知识在不同任务间是通用的。通过迁移这部分层及其权重到新任务，可以帮助新任务快速学习和适应，尤其是在新任务的数据相对有限的情况下。</p></li><li><p><strong>加速学习和减少训练成本</strong>：预训练模型的这些层及其权重作为一种良好的初始化，可以使得新任务的模型更快地收敛到一个较好的性能，从而减少训练时间和成本。</p></li></ol><p>预训练技术虽然强大，但其应用存在一定的门槛和限制：</p><ol><li><p><strong>相似任务的可用性</strong>：有效利用预训练模型的前提是，必须有一个与当前任务高度相似的任务A，并且该任务A已有一个训练良好的模型。</p></li><li><p><strong>模型和权重的可访问性</strong>：即便找到了相似的任务，获取其训练好的模型架构和权重也是一个挑战，尤其是对于个人研究者或小型团队来说。</p></li><li><p><strong>公开可用的预训练模型限制</strong>：尽管存在一些公开的预训练模型（如基于ImageNet的模型），但它们可能不适用于所有类型的任务，特别是在一些专业领域（如医疗、金融）。</p></li><li><p><strong>输入数据的一致性</strong>：要成功应用预训练模型，任务A和任务B的输入数据格式和尺寸必须一致，否则可能需要进行额外的数据预处理或模型调整。</p></li></ol><p>您的分析非常透彻地阐述了预训练和迁移学习的实际应用策略。我将为您总结并提供一个例子来进一步阐释。</p><p>在实际应用中，预训练和迁移学习的区别和操作方法包括：</p><ol><li><p><strong>预训练（Pretraining）</strong>：将一个在任务A上训练好的模型的架构和权重迁移到任务B上，并对整个模型进行再训练。这种方法是期望原始任务的模型为新任务提供一个较好的初始化权重。</p></li><li><p><strong>迁移学习（Transfer Learning）</strong>：将任务A的模型迁移到任务B，但只对模型的一部分（通常是新增的层）进行训练，而保持原始模型的部分层（通常是接近输入的层）的权重固定。这种方法利用原始任务的模型作为一个固定的特征提取器。</p></li><li><p><strong>混合方法</strong>：在实际操作中，往往会采用一种混合策略。一开始可能会锁定迁移过来的层，只训练新增的层。随着训练的进行，可以逐渐解锁一部分层，使其参与到后续的训练中，这样可以平衡保留已学习知识和适应新任务的需要。</p></li></ol><p>预训练模型在编程实践中的应用主要涉及以下几个关键步骤：</p><ol><li><p><strong>选择模型</strong>：首先，选择一个适合任务的模型架构，如残差网络（ResNet）。</p></li><li><p><strong>使用预训练权重</strong>：在实例化模型时，可以选择是否使用预训练权重。这通常通过设置一个参数（如 <code>pretrained=True</code>）来实现。如果设置为 <code>True</code>，则模型将加载预训练的权重；如果为 <code>False</code> 或未设置，则模型将使用随机初始化的权重。</p></li><li><p><strong>下载权重</strong>：当选择使用预训练权重时，权重文件通常会从互联网上自动下载。注意，在下载过程中需要保持网络连接，并确保没有网络代理（VPN）干扰。</p></li><li><p><strong>模型训练</strong>：在下载权重之后，可以根据具体任务对模型进行进一步的训练。根据需要，可以选择训练整个模型或仅训练部分层。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models <span class="keyword">as</span> m</span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"></span><br><span class="line">resnet18_ = m.resnet18()</span><br><span class="line">rs18pt = m.resnet18(pretrained=<span class="literal">True</span>)  <span class="comment"># resnet18_pretrained</span></span><br><span class="line">fcin = rs18pt.fc.in_features</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> rs18pt.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet_pretrained</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 迁移层</span></span><br><span class="line">        self.pretrained = nn.Sequential(rs18pt.conv1,</span><br><span class="line">                                        rs18pt.bn1,</span><br><span class="line">                                        rs18pt.relu,</span><br><span class="line">                                        rs18pt.maxpool,</span><br><span class="line">                                        rs18pt.layer1,</span><br><span class="line">                                        rs18pt.layer2</span><br><span class="line">                                        )</span><br><span class="line">        <span class="comment"># 允许训练的层</span></span><br><span class="line">        self.train_ = nn.Sequential(resnet18_.layer3</span><br><span class="line">                                    , resnet18_.layer4</span><br><span class="line">                                    , resnet18_.avgpool)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出的线性层自己写，以确保输出的类别数量正确</span></span><br><span class="line">        self.fc = nn.Linear(in_features=fcin, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.pretrained(x)</span><br><span class="line">        x = self.train_(x)</span><br><span class="line">        x = x.view(x.shape[<span class="number">0</span>], <span class="number">512</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = MyNet_pretrained()</span><br><span class="line">summary(net, input_size=(<span class="number">10</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>), depth=<span class="number">3</span>, device=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解锁</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.pretrained[<span class="number">5</span>][<span class="number">1</span>].parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>​    可以迁移其他的模型和权重：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#本段代码仅做事例，不可运行</span></span><br><span class="line"><span class="comment">#=======【从url获取模型权重】========</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载状态字典</span></span><br><span class="line">state_dict = torch.hub.load_state_dict_from_url(url)</span><br><span class="line">model.load_state_dict()</span><br><span class="line"><span class="comment">#然后就可以用我们对resnet18使用的一系列手段进行迁移学习了</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#=======【从保存好的权重文件中获取权重】=======</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设 model 是您的模型实例</span></span><br><span class="line"><span class="comment"># 保存当前模型的状态字典到硬盘，以便之后能够重新加载</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;current_model_weights.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果需要，也可以保留在内存中的一份深拷贝</span></span><br><span class="line">original_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...执行一些操作，可能会改变模型的权重...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设您加载了新的权重，进行了一些操作，并想要评估它们</span></span><br><span class="line">new_model_wts = torch.load(<span class="string">&#x27;model_weights.pth&#x27;</span>)</span><br><span class="line">model.load_state_dict(new_model_wts)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...评估 new_model_wts...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果新权重表现不佳，您想要恢复到原始权重</span></span><br><span class="line">model.load_state_dict(original_model_wts)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#=======【从保存好的模型中获取权重】======</span></span><br><span class="line"><span class="comment"># 保存完整模型</span></span><br><span class="line">torch.save(model, <span class="string">&#x27;model.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载完整模型</span></span><br><span class="line">model = torch.load(<span class="string">&#x27;model.pth&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="2-4-模型选择"><a href="#2-4-模型选择" class="headerlink" title="2.4 模型选择"></a>2.4 模型选择</h3><p>首先，我们需要对<strong>各个模型</strong>有较为清晰的认知：    </p><ul><li><strong>ResNet和GoogLeNet</strong>：这两个架构在参数效率上表现良好，适合较大的数据集。它们有许多改进的可能性，但在小数据集上的改进或基于这些架构构建新架构可能较为复杂。</li><li><strong>VGG</strong>：具有简单的架构思想，易于构建和修改。移除线性层可以显著减少参数量，使其成为一个不错的选择。</li><li><strong>AlexNet</strong>：架构最为简单（卷积x2 + 卷积x3），适合在小数据集上进行修改和扩展，可以结合其他网络的思想进行改进。</li></ul><p>第二，我们需要关注<strong>数据</strong>的复杂程度：</p><ul><li><p><strong>图像尺寸和信息量</strong>：大尺寸图像通常含有更多信息，特别是真实拍摄的照片相比人造图像或清洗过的图像更为复杂。</p></li><li><p><strong>标签类别数量</strong>：在分类任务中，标签类别越多意味着信息更加复杂；在回归任务中，数据波动大和分布不明确也代表更高的复杂性。</p></li><li><p><strong>网络架构选择</strong>：对于复杂样本（如大尺寸真实照片或类别多的数据集），需要更深更宽的网络架构，如50层以上的残差网络。例如，ImageNet和由真实照片生成的CIFAR-10数据集需要深层网络。CIFAR-10甚至需要110层残差网络来达到较高的准确率。</p></li><li><p><strong>适应不同数据集的架构深度</strong>：对于相对简单的数据集（如Fashion-MNIST），一般使用20层以下的浅层网络；对于表格数据，甚至可以使用约10层的网络。</p></li></ul><p>第三，我们需要关注数据的规模和我们拥有的<strong>算力</strong>：</p><ul><li><p><strong>数据规模和算力</strong>：数据规模可以从特征量和样本量判断，而算力限制决定了可用网络的深度。选择高参数利用率的架构，如残差网络或简化的VGG，是关键。</p></li><li><p><strong>数据量对训练策略的影响</strong>：大数据量时可以通过减小批量大小慢慢训练；数据不足时可能需要更浅的网络，并增加训练次数或使用数据增强来防止过拟合。</p></li><li><p><strong>资源限制下的策略</strong>：在计算资源受限的情况下，如果数据难度高且数据量小，神经网络可能不是最佳选择。</p></li><li><p><strong>有足够算力时的建议</strong>：如果算力充足，建议在多种架构上进行至少10个epochs的尝试，选择表现最好的模型进行进一步优化。</p></li><li><p><strong>算力有限时的策略</strong>：在算力有限的情况下，首选尝试ResNet18，根据其在10个epochs内的表现决定是否加深网络。如果ResNet18表现不佳或计算资源不足，可以尝试修改AlexNet架构。</p></li><li><p><strong>避免在无GPU环境下使用原版VGG16</strong>，因为其计算资源需求较高。</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE%E6%A8%A1%E5%9E%8B.png" alt=""></p><div class="table-container"><table><thead><tr><th></th><th>偏差大</th><th>偏差小</th></tr></thead><tbody><tr><td>方差大</td><td>模型不适合这个数据<br/>换模型</td><td>过拟合<br>模型很复杂<br/>对某些数据集预测很准确 对某些数据集预测很糟糕</td></tr><tr><td>方差小</td><td>欠拟合<br/>模型相对简单<br/>预测很稳定<br/>但对所有的数据预测都不太准确</td><td>泛化误差小，我们的目标</td></tr></tbody></table></div><h2 id="三-【案例】SVHN街道实景门牌识别"><a href="#三-【案例】SVHN街道实景门牌识别" class="headerlink" title="三 【案例】SVHN街道实景门牌识别"></a>三 【案例】SVHN街道实景门牌识别</h2><p>​    SVHN全称Street View House Number数据集（尺寸为32x32，通道为3，样本量也在10万左右），它是深度学习诞生初期被创造出来的众多数字识别数据集中的一个，也是<strong>唯一一个</strong>基于实拍图片制作而成的数字识别数据集。整个数据集支持<strong>识别</strong>、<strong>检测</strong>、<strong>无监督</strong>三种任务，SVHN数据集也因此具有<strong>三种不同的benchmark</strong>。</p><p>​    我们在SVHN文件夹中会看到两个mat格式的文件。</p><h3 id="1-设置库，导入环境"><a href="#1-设置库，导入环境" class="headerlink" title="1 设置库，导入环境"></a>1 设置库，导入环境</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#配置相关环境</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class="string">&#x27;True&#x27;</span> <span class="comment">#用于避免jupyter环境突然关闭</span></span><br><span class="line">torch.backends.cudnn.benchmark=<span class="literal">True</span> <span class="comment">#用于加速GPU运算的代码</span></span><br><span class="line"><span class="comment">#导入pytorch一个完整流程所需的可能全部的包</span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> T</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models <span class="keyword">as</span> m</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment">#导入作为辅助工具的各类包</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment">#可视化</span></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time <span class="comment">#计算时间、记录时间</span></span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> random <span class="comment">#控制随机性</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> gc <span class="comment">#garbage collector 垃圾回收</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#设置全局的随机数种子，这些随机数种子只能提供有限的控制</span></span><br><span class="line"><span class="comment">#并不能完全令模型稳定下来</span></span><br><span class="line">torch.manual_seed(<span class="number">1412</span>) <span class="comment">#torch</span></span><br><span class="line">random.seed(<span class="number">1412</span>) <span class="comment">#random</span></span><br><span class="line">np.random.seed(<span class="number">1412</span>) <span class="comment">#numpy.random</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>导入数据——定义架构——超参数损失函数——训练函数——训练测试——可视化</p><h3 id="2-数据导入、数据探索、数据增强"><a href="#2-数据导入、数据探索、数据增强" class="headerlink" title="2 数据导入、数据探索、数据增强"></a>2 数据导入、数据探索、数据增强</h3><p>​    通常在第一次导入图像的时候，我们不会使用数据增强的任何手段，而是直接ToTensor()导入进行<strong>查看</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">train = torchvision.datasets.SVHN(root =<span class="string">&#x27;D:\Desktop\QNJS\Dataset\SVHN&#x27;</span></span><br><span class="line">                                 ,split =<span class="string">&quot;train&quot;</span> </span><br><span class="line">                                 ,download = <span class="literal">False</span></span><br><span class="line">                               <span class="comment"># ,transform = T.ToTensor()</span></span><br><span class="line">                                 )</span><br><span class="line">test = torchvision.datasets.SVHN(root =<span class="string">&#x27;D:\Desktop\QNJS\Dataset\SVHN&#x27;</span></span><br><span class="line">                                 ,split =<span class="string">&quot;test&quot;</span></span><br><span class="line">                                 ,download = <span class="literal">False</span></span><br><span class="line">                                 ,transform = T.ToTensor())</span><br><span class="line"><span class="built_in">print</span>(np.unique(train.labels))</span><br><span class="line"><span class="comment">####以上为查看数据部分</span></span><br><span class="line"></span><br><span class="line">trainT = T.Compose([T.RandomCrop(<span class="number">28</span>)</span><br><span class="line">                   ,T.RandomRotation(degrees = [-<span class="number">30</span>,<span class="number">30</span>])</span><br><span class="line">                   ,T.ToTensor()</span><br><span class="line">                   ,T.Normalize(mean = [<span class="number">0.485</span>,<span class="number">0.456</span>,<span class="number">0.406</span>]</span><br><span class="line">                                ,std = [<span class="number">0.229</span>,<span class="number">0.224</span>,<span class="number">0.225</span>])])</span><br><span class="line"></span><br><span class="line">testT = T.Compose([T.CenterCrop(<span class="number">28</span>)</span><br><span class="line">                  ,T.ToTensor()</span><br><span class="line">                  ,T.Normalize(mean = [<span class="number">0.485</span>,<span class="number">0.456</span>,<span class="number">0.406</span>]</span><br><span class="line">                                ,std = [<span class="number">0.229</span>,<span class="number">0.224</span>,<span class="number">0.225</span>])])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train = torchvision.datasets.SVHN(root =<span class="string">&#x27;D:\Desktop\QNJS\Dataset\SVHN&#x27;</span></span><br><span class="line">                                 ,split =<span class="string">&quot;train&quot;</span></span><br><span class="line">                                 ,download = <span class="literal">False</span></span><br><span class="line">                                 ,transform = trainT</span><br><span class="line">                                 )</span><br><span class="line">test = torchvision.datasets.SVHN(root =<span class="string">&#x27;D:\Desktop\QNJS\Dataset\SVHN&#x27;</span></span><br><span class="line">                                 ,split =<span class="string">&quot;test&quot;</span></span><br><span class="line">                                 ,download = <span class="literal">False</span></span><br><span class="line">                                 ,transform = testT</span><br><span class="line">                                )</span><br></pre></td></tr></table></figure><h3 id="3-基于经典架构构筑自己的网络"><a href="#3-基于经典架构构筑自己的网络" class="headerlink" title="3 基于经典架构构筑自己的网络"></a>3 基于经典架构构筑自己的网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#基于小型数据集，首先考虑使用各个经典架构中比较浅、但学习能力又比较强的架构</span></span><br><span class="line"><span class="comment">#比如ResNet18、VGG16、Inception也可以考虑</span></span><br><span class="line">torch.manual_seed(<span class="number">1412</span>)</span><br><span class="line">resnet18_ = m.resnet18()</span><br><span class="line">vgg16_ = m.vgg16() <span class="comment">#VGG本来参数量就很大，因此我个人较少使用vgg16_bn</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyResNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.block1 = nn.Sequential(nn.Conv2d(<span class="number">3</span>,<span class="number">64</span>,kernel_size=<span class="number">3</span></span><br><span class="line">                                              ,stride=<span class="number">1</span>,padding=<span class="number">1</span>,bias=<span class="literal">False</span>)</span><br><span class="line">                                   ,resnet18_.bn1</span><br><span class="line">                                   ,resnet18_.relu) <span class="comment">#删除池化层</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#后续的架构直接从经典架构中选</span></span><br><span class="line">        <span class="comment">#对尺寸很小的数据集而言，我们的深度本来就不深，因此可以试着在特征图数量上有所增加（增加宽度）</span></span><br><span class="line">        self.block2 = resnet18_.layer2 <span class="comment">#2个残差单元</span></span><br><span class="line">        self.block3 = resnet18_.layer3 <span class="comment">#2个残差单元</span></span><br><span class="line">        <span class="comment">#自适应平均池化+线性层，此处都与残差网络一致</span></span><br><span class="line">        self.avgpool = resnet18_.avgpool</span><br><span class="line">        <span class="comment">#输出的线性层自己写，以确保输出的类别数量正确</span></span><br><span class="line">        self.fc = nn.Linear(in_features=<span class="number">256</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.block1(x)</span><br><span class="line">        x = self.block2(x)</span><br><span class="line">        x = self.block3(x)</span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = x.view(x.shape[<span class="number">0</span>],<span class="number">256</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyVgg</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#在9层之后增加一个单独的卷积层，再加入池化层，构成(卷积x2+池化) + (卷积x3+池化)的类似AlexNet的结构</span></span><br><span class="line">        self.features = nn.Sequential(*vgg16_.features[<span class="number">0</span>:<span class="number">9</span>] <span class="comment">#星号用于解码</span></span><br><span class="line">                                     ,nn.Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">                                     ,nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                     ,nn.MaxPool2d(<span class="number">2</span>,<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>))</span><br><span class="line">        <span class="comment">#进入线性层时输入通道数发生变化，因此线性层需要重写</span></span><br><span class="line">        <span class="comment">#输出层也需要重写</span></span><br><span class="line">        self.avgpool = vgg16_.avgpool</span><br><span class="line">        self.fc = nn.Sequential(nn.Linear(<span class="number">7</span>*<span class="number">7</span>*<span class="number">128</span>, out_features=<span class="number">4096</span>,bias=<span class="literal">True</span>)</span><br><span class="line">                                ,*vgg16_.classifier[<span class="number">1</span>:<span class="number">6</span>]</span><br><span class="line">                                ,nn.Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">10</span>,bias=<span class="literal">True</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = x.view(x.shape[<span class="number">0</span>],<span class="number">7</span>*<span class="number">7</span>*<span class="number">128</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 卷积层里面的参数大小是(in,out,kh,kw)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line">summary(MyResNet(),(<span class="number">10</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>),depth=<span class="number">1</span>,device=<span class="string">&quot;cpu&quot;</span>) </span><br><span class="line">summary(MyVgg(),(<span class="number">10</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>),depth=<span class="number">1</span>,device=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment">#残差网络的参数量少很多，但是总计算量是VGG的两倍还多</span></span><br><span class="line"><span class="comment">#同时，VGG模型占用的内存更大，所以VGG需要更大的显存，但在GPU上VGG理论上应该更快</span></span><br><span class="line"><span class="comment">#在这个过程中，我们是从已经实例化的类中直接复制层来使用</span></span><br><span class="line"><span class="comment">#因此我们复用经典架构的部分，参数已经被实例化好了</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#因此实例化具体的MyResNet()时没有参数生成</span></span><br><span class="line">[*MyResNet().block2[<span class="number">0</span>].parameters()][<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>] <span class="comment">#复制的部分参数是一致的</span></span><br><span class="line">[*resnet18_.layer2[<span class="number">0</span>].conv1.parameters()][<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"><span class="comment">#没有复用经典架构的部分，则在我们实例化网络的时候才有参数</span></span><br><span class="line">[*resnet18_.fc.parameters()] <span class="comment">#自己设立的部分参数是不同的</span></span><br><span class="line">[*MyResNet().fc.parameters()]</span><br></pre></td></tr></table></figure><h3 id="4-一套完整的训练函数"><a href="#4-一套完整的训练函数" class="headerlink" title="4 一套完整的训练函数"></a>4 一套完整的训练函数</h3><h4 id="4-1-迭代与预测"><a href="#4-1-迭代与预测" class="headerlink" title="4.1 迭代与预测"></a>4.1 迭代与预测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">IterOnce</span>(<span class="params">net,criterion,opt,x,y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对模型进行一次迭代的函数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    net: 实例化后的架构</span></span><br><span class="line"><span class="string">    criterion: 损失函数</span></span><br><span class="line"><span class="string">    opt: 优化算法</span></span><br><span class="line"><span class="string">    x: 这一个batch中所有的样本</span></span><br><span class="line"><span class="string">    y: 这一个batch中所有样本的真实标签</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    sigma = net.forward(x)</span><br><span class="line">    loss = criterion(sigma,y)</span><br><span class="line">    loss.backward()</span><br><span class="line">    opt.step()</span><br><span class="line">    opt.zero_grad(set_to_none=<span class="literal">True</span>) <span class="comment">#比起设置梯度为0，让梯度为None会更节约内存</span></span><br><span class="line">    yhat = torch.<span class="built_in">max</span>(sigma,<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">    correct = torch.<span class="built_in">sum</span>(yhat == y)</span><br><span class="line">    <span class="keyword">return</span> correct,loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">TestOnce</span>(<span class="params">net,criterion,x,y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对一组数据进行测试并输出测试结果的函数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    net: 经过训练后的架构</span></span><br><span class="line"><span class="string">    criterion：损失函数</span></span><br><span class="line"><span class="string">    x：要测试的数据的所有样本</span></span><br><span class="line"><span class="string">    y：要测试的数据的真实标签</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#对测试，一定要阻止计算图追踪</span></span><br><span class="line">    <span class="comment">#这样可以节省很多内存，加速运算</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): </span><br><span class="line">        sigma = net.forward(x)</span><br><span class="line">        loss = criterion(sigma,y)</span><br><span class="line">        yhat = torch.<span class="built_in">max</span>(sigma,<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">        correct = torch.<span class="built_in">sum</span>(yhat == y)</span><br><span class="line">    <span class="keyword">return</span> correct,loss</span><br></pre></td></tr></table></figure><h4 id="4-2-提前停止"><a href="#4-2-提前停止" class="headerlink" title="4.2 提前停止"></a>4.2 提前停止</h4><p>例如，当连续n次迭代中，损失函数的减小值都低于阈值tol，或者测试集的分数提升值都低于阈值tol的时候，我们就可以令迭代停止了。</p><p>我们需要让本轮迭代的损失与 历史迭代最小损失比较，如果历史最小损失 - 本轮迭代的损失 &gt; tol，我们才认可损失函数减小了。</p><p><strong>白话：让测试集的loss是下降的，不是平稳或上升的。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EarlyStopping</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, patience=<span class="number">5</span>, tol=<span class="number">0.0005</span></span>):</span><br><span class="line">        self.patience = patience</span><br><span class="line">        self.tol = tol</span><br><span class="line">        self.counter = <span class="number">0</span></span><br><span class="line">        self.lowest_loss = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)  <span class="comment"># 使用无穷大作为初始值</span></span><br><span class="line">        self.early_stop = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, val_loss</span>):</span><br><span class="line">        loss_improvement = self.lowest_loss - val_loss</span><br><span class="line">        <span class="keyword">if</span> loss_improvement &gt; self.tol:</span><br><span class="line">            self.lowest_loss = val_loss</span><br><span class="line">            self.counter = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.counter += <span class="number">1</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;\t NOTICE: Early stopping counter &#123;&#125; of &#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.counter, self.patience))</span><br><span class="line">            <span class="keyword">if</span> self.counter &gt;= self.patience:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;\t NOTICE: Early Stopping Actived&#x27;</span>)</span><br><span class="line">                self.early_stop = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.early_stop</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="4-3-训练、测试、监控、保存权重、绘图"><a href="#4-3-训练、测试、监控、保存权重、绘图" class="headerlink" title="4.3 训练、测试、监控、保存权重、绘图"></a>4.3 训练、测试、监控、保存权重、绘图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit_test</span>(<span class="params">net, batchdata, testdata, criterion, opt, epochs, tol, modelname, PATH</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run_epoch</span>(<span class="params">data_loader, training=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;运行一个训练或测试的 epoch，并返回平均损失和准确率。&quot;&quot;&quot;</span></span><br><span class="line">        total_loss, total_correct = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> data_loader:</span><br><span class="line">            x, y = x.to(device), y.to(device)</span><br><span class="line">            <span class="keyword">if</span> training:</span><br><span class="line">                opt.zero_grad()</span><br><span class="line">                net.train()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                net.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">            outputs = net(x)</span><br><span class="line">            loss = criterion(outputs, y)</span><br><span class="line">            <span class="keyword">if</span> training:</span><br><span class="line">                loss.backward()</span><br><span class="line">                opt.step()</span><br><span class="line"></span><br><span class="line">            total_loss += loss.item()</span><br><span class="line">            total_correct += (outputs.argmax(<span class="number">1</span>) == y).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">        avg_loss = total_loss / <span class="built_in">len</span>(data_loader.dataset)</span><br><span class="line">        accuracy = total_correct / <span class="built_in">len</span>(data_loader.dataset)</span><br><span class="line">        <span class="keyword">return</span> avg_loss, accuracy</span><br><span class="line"></span><br><span class="line">    train_loss_history, test_loss_history = [], []</span><br><span class="line">    best_accuracy = <span class="number">0.0</span></span><br><span class="line">    early_stopper = EarlyStopping(tol=tol)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        train_loss, train_accuracy = run_epoch(batchdata, training=<span class="literal">True</span>)</span><br><span class="line">        test_loss, test_accuracy = run_epoch(testdata, training=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span> - Train Loss: <span class="subst">&#123;train_loss:<span class="number">.4</span>f&#125;</span>, &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;Test Loss: <span class="subst">&#123;test_loss:<span class="number">.4</span>f&#125;</span>, Train Acc: <span class="subst">&#123;train_accuracy:<span class="number">.2</span>f&#125;</span>%, &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;Test Acc: <span class="subst">&#123;test_accuracy:<span class="number">.2</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        train_loss_history.append(train_loss)</span><br><span class="line">        test_loss_history.append(test_loss)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> test_accuracy &gt; best_accuracy:</span><br><span class="line">            best_accuracy = test_accuracy</span><br><span class="line">            torch.save(net.state_dict(), os.path.join(PATH, <span class="string">f&#x27;<span class="subst">&#123;modelname&#125;</span>.pt&#x27;</span>))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;\tWeights Saved&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> early_stopper(test_loss):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;\tEarly Stopping Triggered&#x27;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_loss_history, test_loss_history</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line"><span class="comment"># net = YourModel()  # 你的模型实例</span></span><br><span class="line"><span class="comment"># criterion = nn.CrossEntropyLoss()</span></span><br><span class="line"><span class="comment"># opt = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)</span></span><br><span class="line"><span class="comment"># train_loss, test_loss = fit_test(net, train_loader, test_loader, criterion, opt, 10, 0.0001, &quot;model_name&quot;, &quot;./models&quot;)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">full_procedure</span>(<span class="params">net, epochs, bs, modelname, PATH, lr=<span class="number">0.001</span></span>):</span><br><span class="line">    torch.manual_seed(<span class="number">1412</span>)</span><br><span class="line">    batchdata = DataLoader(train, batch_size=bs, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">    testdata = DataLoader(test, batch_size=bs, shuffle=<span class="literal">False</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">    criterion = nn.CrossEntropyLoss(reduction=<span class="string">&quot;sum&quot;</span>)</span><br><span class="line">    opt = optim.RMSprop(net.parameters(), lr=lr)</span><br><span class="line">    <span class="keyword">return</span> fit_test(net, batchdata, testdata, criterion, opt, epochs, <span class="number">10</span>**(-<span class="number">5</span>), modelname, PATH)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plotloss</span>(<span class="params">trainloss, testloss</span>):</span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line">    plt.plot(trainloss, color=<span class="string">&quot;red&quot;</span>, linestyle=<span class="string">&quot;-&quot;</span>, linewidth=<span class="number">2</span>, label=<span class="string">&quot;Train Loss&quot;</span>)</span><br><span class="line">    plt.plot(testloss, color=<span class="string">&quot;orange&quot;</span>, linestyle=<span class="string">&quot;--&quot;</span>, linewidth=<span class="number">2</span>, label=<span class="string">&quot;Test Loss&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Training and Testing Loss Over Epochs&quot;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Epochs&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.grid(<span class="literal">True</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">这些 PyTorch 函数是用来管理和获取关于 GPU 显存的信息。以下是每个函数的解释和可能的输出示例：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">1. **torch.cuda.is_available()**</span></span><br><span class="line"><span class="string">   - **功能**：检查当前环境是否有可用的 GPU。</span></span><br><span class="line"><span class="string">   - **输出示例**：`True` 或 `False`。</span></span><br><span class="line"><span class="string">   - **解释**：如果输出 `True`，表示有至少一个可用的 GPU；`False` 表示没有可用的 GPU。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">2. **torch.cuda.device_count()**</span></span><br><span class="line"><span class="string">   - **功能**：返回可用的 GPU 数量。</span></span><br><span class="line"><span class="string">   - **输出示例**：`1` 或更高的整数。</span></span><br><span class="line"><span class="string">   - **解释**：输出表示系统中可用的 GPU 数量。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">3. **torch.cuda.get_device_name(device)**</span></span><br><span class="line"><span class="string">   - **功能**：获取指定 GPU 的名称。</span></span><br><span class="line"><span class="string">   - **输出示例**：`&#x27;NVIDIA GeForce RTX 3080&#x27;`。</span></span><br><span class="line"><span class="string">   - **解释**：输出指定 GPU 设备的名称。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">4. **torch.cuda.memory_allocated()**</span></span><br><span class="line"><span class="string">   - **功能**：返回当前已分配的 GPU 内存量（以字节为单位）。</span></span><br><span class="line"><span class="string">   - **输出示例**：`1048576`（1MB）。</span></span><br><span class="line"><span class="string">   - **解释**：表示当前已分配给张量和缓存的 GPU 内存量。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">5. **torch.cuda.max_memory_allocated()**</span></span><br><span class="line"><span class="string">   - **功能**：返回自程序开始以来分配的最大 GPU 内存量（以字节为单位）。</span></span><br><span class="line"><span class="string">   - **输出示例**：`2097152`（2MB）。</span></span><br><span class="line"><span class="string">   - **解释**：表示程序运行过程中分配的最大 GPU 内存量。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">6. **torch.cuda.memory_reserved()**</span></span><br><span class="line"><span class="string">   - **功能**：返回缓存内存分配器正在使用的 GPU 内存量（以字节为单位）。</span></span><br><span class="line"><span class="string">   - **输出示例**：`4194304`（4MB）。</span></span><br><span class="line"><span class="string">   - **解释**：表示当前由 PyTorch 的缓存分配器保留的 GPU 内存量。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">7. **torch.cuda.get_device_capability(device)**</span></span><br><span class="line"><span class="string">   - **功能**：获取指定 GPU 的计算能力。</span></span><br><span class="line"><span class="string">   - **输出示例**：`(7, 5)`。</span></span><br><span class="line"><span class="string">   - **解释**：表示 GPU 的计算能力版本，例如 7.5。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">8. **torch.cuda.empty_cache()**</span></span><br><span class="line"><span class="string">   - **功能**：清空 PyTorch 的缓存分配器中未使用的缓存内存，但不会清空由张量占用的内存。</span></span><br><span class="line"><span class="string">   - **输出**：无输出，这是一个执行操作。</span></span><br><span class="line"><span class="string">   - **解释**：此函数用于清理 PyTorch 分配器持有但未使用的内存，有助于减少 GPU 内存碎片化。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">以上函数主要用于监控和管理 GPU 资源，特别在进行大规模的深度学习训练时非常有用，以确保有效利用 GPU 资源，避免内存溢出等问题。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">import torch</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">def print_gpu_info():</span></span><br><span class="line"><span class="string">    if torch.cuda.is_available():</span></span><br><span class="line"><span class="string">        device_count = torch.cuda.device_count()</span></span><br><span class="line"><span class="string">        print(f&quot;Number of GPUs Available: &#123;device_count&#125;&quot;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        for i in range(device_count):</span></span><br><span class="line"><span class="string">            print(f&quot;\nGPU &#123;i&#125;: &#123;torch.cuda.get_device_name(i)&#125;&quot;)</span></span><br><span class="line"><span class="string">            print(f&quot;  - Compute Capability: &#123;torch.cuda.get_device_capability(i)&#125;&quot;)</span></span><br><span class="line"><span class="string">            print(f&quot;  - Memory Allocated: &#123;torch.cuda.memory_allocated(i)&#125; bytes&quot;)</span></span><br><span class="line"><span class="string">            print(f&quot;  - Max Memory Allocated: &#123;torch.cuda.max_memory_allocated(i)&#125; bytes&quot;)</span></span><br><span class="line"><span class="string">            print(f&quot;  - Memory Reserved: &#123;torch.cuda.memory_reserved(i)&#125; bytes&quot;)</span></span><br><span class="line"><span class="string">            torch.cuda.empty_cache()  # 清空当前 GPU 的未使用的缓存内存</span></span><br><span class="line"><span class="string">            print(&quot;  - Cache Memory Cleared&quot;)</span></span><br><span class="line"><span class="string">    else:</span></span><br><span class="line"><span class="string">        print(&quot;No GPU available, using CPU.&quot;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">print_gpu_info()</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h3 id="5-模型选择"><a href="#5-模型选择" class="headerlink" title="5 模型选择"></a>5 模型选择</h3>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>pip&amp;conda</title>
      <link href="/pip-conda/"/>
      <url>/pip-conda/</url>
      
        <content type="html"><![CDATA[<h3 id="查找site-packages文件夹"><a href="#查找site-packages文件夹" class="headerlink" title="查找site-packages文件夹"></a>查找site-packages文件夹</h3><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip -V</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/pip-V.png" alt=""></p><h3 id="创建新环境"><a href="#创建新环境" class="headerlink" title="创建新环境"></a>创建新环境</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n myenv python=3.8</span><br></pre></td></tr></table></figure><h3 id="查看已建环境"><a href="#查看已建环境" class="headerlink" title="查看已建环境"></a>查看已建环境</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda <span class="built_in">env</span> list</span><br></pre></td></tr></table></figure><h3 id="查看已安装包"><a href="#查看已安装包" class="headerlink" title="查看已安装包"></a>查看已安装包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>yolov3-paper</title>
      <link href="/yolov3-paper/"/>
      <url>/yolov3-paper/</url>
      
        <content type="html"><![CDATA[<h1 id="Yolov3-An-incremental-improvement"><a href="#Yolov3-An-incremental-improvement" class="headerlink" title="Yolov3:An incremental improvement"></a>Yolov3:An incremental improvement</h1><h2 id="Abstract—摘要"><a href="#Abstract—摘要" class="headerlink" title="Abstract—摘要"></a><strong>Abstract—摘要</strong></h2><p>我们提出了对YOLO的一些更新! 我们做了一些小的设计改动，使其变得更好。我们还训练了这个新的网络，这个网络非常棒。它比上次大了一点，但更准确。不过它仍然很快，不用担心。在320×320的情况下，YOLOv3在28.2mAP的情况下运行22毫秒，与SSD一样准确，但速度快三倍。当我们看一下旧的0.5 IOU mAP检测指标时，YOLOv3是相当好的。在Titan X上，它在51毫秒内实现了57.9个AP50，而RetinaNet在198毫秒内实现了57.5个AP50，性能相似但快3.8倍。像往常一样，所有的代码都在网上，<a href="https://pjreddie.com/yolo/。">https://pjreddie.com/yolo/。</a></p><blockquote><p><strong>对比YOLOv2：</strong> 做了一些小改进，然后训练了一个更深的模型，准确度有所提升，并且依旧很快。</p><p><strong>对比SSD：</strong> 在320×320的情况下，YOLOv3在28.2mAP的情况下运行22毫秒，与SSD一样准确，但速度快三倍。</p><p><strong>对比RetinaNet：</strong> 在Titan X上，它在51毫秒内实现了57.9个AP50，而RetinaNet在198毫秒内实现了57.5个AP50，性能相似但快3.8倍。</p><p>这是可爱的作者直接拿了RetinaNet的图，然后加上了自己的YOLO曲线，可以看到YOLO的线都已经弄到坐标轴外面去了，这是故意的，用这种方法凸显自己模型更快（还有个原因是因为懒）。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-abstract.png" alt=""></p></blockquote><h2 id="一、Introduction—（随性的）引言"><a href="#一、Introduction—（随性的）引言" class="headerlink" title="一、Introduction—（随性的）引言"></a><strong>一、Introduction—（随性的）引言</strong></h2><p>有时，你一整年全在敷衍了事而不自知。比如今年我就没做太多研究，在推特上挥霍光阴，置 GANs 于不顾。我有一点去年留下的动力[12] [1]；我设法对YOLO做了一些改进。但是，说实话，没有什么超级有趣的东西，只是做了一些小改动，让它变得更好。我还对其他人的研究提供了一些帮助。</p><p>实际上，这也是我们今天来到这里的原因。我们有一个可以上镜的最后期限[4]，我们需要引用我对YOLO的一些随机更新，但我们没有来源。所以准备好接受技术报告吧!</p><p>技术报告的好处是，它们不需要介绍，你们都知道我们为什么在这里。因此，这个介绍的结尾将为本文的其余部分做一个标志。首先，我们将告诉你YOLOv3是什么情况。然后我们会告诉你我们是如何做的。我们也会告诉你一些我们尝试过但没有成功的事情。最后，我们将思考这一切意味着什么。</p><blockquote><p>“有时，你一整年全在敷衍了事而不自知。比如今年我就没做太多研究，在推特上挥霍光阴，置 GANs 于不顾。”——最开始读时我一度以为这是网友的恶搞翻译，直到后来自己看了原文。。。不得不服，果真是大佬才敢那么任性吧！</p><p>很别具一格，没啥实际内容，大家直接往后翻吧hh。就是说这篇论文实际上算是一篇学术报告，是为了给他们即将出现的新论文做引用的。</p></blockquote><h2 id="二、The-Deal—改进的细节"><a href="#二、The-Deal—改进的细节" class="headerlink" title="二、The Deal—改进的细节"></a><strong>二、The Deal—改进的细节</strong></h2><h3 id="2-1-Bounding-Box-Prediction—边界框预测"><a href="#2-1-Bounding-Box-Prediction—边界框预测" class="headerlink" title="2.1 Bounding Box Prediction—边界框预测"></a>2.1 Bounding Box Prediction—边界框预测</h3><p>按照YOLO9000，我们的系统使用维度集群作为锚定框来预测边界框[15]。该网络为每个边界框预测4个坐标，tx, ty, tw, th。如果单元格从图像的左上角偏移（cx，cy），并且边界框先验具有宽度和高度pw，ph，那么预测对应于：</p><script type="math/tex; mode=display">b_x = \sigma(t_x) + c_x</script><script type="math/tex; mode=display">b_y = \sigma(t_y) + c_y</script><script type="math/tex; mode=display">b_w = p_w e^{t_w}</script><script type="math/tex; mode=display">b_h = p_h e^{t_h}</script><script type="math/tex; mode=display">Pr(\text{object}) \ast IOU(b, \text{object}) = \sigma(t_o)</script><p>在训练期间，我们使用平方误差损失之和。如果某个真实框的坐标是ˆt，我们的梯度就是真实框坐标值（从真实框中计算）减去我们的预测坐标值：ˆt<em> - t</em>。这个真实框坐标值可以通过倒置上述方程轻松计算出来。 </p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-deal.png" alt=""></p><p> YOLOv3使用逻辑回归法为每个边界框预测一个目标分数。如果边界框先验比其他边界框先验更多地与目标真实框重叠，则该分数应为1。如果先验边界框不是最好的，但与真实框目标的重叠程度超过了某个阈值，我们就忽略这个预测，遵循[17]。我们使用0.5的阈值。与[17]不同的是，我们的系统只为每个真实框对象分配一个先验边界框。如果没有为一个真实框对象分配一个先验边界框，它不会对坐标或类别的预测产生任何损失，只有目标置信度。</p><blockquote><h4 id="与YOLOv2相同之处"><a href="#与YOLOv2相同之处" class="headerlink" title="与YOLOv2相同之处"></a>与YOLOv2相同之处</h4><p>使用dimension clusters来找到先验anchor boxes，然后通过anchor boxes来预测边界框。</p><h4 id="YOLOv3的改进"><a href="#YOLOv3的改进" class="headerlink" title="YOLOv3的改进"></a>YOLOv3的改进</h4><p>在YOLOv3 中，利用逻辑回归来预测每个边界框的客观性分数( object score )，也就是YOLOv1 论文中说的confidence :</p><ul><li><strong>正样本：</strong> 如果当前预测的包围框比之前其他的任何包围框更好的与ground truth对象重合，那它的置信度就是 1。</li><li><strong>忽略样本：</strong> 如果当前预测的包围框不是最好的，但它和 ground truth对象重合了一定的阈值（这里是0.5）以上，神经网络会忽略这个预测。</li><li><strong>负样本:</strong> 若bounding box 没有与任一ground truth对象对应，那它的置信度就是 0</li></ul><blockquote><p><strong>Q1：为什么YOLOv3要将正样本confidence score设置为1?</strong></p><p>置信度意味着该预测框是或者不是一个真实物体，是一个二分类，所以标签是1、0更加合理。并且在学习小物体时，有很大程度会影响IOU。如果像YOLOv1使用bounding box与ground truth对象的IOU作为confidence，那么confidence score始终很小，无法有效学习，导致检测的Recall不高。</p><p><strong>Q2：为什么存在忽略样本?</strong></p><p>由于YOLOV3采用了多尺度的特征图进行检测，而不同尺度的特征图之间会有重合检测的部分。例如检测一个物体时，在训练时它被分配到的检测框是第一个特征图的第三个bounding box，IOU为0.98，此时恰好第二个特征图的第一个bounding box与该ground truth对象的IOU为0.95，也检测到了该ground truth对象，如果此时给其confidence score强行打0，网络学习的效果会不理想。</p></blockquote><p>与Faster-RCNN 不同，YOLOv3 仅对每一个真实物件分配一个anchor box，若没有分配到anchor box 的真实物件，便不会有坐标误差，仅会具有object score 误差。</p></blockquote><h3 id="2-2-Class-Prediction—类预测（单标签分类改进为多标签分类）"><a href="#2-2-Class-Prediction—类预测（单标签分类改进为多标签分类）" class="headerlink" title="2.2 Class Prediction—类预测（单标签分类改进为多标签分类）"></a>2.2 Class Prediction—类预测（单标签分类改进为多标签分类）</h3><p>每个框都使用多标签分类法预测边界框可能包含的类别。我们不使用softmax，因为我们发现它对于良好的性能是不必要的，相反，我们只是使用独立的逻辑分类器。在训练过程中，我们使用二元交叉熵损失来进行分类预测。</p><p>当我们转向更复杂的领域，如开放图像数据集[7]时，这种提法会有所帮助。在这个数据集中，有许多重叠的标签（即女人和人）。</p><p>使用softmax的假设是，每个框都有一个确切的类别，但情况往往并非如此。多标签方法可以更好地模拟数据。</p><blockquote><h4 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h4><p>每个框使用多标签分类预测边界框可能包含的类</p><h4 id="YOLOv3使用的方法"><a href="#YOLOv3使用的方法" class="headerlink" title="YOLOv3使用的方法"></a>YOLOv3使用的方法</h4><p>（1）YOLOv3 使用的是<strong>logistic 分类器</strong>，而不是之前使用的softmax。</p><p>（2）在YOLOv3 的训练中，便使用了<strong>Binary Cross Entropy ( BCE, 二元交叉熵)</strong> 来进行类别预测。</p><blockquote><p><strong>Q：softmax被替代的原因？</strong></p><p>（1）softmax只适用于单目标多分类(甚至类别是互斥的假设)，但目标检测任务中可能一个物体有多个标签。(属于多个类并且类别之间有相互关系)，比如Person和Women。</p><p>（2）logistic激活函数来完成，这样就能预测每一个类别是or不是。</p></blockquote></blockquote><h3 id="2-3-Predictions-Across-Scales—跨尺度预测"><a href="#2-3-Predictions-Across-Scales—跨尺度预测" class="headerlink" title="2.3 Predictions Across Scales—跨尺度预测"></a>2.3 Predictions Across Scales—跨尺度预测</h3><p>YOLOv3在三个不同的尺度上对框进行预测。我们的系统使用类似于特征金字塔网络[8]的概念从这些尺度上提取特征。从我们的基础特征提取器中，我们添加了几个卷积层。最后一个卷积层预测一个3维张量，编码边界框、对象性和类别预测。在我们与COCO[10]的实验中，我们在每个尺度上预测3个框，所以张量是N×N×[3∗(4+1+80)]，用于4个边界盒的偏移，1个对象性预测，和80个类别预测。</p><p>接下来，我们从前两层中提取特征图，并对其进行2倍的上采样。我们还从网络中的早期特征图中取出一个特征图，用连接法将其与我们的上采样特征合并。这种方法使我们能够从上采样的特征中获得更有意义的语义信息，并从早期的特征图中获得更精细的信息。然后，我们再增加几个卷积层来处理这个合并的特征图，最终预测出一个类似的张量，尽管现在的张量是原来的两倍。</p><p>我们再进行一次同样的设计，以预测最终规模的框。因此，我们对第三个尺度的预测得益于所有先前的计算以及网络早期的细化特征。</p><p>我们仍然使用k-means聚类法来确定我们的边界框预设。我们只是任意地选择了9个聚类和3个尺度，然后在各个尺度上均匀地划分聚类。在COCO数据集上，这9个聚类是。(10 × 13), (16 × 30), (33 × 23), (30 × 61), (62 × 45), (59 × 119), (116 × 90), (156 × 198), (373 × 326)。</p><blockquote><h4 id="YOLOv3的改进-1"><a href="#YOLOv3的改进-1" class="headerlink" title="YOLOv3的改进"></a>YOLOv3的改进</h4><p><strong>灵感来源：</strong> YOLOv3借鉴了FPN的方法，采用多尺度的特征图对不同大小的物体进行检测，以提升小物体的预测能力。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-%E8%B7%A8%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B.png" alt=""></p><p><strong>（1）YOLOv3采用了3个不同尺度的特征图（三个不同卷积层提取的特征）</strong></p><p>YOLOv3通过下采样32倍、16倍和8倍得到3个不同尺度的特征图。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-%E8%B7%A8%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B1.png" alt=""></p><p>例如输入416X416的图像，则会得到<strong>13X13</strong> (416/32)，<strong>26X26</strong>(416/16) 以及<strong>52X52</strong>(416/8)这3个尺度的特征图。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-%E8%B7%A8%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B2.png" alt=""></p><p><strong>（2）YOLOv3每个尺度的特征图上使用3个anchor box。</strong></p><p>使用dimension clusters得到9个聚类中心（anchor boxes），并将这些anchor boxes划分到3个尺度特征图上，尺度更大的特征图使用更小的先验框。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-%E8%B7%A8%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B3.png" alt=""></p><p><strong>（3）YOLOv3对每个尺度下的特征图都进行边界框的预测。</strong></p><p>每种尺度的特征图上可以得到<strong>N × N × [3 ∗ (4 + 1 + 80)]</strong> 的结果（分别是N x N个 gird cell ，3种尺度的anchor boxes，4个边界框偏移值、1个目标预测置信度以及80种类别的预测概率。）</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-%E8%B7%A8%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B4.png" alt=""></p><p>该方法允许从上采样的特征中获取更有意义的语义信息，从早期的特征图中获取更细粒度的信息。</p><h4 id="不同尺度下的预测方法"><a href="#不同尺度下的预测方法" class="headerlink" title="不同尺度下的预测方法"></a>不同尺度下的预测方法</h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-%E8%B7%A8%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B5.png" alt=""></p><p><strong>（1）第一种尺度：</strong></p><p><strong>特征图：</strong> 对原图下采样32x得到(13 x 13)特征图</p><p><strong>预测：</strong> 在上述特征图后添加几个卷积层，最后输出一个 N × N × [3 ∗ (4 + 1 + 80)] 的张量表示预测。——图中第3个红色部分</p><p><strong>最终输出：</strong> [13, 13, 255]</p><p><strong>（2）第二种尺度：</strong></p><p><strong>特征图：</strong> 来源于两种计算</p><p>   1.对原图下采样16x得到 (26 x 26)特征图</p><p>   2.对第一种尺度得到的(13 x 13)特征图进行上采样，得到(26 x 26)特征图。</p><p>两种计算得到的（26 x 26）特征图通过连接合并在一起。</p><p><strong>预测：</strong> 在合并后的特征图后添加几个卷积层，最后输出一个 N × N × [3 ∗ (4 + 1 + 80)] 的张量表示预测。这个张量的大小是尺度一输出张量大小的两倍。——图中第2个红色部分</p><p><strong>最终输出：</strong> [26, 26, 255]</p><p><strong>（3）第三种尺度：</strong></p><p><strong>特征图：</strong> 来源于两种计算</p><p> 1.对原图下采样8x得到 (52 x 52)特征图</p><p> 2.对第二种尺度得到的(26 x 26)特征图进行上采样，得到(52 x 52)特征图。</p><p>两种方式得到的（52 x 52）的特征图通过连接合并在一起。</p><p><strong>预测：</strong> 在合并后的特征图后添加几个卷积层，最后输出一个 N × N × [3 ∗ (4 + 1 + 80)] 的张量表示预测。这个张量的大小是尺度二输出的两倍——图中第1个红色部分。</p><p>对第三尺度的预测受益于所有的先验计算以及网络早期的细粒度特性。</p><p><strong>最终输出：</strong> [52, 52, 255]</p><blockquote><p><strong>Q：合并（加入残差啊思想）的目的：</strong></p><p>在每一种维度输出之前还有一个分支就是和下一路进行concat拼接(上一层进行上采样后拼接)。这样加入残差思想，保留各种维度特征(底层像素+高层语义)。三个尺度就可以预测各种不同大小的物体了。</p></blockquote><h4 id="结构模型示意图"><a href="#结构模型示意图" class="headerlink" title="结构模型示意图"></a>结构模型示意图</h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-%E8%B7%A8%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B6.png" alt=""></p><p>YOLOv3总共输出3个特征图，第一个特征图下采样32倍，第二个特征图下采样16倍，第三个下采样8倍。输入图像经过Darknet-53（无全连接层），再经过YOLOblock生成的特征图被当作两用，第一用为经过3×3卷积层、1×1卷积之后生成特征图一，第二用为经过1×1卷积层加上采样层，与Darnet-53网络的中间层输出结果进行拼接，产生特征图二。同样的循环之后产生特征图三。</p></blockquote><h3 id="2-4-Feature-Extractor—特征提取"><a href="#2-4-Feature-Extractor—特征提取" class="headerlink" title="2.4 Feature Extractor—特征提取"></a>2.4 Feature Extractor—特征提取</h3><p>我们使用一个新的网络来进行特征提取。</p><p>我们的新网络是YOLOv2中使用的网络、Darknet-19和新式的Darknet网络之间的一种混合方法。我们的网络使用连续的3×3和1×1卷积层，但现在也有一些快捷连接，而且明显更大。它有53个卷积层，所以我们把它叫做… 等待它… Darknet-53!</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-net.png" alt=""></p><p>这个新网络比Darknet19强大得多，但仍然比ResNet-101或ResNet-152更有效率。下面是一些ImageNet的结果:</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-result.png" alt=""></p><blockquote><p>作者是将darknet-19里加入了ResNet残差连接，改进之后的模型叫Darknet-53</p><h4 id="Darknet-53主要做了如下改进："><a href="#Darknet-53主要做了如下改进：" class="headerlink" title="Darknet-53主要做了如下改进："></a><strong>Darknet-53主要做了如下改进：</strong></h4><p>（1）<strong>没有采用最大池化层</strong>，转而采用步长为2的卷积层进行下采样。</p><p>（2）为了防止过拟合，在每个卷积层之后加入了<strong>一个BN层和一个Leaky ReLU</strong>。</p><p>（3）引入了<strong>残差网络</strong>的思想，目的是为了让网络可以提取到更深层的特征，同时避免出现梯度消失或爆炸。</p><p>（4）<strong>将网络的中间层和后面某一层的上采样进行张量拼接</strong>，达到多尺度特征融合的目的。</p><p>Darknet-53的性能与最先进的分类器相当，但浮点运算更少，速度更快。Darknet-53还实现了每秒最高的浮点运算。这意味着网络结构更好地利用了GPU，使其评估更高效，从而更快。</p></blockquote><h3 id="2-5-Training—训练"><a href="#2-5-Training—训练" class="headerlink" title="2.5 Training—训练"></a>2.5 Training—训练</h3><p>我们仍然在完整的图像上进行训练，没有硬性的负面挖掘或任何这些东西。我们使用多尺度训练，大量的数据增强，批量归一化，所有标准的东西。我们使用Darknet神经网络框架进行训练和测试[14]。</p><blockquote><p>（1）训练完整的图像，没有硬负面挖掘。</p><p>（2）使用多尺度的训练，大量的数据扩充，批量标准化。</p><p>（3）使用Darknet神经网络框架来训练和测试。</p></blockquote><h2 id="三、How-We-Do—我们怎样做"><a href="#三、How-We-Do—我们怎样做" class="headerlink" title="三、How We Do—我们怎样做"></a>三、How We Do—我们怎样做</h2><p>YOLOv3是相当不错的，见表3。在COCOs怪异的平均AP指标方面，它与SSD的变体相当，但速度快了3倍。虽然在这个指标上，YOLOv3和RetinaNet等模型一样。</p><p>然而，相比于IOU=0.5（或图表中的AP50）的 “老 “检测指标mAP时，YOLOv3显得非常强大。它几乎与RetinaNet持平，远高于SSD的变体。这表明YOLOv3是一个非常强大的检测器，擅长为目标生成合理的框。然而，随着IOU阈值的增加，性能明显下降，表明YOLOv3努力使框与目标完全对齐。</p><p>在过去，YOLO在处理小物体时很吃力。然而，现在我们看到这一趋势发生了逆转。通过新的多尺度预测，我们看到YOLOv3具有相对较高的APS性能。然而，它在中等和较大尺寸物体上的性能相对较差。需要进行更多的调查来了解这个问题的真相。</p><p>当我们在AP50指标上绘制准确度与速度的关系时（见图5），我们看到YOLOv3比其他检测系统有明显的优势。也就是说，它更快、更好。<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-howdo.png" alt=""></p><blockquote><p><strong>APs：</strong> 小目标（area(框大小）&lt;32×32）的AP</p><p><strong>APm：</strong> 中目标（32×32&lt;area&lt;96×96）的AP</p><p><strong>APl：</strong> 大目标（96×96&lt;area）的AP</p><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><ul><li>YOLOv3 擅于预测出「合适」，但无法预测出非常精准的边界框。</li><li>YOLOv3 小目标预测能力提升，但中大目标的预测反而相对较差。</li><li>若将速度考量进来，YOLOv3 整体来说表现非常出色。</li></ul><h4 id="YOLOv3在小目标-密集目标的改进"><a href="#YOLOv3在小目标-密集目标的改进" class="headerlink" title="YOLOv3在小目标\密集目标的改进"></a>YOLOv3在小目标\密集目标的改进</h4><p>1.<strong>grid cell个数增加</strong>，YOLOv1（7×7），YOLOv2（13×13），YOLOv3（13×13+26×26+52×52）</p><p>2.<strong>YOLOv2和YOLOv3可以输入任意大小的图片</strong>，输入图片越大，产生的grid cell越多，产生的预测框也就越多</p><p>3.<strong>专门小目标预先设置了一些固定长宽比的anchor</strong>，直接生成小目标的预测框是比较难的，但是在小预测框基础上再生成小目标的预测框是比较容易的</p><p>4.<strong>多尺度预测（借鉴了FPN）</strong>，既发挥了深层网络的特化语义特征，又整合了浅层网络的细腻度的像素结构信息</p><p>5.对于小目标而言，边缘轮廓是非常重要的，即浅层网络的边缘信息。<strong>在损失函数中有着惩罚小框项</strong></p><p>6.网络结构：<strong>网络加了跨层连接和残差连接（shortcut connection）</strong>，这样可以整合各个层的特征，这样使得网络本身的特征提取能力提升了</p></blockquote><h2 id="四、Things-We-Tried-That-Didn’t-Work—那些我们尝试了但没有奏效的方法"><a href="#四、Things-We-Tried-That-Didn’t-Work—那些我们尝试了但没有奏效的方法" class="headerlink" title="四、Things We Tried That Didn’t Work—那些我们尝试了但没有奏效的方法"></a><strong>四、Things We Tried That Didn’t Work—那些我们尝试了但没有奏效的方法</strong></h2><p>我们在做YOLOv3的时候尝试了很多东西。很多东西都没有成功。以下是我们能记住的东西。</p><p> <strong>锚框的X、Y偏移量预测</strong>。我们尝试使用正常的锚定框预测机制，即用线性激活的方式将x、y偏移量预测为框宽或框高的倍数。我们发现这种提法降低了模型的稳定性，而且效果不是很好。</p><p><strong>线性x，y预测，而不是逻辑预测</strong>。我们尝试用线性激活来直接预测x，y偏移量，而不是用逻辑激活。这导致了mAP下降了几个点。</p><p> <strong>Focal loss</strong>。我们尝试使用焦点损失。它使我们的mAP下降了大约2点。YOLOv3可能已经对焦点损失试图解决的问题很稳健，因为它有独立的对象性预测和条件类预测。因此，对于大多数例子来说，类别预测没有损失？还是什么？我们并不完全确定。</p><p> <strong>双重IOU阈值和真实分配</strong>。Faster R-CNN在训练过程中使用两个IOU阈值。如果一个预测与真实框重叠0.7，它就是一个正面的例子，重叠[0.3 - 0.7]，它就会被忽略，对于所有地面真相对象来说，小于0.3就是一个负面的例子。我们尝试过类似的策略，但没有得到好的结果。</p><p>我们相当喜欢我们目前的表述，它似乎至少处于一个局部最优状态。这些技术中的一些可能最终会产生好的结果，也许它们只是需要一些调整来稳定训练。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-didntwork.png" alt=""></p><blockquote><p>这部分不用深入研究，因为这是作者也没有捣鼓清楚的内容~</p><p>（1）<strong>预测相对于初始anchor宽高倍数的偏移量</strong> ，使用线性激活预测x, y偏移为的anchor box的宽度或高度的倍数。这种方法降低了模型的稳定性，效果不是很好</p><p>（2）<strong>使用线性激活来直接预测x, y偏移量</strong>，而不是逻辑逻辑激活。这导致了一些mAP的下降。</p><p>（3）<strong>Focal loss</strong> （用于图像领域解决数据不平衡造成的模型性能问题，也就是正负样本不均衡，正样本少的问题)。mAPx下降2个点。YOLOv3可能已经对focal loss试图解决的问题很健壮，因为它有独立的目标预测和条件类预测。</p><p>（4）<strong>Faster RCNN在训练中使用两个IOU阈值</strong>。如果预测框与真值 IOU大于0.7的，则边界框作为正样本。如果IOU在0.3-0.7之间它被忽略，小于0.3阈值时，它是一个负样本。我们尝试了类似的策略，但没有得到好的结果。</p></blockquote><h2 id="五、-What-This-All-Means—这一切意味着什么？"><a href="#五、-What-This-All-Means—这一切意味着什么？" class="headerlink" title="五、 What This All Means—这一切意味着什么？"></a><strong>五、 What This All Means—这一切意味着什么？</strong></h2><p>YOLOv3是一个好的检测器。它的速度很快，很准确。它在0.5和0.95 IOU之间的COCO平均AP指标上不那么好。但它在旧的检测指标0.5 IOU上是非常好的。</p><p>我们到底为什么要转换指标？最初的COCO论文中只有这样一句话：”关于评估指标的全面讨论。“一旦评估服务器完成，将增加对评估指标的全面讨论”。Russakovsky等人报告说，人类很难区分0.3和0.5的IOU! “训练人类目测一个IOU为0.3的边界框并将其与一个IOU为0.5的边界框区分开来是非常困难的。” [18] 如果人类很难区分，那么这又有多大关系呢？但也许一个更好的问题是：“既然我们有了这些探测器，我们要用它们做什么？” 很多做这项研究的人都在谷歌和Facebook。</p><p>我想至少我们知道这项技术是在良好的手中，绝对不会被用来收集你的个人信息并出售给…，等等，你是说这正是它将被用来做什么？哦。</p><p>好吧，其他大量资助视觉研究的人是军方，他们从来没有做过任何可怕的事情，比如用新技术杀死很多人，哦，等等…1，我很希望大多数使用计算机视觉的人只是在用它做快乐的好事，比如计算国家公园里斑马的数量[13]，或者跟踪他们的猫在家里徘徊[19]。但是，计算机视觉已经被用于可疑的用途，作为研究人员，我们有责任至少考虑我们的工作可能造成的伤害，并想办法减轻它。我们欠世界这么多。</p><p>最后，请不要@我。(因为我终于退出了Twitter）。</p><blockquote><p>作者希望计算机视觉可以用在好的、对的事情上面。Love&amp;Peace~</p></blockquote><div class="table-container"><table><thead><tr><th></th><th>YOLOv1</th><th>YOLOv2</th><th>YOLOv3</th></tr></thead><tbody><tr><td>输入图像尺寸</td><td>输入的是448×448的三通道图像</td><td>输入的是416×416的三通道图像</td><td>输入的是416×416的三通道图像</td></tr><tr><td>grid cell</td><td>每一张图像划分为7×7=49个grid cell</td><td>每一张图像划分为13×13=169个grid cell</td><td>yolov3会产生三个尺度：13×13、26×26、52×52，也对应着grid cell个数。</td></tr><tr><td>bbox/anchor</td><td>每个grid cell 生成2个 bbox（没有anchor），与真实框IOU最大的那个框负责拟合真实框</td><td>每个grid cell 生成5个anchor框，通过IOU计算选一个anchor产生预测框去拟合真实框</td><td>每个grid cell生成3个anchor框，通过与gt的IOU计算选一个anchor产生预测框去拟合真实框</td></tr><tr><td>输出张量</td><td>输出7 <em> 7 </em> 30维的张量(30的含义:两组bbox的xywh和置信度+20个类别)</td><td>输出13 <em> 13 </em> 125维张量(125的含义：五组anchor的xywh和置信度+20个类别)</td><td>输出三个不同尺寸的张量，但最后都是255，比如S <em> S </em> 255，（255含义:三组anchor里xywh+置信度+分类数(COCO数据集80个分类)，所以就是3 * (80+5)。）</td></tr><tr><td>预测框数量</td><td>7 <em> 7 </em>2 = 98个预测框</td><td>13 <em> 13 </em> 5 = 845个预测框</td><td>( 52 <em> 52 + 26 </em> 26 +13 <em> 13) </em> 3 = 10647个预测框</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>yolov2-paper</title>
      <link href="/yolov2-paper/"/>
      <url>/yolov2-paper/</url>
      
        <content type="html"><![CDATA[<h1 id="YOLO9000-Better-Faster-Stronger"><a href="#YOLO9000-Better-Faster-Stronger" class="headerlink" title="YOLO9000: Better, Faster, Stronger"></a>YOLO9000: Better, Faster, Stronger</h1><h2 id="Abstract—摘要"><a href="#Abstract—摘要" class="headerlink" title="Abstract—摘要"></a>Abstract—摘要</h2><p>我们介绍了YOLO9000，一个最先进的实时目标检测系统，可以检测超过9000个目标类别。首先，我们提出了对YOLO检测方法的各种改进，这些改进既是新的，也是来自先前的工作。改进后的模型YOLOv2在标准检测任务上是最先进的，如PASCAL VOC和COCO。使用一种新的、多尺度的训练方法，同一个YOLOv2模型可以在不同的规模下运行，在速度和准确性之间提供了一个简单的权衡。在67FPS时，YOLOv2在VOC 2007上得到76.8mAP。在40 FPS时，YOLOv2得到78.6 mAP，超过了最先进的方法，如带有ResNet和SSD的Faster R-CNN，同时运行速度仍然很高。最后，我们提出了一种联合训练目标检测和分类的方法。使用这种方法，我们在COCO检测数据集和ImageNet分类数据集上同时训练YOLO9000。我们的联合训练使YOLO9000能够预测没有标记检测数据的目标类别的检测情况。我们在ImageNet检测任务上验证了我们的方法。尽管只有200个类中的44个有检测数据，YOLO9000在ImageNet检测验证集上得到了19.7的mAP。在COCO上没有的的156个类中，YOLO9000得到了16.0 mAP。但YOLO能检测的不仅仅是200个类；它能预测9000多个不同目标类别的检测。而且它仍然是实时运行的。</p><blockquote><h4 id="YOLOv1的不足"><a href="#YOLOv1的不足" class="headerlink" title="YOLOv1的不足"></a>YOLOv1的不足</h4><p>1）定位不准确</p><p>2）和基于region proposal的方法相比召回率较低。</p><h4 id="本文的改进"><a href="#本文的改进" class="headerlink" title="本文的改进"></a>本文的改进</h4><ul><li><strong>YOLO9000：</strong> 先进，实时的目标检测方法，可检测9000多类物体</li><li><strong>多尺度训练方法（ multi-scale training）：</strong> 相同的YOLOv2模型可以在不同的大小下运行，在速度和精度之间提供了一个简单的折中</li><li><strong>mAP表现更好：</strong> 67FPS，在VOC 2007上76.8 mAP，在40 FPS，78.6mAP；而且速度更快。</li><li><strong>提出了一种联合训练目标检测和分类的方法：</strong> 使用该方法在COCO目标检测数据集和Imagenet图像分类数据集上，训练出了YOLO9000</li><li><strong>可以检测出更多的类别：</strong> 即使这些类别没有在目标检测的数据集中出现</li></ul></blockquote><h2 id="一、-Introduction—引言"><a href="#一、-Introduction—引言" class="headerlink" title="一、 Introduction—引言"></a><strong>一、 Introduction—引言</strong></h2><p>通用的目标检测应该是快速、准确的，并且能够识别各种各样的目标。自从引入神经网络以来，检测框架已经变得越来越快和准确。然而，大多数检测方法仍然被限制在一小部分目标上。</p><p>与分类和标记等其他任务的数据集相比，当前的目标检测数据集是有限的。最常见的检测数据集包含几千到几十万张图像，有几十到几百个标签[3] [10] [2]。分类数据集有数以百万计的图像，有数万或数十万个类别[20] [2]。</p><p>我们希望检测能够达到目标分类的水平。然而，为检测而给图像贴标签比为分类或标记贴标签要昂贵得多（标签通常是给用户免费提供的）。因此，我们不太可能在不久的将来看到与分类数据集相同规模的检测数据集。</p><p>我们提出了一种新的方法来利用我们已经拥有的大量分类数据，并利用它来扩大当前检测系统的范围。我们的方法使用目标分类的分层观点，使我们能够将不同的数据集结合在一起。</p><p>我们还提出了一种联合训练算法，使我们能够在检测和分类数据上训练目标检测器。我们的方法利用标记的检测图像来学习精确定位目标，同时使用分类图像来增加其词汇量和鲁棒性。</p><p>使用这种方法，我们训练了YOLO9000，一个实时的目标检测器，可以检测超过9000个不同的物体类别。首先，我们在基础YOLO检测系统的基础上进行改进，以产生YOLOv2，一个最先进的实时检测器。然后，我们使用我们的数据集组合方法和联合训练算法，在ImageNet的9000多个类别以及COCO的检测数据上训练一个模型。</p><p>我们所有的代码和预训练的模型都可以在线获得：<a href="http://pjreddie.com/yolo9000/。">http://pjreddie.com/yolo9000/。</a></p><blockquote><h4 id="目标检测现状的不足"><a href="#目标检测现状的不足" class="headerlink" title="目标检测现状的不足"></a>目标检测现状的不足</h4><ul><li>当前的目标检测数据集是有限的</li><li>目标检测能检测的对象种类非常有限，可检测的物体少</li></ul><h4 id="本文工作"><a href="#本文工作" class="headerlink" title="本文工作"></a>本文工作</h4><ul><li>1）<strong>使用联合数据集：</strong> 利用已有的分类数据集，来拓展目标检测的范围。利用对象分类的分层视图，使得可以将不同数据集组合到一起</li><li>2）<strong>提出联合训练算法：</strong> 可以在检测数据集和分类数据集上训练目标分类器，用标记的目标检测数据集优化定位精度，利用分类图像来增加其词汇量鲁棒性</li><li>3）<strong>改进YOLOv1提出YOLOv2：</strong> 一种最先进的实时检测器</li><li>4）<strong>提出YOLO9000：</strong> 先将YOLO优化成YOLOv2，再用联合方法训练出YOLO9000</li></ul></blockquote><h2 id="二、-Better—更好"><a href="#二、-Better—更好" class="headerlink" title="二、 Better—更好"></a><strong>二、 Better—更好</strong></h2><h3 id="2-1-Batch-Normalization—批量归一化"><a href="#2-1-Batch-Normalization—批量归一化" class="headerlink" title="2.1 Batch Normalization—批量归一化"></a>2.1 Batch Normalization—批量归一化</h3><p>相比于最先进的检测系统，YOLO存在着各种缺陷。与<a href="https://so.csdn.net/so/search?q=Faster&amp;spm=1001.2101.3001.7020">Faster</a> R-CNN相比，对YOLO的错误分析表明，YOLO出现了大量的定位错误。此外，与基于区域建议的方法相比，YOLO的召回率相对较低。因此，我们主要关注的是在保持分类精度的同时，提高召回率和定位准确度。</p><p>计算机视觉通常趋向于更大、更深的网络[6] [18] [17]。更好的性能往往取决于训练更大的网络或将多个模型集合在一起。然而，在YOLOv2中，我们希望有一个更准确的检测器，但仍然是快速的。我们没有扩大我们的网络，而是简化了网络，然后让表征更容易学习。我们将过去工作中的各种想法与我们自己的新概念结合起来，以提高YOLO的性能。在表2中可以看到结果的总结。</p><p><strong>批量归一化</strong> 批量归一化导致收敛性的显著改善，同时消除了对其他形式的规范化的需求[7]。通过在YOLO的所有卷积层上添加批量归一化，我们在mAP上得到了超过2%的改善。批量规范化也有助于规范化模型。有了批归一化，我们可以在不过拟合的情况下去除模型中的dropout。</p><blockquote><h4 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h4><p>CNN在训练过程中网络每层输入的分布一直在改变, 会使训练过程难度加大，对网络的每一层的输入(每个卷积层后)都做了归一化，<strong>这样网络就不需要每层都去学数据的分布，收敛会更快</strong>。</p><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p>在YOLO模型的所有卷积层上添加<strong>Batch Normalization</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/BatchNormalization.png" alt=""></p><h4 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h4><p>mAP获得了2%的提升。Batch Normalization 也有助于规范化模型，可以在舍弃dropout优化后依然不会过拟合。</p></blockquote><h3 id="2-2-High-Resolution-Classifier—高分辨率分类器"><a href="#2-2-High-Resolution-Classifier—高分辨率分类器" class="headerlink" title="2.2 High Resolution Classifier—高分辨率分类器"></a>2.2 High Resolution Classifier—高分辨率分类器</h3><p>高分辨率分类器所有最先进的检测方法都使用在ImageNet上预训练的分类器[16]。从AlexNet开始，大多数分类器在小于256×256的输入图像上运行[8]。最初的YOLO在224×224的情况下训练分类器网络，并将分辨率提高到448以进行检测训练。这意味着网络在切换到检测学习时还必须调整到新的输入分辨率。</p><p>对于YOLOv2，我们首先在ImageNet上以448×448的完整分辨率对分类网络进行微调，并进行10个epoch。这让网络有时间调整其滤波器，以便在更高的分辨率输入下更好地工作。然后，我们再对检测网络的结果微调。这个高分辨率的分类网络使我们的mAP增加了近4%。</p><blockquote><h4 id="分类器介绍"><a href="#分类器介绍" class="headerlink" title="分类器介绍"></a>分类器介绍</h4><p>检测方法都使用在ImageNet上预先训练的分类器作为预训练模型。从AlexNet开始，大多数分类器的输入都小于256×256。</p><h4 id="v1中的使用"><a href="#v1中的使用" class="headerlink" title="v1中的使用"></a>v1中的使用</h4><p>v1中预训练使用的是分类数据集，大小是224×224 ，然后迁移学习，微调时使用YOLO模型做目标检测的时候才将输入变成448 × 448。这样改变尺寸，网络就要多重新学习一部分，会带来性能损失。</p><h4 id="v2中的改进"><a href="#v2中的改进" class="headerlink" title="v2中的改进"></a>v2中的改进</h4><p>v2直接在预训练中输入的就是<strong>448×448</strong>的尺寸，微调的时候也是<strong>448 × 448</strong>。</p><h4 id="效果-1"><a href="#效果-1" class="headerlink" title="效果"></a>效果</h4><p>使mAP增加了近4%</p></blockquote><h3 id="2-3-Convolutional-With-Anchor-Boxes—带有Anchor-Boxes的卷积"><a href="#2-3-Convolutional-With-Anchor-Boxes—带有Anchor-Boxes的卷积" class="headerlink" title="2.3 Convolutional With Anchor Boxes—带有Anchor Boxes的卷积"></a>2.3 Convolutional With Anchor Boxes—带有Anchor Boxes的卷积</h3><p><strong>带有锚框的卷积</strong> YOLO直接使用卷积特征提取器顶部的全连接层来预测边界框的坐标。Faster R-CNN不直接预测坐标，而是使用手工挑选的先验因素来预测边界框[15]。Faster R-CNN中的区域生成网络（RPN）只使用卷积层来预测锚框的偏移量和置信度。由于预测层是卷积，RPN预测了特征图中每个位置的偏移量。预测偏移量而不是坐标可以简化问题，使网络更容易学习。</p><p>我们从YOLO中移除全连接层，并使用锚框来预测边界框。首先，我们消除了一个池化层，使网络卷积层的输出具有更高的分辨率。我们还缩小了网络，使其在分辨率为416×416的输入图像上运行，而不是448×448。我们这样做是因为我们希望在我们的特征图中有奇数个位置，以便只有一个中心单元。目标，尤其是大型目标，往往会占据图像的中心位置，所以在中心位置有一个单一的位置来预测这些目标是很好的，而不是在中心附近的四个位置。YOLO的卷积层对图像进行了32倍的降样，所以通过使用416的输入图像，我们得到了一个13×13的输出特征图。</p><p>引入锚框后，我们将类别预测机制与空间位置分开处理，单独预测每个锚框的类和目标。和原来的YOLO一样，目标预测仍然预测先验框和真实框的IOU，而类别预测则预测在有目标存在下，该类别的条件概率。</p><p>使用锚框，我们得到的准确率会有小幅下降。YOLO每张图片只预测了98个框，但使用锚框后，我们的模型预测了超过一千个框。在没有锚框的情况下，我们的中间模型mAP为69.5，召回率为81%。有了锚框，我们的模型mAP为69.2，召回率为88%。即使mAP下降了，平均召回率的增加意味着我们的模型有更大的改进空间。</p><blockquote><h4 id="什么是Anchor？"><a href="#什么是Anchor？" class="headerlink" title="什么是Anchor？"></a>什么是Anchor？</h4><p><strong>定义：</strong> Anchor（先验框） 就是一组预设的边框，在训练时，以真实的边框位置相对于预设边框的偏移来构建训练样本。 这就相当于，预设边框先大致在可能的位置“框”出来目标，然后再在这些预设边框的基础上进行调整。<strong>简言之就是在图像上预设好的不同大小，不同长宽比的参照框。</strong></p><p><strong>Anchor Box：</strong>一个Anchor Box可以由边框的纵横比和边框的面积（尺度)来定义，相当于一系列预设边框的生成规则，根据Anchor Box，可以在图像的任意位置，生成一系列的边框。由于Anchor Box 通常是以CNN提取到的Feature Map 的点为中心位置，生成边框，所以一个Anchor Box不需要指定中心位置。</p><h4 id="Anchor-Box的构成"><a href="#Anchor-Box的构成" class="headerlink" title="Anchor Box的构成"></a><strong>Anchor Box的构成</strong></h4><ul><li>使用CNN提取的Feature Map的点，来定位目标的位置。</li><li>使用Anchor Box的Scale来表示目标的大小。</li><li>使用Anchor Box的Aspect Ratio来表示目标的形状。</li></ul><h4 id="之前研究"><a href="#之前研究" class="headerlink" title="之前研究"></a>之前研究</h4><p><strong>YOLOv1:</strong> 使用全连接层来直接预测边界框（x,y,w,h,c）其中边界框的坐标是相对于cell的，宽与高是相对于整张图片。由于各个图片中存在不同尺度和长宽比的物体，YOLOv1在训练过程中学习适应不同物体的形状是比较困难的，这也导致YOLOv1在精确定位方面表现较差。</p><p><strong>Faster R-CNN：</strong> 不是直接预测目标边界框，而是使用手工挑选的先验Anchor Boxes。利用RPN预测的边界框是相对于Anchor Boxes的坐标和高宽的偏移offset。RPN在特征图的每个位置预测Anchor Box偏移量而不是坐标，简化了问题，使网络更容易学习。</p><p><strong>YOLOv2的改进</strong><br>（1）删掉全连接层和最后一个pooling层，使得最后的卷积层可以有更高分辨率的特征</p><p>（2）缩小网络操作的输入图像为416×416</p><blockquote><p><strong>Q：为什么是416×416，而不是448×448？</strong></p><p>YOLOv2模型下采样的总步长为32,对于416×416大小的图片，最终得到的特征图大小为13×13（416/32=13），特征图中有奇数个位置，所以只有一个中心单元格。物体往往占据图像的中心，所以最好在中心有一个单独的位置来预测这些物体，而不是在附近的四个位置。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-%E5%A5%87%E5%81%B6%E6%95%B0.png" alt=""></p></blockquote><p>（3）使用Anchor Boxes</p><h4 id="效果-2"><a href="#效果-2" class="headerlink" title="效果"></a>效果</h4><p>使用Anchor，模型的mAP值从69.5降到了69.2，下降了一丢丢，而召回率却从81%提高到了88%。</p><blockquote><p>Q：精度（precision）和召回率（recall）：</p><p><strong>precision：</strong> 预测框中包含目标的比例。</p><p><strong>recall：</strong> 真正目标被检测出来的比例。</p><p>简言之，recall表示得找全，precision表示得找准。\</p></blockquote><h4 id="v2和v1对比"><a href="#v2和v1对比" class="headerlink" title="v2和v1对比"></a>v2和v1对比</h4><div class="table-container"><table><thead><tr><th></th><th><strong>YOLOv1</strong></th><th><strong>YOLOv2</strong></th></tr></thead><tbody><tr><td><strong>初始设置</strong></td><td>初始生成两个boxes，加大了学习复杂度。</td><td>Anchor初始是固定的，但在训练过程中会进行微调。使用Anchor boxes之后，每个位置的各个Anchor box都单独预测一组分类概率值。</td></tr><tr><td><strong>输出公式</strong></td><td>(框数 * 信息数)+分类数</td><td>框数 *(信息数+分类数)</td></tr><tr><td><strong>公式含义</strong></td><td>在YOLOv1中，类别概率是由grid cell来预测的，每个cell都预测2个boxes，每个boxes包含5个值，每个grid cell 携带的是30个信息。但是每个cell只预测一组分类概率值，供2个boxes共享。</td><td>在YOLOv2中，类别概率是属于box的，每个box对应一个类别概率，而不是由cell决定，因此这边每个box对应25个预测值。每个grid cell携带的是 25 × 5 =125个信息，25是 xywh+置信度+分类数，5就是5个Anchor。</td></tr><tr><td><strong>输出框</strong></td><td>7 × 7 × 2 = 98个框</td><td>13 × 13 × 5 = 845个框</td></tr><tr><td><strong>输出值</strong></td><td>7 ×7 × 30</td><td>13 × 13 × 5 × 25</td></tr></tbody></table></div><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1vsv2.png" alt=""></p></blockquote><h3 id="2-4-Dimension-Clusters—维度聚类（K-means聚类确定Anchor初始值）"><a href="#2-4-Dimension-Clusters—维度聚类（K-means聚类确定Anchor初始值）" class="headerlink" title="2.4 Dimension Clusters—维度聚类（K-means聚类确定Anchor初始值）"></a>2.4 Dimension Clusters—维度聚类（K-means聚类确定Anchor初始值）</h3><p><strong>维度集群</strong> 在与YOLO一起使用锚框时，我们遇到了两个问题。第一个问题是，框的尺寸是手工挑选的。网络可以学习适当地调整框，但是如果我们为网络挑选更好的先验锚框来开始，我们可以使网络更容易学习预测好的检测结果。</p><p>我们在训练集的边界框上运行k-means聚类，以自动找到好的先验参数，而不是手工选择先验参数。如果我们使用标准的k-means和欧氏距离，大的框比小的框产生更多的误差。然而，我们真正想要的是能获得好的IOU分数的先验锚框，这与框的大小无关。因此，对于距离度量，我们使用：d( box , centroid )=1−IOU( box , centroid )。</p><p>我们对不同的k值运行k-means，并绘制出最接近中心点的平均IOU，见图2。我们选择k = 5作为模型复杂性和高召回率之间的良好权衡。聚类中心点与手工挑选的锚框有明显不同。短而宽的框较少，高而薄的框较多。</p><p>我们在表1中比较了我们的聚类策略和手工挑选的锚框的平均IOU与最接近的先验。在只有5个先验的情况下，中心点的表现与9个锚框相似，平均IOU分别为61.0，60.9。如果我们使用9个中心点，我们会看到一个高得多的平均IOU。这表明，使用k-means来生成我们的边界框，使模型开始有一个更好的表示，并使任务更容易学习。</p><blockquote><h4 id="使用Anchor的问题一"><a href="#使用Anchor的问题一" class="headerlink" title="使用Anchor的问题一"></a>使用Anchor的问题一</h4><p>Anchor Boxes的尺寸是手工指定了长宽比和尺寸，相当于一个超参数，这违背了YOLO对于目标检测模型的初衷，<strong>因为如果指定了Anchor的大小就没办法适应各种各样的物体了</strong>。</p><h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><p>在训练集的边界框上运行K-means聚类训练bounding boxes，可以自动找到更好的boxes宽高维度。由上面分析已知，设置先验Anchor Boxes的主要目的是为了使得预测框与真值的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标</p><blockquote><p>K-means算法步骤：</p><p>1.选择初始化的K个样本作为初始聚类中心</p><p>2.针对数据集中每个样本，计算它到K个聚类中心的距离，并将其分到距离最小的聚类中心所对应的类中</p><p>3.针对每个类别，重新计算它的聚类中心</p><p>4.重复上面的步骤2、3，直到达到某个终止条件（迭代次数、最小误差变化）</p></blockquote><p><strong>公式：</strong> d(box, centroid) = 1 − IOU(box, centroid) （box:其他框， centroid：聚类中心框）</p><p>如下图，选取不同的k值（聚类的个数）运行K-means算法，并画出平均IOU和K值的曲线图。当k = 5时，可以很好的权衡模型复杂性和高召回率。与手工挑选的相比，K-means算法挑选的检测框形状多为瘦高型。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/K-means.png" alt=""></p><blockquote><p><strong>Q：为什么不尽量选择大的k值？</strong></p><p>因为K越大就生成越多的Anchor，越多的框自然准确率就能上去了，但同时也成倍的增加了模型的复杂度。R-CNN就是因为提取2K个候选框拉跨的。</p></blockquote></blockquote><h3 id="2-5-Direct-location-prediction—直接的位置预测"><a href="#2-5-Direct-location-prediction—直接的位置预测" class="headerlink" title="2.5 Direct location prediction—直接的位置预测"></a>2.5 Direct location prediction—直接的位置预测</h3><p><strong>直接的位置预测</strong>。 当YOLO使用锚框时，我们遇到了第二个问题：模型的不稳定性，特别是在早期迭代中。大部分的不稳定性来自于对框的（x，y）位置的预测。在区域生成网络中，网络预测值tx和ty，（x，y）中心坐标的计算方法是：</p><script type="math/tex; mode=display">x = (t_x \ast w_a) - x_a\\y = (t_y \ast h_a) - y_a</script><p>例如，tx=1的预测会将框向右移动，移动的宽度为锚框的宽度，tx=-1的预测会将框向左移动相同的长度。</p><p>这个公式是不受限制的，所以任何锚框都可以在图像中的任何一点结束，而不管这个框是在哪个位置预测的。在随机初始化的情况下，模型需要很长时间才能稳定地预测出合理的偏移量。</p><p>我们不预测偏移量，而是遵循YOLO的方法，预测相对于网格单元位置的坐标。这使得真实值的界限在0到1之间。我们使用逻辑激活来约束网络的预测，使其落在0~1这个范围内。</p><p>网络在输出特征图中的每个单元预测了5个边界框。该网络为每个边界框预测了5个坐标，即tx、ty、tw、th和to。如果单元格与图像左上角的偏移量为（cx，cy），且先验框的宽度和高度为pw，ph，则预测值对应于：<br>由于我们限制了位置预测，参数化更容易学习，使网络更稳定。使用维度聚类以及直接预测边界框中心位置，比起使用锚框的版本，YOLO提高了近5%。 </p><blockquote><h4 id="使用Anchor的问题二"><a href="#使用Anchor的问题二" class="headerlink" title="使用Anchor的问题二"></a>使用Anchor的问题二</h4><p>模型不稳定，特别是在早期迭代期间。大多数不稳定性来自于对边框(x, y)位置的预测。</p><h4 id="RPN网络的位置预测"><a href="#RPN网络的位置预测" class="headerlink" title="RPN网络的位置预测"></a>RPN网络的位置预测</h4><p><strong>方法：</strong> 预测相对于Anchor Box的坐标的偏移，和相对于Anchor Box高宽的偏移。</p><p><strong>计算公式：</strong> 预测框中心坐标= 输出的偏移量×Anchor宽高+Anchor中心坐标</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-rpn.png" alt=""></p><p><strong>不足：</strong> 这个公式是不受约束的，因此任何锚框可以出现在图像中的任何位置。在随机初始化的情况下，模型需要很长时间才能稳定到预测合理的偏移量。</p><h4 id="YOLOv2的改进"><a href="#YOLOv2的改进" class="headerlink" title="YOLOv2的改进"></a><strong>YOLOv2的改进</strong></h4><p><strong>方法：</strong> <strong>预测边界框中心点相对于对应cell左上角位置的相对偏移值。</strong>将网格归一化为1×1，坐标控制在每个网格内，同时配合sigmod函数将预测值转换到0~1之间的办法，做到每一个Anchor只负责检测周围正负一个单位以内的目标box。</p><p><strong>计算公式：</strong> 一个网格相对于图片左上角的偏移量是cx，cy。先验框的宽度和高度分别是pw和ph，则预测的边界框相对于特征图的中心坐标(bx，by)和宽高bw、bh</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-impro.png" alt=""></p><h4 id="效果-3"><a href="#效果-3" class="headerlink" title="效果"></a>效果</h4><p>使模型更容易稳定训练，mAP值提升了约5%。</p></blockquote><h3 id="2-6-Fine-Grained-Features—细粒度的特征"><a href="#2-6-Fine-Grained-Features—细粒度的特征" class="headerlink" title="2.6 Fine-Grained Features—细粒度的特征"></a>2.6 Fine-Grained Features—细粒度的特征</h3><p>细粒度的特征这个修改后的YOLO在13×13的特征图上预测探测结果。虽然这对大型物体来说是足够的，但它可能会受益于更细粒度的特征来定位较小的物体。Faster R-CNN和SSD都在网络中的各种特征图上运行他们的网络，以获得多个分辨率。我们采取了一种不同的方法，只需要增加一个直通层，从早期的层中提取26×26分辨率的特征。</p><p>直通层通过将相邻的特征堆叠到不同的通道而不是空间位置上，将高分辨率的特征与低分辨率的特征串联起来，类似于ResNet中的恒等映射。这种细粒度的特征。这就把26×26×512的特征图变成了13×13×2048的特征图，它可以与原始特征连接起来。我们的检测器在这个扩展的特征图之上运行，这样它就可以访问细粒度的特征。这使性能有了1%的适度提高。</p><blockquote><h4 id="为什么使用细粒特征？"><a href="#为什么使用细粒特征？" class="headerlink" title="为什么使用细粒特征？"></a>为什么使用细粒特征？</h4><p>这个修改后的YOLO在13 × 13特征图上进行检测。虽然这对于大型对象来说已经足够了，但是对于较小的对象来说，更细粒度的特性可能会使得检测效果更好。</p><h4 id="使用细粒度特征"><a href="#使用细粒度特征" class="headerlink" title="使用细粒度特征"></a>使用细粒度特征</h4><p><strong>Faster R-CNN和SSD：</strong> 使用了多尺度的特征图来分别检测不同大小的物体，前面更精细的特征图可以用来预测小物体。</p><p><strong>YOLOv2：</strong> 不同的方法，为网络简单地添加一个直通层（ passthrough layer），获取前层26×26分辨率特征。</p><h4 id="直通层（-passthrough-layer）"><a href="#直通层（-passthrough-layer）" class="headerlink" title="直通层（ passthrough layer）"></a>直通层（ passthrough layer）</h4><ul><li>将相邻的特征叠加到不同的通道来，将高分辨率的特征与低分辨率的特征连接起来</li><li>将前层26×26×512的特征图转换为13×13×2048的特征图，并与原最后层特征图进行拼接。</li></ul><p><strong>具体计算过程：</strong> YOLO v2提取Darknet-19最后一个maxpooling层的输入，得到26×26×512的特征图。经过1×1×64的卷积以降低特征图的维度，得到26×26×64的特征图，然后经过pass through层的处理变成13x13x256的特征图（抽取原特征图每个2x2的局部区域组成新的channel，即原特征图大小降低4倍，channel增加4倍），再与13×13×1024大小的特征图连接，变成13×13×1280的特征图，最后在这些特征图上做预测。<br><strong>具体操作：</strong></p><p><img src="C:/Users/17145/AppData/Roaming/Typora/typora-user-images/image-20240202225417118.png" alt="image-20240202225417118"></p><p>一个feature map，也就是在最后的池化之前，分成两路：一路是做拆分，分成四块，四块拼成一个长条，另一个是做正常的池化卷积操作，最后两个长条叠加输出。</p><blockquote><p><strong>Q：如何拆分成四块的？</strong></p><p>并不是简单的“两刀切4块”，而是在每个2×2的小区域上都选择左上角块，具体看下图。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-%E6%8B%86%E5%88%86%E5%9B%9B%E4%B8%AA%E5%9D%97.png" alt=""></p></blockquote><p><strong>注意：</strong> 这里的叠加不是ResNet里的add，而是拼接，是DenseNet里的concat。</p><h4 id="效果-4"><a href="#效果-4" class="headerlink" title="效果"></a>效果</h4><p>提升了1%的mAP</p></blockquote><h3 id="2-7-Multi-Scale-Training—多尺度的训练"><a href="#2-7-Multi-Scale-Training—多尺度的训练" class="headerlink" title="2.7 Multi-Scale Training—多尺度的训练"></a>2.7 Multi-Scale Training—多尺度的训练</h3><p><strong>多尺度的训练</strong>原始的YOLO使用448×448的输入分辨率。通过添加锚框，我们将分辨率改为416×416。然而，由于我们的模型只使用卷积层和池化层，因此可以实时调整大小。我们希望YOLOv2能够鲁棒地运行在不同尺寸的图像上，所以我们将多尺度训练应用到模型中。</p><p>我们不需要修改输入图像的大小，而是每隔几个迭代就改变网络。每10个批次，我们的网络就会随机选择一个新的图像尺寸。由于我们的模型缩减了32倍，我们从以下32的倍数中抽取：{320, 352, …, 608}。因此，最小的选项是320 × 320，最大的是608 × 608。我们将调整网络的尺寸，然后继续训练。</p><p>这种制度迫使网络学会在各种输入维度上进行良好的预测。这意味着同一个网络可以预测不同分辨率下的检测结果。网络在较小的尺寸下运行得更快，因此YOLOv2在速度和准确性之间提供了一个简单的权衡。</p><p>在低分辨率下，YOLOv2作为一个廉价、相当准确的检测器运行。在288×288时，它以超过90 FPS的速度运行，其mAP几乎与Faster R-CNN一样好。这使它成为较小的GPU、高帧率视频或多个视频流的理想选择。</p><p>在高分辨率下，YOLOv2是一个最先进的检测器，在VOC 2007上的mAP为78.6，而运行速度仍高于实时速度。</p><blockquote><h4 id="YOLOv1"><a href="#YOLOv1" class="headerlink" title="YOLOv1"></a>YOLOv1</h4><p><strong>方法：</strong> 使用448×448的固定分辨率输入。</p><h4 id="YOLOv2的改进-1"><a href="#YOLOv2的改进-1" class="headerlink" title="YOLOv2的改进"></a>YOLOv2的改进</h4><p><strong>原理：</strong> YOLOv2模型只使用了卷积和池化层，所以可以动态调整输入大小。每隔几次迭代就改变网络，而不是固定输入图像的大小。</p><p><strong>做法：</strong> 网络每10批训练后随机选择一个新的图像尺寸大小。由于模型下采样了32倍，从以下32的倍数{320,352，…，608}作为图像维度的选择。将网络输入调整到那个维度，并继续训练。</p><p><strong>作用：</strong> 这种机制使得网络可以更好地预测不同尺寸的图片，意味着同一个网络可以进行不同分辨率的检测任务，在输入size较大时，训练速度较慢，在输入size较小时，训练速度较快，而multi-scale training又可以提高准确率，因此算是准确率和速度都取得一个不错的平衡。</p><p><strong>YOLOv2和其他网络成绩对比：</strong><br>在小尺寸图片检测中，YOLOv2成绩很好，输入为228 × 228的时候，帧率达到90FPS，mAP几乎和Faster R-CNN的水准相同。使得其在低性能GPU、高帧率视频、多路视频场景中更加适用。在大尺寸图片检测中，YOLOv2达到了先进水平，VOC2007 上mAP为78.6%，仍然高于平均水准。<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-%E5%A4%9A%E5%B0%BA%E5%BA%A6%E8%AE%AD%E7%BB%83.png" alt=""></p></blockquote><h3 id="2-8-Further-Experiments—进一步的实验"><a href="#2-8-Further-Experiments—进一步的实验" class="headerlink" title="2.8 Further Experiments—进一步的实验"></a>2.8 Further Experiments—进一步的实验</h3><p><strong>进一步的实验</strong>我们训练YOLOv2对VOC 2012进行检测。表4显示了YOLOv2与其他最先进的检测系统的性能比较。YOLOv2实现了73.4 mAP，同时运行速度远远超过比较的方法。我们还对COCO进行了训练，并在表5中与其他方法进行了比较。在VOC指标（IOU = 0.5）上，YOLOv2得到44.0 mAP，与SSD和Faster R-CNN相当。</p><blockquote><p>作者在VOC2012上对YOLOv2进行训练，下图是和其他方法的对比。YOLOv2精度达到了73.4%，并且速度更快。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-%E8%BF%9B%E4%B8%80%E6%AD%A5%E8%AE%AD%E7%BB%83.png" alt=""></p><p>同时YOLOV2也在COCO上做了测试（IOU=0.5），也和Faster R-CNN、SSD作了成绩对比。总的来说，比上不足，比下有余。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-coco.png" alt=""></p></blockquote><h2 id="三、Faster—更快"><a href="#三、Faster—更快" class="headerlink" title="三、Faster—更快"></a>三、Faster—更快</h2><p>我们希望检测是准确的，但我们也希望它是快速的。大多数检测的应用，如机器人或自动驾驶汽车，都依赖于低延迟的预测。为了最大限度地提高性能，我们在设计YOLOv2时从头到尾都是快速的。</p><p>大多数检测框架依靠VGG-16作为基础特征提取器[17]。VGG-16是一个强大的、准确的分类网络，但它是不必要的复杂。VGG-16的卷积层需要306.9亿次浮点运算来处理一张224×224分辨率的图像。</p><p>YOLO框架使用一个基于Googlenet架构的定制网络[19]。这个网络比VGG-16更快，一个前向通道只用了85.2亿次运算。然而，它的准确性比VGG16略差。对于224×224的单张图像，前5名的准确率，YOLO在ImageNet上的自定义模型精度为88.0%，而VGG-16为90.0%。</p><blockquote><p>这一段开头先批评一波VGG，说VGG慢的不行，所以YOLOv1用的GoogLeNet，也就是Inceptionv1。速度很快，但是对比VGG精度稍微有所下降</p><p>通常目标检测框架： 大多数检测框架依赖于VGG-16作为基本的特征提取器。VGG-16是一个强大、精确的分类网络，但是它计算复杂。</p><p>YOLO框架： 使用基于GoogLeNet架构的自定义网络。虽说整体mAP 表现较VGG-16 差一些，但是却换来更快速、更少的预测运算。</p><p>YOLOv2 框架： 使用的是一个全新的架构: Darknet-19</p></blockquote><h3 id="3-1-Darknet-19"><a href="#3-1-Darknet-19" class="headerlink" title="3.1 Darknet-19"></a>3.1 Darknet-19</h3><p><strong>Darknet-19</strong>我们提出一个新的分类模型，作为YOLOv2的基础。我们的模型建立在先前的网络设计工作以及该领域的常识之上。与VGG模型类似，我们主要使用3×3的过滤器，并在每个池化步骤后将通道的数量增加一倍[17]。按照网络中的网络（NIN）的工作，我们使用全局平均池来进行预测，以及使用1×1滤波器来压缩3×3卷积之间的特征表示[9]。我们使用批量归一化来稳定训练，加速收敛，并使模型正规化[7]。</p><p>我们的最终模型，称为Darknet-19，有19个卷积层和5个maxpooling层。完整的描述见表6。Darknet-19只需要55.8亿次操作来处理一幅图像，却在ImageNet上达到了72.9%的最高准确率和91.2%的top-5准确率。</p><blockquote><h4 id="Darknet-19介绍"><a href="#Darknet-19介绍" class="headerlink" title="Darknet-19介绍"></a><strong>Darknet-19介绍</strong></h4><p>一个新的分类模型作为YOLOv2的基础框架。与VGG模型类似，主要使用3×3的卷积，并在每个池化步骤后加倍通道数。使用全局平均池进行预测，并使用1×1卷积压缩特征图通道数以降低模型计算量和参数，每个卷积层后使用BN层以加快模型收敛同时防止过拟合。最后用average pooling层代替全连接层进行预测。</p><p><strong>Darknet-19细节：</strong> 有19个卷积层和5个maxpooling层。（v1的GooLeNet是4个卷积层和2个全连接层）</p><p><strong>结构如下：</strong>（这是分类的模型，不是目标检测的模型）<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-dark19.png" alt=""></p><p>采用 YOLOv2，模型的mAP值没有显著提升，但计算量减少了。</p><blockquote><p><strong>Q：为什么去掉全连接层了呢？</strong></p><p>因为全连接层容易过拟合，训练慢。（参数太多）如下图，YOLOv1中通过全连接层将7×7×1024的特征图变换为7×7×30的特征图。但是这种变换完全可以通过一个3×3的卷积核做到，从而节省参数。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-dark.png" alt=""></p></blockquote></blockquote><h3 id="3-2-Training-for-classification—分类的训练"><a href="#3-2-Training-for-classification—分类的训练" class="headerlink" title="3.2 Training for classification—分类的训练"></a>3.2 Training for classification—分类的训练</h3><p><strong>分类的训练</strong> 我们使用随机梯度下降法在标准的ImageNet 1000类分类数据集上训练网络160次，使用Darknet神经网络框架[13]，起始学习率为0.1，多项式速率衰减为4次方，权重衰减为0.0005，动量为0.9。在训练过程中，我们使用标准的数据增强技巧，包括随机作物、旋转、色调、饱和度和曝光度的转变。</p><p>如上所述，在对224×224的图像进行初始训练后，我们在更大的尺寸（448）上对我们的网络进行微调。在这种微调中，我们用上述参数进行训练，但只用了10个epoch，并以10-3的学习率开始。在这个更高的分辨率下，我们的网络达到了76.5%的最高准确率和93.3%的Top-5准确率。</p><blockquote><p><strong>参数设置</strong><br><strong>（1）训练数据集：</strong> 标准ImageNet 1000类分类数据集</p><p><strong>（2）训练参数：</strong> 对网络进行160个epochs的训练，使用初始学习率为0.1随机梯度下降法、4的多项式率衰减法、0.0005的权值衰减法和0.9的动量衰减法</p><p><strong>（3）模型：</strong> 使用的是Darknet神经网络框架。</p><p><strong>（4）数据增强：</strong> 在训练中使用标准的数据增强技巧，包括随机的裁剪、旋转、色相、饱和度和曝光变化。</p><p>如上所述，在最初的224×224图像训练之后，然后放到448 × 448上微调，但只训练约10个周期。在这个高分辨率下，网络达到很高精度。微调时，10epoch，初始lr0.001。</p><p><strong>结果：</strong> 高分辨率下训练的分类网络在top-1准确率76.5%，top-5准确率93.3%。</p></blockquote><h3 id="3-3-Training-for-detection—检测的训练"><a href="#3-3-Training-for-detection—检测的训练" class="headerlink" title="3.3 Training for detection—检测的训练"></a>3.3 Training for detection—检测的训练</h3><p><strong>检测的训练</strong> 我们对这个网络进行了修改，去掉了最后一个卷积层，而是增加了三个3×3的卷积层，每个卷积层有1024个过滤器，然后是最后一个1×1的卷积层，输出的数量是我们检测所需的。对于VOC，我们预测5个框的5个坐标，每个框有20个类别，所以有125个过滤器。我们还从最后的3×3×512层向第二个卷积层添加了一个直通层，以便我们的模型可以使用细粒度的特征。</p><p>我们用10-3的起始学习率训练网络160个epoch，在60和90个epoch时除以10。我们使用0.0005的权重衰减和0.9的动量。我们使用与YOLO和SSD类似的数据增强，包括随机裁剪、颜色转换等。我们在COCO和VOC上使用同样的训练策略。</p><blockquote><h4 id="网络微调"><a href="#网络微调" class="headerlink" title="网络微调"></a><strong>网络微调</strong></h4><ul><li>移除最后一个卷积层、global avgpooling层和softmax</li><li>增加3个3x3x1024的卷积层</li><li>增加passthrough层</li><li>增加一个1×1个卷积层作为网络输出层。输出的channel数为num<em> anchors×(5+num</em> calsses)（num_anchors在文中为5，num _classes=20是类别个数，5是坐标值和置信度）</li></ul><h4 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h4><p><strong>（1）网络最后一层即1X1卷积层卷积核个数同网络输出维度相同：</strong> 对于VOC，预测5个边界框，每个边界框有5个坐标，每个边界框有20个类，所以最后一个1×1卷积层有125个卷积核。</p><p><strong>（2）passthrough层：</strong> 倒数第二个3X3卷积到最后一个3X3卷积层增加passthrough层。模型可以使用细粒度的特征。</p><p><strong>（3）训练参数：</strong> 10−3的起始学习率对网络进行160个周期的训练，并在60和90个周期时将其除以10。使用重量衰减为0.0005，动量为0.9。</p><h4 id="YOLOv2的训练"><a href="#YOLOv2的训练" class="headerlink" title="YOLOv2的训练"></a>YOLOv2的训练</h4><p>（1）在ImageNet训练Draknet-19，模型输入为224×224，共160个epochs</p><p>（2）将网络的输入调整为448×448,继续在ImageNet数据集上finetune分类模型，训练10 个epochs。参数除了epoch和learning rate改变外，其他都没变，这里learning rate改为0.001。</p><p>（3）修改Darknet-16分类模型为检测模型（看上面的网络微调部分），并在监测数据集上继续finetune模型<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-train.png" alt=""></p></blockquote><h2 id="四、Stronger—更强（YOLO9000部分）"><a href="#四、Stronger—更强（YOLO9000部分）" class="headerlink" title="四、Stronger—更强（YOLO9000部分）"></a>四、Stronger—更强（YOLO9000部分）</h2><p>我们提出了一种对分类和检测数据进行联合训练的机制。我们的方法使用标记为检测的图像来学习特定的检测信息，如边界框坐标预测和目标类，以及如何对普通目标进行分类。它使用只有类别标签的图像来扩大它可以检测的类别的数量。</p><p>在训练过程中，我们混合了来自检测和分类数据集的图像。当我们的网络看到被标记为检测的图像时，我们可以根据完整的YOLOv2损失函数进行反向传播。当它看到一个分类图像时，我们只从架构的分类特定部分反向传播损失。</p><p>这种方法带来了一些挑战。检测数据集只有常见的物体和一般的标签，如 “狗 “或 “船”。分类数据集有更广泛和更深入的标签范围。ImageNet有一百多个狗的品种，包括 “诺福克梗”、”约克夏梗 “和 “贝灵顿梗”。如果我们想在这两个数据集上进行训练，我们需要一个连贯的方法来合并这些标签。</p><p>大多数分类方法在所有可能的类别中使用softmax层来计算最终的概率分布。使用softmax时，假定这些类别是相互排斥的。这给合并数据集带来了问题，例如，你不会想用这个模型来合并ImageNet和COCO，因为 “诺福克梗 “和 “狗 “这两个类别并不相互排斥。</p><p>我们可以使用一个多标签模型来结合数据集，而这个模型并不假定相互排斥。这种方法忽略了我们所知道的关于数据的所有结构，例如，所有的COCO类都是互斥的。</p><blockquote><h3 id="YOLOv2和YOLO9000的关系"><a href="#YOLOv2和YOLO9000的关系" class="headerlink" title="YOLOv2和YOLO9000的关系"></a>YOLOv2和YOLO9000的关系</h3><p>YOLOv2和YOLO9000算法在2017年CVPR上被提出，重点解决YOLOv1召回率和定位精度方面的误差。</p><p><strong>YOLOv2：</strong> 是在YOLOv1的基础上改进得到，改进之处主要有：Batch Normalization (批量归一化)、High Resolution Classfier(高分辨率的分类器)、Convolutional With Anchor Boxes (带锚框的卷积)、Dimension Clusters (维度聚类)、Direct location prediction (直接位置预测)、Fine-Grained Feature (细粒度特性)、Multi-Scale Training (多尺度训练)，它的特点是“更好，更快，更强”。</p><p><strong>YOLO9000：</strong> 的主要检测网络也是YOLO v2，同时使用WordTree来混合来自不同的资源的训练数据，并使用联合优化技术同时在ImageNet和COCO数据集上进行训练，目的是利用数量较大的分类数据集来帮助训练检测模型，因此，YOLO9000的网络结构允许实时地检测超过9000种物体分类，进一步缩小了检测数据集与分类数据集之间的大小代沟。</p><h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p>联合coco目标检测数据集和imagenet分类数据集。</p><ul><li>输入的若为目标检测标签的，则在模型中反向传播目标检测的损失函数。</li><li>输入的若为分类标签的，则反向传播分类的损失函数</li></ul><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>coco的数据集标签分类的比较粗，比如狗，猫，而imagenet分类则比较细化，比如二哈狗，金毛狗。</li><li>这时候如果用softmax进行最后的分类，则会产生问题，因为softmax输出最大概率的那个分类，各种分类之间彼此互斥，若狗，二哈狗，金毛狗在一起的话就会出问题。</li><li>所以要联合训练，必须让标签有一定程度上的一致性。</li></ul></blockquote><h3 id="4-1-Hierarchical-classification—分层分类"><a href="#4-1-Hierarchical-classification—分层分类" class="headerlink" title="4.1 Hierarchical classification—分层分类"></a>4.1 Hierarchical classification—分层分类</h3><p><strong>分层分类</strong> ImageNet的标签是从WordNet中提取的，WordNet是一个语言数据库，用于构造概念和它们之间的关系[12]。在WordNet中，”Norfolk terrier “和 “Yorkshire terrier “都是 “terrier “的外来语，而 “terrier “是 “猎狗 “的一种，是 “狗 “的一种，是 “犬类 “的一种等等。大多数分类方法都假定标签有一个平面结构，然而对于结合数据集来说，结构正是我们所需要的。</p><p>WordNet的结构是一个有向图，而不是一棵树，因为语言是复杂的。例如，”狗 “既是 “犬类 “的一种类型，也是 “家畜 “的一种类型，它们都是WordNet中的主题词。我们没有使用完整的图结构，而是通过从ImageNet中的概念建立一棵分层的树来简化这个问题。</p><p>为了建立这棵树，我们检查了ImageNet中的视觉名词，并查看了它们通过WordNet图到根节点的路径，在这个例子中是 “物理对象”。许多同义词在图中只有一条路径，因此我们首先将所有这些路径添加到我们的树上。然后，我们反复检查我们剩下的概念，并添加路径，使树的增长尽可能少。因此，如果一个概念有两条通往根的路径，其中一条路径会给我们的树增加三条边，而另一条只增加一条边，我们就选择较短的路径。</p><p>最后的结果是WordTree，一个视觉概念的分层模型。为了用WordTree进行分类，我们在每个节点上预测条件概率，即在给定的同义词中，每个同义词的概率。如果我们想计算一个特定节点的绝对概率，我们只需沿着树的路径到根节点，然后乘以条件概率。</p><p>为了分类的目的，我们假设该图像包含一个物体。Pr(物理对象) = 1。</p><p>为了验证这种方法，我们在使用1000类ImageNet建立的WordTree上训练Darknet-19模型。为了建立WordTree1k，我们加入了所有的中间节点，将标签空间从1000扩大到1369。在训练过程中，我们在树上传播基础事实标签，这样，如果一张图片被标记为 “诺福克梗”，它也会被标记为 “狗 “和 “哺乳动物”，等等。为了计算条件概率，我们的模型预测了一个由1369个值组成的向量，我们计算了所有作为同一概念的假名的系统集的softmax，见图5。<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E5%88%86%E5%B1%82%E5%88%86%E7%B1%BB.png" alt=""></p><p>使用与之前相同的训练参数，我们的分层式Darknet-19达到了71.9%的top-1准确率和90.4%的top-5准确率。尽管增加了369个额外的概念，并让我们的网络预测树状结构，但我们的准确率只下降了一点。以这种方式进行分类也有一些好处。在新的或未知的对象类别上，性能会优雅地下降。例如，如果网络看到一张狗的照片，但不确定它是什么类型的狗，它仍然会以高置信度预测 “狗”，但在假名中分布的置信度会降低。</p><p>这种表述也适用于检测。现在，我们不是假设每张图片都有一个物体，而是使用YOLOv2的物体性预测器来给我们提供Pr（物理物体）的值。检测器会预测出一个边界框和概率树。我们向下遍历这棵树，在每一个分叉处采取最高的置信度路径，直到我们达到某个阈值，我们就可以预测那个物体类别。</p><blockquote><ul><li>ImageNet的标签是从WordNet中提取的，WordNet是一个语言数据库，用于构造概念和它们之间的关系。</li><li>WordNet的结构是一个有向图，而不是一棵树，因为语言是复杂的。</li><li><p>作者们并不采用整个WordNet 的图结构，而是从中抽取其视觉名词重新制作一个树状结构。</p></li><li><p>在WordTree结构上进行操作，需要预测的是每一个节点相对于父节点的条件概率，要计算某个几点的绝对概率 或者说联合概率，就直接从他乘到根节点。</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-%E6%9C%89%E5%90%91%E5%9B%BE.png" alt=""></p></blockquote><h3 id="4-2-Dataset-combination-with-WordTree—用WordTree组合数据集"><a href="#4-2-Dataset-combination-with-WordTree—用WordTree组合数据集" class="headerlink" title="4.2 Dataset combination with WordTree—用WordTree组合数据集"></a>4.2 Dataset combination with WordTree—用WordTree组合数据集</h3><p><strong>用WordTree组合数据集。</strong>我们可以使用WordTree以合理的方式将多个数据集组合在一起。我们只需将数据集中的类别映射到树上的同位素。图6显示了一个使用WordTree来结合ImageNet和COCO的标签的例子。WordNet是非常多样化的，所以我们可以将这种技术用于大多数数据集。</p><blockquote><p>原始正常的数据集中数据结构是WordNet(有向图)。作者改造成了WordTree(树)。</p><p><strong>WordTree的生成方式如下：</strong></p><ul><li>遍历Imagenet的label，然后在WordNet中寻找该label到根节点(指向一个物理对象)的路径；</li><li>如果路径只有一条，那么就将该路径直接加入到分层树结构中；</li><li>否则，从剩余的路径中选择一条最短路径，加入到分层树。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-wordtree.png" alt=""></p><p>混合后的数据集形成一个有9418类的WordTree。生成的WordTree模型如下图所示。另外考虑到COCO数据集相对于ImageNet数据集数据量太少了，为了平衡两个数据集，作者进一步对COCO数据集过采样，使COCO数据集与ImageNet数据集的数据量比例接近1：4。</p></blockquote><h3 id="4-3-Joint-classification-and-detection—联合分类和检测"><a href="#4-3-Joint-classification-and-detection—联合分类和检测" class="headerlink" title="4.3 Joint classification and detection—联合分类和检测"></a>4.3 Joint classification and detection—联合分类和检测</h3><p><strong>联合分类和检测</strong> 现在我们可以使用WordTree结合数据集，我们可以训练分类和检测的联合模型。我们想训练一个极大规模的检测器，所以我们使用COCO检测数据集和ImageNet完整版本中的前9000个类来创建我们的联合数据集。我们还需要评估我们的方法，所以我们加入了ImageNet检测挑战中尚未包括的任何类别。这个数据集的相应WordTree有9418个类。ImageNet是一个更大的数据集，所以我们通过对COCO的过度采样来平衡数据集，使ImageNet只比它大4:1。</p><p>使用这个数据集，我们训练YOLO9000。我们使用基本的YOLOv2架构，但只有3个先验因素，而不是5个，以限制输出大小。当我们的网络看到一个检测图像时，我们像平常一样反向传播损失。对于分类损失，我们只在标签的相应级别或以上反向传播损失。例如，如果标签是 “狗”，我们不给树上更远的预测分配任何错误，”德国牧羊犬 “与 “金毛猎犬”，因为我们没有这些信息。</p><p>当它看到一个分类图像时，我们只反向传播分类损失。要做到这一点，我们只需找到预测该类的最高概率的边界框，并计算其预测树上的损失。我们还假设预测框与地面真实标签至少有0.3 IOU的重叠，我们根据这一假设反向传播对象性损失。</p><p>通过这种联合训练，YOLO9000学会了使用COCO中的检测数据来寻找图像中的物体，并学会了使用ImageNet中的数据对这些物体进行分类。</p><p>我们在ImageNet检测任务上评估了YOLO9000。ImageNet的检测任务与COCO共享44个对象类别，这意味着YOLO9000只看到了大多数测试图像的分类数据，而不是检测数据。YOLO9000总体上得到了19.7的mAP，在它从未见过任何标记的检测数据的156个不相干的对象类别上得到了16.0的mAP。这个mAP比DPM取得的结果要高，但是YOLO9000是在不同的数据集上训练的，只有部分监督[4]。它还同时检测了9000个其他物体类别，而且都是实时的。</p><p>当我们分析YOLO9000在ImageNet上的表现时，我们看到它能很好地学习新的动物物种，但在学习服装和设备等类别时却很困难。新的动物更容易学习，因为对象性预测可以很好地从COCO中的动物中概括出来。相反，COCO没有任何类型的衣服的边界框标签，只有人的标签，所以YOLO9000在为 “太阳镜 “或 “游泳裤 “等类别建模时很吃力。</p><blockquote><h3 id="YOLO9000是怎样进行联合训练的？"><a href="#YOLO9000是怎样进行联合训练的？" class="headerlink" title="YOLO9000是怎样进行联合训练的？"></a>YOLO9000是怎样进行联合训练的？</h3><p>YOLO9000采用 YOLO v2的结构，<strong>Anchorbox由原来的5调整到3</strong>，对每个Anchorbox预测其对应的边界框的位置信息x , y , w , h和置信度以及所包含的物体分别属于9418类的概率，所以每个Anchorbox需要预测4+1+9418=9423个值。每个网格需要预测3×9423=28269个值。在训练的过程中，当网络遇到来自检测数据集的图片时，用完整的 YOLO v2 loss进行反向传播计算，当网络遇到来自分类数据集的图片时，只用分类部分的loss进行反向传播。</p><h3 id="YOLO-9000是怎么预测的？"><a href="#YOLO-9000是怎么预测的？" class="headerlink" title="YOLO 9000是怎么预测的？"></a>YOLO 9000是怎么预测的？</h3><p>WordTree中每个节点的子节点都属于同一个子类，分层次的对每个子类中的节点进行一次softmax处理，以得到同义词集合中的每个词的下义词的概率。当需要预测属于某个类别的概率时，需要预测该类别节点的条件概率。即在WordTree上找到该类别名词到根节点的路径，计算路径上每个节点的概率之积。<strong>预测时， YOLO v2得到置信度，同时会给出边界框位置以及一个树状概率图，沿着根节点向下，沿着置信度最高的分支向下，直到达到某个阈值，最后到达的节点类别即为预测物体的类别。</strong></p></blockquote><h3 id="五、Conclusion—结论"><a href="#五、Conclusion—结论" class="headerlink" title="五、Conclusion—结论"></a><strong>五、Conclusion—结论</strong></h3><p>我们介绍了YOLOv2和YOLO9000，实时检测系统。YOLOv2是最先进的，在各种检测数据集上比其他检测系统快。此外，它可以在各种图像尺寸下运行，在速度和准确性之间提供平稳的权衡。</p><p>YOLO9000是一个实时框架，通过联合优化检测和分类来检测9000多个物体类别。我们使用WordTree来结合各种来源的数据和我们的联合优化技术，在ImageNet和COCO上同时训练。YOLO9000是朝着缩小检测和分类之间的数据集大小差距迈出的有力一步。</p><p>我们的许多技术可以在目标检测之外进行推广。我们对ImageNet的WordTree表示为图像分类提供了一个更丰富、更详细的输出空间。使用分层分类的数据集组合在分类和分割领域将是有用的。像多尺度训练这样的训练技术可以在各种视觉任务中提供好处。</p><p>对于未来的工作，我们希望将类似的技术用于弱监督的图像分割。我们还计划在训练过程中使用更强大的匹配策略为分类数据分配弱标签来提高我们的检测结果。计算机视觉有着得天独厚的大量标记数据。我们将继续寻找方法，将不同来源和结构的数据结合起来，为视觉世界建立更强大的模型。</p><blockquote><p><strong>YOLOv2</strong> 是最先进的，在各种检测数据集上比其他检测系统更快。此外，它可以在各种图像大小下运行，以在速度和精度之间提供平滑的折中。</p><p><strong>对比yolov1所作出的改进：</strong></p><ul><li>加了BN（卷积后，激活函数前）;</li><li>加了高分辨率分类器;加了anchor(聚类得到个数,1个gird cell 生成5个anchor);限制预测框；</li><li>加入细粒度特征(类似于concat的残差)加入对尺度训练改进骨干网络(GoogleNet 变darknet-19)通过WordTree将不同数据集结合联合训练。</li><li>用一种新颖的方法扩充了数据集。</li></ul><p><strong>YOLO9000</strong> 是一个实时框架，通过联合优化检测和分类，可检测9000多个对象类别。我们使用WordTree合并来自不同来源的数据，并使用我们的联合优化技术在ImageNet和CoCo上同时进行训练。</p><p><strong>WordTree</strong> 的概念可以让分类标注提供更大的运用空间，并且可以利用来进行弱监督学习，也可以利用这样的概念结合各种不同任务的资料集，对于分类有很大的助益。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>yolov1-paper</title>
      <link href="/yolov1-paper/"/>
      <url>/yolov1-paper/</url>
      
        <content type="html"><![CDATA[<h1 id="You-Only-Look-Once-Unified-Real-Time-Object-Detection"><a href="#You-Only-Look-Once-Unified-Real-Time-Object-Detection" class="headerlink" title="You Only Look Once: Unified, Real-Time Object Detection"></a>You Only Look Once: Unified, Real-Time Object Detection</h1><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="1-Yolo模型演进"><a href="#1-Yolo模型演进" class="headerlink" title="1. Yolo模型演进"></a>1. Yolo模型演进</h3><div class="table-container"><table><thead><tr><th>版本</th><th>发布日期</th><th>备注</th></tr></thead><tbody><tr><td>YOLOv1</td><td>CVPR 2016</td><td>初始版本</td></tr><tr><td>YOLOv2/YOLO9000</td><td>CVPR 2017</td><td>改进版本，引入YOLO9000</td></tr><tr><td>YOLOv3</td><td>2018</td><td>进一步改进</td></tr><tr><td>YOLOv4</td><td>2020.4.24</td><td>性能优化</td></tr><tr><td>YOLOv5</td><td>2020.5.2</td><td>Ultralytics公司发布</td></tr><tr><td>v5.0</td><td>2021.4.12</td><td>YOLOv5的一个版本</td></tr></tbody></table></div><h3 id="2-计算机视觉的任务"><a href="#2-计算机视觉的任务" class="headerlink" title="2. 计算机视觉的任务"></a>2. 计算机视觉的任务</h3><div class="table-container"><table><thead><tr><th>任务类型</th><th>输入</th><th>输出</th><th>描述</th></tr></thead><tbody><tr><td>图像分类</td><td>图像</td><td>类别</td><td>输入图像，预测图像的类别</td></tr><tr><td><strong>目标检测</strong></td><td>图像</td><td>类别+框</td><td>输入图像，预测图像中对象的类别及其位置（边界框）,目标定位/目标识别</td></tr><tr><td>语义分割</td><td>图像</td><td>像素分类</td><td>输入图像，对每个像素进行分类，所有实例被视为一个整体（如所有人）</td></tr><tr><td><strong>实例分割</strong></td><td>图像</td><td>像素分类+实例区分</td><td>输入图像，对每个像素进行分类，并区分同一类别的不同实例（如不同的人）</td></tr></tbody></table></div><ul><li>只有<strong>目标检测</strong>和<strong>实例分割</strong>实现了<code>单实例</code>的识别</li></ul><h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><h3 id="Abstract—摘要"><a href="#Abstract—摘要" class="headerlink" title="Abstract—摘要"></a>Abstract—摘要</h3><p>我们提出的YOLO是一种新的目标检测方法。以前的目标检测方法通过重新利用分类器来执行检测。与先前的方案不同，我们将目标检测看作回归问题从空间上<strong>定位边界框（bounding box）并预测该框的类别概率</strong>。我们使用<strong>单个神经网络</strong>，在一次评估中直接从完整图像上预测边界框和类别概率。由于整个检测流程仅用一个网络，所以可以直接对检测性能进行<strong>端到端</strong>的优化。</p><p>我们的统一架构速度极快。我们的基本YOLO模型以<strong>45 fps</strong>（帧/秒）的速度实时处理图像。该网络的一个较小版本——Fast YOLO，以<strong>155 fps</strong>这样惊人的速度运行，同时仍然达到其他实时检测器的两倍。与最先进的（state-of-the-art，SOTA）检测系统相比，YOLO虽然产生了较多的定位误差，但它几乎不会发生把背景预测为目标这样的假阳性（False Positive）的错误。最后，YOLO能<strong>学习到泛化性很强的目标表征</strong>。当从自然图像学到的模型用于其它领域如艺术画作时，它的表现都优于包括DPM和R-CNN在内的其它检测方法。</p><blockquote><h4 id="之前的方法（RCNN系列）"><a href="#之前的方法（RCNN系列）" class="headerlink" title="之前的方法（RCNN系列）"></a>之前的方法（RCNN系列）</h4><p>（1）通过region proposal产生大量的可能包含待检测物体的<strong>potential bounding box</strong></p><p>（2）再用分类器去判断每个<strong>bounding box</strong>里是否包含有物体，以及物体所属类别的<strong>probability或者 confidence</strong></p><p>（3）最后回归预测</p><h4 id="YOLO的简介："><a href="#YOLO的简介：" class="headerlink" title="YOLO的简介："></a>YOLO的简介：</h4><p>本文将检测变为一个regression problem（回归问题），YOLO 从输入的图像，仅仅经过一个神经网络，直接得到一些bounding box以及每个bounding box所属类别的概率。</p><p>因为整个的检测过程仅仅有一个网络，所以它可以直接进行end-to-end的优化。</p></blockquote><h3 id="一、Introduction—前言"><a href="#一、Introduction—前言" class="headerlink" title="一、Introduction—前言"></a>一、<strong>Introduction—前言</strong></h3><p>人们只需瞄一眼图像，立即知道图像中的物体是什么，它们在哪里以及它们如何相互作用。人类的视觉系统是快速和准确的，使得我们在无意中就能够执行复杂的任务，如驾驶。快速且准确的目标检测算法可以让计算机在没有专门传感器的情况下驾驶汽车，使辅助设备能够向人类用户传达实时的场景信息，并解锁通用、响应性的机器人系统的潜能。</p><p>目前的检测系统通过<strong>重用分类器</strong>来执行检测。为了检测目标，这些系统为该目标提供一个分类器，在测试图像的不同的位置和不同的尺度上对其进行评估。像deformable parts models（<strong>DPM</strong>，可变形部分模型）这样的系统使用<strong>滑动窗口方法</strong>，其分类器在整个图像上<strong>均匀间隔</strong>的位置上运行[10]。</p><p><strong>我们将目标检测看作是一个单一的回归问题，直接从图像像素得到边界框坐标和类别概率。</strong>使用我们的系统——You Only Look Once（YOLO），便能得到图像上的物体是什么和物体的具体位置。</p><p>YOLO非常简单（见图1），它仅用单个卷积网络就能同时预测多个边界框和它们的类别概率。YOLO<strong>在整个图像上训练，并能直接优化检测性能</strong>。与传统的目标检测方法相比，这种统一的模型下面所列的<strong>一些优点</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Yolov1-Figure1.png" alt="image-20240105230745471"></p><blockquote><h4 id="之前的研究："><a href="#之前的研究：" class="headerlink" title="之前的研究："></a>之前的研究：</h4><p><strong>DPM：</strong> 系统为检测对象使用分类器，并在测试图像的不同位置和尺度对其进行评估</p><p><strong>R-CNN：</strong>SS方法提取候选框＋CNN＋分类+回归。</p><h4 id="YOLO处理步骤："><a href="#YOLO处理步骤：" class="headerlink" title="YOLO处理步骤："></a>YOLO处理步骤：</h4><p>(1)将输入图像的大小调整为448×448，分割得到7*7网格；</p><p>(2)通过CNN提取特征和预测；</p><p>(3)利用非极大值抑制（NMS）进行筛选</p></blockquote><p><strong>第一，YOLO速度非常快</strong>。由于我们将检测视为<strong>回归问题</strong>，所以我们不需要复杂的流程。测试时，我们在一张新图像上简单的运行我们的神经网络来预测检测结果。在Titan X GPU上不做批处理的情况下，YOLO的基础版本以每秒45帧的速度运行，而快速版本运行速度超过150fps。这意味着我们可以在不到25毫秒的延迟内实时处理流媒体视频。此外，YOLO实现了其它实时系统两倍以上的平均精度。关于我们的系统在网络摄像头上实时运行的演示，请参阅我们的项目网页：YOLO: Real-Time Object Detection。</p><p><strong>第二，YOLO是在整个图像上进行推断的</strong>。与基于滑动窗口和候选框的技术不同，YOLO在训练期间和测试时都会顾及到整个图像，所以它隐式地包含了关于类的上下文信息以及它们的外观。Fast R-CNN是一种很好的检测方法[14]，但由于它看不到更大的上下文，会将背景块误检为目标。与Fast R-CNN相比，YOLO的背景误检数量少了一半。</p><p><strong>第三，YOLO能学习到目标的泛化表征</strong>（generalizable representations of objects）。把在自然图像上进行训练的模型，用在艺术图像进行测试时，YOLO大幅优于DPM和R-CNN等顶级的检测方法。由于YOLO具有高度泛化能力，因此在应用于新领域或碰到意外的输入时不太可能出故障。</p><p><strong>YOLO在精度上仍然落后于目前最先进的检测系统</strong>。虽然它可以快速识别图像中的目标，但它在定位某些物体尤其是小的物体上精度不高。</p><p>我们在实验中会进一步探讨精度／时间的权衡。我们所有的训练和测试代码都是开源的，而且各种预训练模型也都可以下载。</p><blockquote><h4 id="YOLO的定义："><a href="#YOLO的定义：" class="headerlink" title="YOLO的定义："></a>YOLO的定义：</h4><p>YOLO将目标检测重新定义为<strong>单个回归问题</strong>，<strong>从图像像素直接到边界框坐标和类概率</strong>。YOLO可以在一个图像来预测：哪些对象是存在的？它们在哪里？</p><p>如 Figure 1：将图像输入单独的一个 CNN 网络，就会预测出 bounding box，以及这些 bounding box 所属类别的概率。</p><p>YOLO 用<strong>一整幅图像</strong>来训练，同时可以直接优化性能检测。</p><p>性能检测对比：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolo%E6%A3%80%E6%B5%8B%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94.png" alt="yolo检测性能一笔"></p><h4 id="YOLO的优点："><a href="#YOLO的优点：" class="headerlink" title="YOLO的优点："></a>YOLO的优点：</h4><p>1）<strong>YOLO的速度非常快。</strong>能够达到实时的要求。在 Titan X 的 GPU 上 能够达到 45 帧每秒。</p><p>2）<strong>YOLO在做预测时使用的是全局图像。</strong>与FastR-CNN相比，YOLO产生的背景错误数量不到一半。</p><p>3）<strong>YOLO 学到物体更泛化的特征表示。</strong>因此当应用于新域或意外输入时，不太可能崩溃。</p></blockquote><h3 id="二、Uniﬁed-Detection—统一检测"><a href="#二、Uniﬁed-Detection—统一检测" class="headerlink" title="二、Uniﬁed Detection—统一检测"></a><strong>二、Uniﬁed Detection—统一检测</strong></h3><p><strong>我们将目标检测的独立部分(the separate components )整合到单个神经网络中。</strong>我们的网络使用整个图像的特征来预测每个边界框。它还可以同时预测一张图像中的所有类别的所有边界框。这意味着我们的网络对整张图像和图像中的所有目标进行全局推理(reason globally)。YOLO设计可实现端到端训练和实时的速度，同时保持较高的平均精度。</p><p><strong>我们的系统将输入图像分成 S×S 的网格</strong>。如果目标的中心落入某个网格单元(grid cell)中，那么该网格单元就负责检测该目标。</p><p><strong>每个网格单元都会预测 B个 边界框和这些框的置信度分数（confidence scores）</strong>。这些置信度分数反映了该模型对那个框内是否包含目标的置信度，以及它对自己的预测的准确度的估量。在形式上，我们将<strong>置信度</strong>定义为 <code>confidence = Pr(Object) * IOUpred truth</code> 。如果该单元格中不存在目标（即<code>Pr(Object)=0</code>），则置信度分数应为 0。否则（即<code>Pr(Object)=1</code>），我们希望置信度分数等于预测框（predict box）与真实标签框（ground truth）之间联合部分的交集（IOU）。</p><p>每个网格单元还预测了C类的条件概率，<code>Pr(Classi|Object)</code>。这些概率是以包含目标的网格单元为条件的。我们只预测每个网格单元的一组类别概率，而不考虑框B的数量。</p><p>在测试时，我们将条件类概率和单个框的置信度预测相乘：</p><script type="math/tex; mode=display">\Pr(\text{Class}_i|\text{Object}) \cdot \Pr(\text{Object}) \cdot \text{IOU}^{\text{truth}}_{\text{pred}} = \Pr(\text{Class}_i) \cdot \text{IOU}^{\text{truth}}_{\text{pred}}</script><p>这给我们提供了每个框的特定类别的置信度分数。这些分数既是对该类出现在框里的概率的编码，也是对预测的框与目标的匹配程度的编码。</p><blockquote><h4 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h4><p>YOLO将目标检测问题作为<strong>回归问题</strong>。会将输入图像分成S×S的网格，如果一个物体的中心点落入到一个cell中，那么该cell就要负责预测该物体，一个格子只能预测一个物体，会生成两个预测框。</p><h4 id="对于每个grid-cell："><a href="#对于每个grid-cell：" class="headerlink" title="对于每个grid cell："></a>对于每个grid cell：</h4><p>1）预测B个边界框，每个框都有一个置信度分数（confidence score）这些框大小尺寸等等都随便，只有一个要求，就是<strong>生成框的中心点必须在grid cell里</strong>。</p><p>2）每个边界框包含5个元素：<strong>(x,y,w,h)</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolo-xywhc.png" alt="yolo-xywhc"></p><ul><li><strong>x，y：</strong>是指bounding box的预测框的中心坐标相较于该bounding box归属的grid cell左上角的偏移量，在0-1之间。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolo-xy.png" alt=""></p><p>在上图中，<strong>绿色虚线框</strong>代表grid cell，<strong>绿点</strong>表示该grid cell的左上角坐标，为（0，0）；<strong>红色和蓝色框</strong>代表该grid cell包含的两个bounding box，<strong>红点和蓝点</strong>表示这两个bounding box的中心坐标。有一点很重要，bounding box的中心坐标一定在该grid cell内部，因此，红点和蓝点的坐标可以归一化在0-1之间。在上图中，红点的坐标为（0.5，0.5），即x=y=0.5，蓝点的坐标为（0.9，0.9），即x=y=0.9。</p><ul><li><strong>w，h：</strong> 是指该bounding box的宽和高，但也归一化到了0-1之间，表示相较于原始图像的宽和高（即448个像素）。比如该bounding box预测的框宽是44.8个像素，高也是44.8个像素，则w=0.1，h=0.1。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolo-wh.png" alt=""></p><p><strong>红框</strong>的x=0.8，y=0.5，w=0.1，h=0.2。</p><p>3）不管框 B 的数量是多少，<strong>每个Grid Cell只负责预测一个目标</strong>。</p><p>4）预测 C 个条件概率类别（物体属于每一种类别的可能性）</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-figure2.png" alt=""></p><blockquote><p><strong>综上，S×S 个网格，每个网格要预测 B个bounding box （中间上图），还要预测 C 个类（中间下图）。</strong>将两图合并，网络输出就是一个 S × S × (5×B+C)。（S x S个网格，每个网格都有B个预测框，每个框又有5个参数，再加上每个网格都有C个预测类）</p><h4 id="预测特征组成"><a href="#预测特征组成" class="headerlink" title="预测特征组成"></a>预测特征组成</h4><p>最终的预测特征由边框的位置、边框的置信度得分以及类别概率组成，这三者的含义如下：</p><ul><li><strong>边框位置x,y,w,h：</strong> 对每一个边框需要预测其中心坐标及宽、高这4个量， 两个边框共计8个预测值边界框宽度w和高度h用图像宽度和高度归一化。因此 x,y,w,h 都在0和1之间。</li><li><strong>置信度得分(box confidence score) c ：</strong> 框包含一个目标的可能性以及边界框的准确程度。类似于Faster RCNN 中是前景还是背景。由于有两个边框，因此会存在两个置信度预测值。</li><li><strong>类别概率：</strong> 由于PASCAL VOC数据集一共有20个物体类别，因此这里预测的是边框属于哪一个类别。</li></ul><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><ul><li>一个cell预测的两个边界框共用一个类别预测， 在<strong>训练</strong>时会选取与标签IoU更大的一个边框负责回归该真实物体框，在<strong>测试</strong>时会选取置信度更高的一个边框，另一个会被舍弃，因此7×7=49个gird cell最多只能预测49个物体。</li><li>因为每一个 grid cell只能有一个分类，也就是他只能预测一个物体，这也是导致YOLO对小目标物体性能比较差的原因。<strong>如果所给图片极其密集，导致 grid cell里可能有多个物体，但是YOLO模型只能预测出来一个，那这样就会忽略在本grid cell内的其他物体。</strong></li></ul></blockquote><h4 id="2-1-Network-Design—网络设计"><a href="#2-1-Network-Design—网络设计" class="headerlink" title="2.1 Network Design—网络设计"></a>2.1 Network Design—网络设计</h4><p>我们将此模型作为卷积神经网络来实现，并在Pascal VOC检测数据集[9]上进行评估。网络的初始卷积层从图像中提取特征，而全连接层负责预测输出概率和坐标。</p><p>我们的网络架构受图像分类模型<strong>GoogLeNet</strong>的启发[34]。我们的网络有24个卷积层，后面是2个全连接层。我们只使用1×1降维层，后面是3×3卷积层，这与Lin等人[22]类似，而不是GoogLeNet使用的Inception模块。</p><p>我们还训练了快速版本的YOLO，旨在推动快速目标检测的界限。快速YOLO使用具有较少卷积层（9层而不是24层）的神经网络，在这些层中使用较少的卷积核。除了网络规模之外，基本版YOLO和快速YOLO的所有训练和测试参数都是相同的。</p><p>我们网络的最终输出是7×7×30的预测张量。</p><blockquote><h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p>YOLO网络结构借鉴了 GoogLeNet。输入图像的尺寸为448×448，经过24个卷积层，2个全连接的层（FC），最后在reshape操作，输出的特征图大小为7×7×30。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-net.png" alt=""></p><h4 id="网络详解"><a href="#网络详解" class="headerlink" title="网络详解"></a>网络详解</h4><p>1）YOLO主要是建立一个CNN网络生成预测<strong>7×7×1024</strong> 的张量 。</p><p>2）然后使用两个全连接层执行线性回归，以进行<strong>7×7×2</strong> 边界框预测。将具有高置信度得分（大于0.25）的结果作为最终预测。</p><p>3）在3×3的卷积后通常会接一个通道数更低<strong>1×1</strong>的卷积，这种方式既降低了计算量，同时也提升了模型的非线性能力。</p><p>4）除了最后一层使用了线性激活函数外，其余层的激活函数为 <strong>Leaky ReLU</strong> 。</p><p>5）在训练中使用了 <strong>Dropout</strong> 与数据增强的方法来防止过拟合。</p><p>6）对于最后一个卷积层，它输出一个形状为 <strong>(7, 7, 1024)</strong> 的张量。 然后张量展开。使用2个全连接层作为一种线性回归的形式，它输出1470个参数，然后reshape为 <strong>(7, 7, 30)</strong> 。</p><p>张量剖面图</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E5%BC%A0%E9%87%8F%E5%89%96%E9%9D%A2%E5%9B%BE.png" alt=""></p><ul><li><strong>7×7：</strong> 一共划分成7×7的网格。</li><li><strong>30：</strong> 30包含了两个预测框的参数和Pascal VOC的类别参数：每个预测框有5个参数：x,y,w,h,confidence。另外，Pascal VOC里面还有20个类别；所以最后的30实际上是由<code>5x2+20</code>组成的，也就是说这一个30维的向量就是一个gird cell的信息。</li><li><strong>7×7×30：</strong> 总共是7 × 7个gird cell一共就是7 × 7 ×（2 × 5+ 20）= 7 × 7 × 30 tensor = 1470 </li></ul></blockquote><h4 id="2-2-Training—训练"><a href="#2-2-Training—训练" class="headerlink" title="2.2 Training—训练"></a>2.2 Training—训练</h4><p>我们在ImageNet的1000类竞赛数据集[30]上<strong>预训练我们的卷积层</strong>。对于预训练，我们使用图3中的前20个卷积层，接着是平均池化层和全连接层。我们对这个网络进行了大约一周的训练，并且在ImageNet 2012验证集上获得了单一裁剪图像88%的top-5准确率，与Caffe模型池中的GoogLeNet模型相当。我们使用<strong>Darknet</strong>框架进行所有的训练和推断[26]。</p><p>然后我们转换模型来执行检测训练。Ren等人表明，<strong>预训练网络中增加卷积层和连接层可以提高性能</strong>[29]。按照他们的方法，我们<strong>添加了四个卷积层和两个全连接层，这些层的权重都用随机值初始化</strong>。检测通常需要细粒度的视觉信息，因此我们将网络的输入分辨率从224×224改为448×448。</p><p><strong>模型的最后一层预测类概率和边界框坐标</strong>。我们通过图像宽度和高度来规范边界框的宽度和高度，使它们落在0和1之间。我们将边界框x和y坐标参数化为特定网格单元位置的<strong>偏移量</strong>，所以它们的值被限定在在0和1之间。</p><p><strong>模型的最后一层</strong>使用线性激活函数，而<strong>所有其它的层</strong>使用下面的Leaky-ReLU：</p><script type="math/tex; mode=display">\phi(x) = \begin{cases} x, & \text{if } x > 0 \\0.1x, & \text{otherwise}\end{cases}</script><p>我们对模型输出的<strong>平方和误差</strong>(sum-squared error)进行优化。我们选择使用平方和误差，是因为它易于优化，但是它并不完全符合最大化平均精度（average precision）的目标。它给分类误差与定位误差的权重是一样的，这点可能并不理想。<strong>另外</strong>，每个图像都有<strong>很多网格单元并没有包含任何目标</strong>，这将这些单元格的“置信度”分数推向零，通常<strong>压制了包含目标的单元格的梯度</strong>。这可能导致模型不稳定，从而导致训练在早期就发散(diverge)。</p><p>为了弥补平方和误差的缺陷，我们增加了边界框坐标预测的损失，并减少了不包含目标的框的置信度预测的损失。我们使用两个参数来实现这一点。</p><script type="math/tex; mode=display">\lambda_{\text{coord}} = 5 \quad \text{and} \quad \lambda_{\text{noobj}} = 0.5</script><p>平方和误差对大框和小框的误差权衡是一样的，而我们的错误指标(error metric)应该要体现出，<strong>大框的小偏差的重要性不如小框的小偏差的重要性。</strong>为了部分解决这个问题，我们直接预测边界框宽度和高度的<strong>平方根</strong>，而不是宽度和高度。</p><p><strong>YOLO为每个网格单元预测多个边界框。</strong>在训练时，每个目标我们只需要一个边界框预测器来负责。若某预测器的预测值与目标的实际值的IOU值最高，则这个预测器被指定为“负责”预测该目标。这导致边界框预测器的专业化。每个预测器可以更好地预测特定大小，方向角，或目标的类别，从而改善整体召回率(recall)。</p><p><strong>在训练期间，我们优化以下多部分损失函数：</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-loss.png" alt=""></p><p>注意，如果目标存在于该网格单元中（前面讨论的条件类别概率），则损失函数仅惩罚(penalizes)分类错误。如果预测器“负责”实际边界框（即该网格单元中具有最高IOU的预测器），则它也仅惩罚边界框坐标错误。</p><p>我们用Pascal VOC 2007和2012的训练集和验证数据集进行了大约 135个epoch 的网络训练。因为我们仅在Pascal VOC 2012上进行测试，所以我们的训练集里包含了Pascal VOC 2007的测试数据。在整个训练过程中，我们使用：batch size=64，momentum=0.9，decay=0.0005。</p><p>我们的学习率（learning rate）计划如下：在第一个epoch中，我们将学习率从0.001慢慢地提高到 0.01。如果从大的学习率开始训练，我们的模型通常会由于不稳定的梯度而发散(diverge)。我们继续以0.01进行75个周期的训练，然后以0.001进行30个周期的训练，最后以0.0001进行30个周期的训练。</p><p><strong>为避免过拟合，我们使用了Dropout和大量的数据增强</strong>。 在第一个连接层之后的dropout层的丢弃率设置为0.5，以防止层之间的相互适应[18]。 对于数据增强(data augmentation)，我们引入高达20％的原始图像大小的随机缩放和平移(random scaling and translations )。我们还在 HSV 色彩空间中以高达 1.5 的因子随机调整图像的曝光度和饱和度。</p><blockquote><h4 id="预训练分类网络"><a href="#预训练分类网络" class="headerlink" title="预训练分类网络"></a>预训练分类网络</h4><p>在 ImageNet 1000数据集上预训练一个分类网络，<strong>这个网络使用Figure3中的前20个卷积层，然后是一个平均池化层和一个全连接层。</strong>（此时网络输入是224×224）。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-ImageNet%E9%A2%84%E8%AE%AD%E7%BB%83%E7%BD%91%E7%BB%9C.png" alt=""></p><h4 id="训练检测网络"><a href="#训练检测网络" class="headerlink" title="训练检测网络"></a>训练检测网络</h4><p>经过上一步的预训练，就已经把主干网络的前20个卷积层给训练好了，前20层的参数已经学到了图片的特征。接下来的步骤本质就是迁移学习，<strong>在训练好的前20层卷积层后加上4层卷积层和2层全连接层，然后在目标检测的任务上进行迁移学习。</strong></p><p>在整个网络（24+2）的训练过程中，除最后一层采用ReLU函数外，其他层均采用leaky ReLU激活函数。leaky ReLU相对于ReLU函数可以解决在输入为负值时的零梯度问题。YOLOv1中采用的leaky ReLU函数的表达式为：</p><script type="math/tex; mode=display">\phi(x) = \begin{cases} x, & \text{if } x > 0 \\0.1x, & \text{otherwise}\end{cases}</script><h4 id="NMS非极大值抑制"><a href="#NMS非极大值抑制" class="headerlink" title="NMS非极大值抑制"></a>NMS非极大值抑制</h4><p><strong>概念：</strong>NMS算法主要解决的是一个目标被多次检测的问题，意义主要在于在一个区域里交叠的很多框选一个最优的。</p><p>1）对于上述的98列数据，先看某一个类别，也就是只看98列的这一行所有数据，<strong>先拿出最大值概率的那个框，剩下的每一个都与它做比较</strong>，如果两者的IoU大于某个阈值，则认为这俩框重复识别了同一个物体，就将其中低概率的重置成0。</p><p>2）最大的那个框和其他的框比完之后，<strong>再从剩下的框找最大的，继续和其他的比</strong>，依次类推对所有类别进行操作。 注意，这里不能直接选择最大的，因为<strong>有可能图中有多个该类别的物体</strong>，所以IoU如果小于某个阈值，则会被保留。</p><p>3）最后得到一个稀疏矩阵，因为里面有很多地方都被重置成0，拿出来不是0的地方拿出来概率和类别，就得到最后的目标检测结果了。</p><p><strong>注意：</strong> <strong>NMS只发生在预测阶段，训练阶段是不能用NMS的</strong>，因为在训练阶段不管这个框是否用于预测物体的，他都和损失函数相关，不能随便重置成0。</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p><strong>损失函数包括：</strong></p><ul><li>localization loss -&gt; 坐标损失</li><li>confidence loss -&gt; 置信度损失</li><li>classification loss -&gt; 分类损失</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-loss%E8%A7%A3%E9%87%8A.png" alt=""></p><h5 id="损失函数详解："><a href="#损失函数详解：" class="headerlink" title="损失函数详解："></a>损失函数详解：</h5><p><strong>（1）坐标损失</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E5%9D%90%E6%A0%87%E6%8D%9F%E5%A4%B1.png" alt=""></p><ul><li>第一行： 负责检测物体的框中心点（x, y）定位误差。</li><li>第二行： 负责检测物体的框的高宽（w,h)定位误差，这个根号的作用就是为了修正对大小框一视同仁的缺点，削弱大框的误差。</li></ul><blockquote><h5 id="加根号的作用："><a href="#加根号的作用：" class="headerlink" title="加根号的作用："></a>加根号的作用：</h5><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E5%A4%A7%E6%A1%86%2B%E5%B0%8F%E6%A1%86.png" alt=""></p><p>在上图中，大框和小框的bounding box和ground truth都是差了一点。但对于实际预测来讲，大框（大目标）差的这一点也许没啥事儿，而小框（小目标）差的这一点可能就会导致bounding box的方框和目标差了很远。而如果还是使用第一项那样直接算平方和误差，就相当于把大框和小框一视同仁了，这样显然不合理。而如果使用开根号处理，就会一定程度上改善这一问题 。即要想办法<strong>夸大</strong>小框的偏差<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E5%A4%B8%E5%A4%A7%E5%B0%8F%E6%A1%86.png" alt=""></p><p>这样一来，同样是差一点，小框产生的误差会更大，即对小框惩罚的更严重。</p></blockquote><p><strong>（2）置信度损失</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E7%BD%AE%E4%BF%A1%E5%BA%A6%E6%8D%9F%E5%A4%B1.png" alt=""></p><ul><li>第一行： 负责检测物体的那个框的置信度误差。</li><li>第二行： 不负责检测物体的那个框的置信度误差。</li></ul><p><strong>（3）分类损失</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1.png" alt=""></p><p>负责检测物体的grid cell分类的误差。</p><h4 id="特殊符号的含义："><a href="#特殊符号的含义：" class="headerlink" title="特殊符号的含义："></a>特殊符号的含义：</h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E7%89%B9%E6%AE%8A%E7%AC%A6%E5%8F%B7%E5%90%AB%E4%B9%89.png" alt=""></p></blockquote><h4 id="2-3-Inference—推论"><a href="#2-3-Inference—推论" class="headerlink" title="2.3 Inference—推论"></a>2.3 Inference—推论</h4><p>就像在训练中一样，预测测试图像的检测只需要一次网络评估。在Pascal VOC上，每张图像上网络预测 98 个边界框和每个框的类别概率。YOLO在测试时非常快，因为它只需要一次网络评估(network evaluation)，这与基于分类器的方法不同。</p><p><strong>网格设计强化了边界框预测中的空间多样性</strong>。通常一个目标落在哪一个网格单元中是很明显的，而网络只能为每个目标预测一个边界框。然而，一些大的目标或接近多个网格单元的边界的目标能被多个网格单元定位。<strong>非极大值抑制(Non-maximal suppression,NMS)可以用来修正这些多重检测</strong>。非最大抑制对于YOLO的性能的影响不像对于R-CNN或DPM那样重要，但也能增加2−3%的mAP。</p><blockquote><p>1）预测测试图像的检测只需要一个网络评估。</p><p>2）测试时间快</p><p>3）当图像中的物体较大，或者处于 grid cells 边界的物体，可能在多个 cells 中被定位出来。</p><p>4）利用NMS去除重复检测的物体，使mAP提高，但和RCNN等相比不算大。</p></blockquote><h4 id="2-4-Limitations-of-YOLO—YOLO的局限性"><a href="#2-4-Limitations-of-YOLO—YOLO的局限性" class="headerlink" title="2.4 Limitations of YOLO—YOLO的局限性"></a>2.4 Limitations of YOLO—YOLO的局限性</h4><p>由于每个格网单元只能预测两个框，并且只能有一个类，因此YOLO对边界框预测<strong>施加了很强的空间约束</strong>。这个空间约束限制了我们的模型可以预测的邻近目标的数量。我们的模型难以预测群组中出现的小物体（比如鸟群）。</p><p>由于我们的模型学习是从数据中预测边界框，因此它<strong>很难泛化</strong>到新的、不常见的长宽比或配置的目标。我们的模型也使用<strong>相对较粗糙的特征</strong>来预测边界框，因为输入图像在我们的架构中<strong>历经了多个下采样层</strong>(downsampling layers)。</p><p>最后，我们的训练基于一个逼近检测性能的损失函数，这个损失函数<strong>无差别地</strong>处理小边界框与大边界框的误差。大边界框的小误差通常是无关要紧的，但小边界框的小误差对IOU的影响要大得多。我们的<strong>主要错误来自于不正确的定位</strong>。</p><blockquote><p>1）对于图片中一些群体性小目标检测效果比较差。因为yolov1网络到后面感受野较大，小目标的特征无法再后面7×7的grid中体现，<strong>针对这一点，yolov2已作了一定的修改，加入前层（感受野较小）的特征进行融合。</strong></p><p>2）原始图片只划分为7x7的网格，当两个物体靠的很近时（挨在一起且中点都落在同一个格子上的情况），效果比较差。因为yolov1的模型决定了一个grid只能预测出一个物体，所以就会丢失目标，针对这一点，<strong>yolov2引入了anchor的概念，一个grid有多少个anchor理论上就可以预测多少个目标</strong>。<br>3）每个网格只对应两个bounding box，当物体的长宽比不常见(也就是训练数据集覆盖不到时)，效果较差。</p><p>4）最终每个网格只对应一个类别，容易出现漏检(物体没有被识别到)。</p></blockquote><h3 id="三、Comparison-to-Other-Detection-Systems—与其他目标检测算法的比较"><a href="#三、Comparison-to-Other-Detection-Systems—与其他目标检测算法的比较" class="headerlink" title="三、Comparison to Other Detection Systems—与其他目标检测算法的比较"></a>三、Comparison to Other Detection Systems—与其他目标检测算法的比较</h3><p>目标检测是计算机视觉中的核心问题。检测流程通常是<strong>首先</strong>从输入图像上提取一组鲁棒特征（Haar [25]，SIFT [23]，HOG [4]，卷积特征[6]）。<strong>然后</strong>，分类器[36,21,13,10]或定位器[1,32]被用来识别特征空间中的目标。这些分类器或定位器或在整个图像上或在图像中的一些子区域上以滑动窗口的方式运行[35,15,39]。我们将YOLO检测系统与几种顶级检测框架进行比较，突出了关键的相似性和差异性。</p><p><strong>Deformable parts models</strong>。可变形部分模型（DPM）使用<strong>滑动窗口</strong>方法进行目标检测[10]。DPM使用不相交的流程来提取静态特征，对区域进行分类，预测高评分区域的边界框等。我们的系统用单个卷积神经网络替换所有这些不同的部分。<strong>网络同时进行特征提取，边界框预测，非极大值抑制和上下文推理</strong>。网络的特征feature是在<strong>在线</strong>(in-line)训练出来的而不是静态，因此可以根据特定的检测任务进行优化。我们的统一架构比DPM更快，更准确。</p><p><strong>R-CNN</strong>。R-CNN及其变体(variants)使用<strong>区域候选</strong>而不是滑动窗口来查找图像中的目标。选择性搜索[35]生成潜在的边界框(Selective Search generates potential bounding boxes)，卷积网络提取特征，SVM对框进行评分，线性模型调整边界框，非最大抑制消除重复检测(eliminates duplicate detections)。 这个复杂流水线的每个阶段都必须独立地进行精确调整(precisely tuned independently)，所得到的系统非常缓慢，在测试时间每个图像需要超过40秒[14]</p><p>YOLO与R-CNN有一些相似之处。每个网格单元提出潜在的边界框并使用卷积特征对这些框进行评分。然而，我们的系统对网格单元的候选框<strong>施加空间限制</strong>，这有助于缓解对同一目标的多次检测的问题。 我们的系统还<strong>生成了更少的边界框</strong>，每张图像只有98个，而选择性搜索则有约2000个。最后，我们的系统将这些单独的组件(individual components)组合成一个单一的、共同优化的模型。</p><p><strong>其它快速检测器</strong>。 Fast R-CNN 和 Faster R-CNN 通过共享计算和使用神经网络替代选择性搜索[14]，[28]来提出候选区域来加速 R-CNN 框架。虽然它们提供了比 R-CNN 更快的速度和更高的准确度，但仍然不能达到实时性能。</p><p>许多研究工作集中在加快DPM流程上[31] [38] [5]。它们加速HOG计算，使用级联(cascades)，并将计算推动到（多个）GPU上。但是，实际上只有30Hz的DPM [31]可以实时运行。</p><p>YOLO并没有试图优化大型检测流程的单个组件，相反，而是完全抛弃(throws out…entirely)了大型检测流程，并通过设计来提高速度。</p><p>像人脸或行人等单个类别的检测器可以高度优化，因为他们只需处理较少的多样性[37]。YOLO是一种通用的检测器，它可以同时(simultaneously)检测多个目标。</p><p><strong>Deep MultiBox</strong>。与R-CNN不同，Szegedy等人 训练一个卷积神经网络来预测感兴趣的区域(regions of interest,ROI)[8]，而不是使用选择性搜索。 MultiBox还可以通过用单个类别预测替换置信度预测来执行单个目标检测。 但是，MultiBox无法执行一般的目标检测，并且<strong>仍然只是较大检测流水线中的一部分</strong>，需要进一步的图像补丁分类。 YOLO和MultiBox都使用卷积网络来预测图像中的边界框，但<strong>YOLO是一个完整的检测系统</strong>。</p><p><strong>OverFeat</strong>。Sermanet等人训练了一个卷积神经网络来执行定位，并使该定位器进行检测[32]。OverFeat高效地执行滑动窗口检测，但它仍然是一个<strong>不相交的系统(</strong>disjoint system)。OverFeat优化了定位功能，而不是检测性能。像DPM一样，定位器在进行预测时只能看到局部信息。OverFeat无法推断全局上下文，因此需要大量的后处理来产生连贯的检测。</p><p><strong>MultiGrasp</strong>。我们的系统在设计上类似于Redmon等[27]的抓取检测。 <strong>我们的网格边界框预测方法基于MultiGrasp系统进行回归分析</strong>。 然而，抓取检测比物体检测要简单得多。 MultiGrasp只需要为包含一个目标的图像预测一个可抓取区域。 它不必估计目标的大小，位置或边界或预测它的类别，只需找到适合抓取的区域就可以了。 而<strong>YOLO则是预测图像中多个类的多个目标的边界框和类概率</strong>。</p><blockquote><h4 id="DPM"><a href="#DPM" class="headerlink" title="DPM"></a>DPM</h4><p>用传统的HOG特征方法，也用的是传统的支持向量机SVM分类器，然后人工造一个模板，再用滑动窗口方法不断的暴力搜索整个待识别图，去套那个模板。这个方法比较大的问题就是在于设计模板，计算量巨大，而且是个静态的，没办法匹配很多变化的东西，鲁棒性差。</p><h4 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h4><ul><li>第一阶段：每个图片使用选择性搜索SS方法提取2000个候选框。</li><li>第二阶段：将每个候选框送入CNN网络进行分类(使用的SVM)。</li></ul><p>YOLO对比他们俩都很强，YOLO和R-CNN也有相似的地方，比如也是提取候选框，YOLO的候选框就是上面说过的那98个 bounding boxes，也是用到了NMS非极大值抑制，也用到了CNN提取特征。</p><h4 id="Other-Fast-Detectors"><a href="#Other-Fast-Detectors" class="headerlink" title="Other Fast Detectors"></a>Other Fast Detectors</h4><p>Fast和Faster R-CNN ：这俩模型都是基于R-CNN的改版，速度和精度都提升了很多，但是也没办法做到实时监测，也就是说FPS到不了30，作者在这里并没有谈准确度的问题，实际上YOLO的准确度在这里是不占优势的，甚至于比他们低。</p><h4 id="Deep-MultiBox"><a href="#Deep-MultiBox" class="headerlink" title="Deep MultiBox"></a>Deep MultiBox</h4><p>训练卷积神经网络来预测感兴趣区域，而不是使用选择性搜索。多盒也可以用单个类预测替换置信预测来执行单个目标检测。YOLO和MultiBox都使用卷积网络来预测图像中的边界框，但YOLO是一个完整的检测系统。</p><h4 id="OverFeat"><a href="#OverFeat" class="headerlink" title="OverFeat"></a>OverFeat</h4><p>OverFeat有效地执行滑动窗口检测，优化了定位，而不是检测性能。与DPM一样，定位器在进行预测时只看到本地信息。OverFeat不能推理全局环境。</p><h4 id="MultiGrasp"><a href="#MultiGrasp" class="headerlink" title="MultiGrasp"></a>MultiGrasp</h4><p>YOLO在设计上与Redmon等人的抓取检测工作相似。边界盒预测的网格方法是基于多重抓取系统的回归到抓取。</p><p>总之，作者就是给前人的工作都数落一遍，凸显自己模型的厉害（学到了！）</p></blockquote><h3 id="四、Experiments—实验"><a href="#四、Experiments—实验" class="headerlink" title="四、Experiments—实验"></a><strong>四、Experiments—实验</strong></h3><h4 id="4-1-Comparison-to-Other-RealTime-Systems—与其他实时系统的比较"><a href="#4-1-Comparison-to-Other-RealTime-Systems—与其他实时系统的比较" class="headerlink" title="4.1 Comparison to Other RealTime Systems—与其他实时系统的比较"></a>4.1 Comparison to Other RealTime Systems—与其他实时系统的比较</h4><p>目标检测方面的许多研究工作都集中在使标准的检测流程更快[5]，[38]，[31]，[14]，[17]，[28]。然而，只有Sadeghi等人实际上产生了一个实时运行的检测系统（每秒30帧或更好）[31]。我们将YOLO与DPM的GPU实现进行了比较，其在30Hz或100Hz下运行。虽然其它的算法没有达到实时性的标准，我们也比较了它们的<strong>mAP和速度</strong>的关系，从而探讨目标检测系统中<strong>精度和性能之间的权衡</strong>。</p><p><strong>Fast YOLO</strong>是PASCAL上最快的目标检测方法；据我们所知，它是现有的最快的目标检测器。具有52.7%的mAP，实时检测的精度是以前的方法的两倍以上。<strong>普通版YOLO</strong>将mAP推到63.4%的同时保持了实时性能。</p><p>我们还使用VGG-16训练YOLO。 这个模型比<strong>普通版YOLO</strong>更精确，但也更慢。 它的作用是与依赖于VGG-16的其他检测系统进行比较，但由于它比实时更慢，所以本文的其他部分将重点放在我们更快的模型上。</p><p>最快的DPM可以在不牺牲太多mAP的情况下有效加速DPM，但仍然会将实时性能降低2倍[38]。与神经网络方法相比，DPM的检测精度相对较低，这也是限制它的原因。</p><p>减去R的R-CNN用静态侯选边界框取代选择性搜索[20]。虽然速度比R-CNN更快，但它仍然无法实时，并且由于该方法无法找到好的边界框，准确性受到了严重影响。</p><p>Fast R-CNN加快了R-CNN的分类阶段，但它仍然依赖于选择性搜索，每个图像需要大约2秒才能生成边界候选框。因此，它虽然具有较高的mAP，但的速度是0.5 fps，仍然远未达到实时。</p><p>最近的Faster R-CNN用神经网络替代了选择性搜索来候选边界框，类似于Szegedy等人[8]的方法。在我们的测试中，他们最准确的模型达到了 7fps，而较小的、不太准确的模型以18 fps运行。 Faster R-CNN的VGG-16版本比YOLO高出10mAP，但比YOLO慢了6倍。 Zeiler-Fergus 版本的Faster R-CNN只比YOLO慢2.5倍，但也不如YOLO准确。</p><blockquote><p>Table 1 在Pascal VOC 2007 上与其他检测方法的对比</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E5%9C%A8Pascal%20VOC%202007%20%E4%B8%8A%E4%B8%8E%E5%85%B6%E4%BB%96%E6%A3%80%E6%B5%8B%E6%96%B9%E6%B3%95%E7%9A%84%E5%AF%B9%E6%AF%94.png" alt=""></p><p><strong>结论：</strong>实时目标检测（FPS&gt;30），YOLO最准，Fast YOLO最快。 </p></blockquote><h4 id="4-2-VOC-2007-Error-Analysis—VOC-2007误差分析"><a href="#4-2-VOC-2007-Error-Analysis—VOC-2007误差分析" class="headerlink" title="4.2 VOC 2007 Error Analysis—VOC 2007误差分析"></a>4.2 VOC 2007 Error Analysis—VOC 2007误差分析</h4><p>为了进一步研究YOLO和最先进的检测器之间的差异，我们详细分析了VOC 2007的分类(breakdown)结果。我们将YOLO与Fast R-CNN进行比较，因为Fast R-CNN是PASCAL上性能最高的检测器之一并且它的检测代码是可公开得到的。</p><p>我们使用Hoiem等人的方法和工具[19]，对于测试的每个类别，我们查看该类别的前N个预测。每个预测都或是正确的，或是根据错误的类型进行分类：</p><ul><li>Correct: correct class and IOU&gt;0.5</li><li>Localization: correct class, 0.1&lt;IOU&lt;0.5</li><li>Similar: class is similar, IOU&gt;0.1</li><li>Other: class is wrong, IOU&gt;0.1</li><li>Background: IOU&lt;0.1 for any object（所有目标的IOU都&lt;0.1）</li></ul><p>YOLO难以正确地定位目标，因此定位错误比YOLO的所有其他错误总和都要多。Fast R-CNN定位错误更少，但把背景误认成目标的错误比较多。它的最高检测结果中有13.6％是不包含任何目标的误报(false positive，背景)。 Fast R-CNN把背景误认成目标的概率比YOLO高出3倍。 </p><blockquote><p>本文使用HoeMm等人的方法和工具。对于测试时间的每个类别，查看该类别的N个预测。每个预测要么是正确的，要么是基于错误类型进行分类的：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E6%B5%8B%E8%AF%95%E7%B1%BB%E5%88%AB.png" alt=""></p><p><strong>参数含义:</strong></p><ul><li><strong>Correct：</strong>正确分类，且预测框与ground truth的IOU大于0.5，既预测对了类别，预测框的位置和大小也很合适。 </li><li><strong>Localization：</strong>正确分类，但预测框与ground truth的IOU大于0.1小于0.5，即虽然预测对了类别，但预测框的位置不是那么的严丝合缝，不过也可以接受。</li><li><strong>Similar：</strong> 预测了相近的类别，且预测框与ground truth的IOU大于0.1。即预测的类别虽不正确但相近，预测框的位置还可以接受。</li><li><strong>Other：</strong>预测类别错误，预测框与ground truth的IOU大于0.1。即预测的类别不正确，但预测框还勉强把目标给框住了。</li><li><strong>Background：</strong>预测框与ground truth的IOU小于0.1，即该预测框的位置为背景，没有目标。</li></ul><p>Figure 4 显示了所有20个类中每种错误类型的平均细分情况</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E9%94%99%E8%AF%AF%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%B9%B3%E5%9D%87%E7%BB%86%E5%88%86.png" alt=""></p><p><strong>结论：</strong>YOLO定位错误率高于Fast R-CNN；Fast R-CNN背景预测错误率高于YOLO </p></blockquote><h4 id="4-3-Combining-Fast-R-CNN-and-YOLO—Fast-R-CNN与YOLO的结合"><a href="#4-3-Combining-Fast-R-CNN-and-YOLO—Fast-R-CNN与YOLO的结合" class="headerlink" title="4.3 Combining Fast R-CNN and YOLO—Fast R-CNN与YOLO的结合"></a>4.3 Combining Fast R-CNN and YOLO—Fast R-CNN与YOLO的结合</h4><p><strong>YOLO误认背景为目标的情况比Fast R-CNN少得多</strong>。 通过使用YOLO消除Fast R-CNN的背景检测，我们获得了显著的性能提升。 对于R-CNN预测的每个边界框，我们检查YOLO是否预测了一个相似的框。 如果确实如此，那么我们会根据YOLO预测的概率和两个框之间的重叠情况提高预测值。</p><p>最好的Fast R-CNN模型在VOC 2007测试集中达到了 71.8％ 的mAP。 当与YOLO合并时，其mAP增加了 3.2％ 至 75.0％。 我们还尝试将顶级Fast R-CNN模型与其他几个版本的Fast R-CNN结合起来。 这写的结合的平均增长率在 0.3％ 至 0.6％ 之间。</p><p>结合YOLO后获得的性能提高不仅仅是模型集成的副产品，因为结合不同版本的Fast R-CNN几乎没有什么益处。 相反，正是因为YOLO在测试时出现了各种各样的错误，所以它在提高Fast R-CNN的性能方面非常有效。</p><p>不幸的是，这种组合不会从YOLO的速度中受益，因为我们分别运行每个模型，然后合并结果。 但是，由于YOLO速度如此之快，与Fast R-CNN相比，它不会增加任何显著的计算时间。</p><p>Table2 模型组合在VOC 2007上的实验结果对比</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E6%A8%A1%E5%9E%8B%E7%BB%84%E5%90%88%E5%9C%A8VOC%202007%E4%B8%8A%E7%9A%84%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E5%AF%B9%E6%AF%94.png" alt=""></p><p><strong>结论：</strong>因为YOLO在测试时犯了各种错误，所以它在提高快速R-CNN的性能方面非常有效。但是这种组合并不受益于YOLO的速度，由于YOLO很快，和Fast R-CNN相比，它不增加任何有意义的计算时间。</p><h4 id="4-4-VOC-2012-Results—VOC-2012结果"><a href="#4-4-VOC-2012-Results—VOC-2012结果" class="headerlink" title="4.4 VOC 2012 Results—VOC 2012结果"></a>4.4 VOC 2012 Results—VOC 2012结果</h4><p>在VOC 2012测试集中，YOLO的mAp得分是57.9％。这比现有最先进的技术水平低，更接近使用VGG-16的原始的R-CNN，见表3。与其最接近的竞争对手相比，我们的系统很难处理小物体上(struggles with small objects)。在瓶子、羊、电视/监视器等类别上，YOLO得分比R-CNN和Feature Edit低8-10％。然而，在其他类别，如猫和火车YOLO取得了更好的表现。</p><p>我们的Fast R-CNN + YOLO模型组合是性能最高的检测方法之一。 Fast R-CNN与YOLO的组合提高了2.3％，在公共排行榜上提升了5个位置。</p><blockquote><p>Table 3 在VOC2012上mAP排序</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-VOC2012%E4%B8%8AmAP%E6%8E%92%E5%BA%8F.png" alt=""></p><p><strong>结论：</strong>Fast R-CNN从与YOLO的组合中得到2.3%的改进，在公共排行榜上提升了5个百分点。</p></blockquote><h4 id="4-5-Generalizability-Person-Detection-in-Artwork—泛化性：图像中的人物检测"><a href="#4-5-Generalizability-Person-Detection-in-Artwork—泛化性：图像中的人物检测" class="headerlink" title="4.5 Generalizability: Person Detection in Artwork—泛化性：图像中的人物检测"></a>4.5 Generalizability: Person Detection in Artwork—泛化性：图像中的人物检测</h4><p>用于目标检测的学术数据集的训练和测试数据是<strong>服从同一分布</strong>的。但在现实世界的应用中，很难预测所有可能的用例，他的测试数据可能与系统已经看到的不同[3]。我们将YOLO与其他检测系统在毕加索(Picasso)数据集[12]和人物艺术(People-Art)数据集[3]上进行了比较，这两个数据集用于测试艺术品上的人物检测。</p><p>作为参考(for reference)，我们提供了VOC 2007的人形检测的AP，其中所有模型仅在VOC 2007数据上训练。在Picasso数据集上测试的模型在是在VOC 2012上训练，而People-Art数据集上的模型则在VOC 2010上训练。</p><p>R-CNN在VOC 2007上有很高的AP值。然而，当应用于艺术图像时，R-CNN显着下降。R-CNN使用选择性搜索来调整自然图像的候选边界框。R-CNN在分类器阶段只能看到小区域，而且需要有很好的候选框。</p><p>DPM在应用于艺术图像时可以很好地保持其AP。之前的研究认为DPM表现良好，因为它具有强大的物体形状和布局空间模型。虽然DPM不会像R-CNN那样退化，但它的AP本来就很低。</p><p>YOLO在VOC 2007上表现出色，其应用于艺术图像时其AP降低程度低于其他方法。与DPM一样，YOLO模拟目标的大小和形状，以及目标之间的关系和目标通常出现的位置之间的关系。艺术图像和自然图像在像素级别上有很大不同，但它们在物体的大小和形状方面相似，因此YOLO仍然可以预测好的边界框和检测结果。</p><blockquote><p>Figure 5 通用性（Picasso 数据集和 People-Art数据集）</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E9%80%9A%E7%94%A8%E6%80%A7%EF%BC%88Picasso%20%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%20People-Art%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%89.png" alt=""></p><p><strong>结论：</strong>YOLO都具有很好的检测结果</p></blockquote><h3 id="五、Real-Time-Detection-In-The-Wild—自然环境下的实时检测"><a href="#五、Real-Time-Detection-In-The-Wild—自然环境下的实时检测" class="headerlink" title="五、Real-Time Detection In The Wild—自然环境下的实时检测"></a><strong>五、Real-Time Detection In The Wild—自然环境下的实时检测</strong></h3><p>YOLO是一款快速，精确的物体检测器，非常适合计算机视觉应用。 我们将YOLO连接到网络摄像头，并验证它是否保持实时性能，包括从摄像头获取图像并显示检测结果的时间。</p><p>由此产生的系统是互动的和参与的。 虽然YOLO单独处理图像，但当连接到网络摄像头时，它的功能类似于跟踪系统，可在目标移动并在外观上发生变化时检测目标。 系统演示和源代码可在我们的项目网站上找到：<a href="http://pjreddie.com/yolo/">YOLO: Real-Time Object Detection</a>。</p><blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E6%95%88%E6%9E%9C.png" alt=""></p><p><strong>结论：</strong>将YOLO连接到一个网络摄像头上，并验证它是否保持了实时性能，包括从摄像头中获取图像和显示检测结果的时间。结果证明效果很好，如上图所示，除了第二行第二个将人误判为飞机以外，别的没问题。</p></blockquote><h3 id="六、Conclusion—结论"><a href="#六、Conclusion—结论" class="headerlink" title="六、Conclusion—结论"></a><strong>六、Conclusion—结论</strong></h3><p>我们介绍YOLO——一种用于物体检测的统一模型。 我们的模型构造简单，可以直接在完整图像上训练。 与基于分类器的方法不同，YOLO是通过与检测性能直接对应的损失函数进行训练的，并且整个模型是一起训练的。</p><p>快速YOLO是文献中最快的通用目标检测器，YOLO推动实时对象检测的最新技术。 YOLO还能很好地推广到新领域，使其成为快速，鲁棒性强的应用的理想选择。</p><blockquote><h4 id="到底什么是YOLO？"><a href="#到底什么是YOLO？" class="headerlink" title="到底什么是YOLO？"></a>到底什么是YOLO？</h4><ul><li>YOLO眼里目标检测是一个回归问题</li><li><p>一次性喂入图片，然后给出bbox和分类概率</p></li><li><p>简单来说，只看一次就知道图中物体的类别和位置</p></li></ul><h4 id="YOLO过程总结："><a href="#YOLO过程总结：" class="headerlink" title="YOLO过程总结："></a>YOLO过程总结：</h4><p><strong>训练阶段:</strong></p><p>首先将一张图像分成 S × S个 gird cell，然后将它一股脑送入CNN，生成S × S × (B × 5 + C）个结果，最后根据结果求Loss并反向传播梯度下降。</p><p><strong>预测、验证阶段：</strong></p><p>首先将一张图像分成 S × S网格(gird cell)，然后将它一股脑送入CNN，生成S × S × (B × 5 + C）个结果，最后用NMS选出合适的预选框。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(16)-深度视觉入门</title>
      <link href="/Pytorch-16/"/>
      <url>/Pytorch-16/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch_receptive_field <span class="keyword">import</span> receptive_field</span><br></pre></td></tr></table></figure><h2 id="一、卷积相关知识"><a href="#一、卷积相关知识" class="headerlink" title="一、卷积相关知识"></a>一、卷积相关知识</h2><h3 id="1-卷积"><a href="#1-卷积" class="headerlink" title="1. 卷积"></a>1. 卷积</h3><ul><li><code>二维矩阵的卷积</code>表示其中一矩阵旋转<code>180°</code>后，与另一个矩阵<code>求点积(依次相乘求和)</code>的结果<ul><li><code>卷</code>就是旋转，<code>积</code>就是点积.</li></ul></li><li>如今不进行旋转这一步,故本质是<code>互相关</code></li><li><strong>相关概念：</strong><ul><li>卷积核</li><li>感受野</li><li>特征图</li></ul></li></ul><h3 id="2-卷积遇见深度学习"><a href="#2-卷积遇见深度学习" class="headerlink" title="2. 卷积遇见深度学习"></a>2. 卷积遇见深度学习</h3><ul><li><code>检测边缘</code>、<code>锐化</code>、<code>模糊</code>、<code>图像降噪</code>等卷积相关的操作，是在从图像中<strong>提取部分信息</strong>，也被叫做<code>“特征提取”技术</code>。</li><li>通过学习寻找卷积核<ul><li><code>感受野</code>与<code>卷积核</code>点积，其操作与<code>DNN中的权重与特征相乘</code>十分相似</li><li><code>卷积层</code>被建立后，<code>卷积核的值</code>会被随机生成，之后就可以被<code>自动学习</code>出来了</li><li>以此来实现<code>自动找出最佳卷积</code>,提取出<code>对分类最有利的特征</code></li></ul></li><li><strong>参数共享</strong>：卷积带来参数量骤减<ul><li>预测效果好，计算量小</li></ul></li><li><strong>稀疏交互</strong>：获取更深入的特征<ul><li><code>DNN</code>中，上层的任意神经元都必须和<code>下层的每个神经元相连</code></li><li><code>CNN</code>中，下层的一个神经元只和<code>上层中被扫描的那些神经元</code>有关</li><li>让其拥有提取<code>更深特征</code>的能力</li></ul></li><li>绝对不是我们认为的<code>先提取细节</code>，<code>再拼接局部</code>，<code>最后组成图像</code></li></ul><h3 id="3-PyTorch构筑CNN"><a href="#3-PyTorch构筑CNN" class="headerlink" title="3. PyTorch构筑CNN"></a><strong>3. </strong>PyTorch<strong>构筑</strong>CNN</h3><div class="table-container"><table><thead><tr><th style="text-align:center">卷积层类型</th><th style="text-align:center">应用数据类型</th><th style="text-align:left">说明</th></tr></thead><tbody><tr><td style="text-align:center">Conv1d</td><td style="text-align:center">时序数据</td></tr><tr><td style="text-align:center">Conv2d</td><td style="text-align:center">图像数据</td></tr><tr><td style="text-align:center">Conv3d</td><td style="text-align:center">视频数据</td></tr></tbody></table></div><h3 id="4-普通卷积的参数"><a href="#4-普通卷积的参数" class="headerlink" title="4. 普通卷积的参数"></a>4. 普通卷积的参数</h3><ul><li><p>CLASS torch.nn.Conv2d (<code>in_channels</code>, <code>out_channels</code>, <code>kernel_size</code>, <code>stride</code>=1, <code>padding</code>=0, <strong>dilation=1</strong>,<strong>groups=1</strong>, bias=True, padding_mode=’zeros’)</p><ul><li>卷积核尺寸<code>kernel_size</code><ul><li>必填, 整数或数组</li><li>最好是<code>正方形(奇数)</code><ul><li>3x3/5x5/7x7</li><li>奇数保证图像的信息是<code>不断的向中心来压缩</code>；若<code>偶数</code>相当于把像素<code>往一个角去压缩</code>,导致<code>失真</code></li></ul></li></ul></li><li>输入与输出:<code>in_channels</code>，<code>out_channels</code><ul><li><strong>in_channels=3/三通道</strong>:不同数值的<code>3个卷积核</code>与<code>3通道</code>卷积后三个新通道相加得<code>1个feature_map</code><ul><li>无论多少通道进来，最终都是<code>1个feature_map</code></li></ul></li><li><strong>out_channels/扫描次数</strong>:得到<code>x个feature_map</code><ul><li>当前层的out_channels就是下一层的in_channels</li></ul></li><li><strong>in_channels*out_channels=卷积核数量</strong></li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">data = torch.ones(size=(<span class="number">10</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)) <span class="comment">#10张尺寸为28*28的、拥有3个通道的图像</span></span><br><span class="line"></span><br><span class="line">conv1 = nn.Conv2d(in_channels = <span class="number">3</span>  <span class="comment"># 全部通道的扫描值被合并</span></span><br><span class="line">                ,out_channels = <span class="number">6</span>  <span class="comment"># 6个卷积核形成6个feature map</span></span><br><span class="line">                 ,kernel_size = <span class="number">3</span>) <span class="comment"># 3x3的卷积核</span></span><br><span class="line">conv2 = nn.Conv2d(in_channels = <span class="number">6</span>  <span class="comment"># 对下一层网络来说，输入的是上层生成的6个feature map</span></span><br><span class="line">                         ,out_channels = <span class="number">4</span> <span class="comment">#全部特征图的扫描值被合并，4个卷积核形成4个新的feature map</span></span><br><span class="line">                 ,kernel_size = <span class="number">3</span>)</span><br><span class="line"><span class="comment">#通常在网络中，我们不会把参数都写出来，只会写成：</span></span><br><span class="line"><span class="comment">#conv1 = nn.Conv2d(3,6,3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看一下通过卷积后的数据结构</span></span><br><span class="line"><span class="built_in">print</span>(conv1(data).shape)  <span class="comment"># torch.Size([10, 6, 26, 26])</span></span><br><span class="line"><span class="built_in">print</span>(conv2(conv1(data)).shape) <span class="comment"># torch.Size([10, 4, 24, 24])</span></span><br></pre></td></tr></table></figure><ul><li><p>偏差bias</p><ul><li>整数或数组(1)</li><li><strong>布尔值</strong>，<code>&quot;True&quot;</code>则代表在卷积层中<code>使用偏置</code>，反之则不使用偏置</li></ul></li><li><p>步长: <code>stride</code></p><ul><li>加速对特征图的扫描，并加速缩小特征图，令计算更快</li></ul></li><li><p>填充:<code>padding</code></p><ul><li><p>整数或数组(0)</p></li><li><p>扫描不均衡(有多有少扫)</p></li><li><p>扫描不完全(边缘被舍弃)</p></li></ul></li><li><p>填充模式:<code>padding_mode</code></p><ul><li><p><code>zero_padding</code>:零填充</p></li><li><p><code>circular</code>:环形填充</p></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 感受特征图尺寸的变化</span></span><br><span class="line">data = torch.ones(size=(<span class="number">10</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)) <span class="comment">#10张尺寸为28*28的、拥有3个通道的图像</span></span><br><span class="line">conv1 = nn.Conv2d(<span class="number">3</span>,<span class="number">6</span>,<span class="number">3</span>) </span><br><span class="line">conv2 = nn.Conv2d(<span class="number">6</span>,<span class="number">4</span>,<span class="number">3</span>) </span><br><span class="line">conv3 = nn.Conv2d(<span class="number">4</span>,<span class="number">16</span>,<span class="number">5</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>) </span><br><span class="line">conv4 = nn.Conv2d(<span class="number">16</span>,<span class="number">3</span>,<span class="number">5</span>,stride=<span class="number">3</span>,padding=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#conv1，输入结构28*28</span></span><br><span class="line"><span class="comment">#(28 + 0 - 3)/1 + 1 = 26</span></span><br><span class="line"><span class="comment">#验证一下</span></span><br><span class="line"><span class="built_in">print</span>(conv1(data).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#conv2，输入结构26*26</span></span><br><span class="line"><span class="comment">#(26 + 0 - 3)/1 + 1 = 24</span></span><br><span class="line"><span class="comment">#验证</span></span><br><span class="line"><span class="built_in">print</span>(conv2(conv1(data)).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#conv3，输入结构24*24</span></span><br><span class="line"><span class="comment">#(24 + 2 - 5)/2 + 1 = 11，扫描不完全的部分会被舍弃</span></span><br><span class="line"><span class="built_in">print</span>(conv3(conv2(conv1(data))).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#conv4，输入结构11*11</span></span><br><span class="line"><span class="comment">#(11 + 4 - 5)/3 + 1 = 4.33，扫描不完全的部分会被舍弃</span></span><br><span class="line"><span class="built_in">print</span>(conv4(conv3(conv2(conv1(data)))).shape)</span><br></pre></td></tr></table></figure><h3 id="5-池化层"><a href="#5-池化层" class="headerlink" title="5. 池化层"></a>5. 池化层</h3><ul><li><strong>最大池化</strong>(Max Pooling)/nn.MaxPool</li><li><strong>平局池化</strong>(Avg Pooling)/nn.AvgPool</li><li>特点：<ul><li>提供<code>非线性变化</code></li><li>有一定的<code>平移不变性</code></li><li>池化层<code>不会增加参数量</code></li><li><code>统一规律</code>对所有的feature_map<code>一次降维</code></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">data = torch.ones(size=(<span class="number">10</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line"></span><br><span class="line">conv1 = nn.Conv2d(<span class="number">3</span>,<span class="number">6</span>,<span class="number">3</span>) <span class="comment">#(28 + 0 - 3)/1 + 1 = 26</span></span><br><span class="line">conv3 = nn.Conv2d(<span class="number">6</span>,<span class="number">16</span>,<span class="number">5</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>) <span class="comment"># (26 + 2 - 5)/2 +1 = 12</span></span><br><span class="line">pool1 = nn.MaxPool2d(<span class="number">2</span>) <span class="comment">#唯一需要输入的参数，kernel_size=2，则默认使用(2,2)结构的核</span></span><br><span class="line"><span class="comment"># (12 + 0 - 2)/2 + 1 =6</span></span><br><span class="line"><span class="comment">#验证一下</span></span><br><span class="line">pool1(conv3(conv1(data))).shape  <span class="comment"># torch.Size([10, 16, 6, 6])</span></span><br></pre></td></tr></table></figure><h3 id="6-Droupout2d与BatchNorm2d"><a href="#6-Droupout2d与BatchNorm2d" class="headerlink" title="6. Droupout2d与BatchNorm2d"></a>6. Droupout2d与BatchNorm2d</h3><ul><li><p>控制过拟合，提升模型的泛化能力</p></li><li><p>CLASS torch.nn.BatchNorm2d (num_features, eps=1e-05, momentum=0.1, affine=True,track_running_stats=True)</p><ul><li><p>要填写的参数<code>几乎只有num_features</code></p></li><li><p><code>输入神经元</code>的个数/<code>上一层特征图</code>的数量</p></li><li><p><strong><em>**</em></strong>不改变feature_map的数量<strong>*</strong>***</p></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = torch.ones(size=(<span class="number">10</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line">conv1 = nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,padding=<span class="number">2</span>)  <span class="comment"># 28</span></span><br><span class="line">bn1 = nn.BatchNorm2d(<span class="number">32</span>) </span><br><span class="line">bn1(conv1(data)).shape <span class="comment">#不会改变feature map的形状 torch.Size([10, 32, 28, 28])</span></span><br></pre></td></tr></table></figure><ul><li><strong>CLASS torch.nn.Dropout2d (p=0.5, inplace=False)</strong><ul><li>Dropout层本身<code>不带有任何需要学习的参数</code>，因此<code>不会影响参数量。</code></li><li>留下的神经元数值会由此除以<code>(1-p)</code></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = torch.ones(size=(<span class="number">10</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line">conv1 = nn.Conv2d(<span class="number">1</span>,<span class="number">32</span>,<span class="number">5</span>,padding=<span class="number">2</span>)</span><br><span class="line">dp1 = nn.Dropout2d(<span class="number">0.5</span>)</span><br><span class="line">dp1(conv1(data)).shape <span class="comment">#不会改变feature map的形状  torch.Size([10, 32, 28, 28])</span></span><br></pre></td></tr></table></figure><h2 id="二、经典网络"><a href="#二、经典网络" class="headerlink" title="二、经典网络"></a>二、经典网络</h2><h3 id="1-LeNet-5—1998"><a href="#1-LeNet-5—1998" class="headerlink" title="1. LeNet-5—1998"></a>1. LeNet-5—1998</h3><p>LeNet-5是LeNet系列的最终稳定版，它被美国银行用于手写数字识别，该网络有以下特点：</p><ul><li>所有卷积核均为5x5，步长为1</li><li>所有池化方式为平均池化</li><li>所有激活函数采用Sigmoid</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/lenet5&#39;snet.png" alt=""></p><h4 id="1-1-net-py"><a href="#1-1-net-py" class="headerlink" title="1.1 net.py"></a>1.1 net.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个网络模型类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyLeNet5</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 初始化网络</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyLeNet5, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>, padding=<span class="number">2</span>)  <span class="comment"># 6,28,28</span></span><br><span class="line">        self.pool1 = nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)  <span class="comment"># 6,14,14</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)  <span class="comment"># 16,10,10</span></span><br><span class="line">        self.pool2 = nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)  <span class="comment"># 16,5,5</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)  <span class="comment"># 假设有10个类别</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.sigmoid(self.conv1(x))</span><br><span class="line">        x = self.pool1(x)</span><br><span class="line">        x = F.sigmoid(self.conv2(x))</span><br><span class="line">        x = self.pool2(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x = torch.rand((<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))  <span class="comment"># 或者您可以调整为32x32的输入，如果MNIST数据集已经是28x28，则可以保持这个尺寸</span></span><br><span class="line">    model = MyLeNet5()</span><br><span class="line">    y = model(x)</span><br><span class="line">    <span class="built_in">print</span>(y)  <span class="comment"># 这将打印出未经softmax处理的logits</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="1-2-train-py"><a href="#1-2-train-py" class="headerlink" title="1.2 train.py"></a>1.2 train.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> main <span class="keyword">import</span> MyLeNet5</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换为Tensor格式</span></span><br><span class="line">data_transforms = transforms.Compose([transforms.ToTensor()])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载训练数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>,train=<span class="literal">True</span>,transform=data_transforms,download=<span class="literal">True</span>)</span><br><span class="line">train_dataloader = torch.utils.data.DataLoader(dataset =train_dataset, batch_size = <span class="number">16</span>, shuffle = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载测试数据集</span></span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>,train=<span class="literal">False</span>,transform=data_transforms,download=<span class="literal">True</span>)</span><br><span class="line">test_dataloader = torch.utils.data.DataLoader(dataset =test_dataset, batch_size = <span class="number">16</span>, shuffle = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># GPU训练</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用网络模型</span></span><br><span class="line">model = MyLeNet5().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义交叉熵损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optm = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率每隔10轮，变为原来的0.1</span></span><br><span class="line">lrs = lr_scheduler.StepLR(optm,step_size=<span class="number">10</span>,gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">dataloader,model,loss_fn,optm</span>):</span><br><span class="line">    loss,accuracy,n = <span class="number">0.0</span>, <span class="number">0.0</span> ,<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batchm,(X,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        X,y = X.to(device),y.to(device)</span><br><span class="line">        output = model(X)</span><br><span class="line">        cur_loss = loss_fn(output,y)</span><br><span class="line">        _,pred  = torch.<span class="built_in">max</span>(output,axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        cur_accuracy= torch.<span class="built_in">sum</span>(y==pred)/output.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        optm.zero_grad()</span><br><span class="line">        cur_loss.backward()</span><br><span class="line">        optm.step()</span><br><span class="line"></span><br><span class="line">        loss += cur_loss.item()</span><br><span class="line">        accuracy += cur_accuracy</span><br><span class="line">        n = n+<span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;train_loss&quot;</span> + <span class="built_in">str</span>(loss/n))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;train_acc&quot;</span> + <span class="built_in">str</span>(accuracy / n))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">val</span>(<span class="params">dataloader,model,loss_fn</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    loss,accuracy,n = <span class="number">0.0</span>, <span class="number">0.0</span> ,<span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> batchm, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            output = model(X)</span><br><span class="line">            cur_loss = loss_fn(output, y)</span><br><span class="line">            _, pred = torch.<span class="built_in">max</span>(output, axis=<span class="number">1</span>)</span><br><span class="line">            cur_accuracy = torch.<span class="built_in">sum</span>(pred == y).item() / y.size(<span class="number">0</span>)</span><br><span class="line">            loss += cur_loss.item()</span><br><span class="line">            accuracy += cur_accuracy</span><br><span class="line">            n = n + <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;val_loss&quot;</span> + <span class="built_in">str</span>(loss / n))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;val_acc&quot;</span> + <span class="built_in">str</span>(accuracy / n))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> accuracy/n</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line">epoch = <span class="number">50</span></span><br><span class="line">min_acc = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;epoch<span class="subst">&#123;t+<span class="number">1</span>&#125;</span>\n-----------&quot;</span>)</span><br><span class="line">    train(train_dataloader,model,loss_fn,optm)</span><br><span class="line">    a = val(test_dataloader,model,loss_fn)</span><br><span class="line">    <span class="comment"># 保存最好的模型权重</span></span><br><span class="line">    <span class="keyword">if</span> a &gt; min_acc:</span><br><span class="line">        folder = <span class="string">&#x27;save_model&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(folder):</span><br><span class="line">            os.mkdir(folder)</span><br><span class="line">        min_acc = a</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;save best model&#x27;</span>)</span><br><span class="line">        torch.save(model.state_dict(),<span class="string">&#x27;save_model/best_model.pth&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Done&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="1-3-test-py"><a href="#1-3-test-py" class="headerlink" title="1.3 test.py"></a>1.3 test.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> main <span class="keyword">import</span> MyLeNet5</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToPILImage</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转化为tensor格式</span></span><br><span class="line">data_transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载训练数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=data_transform, download=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 给训练集创建一个数据加载器, shuffle=True用于打乱数据集，每次都会以不同的顺序返回。</span></span><br><span class="line">train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=<span class="number">16</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 加载训练数据集</span></span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=data_transform, download=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 给训练集创建一个数据加载器, shuffle=True用于打乱数据集，每次都会以不同的顺序返回。</span></span><br><span class="line">test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=<span class="number">16</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#  如果显卡可用，则用显卡进行训练</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用net里面定义的模型，如果GPU可用则将模型转到GPU</span></span><br><span class="line">model = MyLeNet5().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 train.py 里训练好的模型</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">r&quot;D:\Desktop\QNJS\Model\LeNet-5\save_model\best_model.pth&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取预测结果</span></span><br><span class="line">classes = [</span><br><span class="line">    <span class="string">&quot;0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;1&quot;</span>,</span><br><span class="line">    <span class="string">&quot;2&quot;</span>,</span><br><span class="line">    <span class="string">&quot;3&quot;</span>,</span><br><span class="line">    <span class="string">&quot;4&quot;</span>,</span><br><span class="line">    <span class="string">&quot;5&quot;</span>,</span><br><span class="line">    <span class="string">&quot;6&quot;</span>,</span><br><span class="line">    <span class="string">&quot;7&quot;</span>,</span><br><span class="line">    <span class="string">&quot;8&quot;</span>,</span><br><span class="line">    <span class="string">&quot;9&quot;</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把tensor转成Image， 方便可视化</span></span><br><span class="line">show = ToPILImage()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入验证阶段</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># 对test_dataset里10000张手写数字图片进行推理</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    x, y = test_dataset[i][<span class="number">0</span>], test_dataset[i][<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># tensor格式数据可视化</span></span><br><span class="line">    show(x).show()</span><br><span class="line">    <span class="comment"># 扩展张量维度为4维</span></span><br><span class="line">    x = Variable(torch.unsqueeze(x, dim=<span class="number">0</span>).<span class="built_in">float</span>(), requires_grad=<span class="literal">False</span>).to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        pred = model(x)</span><br><span class="line">        <span class="comment"># 得到预测类别中最高的那一类，再把最高的这一类对应classes中的哪一类标签</span></span><br><span class="line">        predicted, actual = classes[torch.argmax(pred[<span class="number">0</span>])], classes[y]</span><br><span class="line">        <span class="comment"># 最终输出的预测值与真实值</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;predicted: &quot;<span class="subst">&#123;predicted&#125;</span>&quot;, actual:&quot;<span class="subst">&#123;actual&#125;</span>&quot;&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="2-AlexNet—2012"><a href="#2-AlexNet—2012" class="headerlink" title="2. AlexNet—2012"></a>2. AlexNet—2012</h3><ul><li>AlexNet证明了卷积神经网络特征提取远胜于人工特征，将ILSVRC错误率从25.8%降至15.3%。</li><li>输入→(卷积+池化)→(卷积+池化)→(卷积x3+池化)→(线性x3)→输出</li><li>使用小卷积核、多通道和深层网络，引导后续CNN发展。</li><li>采用ReLU激活函数，解决Sigmoid和Tanh的问题。<ul><li><strong>梯度消失问题</strong>：<ul><li>对于 Sigmoid 和 Tanh 函数，当输入值过大或过小时，梯度趋近于零。</li><li>在深层网络中导致权重更新缓慢，学习过程变得低效。</li></ul></li><li><strong>非零中心化问题</strong>：<ul><li>Sigmoid 函数输出严格为正，非零中心化，导致优化过程复杂化。</li><li>Tanh 虽零中心化，但同样受梯度消失影响。</li></ul></li><li><strong>ReLU 激活函数的优势：</strong><ul><li>ReLU 在正数部分梯度恒为 1，避免梯度消失，适用于深层网络。</li><li>ReLU 计算简单，运算效率高于 Sigmoid 和 Tanh。</li><li>ReLU 存在“死亡ReLU”问题（输入负时输出为零），但总体性能优于 Sigmoid 和 Tanh。</li></ul></li></ul></li><li>引入Dropout控制过拟合；图像增强技术扩大数据集，进一步减轻过拟合。</li><li>GPU加速训练，实现有效训练。</li><li>小步幅池化产生重叠区域，有助于缓解过拟合。</li></ul><h4 id="2-1-AlexNet-py"><a href="#2-1-AlexNet-py" class="headerlink" title="2.1 AlexNet.py"></a>2.1 AlexNet.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyAlexNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyAlexNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>)  <span class="comment"># [None, 3, 224, 224] --&gt; [None, 96, 55, 55]</span></span><br><span class="line">        self.pool1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>)  <span class="comment"># [None, 96, 55, 55] --&gt; [None, 96, 27, 27]</span></span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">96</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)  <span class="comment"># [None, 96, 27, 27] --&gt; [None, 256, 27, 27]</span></span><br><span class="line">        self.pool2 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)  <span class="comment"># [None, 256, 27, 27] --&gt; [None, 256, 13, 13]</span></span><br><span class="line">        self.conv3 = nn.Conv2d(in_channels=<span class="number">256</span>, out_channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)  <span class="comment"># [None, 256, 13, 13] --&gt; [None, 384, 13, 13]</span></span><br><span class="line">        self.conv4 = nn.Conv2d(in_channels=<span class="number">384</span>, out_channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)  <span class="comment"># [None, 384, 13, 13] --&gt; [None, 384, 13, 13]</span></span><br><span class="line">        self.conv5 = nn.Conv2d(in_channels=<span class="number">384</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)  <span class="comment"># [None, 384, 13, 13] --&gt; [None, 256, 13, 13]</span></span><br><span class="line">        self.pool3 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)  <span class="comment"># [None, 256, 13, 13] --&gt; [None, 256, 6, 6]</span></span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">256</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">2048</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">2048</span>, <span class="number">2048</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">2048</span>, <span class="number">1000</span>)</span><br><span class="line">        self.fc4 = nn.Linear(<span class="number">1000</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = self.pool1(x)</span><br><span class="line"></span><br><span class="line">        x = F.relu(self.conv2(x))</span><br><span class="line">        x = self.pool2(x)</span><br><span class="line"></span><br><span class="line">        x = F.relu(self.conv3(x))</span><br><span class="line">        x = F.relu(self.conv4(x))</span><br><span class="line">        x = F.relu(self.conv5(x))</span><br><span class="line">        x = self.pool3(x)</span><br><span class="line"></span><br><span class="line">        x = x.view(-<span class="number">1</span>,<span class="number">256</span>*<span class="number">6</span>*<span class="number">6</span>)</span><br><span class="line">        x = F.relu(F.dropout(self.fc1(x), <span class="number">0.5</span>))</span><br><span class="line">        x = F.relu(F.dropout(self.fc2(x), <span class="number">0.5</span>))</span><br><span class="line">        x = F.relu(F.dropout(self.fc3(x), <span class="number">0.5</span>))</span><br><span class="line">        x = self.fc4(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x = torch.rand([<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>])</span><br><span class="line">    model = MyAlexNet()</span><br><span class="line">    summary(model, input_size=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br></pre></td></tr></table></figure><h4 id="2-2-split-data-py"><a href="#2-2-split-data-py" class="headerlink" title="2.2 split_data.py"></a>2.2 split_data.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os  <span class="comment"># 导入os模块，用于处理文件和目录</span></span><br><span class="line"><span class="keyword">from</span> shutil <span class="keyword">import</span> copy  <span class="comment"># 从shutil模块导入copy函数，用于复制文件</span></span><br><span class="line"><span class="keyword">import</span> random  <span class="comment"># 导入random模块，用于生成随机数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个函数，用于创建不存在的目录</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mkfile</span>(<span class="params">file</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file):</span><br><span class="line">        os.makedirs(file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取data文件夹下所有子文件夹的名称（即分类的类名）</span></span><br><span class="line">file_path = <span class="string">r&#x27;D:\Desktop\QNJS\Model\AlexNet\data_name&#x27;</span></span><br><span class="line">flower_class = [cla <span class="keyword">for</span> cla <span class="keyword">in</span> os.listdir(file_path)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建训练集 train 文件夹，并在其下为每个类别创建子目录</span></span><br><span class="line">mkfile(<span class="string">&#x27;data/train&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> cla <span class="keyword">in</span> flower_class:</span><br><span class="line">    mkfile(<span class="string">&#x27;data/train/&#x27;</span> + cla)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建验证集 val 文件夹，并在其下为每个类别创建子目录</span></span><br><span class="line">mkfile(<span class="string">&#x27;data/val&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> cla <span class="keyword">in</span> flower_class:</span><br><span class="line">    mkfile(<span class="string">&#x27;data/val/&#x27;</span> + cla)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置划分比例，训练集 : 验证集 = 8 : 2</span></span><br><span class="line">split_rate = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历每个类别并按比例分割成训练集和验证集</span></span><br><span class="line"><span class="keyword">for</span> cla <span class="keyword">in</span> flower_class:</span><br><span class="line">    cla_path = file_path + <span class="string">&#x27;/&#x27;</span> + cla + <span class="string">&#x27;/&#x27;</span>  <span class="comment"># 获取某一类别的完整路径</span></span><br><span class="line">    images = os.listdir(cla_path)  <span class="comment"># 列出该路径下的所有图像</span></span><br><span class="line">    num = <span class="built_in">len</span>(images)  <span class="comment"># 计算图像总数</span></span><br><span class="line">    eval_index = random.sample(images, k=<span class="built_in">int</span>(num * split_rate))  <span class="comment"># 随机选择部分图像作为验证集(1/5)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历该类别的所有图像，进行分类处理</span></span><br><span class="line">    <span class="keyword">for</span> index, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">        <span class="comment"># 如果图像在验证集中，则复制到验证集目录</span></span><br><span class="line">        <span class="keyword">if</span> image <span class="keyword">in</span> eval_index:</span><br><span class="line">            image_path = cla_path + image</span><br><span class="line">            new_path = <span class="string">&#x27;data/val/&#x27;</span> + cla</span><br><span class="line">            copy(image_path, new_path)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 否则，复制到训练集目录</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            image_path = cla_path + image</span><br><span class="line">            new_path = <span class="string">&#x27;data/train/&#x27;</span> + cla</span><br><span class="line">            copy(image_path, new_path)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印处理进度</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\r[&#123;&#125;] processing [&#123;&#125;/&#123;&#125;]&quot;</span>.<span class="built_in">format</span>(cla, index + <span class="number">1</span>, num), end=<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 当一个类别处理完毕后换行</span></span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有类别处理完毕，打印完成信息</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;processing done!&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="2-3-train-py"><a href="#2-3-train-py" class="headerlink" title="2.3 train.py"></a>2.3 train.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入所需的库</span></span><br><span class="line"><span class="keyword">import</span> torch  <span class="comment"># PyTorch 深度学习框架</span></span><br><span class="line"><span class="keyword">import</span> os  <span class="comment"># 用于操作文件和目录</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  <span class="comment"># PyTorch 的神经网络模块</span></span><br><span class="line"><span class="keyword">from</span> AlexNet <span class="keyword">import</span> MyAlexNet  <span class="comment"># 从 AlexNet 文件导入 MyAlexNet 类</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler  <span class="comment"># 用于调整学习率的工具</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms  <span class="comment"># PyTorch 图像转换工具</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader  <span class="comment"># 用于创建数据加载器的工具</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  <span class="comment"># 用于绘图的库</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> ImageFolder</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> UnidentifiedImageError</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 Matplotlib 绘图时的字体和字符处理</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 设置字体为 SimHei 以支持中文显示</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 正常显示负号</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练数据集和验证数据集的根目录路径</span></span><br><span class="line">ROOT_TRAIN = <span class="string">r&#x27;D:\Desktop\QNJS\Model\AlexNet\data\train&#x27;</span>  <span class="comment"># 训练集路径</span></span><br><span class="line">ROOT_TEST = <span class="string">r&#x27;D:\Desktop\QNJS\Model\AlexNet\data\val&#x27;</span>  <span class="comment"># 验证集路径</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义图像归一化转换</span></span><br><span class="line">normalize = transforms.Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练数据的转换流程</span></span><br><span class="line">train_transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),  <span class="comment"># 调整图像大小为 224x224</span></span><br><span class="line">    transforms.RandomVerticalFlip(),  <span class="comment"># 随机垂直翻转图像</span></span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 将图像转换为 Tensor</span></span><br><span class="line">    normalize])  <span class="comment"># 应用归一化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义验证数据的转换流程</span></span><br><span class="line">val_transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),  <span class="comment"># 调整图像大小为 224x224</span></span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 将图像转换为 Tensor</span></span><br><span class="line">    normalize])  <span class="comment"># 应用归一化</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义自定义的图像数据集类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomImageFolder</span>(<span class="title class_ inherited__">ImageFolder</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">super</span>(CustomImageFolder, self).__getitem__(index)</span><br><span class="line">        <span class="keyword">except</span> UnidentifiedImageError:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Unidentified image at index <span class="subst">&#123;index&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> self[<span class="number">0</span>]  <span class="comment"># 返回数据集中的第一个图像作为替代</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用自定义的图像数据集类来加载训练和验证数据集</span></span><br><span class="line">train_dataset = CustomImageFolder(ROOT_TRAIN, transform=train_transform)</span><br><span class="line">val_dataset = CustomImageFolder(ROOT_TEST, transform=val_transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据加载器</span></span><br><span class="line">train_dataloader = DataLoader(train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">val_dataloader = DataLoader(val_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检测是否有可用的 GPU，否则使用 CPU</span></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化 MyAlexNet 模型并将其移动到设备上（GPU 或 CPU）</span></span><br><span class="line">model = MyAlexNet().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义交叉熵损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器（随机梯度下降）</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义学习率调度器，每10轮降低学习率</span></span><br><span class="line">lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=<span class="number">10</span>, gamma=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">dataloader, model, loss_fn, optimizer</span>):</span><br><span class="line">    loss, current, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch, (x, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        image, y = x.to(device), y.to(device)  <span class="comment"># 将数据移动到相应的设备</span></span><br><span class="line">        output = model(image)  <span class="comment"># 获取模型输出</span></span><br><span class="line">        cur_loss = loss_fn(output, y)  <span class="comment"># 计算损失</span></span><br><span class="line">        _, pred = torch.<span class="built_in">max</span>(output, axis=<span class="number">1</span>)  <span class="comment"># 获取预测结果</span></span><br><span class="line">        cur_acc = torch.<span class="built_in">sum</span>(y == pred) / output.shape[<span class="number">0</span>]  <span class="comment"># 计算准确率</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 清除之前的梯度</span></span><br><span class="line">        cur_loss.backward()  <span class="comment"># 反向传播计算当前的梯度</span></span><br><span class="line">        optimizer.step()  <span class="comment"># 更新模型参数</span></span><br><span class="line">        loss += cur_loss.item()  <span class="comment"># 累加损失</span></span><br><span class="line">        current += cur_acc.item()  <span class="comment"># 累加准确率</span></span><br><span class="line">        n += <span class="number">1</span>  <span class="comment"># 计数增加</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算平均损失和准确率</span></span><br><span class="line">    train_loss = loss / n</span><br><span class="line">    train_acc = current / n</span><br><span class="line">    <span class="comment"># 输出训练的损失和准确率</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;train_loss&#x27;</span> + <span class="built_in">str</span>(train_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;train_acc&#x27;</span> + <span class="built_in">str</span>(train_acc))</span><br><span class="line">    <span class="keyword">return</span> train_loss, train_acc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个验证函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">val</span>(<span class="params">dataloader, model, loss_fn</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式</span></span><br><span class="line">    loss, current, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 在不跟踪梯度的情况下执行前向传播</span></span><br><span class="line">        <span class="keyword">for</span> batch, (x, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">            image, y = x.to(device), y.to(device)  <span class="comment"># 将数据移动到相应的设备</span></span><br><span class="line">            output = model(image)  <span class="comment"># 获取模型输出</span></span><br><span class="line">            cur_loss = loss_fn(output, y)  <span class="comment"># 计算损失</span></span><br><span class="line">            _, pred = torch.<span class="built_in">max</span>(output, axis=<span class="number">1</span>)  <span class="comment"># 获取预测结果</span></span><br><span class="line">            cur_acc = torch.<span class="built_in">sum</span>(y == pred) / output.shape[<span class="number">0</span>]  <span class="comment"># 计算准确率</span></span><br><span class="line">            loss += cur_loss.item()  <span class="comment"># 累加损失</span></span><br><span class="line">            current += cur_acc.item()  <span class="comment"># 累加准确率</span></span><br><span class="line">            n += <span class="number">1</span>  <span class="comment"># 计数增加</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算平均损失和准确率</span></span><br><span class="line">    val_loss = loss / n</span><br><span class="line">    val_acc = current / n</span><br><span class="line">    <span class="comment"># 输出验证的损失和准确率</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;val_loss&#x27;</span> + <span class="built_in">str</span>(val_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;val_acc&#x27;</span> + <span class="built_in">str</span>(val_acc))</span><br><span class="line">    <span class="keyword">return</span> val_loss, val_acc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义绘制损失和准确率图的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matplot_loss</span>(<span class="params">train_loss, val_loss</span>):</span><br><span class="line">    plt.plot(train_loss, label=<span class="string">&#x27;train_loss&#x27;</span>)</span><br><span class="line">    plt.plot(val_loss, label=<span class="string">&#x27;val_loss&#x27;</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;训练集和验证集loss值对比图&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matplot_acc</span>(<span class="params">train_acc, val_acc</span>):</span><br><span class="line">    plt.plot(train_acc, label=<span class="string">&#x27;train_acc&#x27;</span>)</span><br><span class="line">    plt.plot(val_acc, label=<span class="string">&#x27;val_acc&#x27;</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;acc&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;训练集和验证集acc值对比图&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练过程</span></span><br><span class="line">loss_train = []</span><br><span class="line">acc_train = []</span><br><span class="line">loss_val = []</span><br><span class="line">acc_val = []</span><br><span class="line"></span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line">min_acc = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;epoch<span class="subst">&#123;t + <span class="number">1</span>&#125;</span>\n-----------&quot;</span>)</span><br><span class="line">    train_loss, train_acc = train(train_dataloader, model, loss_fn, optimizer)</span><br><span class="line">    val_loss, val_acc = val(val_dataloader, model, loss_fn)</span><br><span class="line">    lr_scheduler.step()  <span class="comment"># 更新学习率</span></span><br><span class="line"></span><br><span class="line">    loss_train.append(train_loss)</span><br><span class="line">    acc_train.append(train_acc)</span><br><span class="line">    loss_val.append(val_loss)</span><br><span class="line">    acc_val.append(val_acc)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存最好的模型权重</span></span><br><span class="line">    <span class="keyword">if</span> val_acc &gt; min_acc:</span><br><span class="line">        folder = <span class="string">&#x27;save_model&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(folder):</span><br><span class="line">            os.mkdir(<span class="string">&#x27;save_model&#x27;</span>)</span><br><span class="line">        min_acc = val_acc</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;save best model, 第<span class="subst">&#123;t+<span class="number">1</span>&#125;</span>轮&quot;</span>)</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">&#x27;save_model/best_model.pth&#x27;</span>)</span><br><span class="line">    <span class="comment"># 保存最后一轮的权重文件</span></span><br><span class="line">    <span class="keyword">if</span> t == epoch-<span class="number">1</span>:</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">&#x27;save_model/last_model.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制损失和准确率图</span></span><br><span class="line">matplot_loss(loss_train, loss_val)  <span class="comment"># 绘制训练集和验证集的损失对比图</span></span><br><span class="line">matplot_acc(acc_train, acc_val)  <span class="comment"># 绘制训练集和验证集的准确率对比图</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Done!&#x27;</span>)  <span class="comment"># 训练完成后打印完成信息</span></span><br></pre></td></tr></table></figure><h4 id="2-4-test-py"><a href="#2-4-test-py" class="headerlink" title="2.4 test.py"></a>2.4 test.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  <span class="comment"># 导入 PyTorch 库</span></span><br><span class="line"><span class="keyword">from</span> AlexNet <span class="keyword">import</span> MyAlexNet  <span class="comment"># 从 AlexNet 文件导入 MyAlexNet 类</span></span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable  <span class="comment"># 导入用于创建可微分变量的工具</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms  <span class="comment"># 导入图像数据集和转换工具</span></span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, ToPILImage  <span class="comment"># 导入转换为Tensor和PIL Image的工具</span></span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> ImageFolder  <span class="comment"># 导入处理文件夹数据集的工具</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader  <span class="comment"># 导入创建数据加载器的工具</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练集和验证集的文件路径</span></span><br><span class="line">ROOT_TRAIN = <span class="string">r&#x27;D:\Desktop\QNJS\Model\AlexNet\data\train&#x27;</span></span><br><span class="line">ROOT_TEST = <span class="string">r&#x27;D:\Desktop\QNJS\Model\AlexNet\data\val&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义图像归一化转换</span></span><br><span class="line">normalize = transforms.Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练数据的转换流程</span></span><br><span class="line">train_transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),  <span class="comment"># 调整图像大小为 224x224</span></span><br><span class="line">    transforms.RandomVerticalFlip(),  <span class="comment"># 随机垂直翻转图像</span></span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 将图像转换为 Tensor</span></span><br><span class="line">    normalize])  <span class="comment"># 应用归一化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义验证数据的转换流程</span></span><br><span class="line">val_transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),  <span class="comment"># 调整图像大小为 224x224</span></span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 将图像转换为 Tensor</span></span><br><span class="line">    normalize])  <span class="comment"># 应用归一化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 ImageFolder 加载训练和验证数据集</span></span><br><span class="line">train_dataset = ImageFolder(ROOT_TRAIN, transform=train_transform)</span><br><span class="line">val_dataset = ImageFolder(ROOT_TEST, transform=val_transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据加载器</span></span><br><span class="line">train_dataloader = DataLoader(train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">val_dataloader = DataLoader(val_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查是否有可用的 GPU，否则使用 CPU</span></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化 MyAlexNet 模型并将其移动到设备上（GPU 或 CPU）</span></span><br><span class="line">model = MyAlexNet().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载训练好的模型权重</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">r&quot;D:\Desktop\QNJS\Model\AlexNet\save_model\best_model.pth&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义类别</span></span><br><span class="line">classes = [<span class="string">&quot;cat&quot;</span>, <span class="string">&quot;dog&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个将张量转换为 PIL 图像的对象</span></span><br><span class="line">show = ToPILImage()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置模型为评估模式</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 循环10次，获取前10个图像的预测结果</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">13</span>,<span class="number">14</span>):</span><br><span class="line">    x, y = val_dataset[i][<span class="number">0</span>], val_dataset[i][<span class="number">1</span>]  <span class="comment"># 获取图像及其标签</span></span><br><span class="line">    show(x).show()  <span class="comment"># 显示图像</span></span><br><span class="line">    <span class="comment"># 将图像数据转换为模型可以接受的形式</span></span><br><span class="line">    x = Variable(torch.unsqueeze(x, dim=<span class="number">0</span>).<span class="built_in">float</span>(), requires_grad=<span class="literal">True</span>).to(device)</span><br><span class="line">    <span class="comment"># x = torch.tensor(x).to(device)  # 去掉这行</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 在不计算梯度的情况下执行前向传播</span></span><br><span class="line">        pred = model(x)  <span class="comment"># 获取模型预测结果</span></span><br><span class="line">        <span class="comment"># 获取预测和实际类别名称</span></span><br><span class="line">        predicted, actual = classes[torch.argmax(pred[<span class="number">0</span>])], classes[y]</span><br><span class="line">        <span class="comment"># 打印预测和实际类别</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;predicted:&quot;<span class="subst">&#123;predicted&#125;</span>&quot;, Actual:&quot;<span class="subst">&#123;actual&#125;</span>&quot;&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="三、架构对学习能力-鲁棒性的影响"><a href="#三、架构对学习能力-鲁棒性的影响" class="headerlink" title="三、架构对学习能力/鲁棒性的影响"></a>三、架构对学习能力/鲁棒性的影响</h2><ul><li><p>深度学习中的经典架构主要是基本准则，通常无法直接解决具体问题。</p></li><li><p>经典架构可能需大幅修改以适应新数据。</p></li><li>评估模型时考虑效果、效率和可解释性。</li></ul><h3 id="1-深度"><a href="#1-深度" class="headerlink" title="1. 深度"></a>1. 深度</h3><h4 id="1-1-深度与效果"><a href="#1-1-深度与效果" class="headerlink" title="1.1 深度与效果"></a>1.1 <strong>深度与效果</strong></h4><ul><li>深度学习中，更深的网络通常具有更强的学习能力。</li><li>加深模型的层数是提升性能的常用方法。</li></ul><h4 id="1-2-卷积神经网络的深度"><a href="#1-2-卷积神经网络的深度" class="headerlink" title="1.2 卷积神经网络的深度"></a>1.2 卷积神经网络的深度</h4><ul><li>网络深度指带权重层的数量，也是总层数。</li><li>深度越大，参数量和网络规模越大。</li><li>使用“广”来描述特征图数量较多的卷积层。</li></ul><h4 id="1-3-深度的限制因素"><a href="#1-3-深度的限制因素" class="headerlink" title="1.3 深度的限制因素"></a>1.3 深度的限制因素</h4><ul><li>输入图像尺寸限制了网络的最大深度。</li><li>实际可构建的网络深度通常有限。</li></ul><h4 id="1-4-卷积神经网络设计原则"><a href="#1-4-卷积神经网络设计原则" class="headerlink" title="1.4 卷积神经网络设计原则"></a>1.4 卷积神经网络设计原则</h4><ul><li>全连接层(FC)通常较大但层数少，多为1-3层，尺寸不超9x9。</li><li>使用小卷积核(如5x5, 3x3等)以保持信息和减少参数。</li><li>大卷积核可能迅速减少特征图尺寸，限制网络深度。</li><li>池化层的padding应小于池化核尺寸的1/2，否则可能引发错误。</li><li>卷积层的padding通常设小于卷积核尺寸以避免增加无效计算和噪声。</li></ul><h4 id="1-5-特征图尺寸变化策略"><a href="#1-5-特征图尺寸变化策略" class="headerlink" title="1.5  特征图尺寸变化策略"></a>1.5  特征图尺寸变化策略</h4><ul><li><strong>宽高减半或更多</strong>：步长大于2或使用大卷积核时可显著减小特征图。（躺平操作）</li><li><strong>逐层递减架构</strong>：不用池化层，通过卷积层的填充与卷积核尺寸搭配，每次卷积后减小2或4像素。</li><li><strong>重复架构</strong>：通过卷积层保持特征图尺寸不变，将尺寸减小工作交给池化层。</li></ul><h4 id="1-6-架构选择与效率对比"><a href="#1-6-架构选择与效率对比" class="headerlink" title="1.6 架构选择与效率对比"></a>1.6 架构选择与效率对比</h4><ul><li><strong>逐层递减架构</strong>：递减2或4。</li><li><strong>重复架构</strong>：保持特征图尺寸不变，依赖池化层减小尺寸。（3 1/5 2搭配）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E9%80%92%E5%87%8F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%87%8D%E5%A4%8D%E6%9E%B6%E6%9E%84.png" alt=""></p><ul><li>“重复”架构优于“递减”架构，因为计算效率高。在Fashion-MNIST上“重复”架构更稳定、表现更好。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/ISLVRC%E7%BD%91%E7%BB%9C%E5%B1%82%E6%95%B0%E5%8E%86%E5%8F%B2.png" alt=""></p><h4 id="1-7-VGGNet与网络深度"><a href="#1-7-VGGNet与网络深度" class="headerlink" title="1.7 VGGNet与网络深度"></a>1.7 VGGNet与网络深度</h4><ul><li>VGGNet强化学习能力，通过多个卷积层增加深度。</li><li>VGGNet优于小型数据集，未获ILSVRC冠军。</li><li>架构分为五块，每次池化划分新块，“n个卷积+1个池化”为一块。</li></ul><h4 id="1-8-深度与效益"><a href="#1-8-深度与效益" class="headerlink" title="1.8 深度与效益"></a>1.8 深度与效益</h4><ul><li>VGG架构多版本：A、B(VGG13)、C、D(VGG16)、E(VGG19)。</li><li>VGG19效果微小提升，深度增加不保证效果提升。</li><li>准确率提升递减，深度和效果关系非线性。</li><li>参数量大，VGG19达1.43亿，对计算资源要求高。</li><li>有效深度提升需降低成本，2017年前达到约220层。</li><li>探索新方法提高深度与性能，而非单纯加深。</li></ul><h4 id="1-9-VGG16的复现—2014"><a href="#1-9-VGG16的复现—2014" class="headerlink" title="1.9 VGG16的复现—2014"></a>1.9 VGG16的复现—2014</h4><ul><li>输入→（卷积x2+池化）x2 →（卷积x3+池化）x3 → FC层x3 →输出</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/VGG16.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VGG16</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># block1</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.pool1 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># block2</span></span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv4 = nn.Conv2d(<span class="number">128</span>, <span class="number">128</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.pool2 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># block3</span></span><br><span class="line">        self.conv5 = nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv6 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv7 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.pool3 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># block4</span></span><br><span class="line">        self.conv8 = nn.Conv2d(<span class="number">256</span>, <span class="number">512</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv9 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv10 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.pool4 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># block5</span></span><br><span class="line">        self.conv11 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv12 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv13 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.pool5 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># FC层</span></span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>)</span><br><span class="line">        self.linear3 = nn.Linear(<span class="number">4096</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = self.pool1(F.relu(self.conv2(x)))</span><br><span class="line">        x = F.relu(self.conv3(x))</span><br><span class="line">        x = self.pool2(F.relu(self.conv4(x)))</span><br><span class="line">        x = F.relu(self.conv5(x))</span><br><span class="line">        x = F.relu(self.conv6(x))</span><br><span class="line">        x = self.pool3(F.relu(self.conv7(x)))</span><br><span class="line">        x = F.relu(self.conv8(x))</span><br><span class="line">        x = F.relu(self.conv9(x))</span><br><span class="line">        x = self.pool4(F.relu(self.conv10(x)))</span><br><span class="line">        x = F.relu(self.conv11(x))</span><br><span class="line">        x = F.relu(self.conv12(x))</span><br><span class="line">        x = self.pool5(F.relu(self.conv13(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">        x = F.relu(self.linear1(F.dropout(x, p=<span class="number">0.5</span>)))</span><br><span class="line">        x = F.relu(self.linear2(F.dropout(x, p=<span class="number">0.5</span>)))</span><br><span class="line">        output = F.softmax(self.linear3(x), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">vgg = VGG16()</span><br><span class="line">summary(vgg, input_size=(<span class="number">10</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>), device=<span class="string">&quot;cuda&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="1-10-深度影响效果的三大解释"><a href="#1-10-深度影响效果的三大解释" class="headerlink" title="1.10 深度影响效果的三大解释"></a>1.10 深度影响效果的三大解释</h4><ul><li>深度网络在相同资源下解决复杂问题能力超过浅层网络，能拟合更复杂函数。</li><li>深度网络更可能避免落入大局部最小值，通常表现更佳。</li><li>网络深度增加扩大感受野，通常提升模型效果。</li></ul><h3 id="2-感受野"><a href="#2-感受野" class="headerlink" title="2. 感受野"></a>2. 感受野</h3><h4 id="2-1-认识感受野"><a href="#2-1-认识感受野" class="headerlink" title="2.1 认识感受野"></a>2.1 认识感受野</h4><ul><li>感受野是神经元对应的<code>输入图像</code>区域。</li><li>特征图的每个像素是一个神经元，受限于扫描的<code>原始图像</code>部分。</li></ul><h4 id="2-2-感受野形象俯视"><a href="#2-2-感受野形象俯视" class="headerlink" title="2.2 感受野形象俯视"></a>2.2 感受野形象俯视</h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E6%84%9F%E5%8F%97%E9%87%8E%E5%BD%A2%E8%B1%A1.png" alt=""></p><h4 id="2-3-感受野与架构效率"><a href="#2-3-感受野与架构效率" class="headerlink" title="2.3 感受野与架构效率"></a>2.3 感受野与架构效率</h4><ul><li><strong>感受野扩大</strong>：随网络深度增加，感受野逐渐增大。</li><li><strong>重复架构优势</strong>：通过多次卷积和池化，感受野效率更高。</li><li><strong>池化层作用</strong>：池化层在减小特征图尺寸的同时，加倍扩大感受野。</li><li><strong>重复vs递减架构</strong>：重复架构下感受野增长效率高于递减架构。<ul><li><code>池化层</code>可以让感受野指数级增长</li></ul></li></ul><div class="table-container"><table><thead><tr><th>架构</th><th>进入FC之前，每个神经元上的感受野大小</th><th>ImageNet数据集上TOP5错误率</th></tr></thead><tbody><tr><td>LeNet5</td><td>16</td><td>约25%</td></tr><tr><td>AlexNet</td><td>195</td><td>16.4% (相较LeNet降低约15%以上)</td></tr><tr><td>VGG16</td><td>212</td><td>7.3%</td></tr></tbody></table></div><ul><li>对像素级预测任务，感受野大小至关重要。即保证感受野足够大以提高语义分割和光流估计等任务精确度。</li></ul><h4 id="2-4-感受野的范围和深度"><a href="#2-4-感受野的范围和深度" class="headerlink" title="2.4 感受野的范围和深度"></a>2.4 感受野的范围和深度</h4><ul><li>感受野理论上无上限，可覆盖整个输入图像。</li><li>深度网络中，感受野随深度增加而扩大，边缘神经元感受野可能超出图像。</li><li>在现代深度网络中，<strong>感受野通常远大于输入图像尺寸</strong>。</li></ul><h1 id="ConvNet-Models-and-Their-Receptive-Fields"><a href="#ConvNet-Models-and-Their-Receptive-Fields" class="headerlink" title="ConvNet Models and Their Receptive Fields"></a>ConvNet Models and Their Receptive Fields</h1><div class="table-container"><table><thead><tr><th>ConvNet Model</th><th>Receptive Field (r)</th><th>Effective Stride (S)</th><th>Effective Padding (P)</th><th>Model Year</th></tr></thead><tbody><tr><td>alexnet_v2</td><td>195</td><td>32</td><td>64</td><td>2014</td></tr><tr><td>vgg_16</td><td>212</td><td>32</td><td>90</td><td>2014</td></tr><tr><td>mobilenet_v1</td><td>315</td><td>32</td><td>126</td><td>2017</td></tr><tr><td>mobilenet_v1_075</td><td>315</td><td>32</td><td>126</td><td>2017</td></tr><tr><td>resnet_v1_50</td><td>483</td><td>32</td><td>239</td><td>2015</td></tr><tr><td>inception_v2</td><td>699</td><td>32</td><td>318</td><td>2015</td></tr><tr><td>resnet_v1_101</td><td>1027</td><td>32</td><td>511</td><td>2015</td></tr><tr><td>inception_v3</td><td>1311</td><td>32</td><td>618</td><td>2015</td></tr><tr><td>resnet_v1_152</td><td>1507</td><td>32</td><td>751</td><td>2015</td></tr><tr><td>resnet_v1_200</td><td>1763</td><td>32</td><td>879</td><td>2015</td></tr><tr><td>inception_v4</td><td>2071</td><td>32</td><td>998</td><td>2016</td></tr><tr><td>inception_resnet_v2</td><td>3039</td><td>32</td><td>1482</td><td>2016</td></tr></tbody></table></div><h4 id="2-5-深度卷积网络中的有效感受野"><a href="#2-5-深度卷积网络中的有效感受野" class="headerlink" title="2.5 深度卷积网络中的有效感受野"></a>2.5 深度卷积网络中的有效感受野</h4><ul><li><strong>中心像素点(有效感受野)</strong>：位于图像中心的像素点重叠路径多，对最终特征图影响更大，信息表现更清晰。</li><li><strong>边缘像素点</strong>：像素点位置越靠近图像边缘，对特征图的影响力迅速衰减，显示边缘像素点的影响较小。</li><li>故<strong>集中图像中心信息，用黑边代替边缘，提高模型性能。</strong></li></ul><h4 id="2-6-扩大感受野：膨胀卷积"><a href="#2-6-扩大感受野：膨胀卷积" class="headerlink" title="2.6 扩大感受野：膨胀卷积"></a>2.6 扩大感受野：膨胀卷积</h4><ul><li><p>提升感受野的方法</p><ul><li><strong>加深网络深度</strong>：每增加一个卷积层，感受野线性增加。（参数增加）</li><li><strong>池化层</strong>：利用池化层快速减小特征图尺寸，间接扩大感受野。（特征图尺寸缩减）</li><li><strong>膨胀卷积</strong>：通过膨胀卷积，即空洞卷积，增加感受野覆盖范围。<ul><li>膨胀卷积不改变相邻像素值，与填充不同。</li><li>膨胀率决定计算点周围扩充像素的圈数。</li><li>膨胀卷积在计算点上扩展面积，增加间隙，跳过间隙计算。</li></ul></li></ul></li><li><p>map计算公式</p></li></ul><script type="math/tex; mode=display">H_{out} = \frac{H_{in} + 2P - \text{dilation}[0] \times (K_H - 1) - 1}{S[0]} + 1</script><script type="math/tex; mode=display">W_{out} = \frac{W_{in} + 2P - \text{dilation}[1] \times (K_W - 1) - 1}{S[1]} + 1</script><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/dilation%E5%9B%BE%E4%BE%8B.png" alt=""></p><ul><li>应用<ul><li>膨胀卷积可能导致信息丢失，需评估其在<code>多层网络</code>中的整体效果。</li><li><strong>优势</strong>：膨胀卷积通过扩大感受野，对语义分割等像素级预测任务表现优异。</li><li><strong>局限性</strong>：膨胀卷积可能导致信息损失，尤其是在小物体分割中不太适用，而适合大物体分割。</li></ul></li></ul><h4 id="2-7-感受野的手动计算"><a href="#2-7-感受野的手动计算" class="headerlink" title="2.7 感受野的手动计算"></a>2.7 感受野的手动计算</h4><script type="math/tex; mode=display">r_l = r_{l-1} + (k_l - 1) \cdot \prod_{i=0}^{l-1} s_i</script><ul><li><strong>LeNet</strong></li></ul><div class="table-container"><table><thead><tr><th>Layer</th><th>Parameters &amp; Structure</th><th>Receptive Field Calculation</th><th>Receptive Field Size</th></tr></thead><tbody><tr><td>Original Image</td><td>stride 1</td><td>-</td><td>1</td></tr><tr><td>conv1</td><td>kernel_size 5x5, stride 1</td><td>1+(5-1)*1</td><td>5</td></tr><tr><td>pool1</td><td>kernel_size 2x2, stride 2</td><td>5+(2-1)*(1*1)</td><td>6</td></tr><tr><td>conv2</td><td>kernel_size 5x5, stride 1</td><td>6+(5-1)*(1*1*2)</td><td>14</td></tr><tr><td>pool2</td><td>kernel_size 2x2, stride 2</td><td>14+(2-1)*(1*1*2*1)</td><td>16</td></tr></tbody></table></div><ul><li><strong>AlexNet</strong></li></ul><div class="table-container"><table><thead><tr><th>Layer</th><th>Parameters &amp; Structure</th><th>Receptive Field Calculation</th><th>Receptive Field Size</th></tr></thead><tbody><tr><td>Original Image</td><td>stride 1</td><td></td><td>1</td></tr><tr><td>conv1</td><td>kernel_size 11x11, stride 4</td><td><code>1+(11-1) * 1</code></td><td>11</td></tr><tr><td>pool1</td><td>kernel_size 3x3, stride 2</td><td><code>11+(3-1) * (1 * 4)</code></td><td>19</td></tr><tr><td>conv2</td><td>kernel_size 5x5, stride 1</td><td><code>19+(5-1) * (1 * 4  * 2)</code></td><td>51</td></tr><tr><td>pool2</td><td>kernel_size 3x3, stride 2</td><td><code>51+(3-1) * (1 * 4 * 2 * 1)</code></td><td>67</td></tr><tr><td>conv3</td><td>kernel_size 3x3, stride 1</td><td><code>67+(3-1) * (1 * 4 * 2 * 1 * 2)</code></td><td>99</td></tr><tr><td>conv4</td><td>kernel_size 3x3, stride 1</td><td><code>99+(3-1) * (1 * 4 * 2 * 1 * 2 * 1)</code></td><td>131</td></tr><tr><td>conv5</td><td>kernel_size 3x3, stride 1</td><td><code>131+(3-1) * (1 * 4 * 2 * 1 * 2 * 1 * 1)</code></td><td>163</td></tr><tr><td>pool3</td><td>kernel_size 3x3, stride 2</td><td><code>163+(3-1) * (1 * 4 * 2 * 1 * 2 * 1 * 1 * 1)</code></td><td>195</td></tr></tbody></table></div><h4 id="2-8-感受野的自动计算"><a href="#2-8-感受野的自动计算" class="headerlink" title="2.8 感受野的自动计算"></a>2.8 感受野的自动计算</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch_receptive_field <span class="keyword">import</span> receptive_field</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet5</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 更换为最大池化层，因为torch_receptive_field库只认最大池化层，其他池化层一律报错</span></span><br><span class="line">        self.pool1 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool2 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># FC层是不参与感受野计算的，因此torch_receptive_field不接受FC层的输入</span></span><br><span class="line">        <span class="comment"># 因此在测试架构时，你可以直接不写fc层</span></span><br><span class="line">        <span class="comment"># self.fc1 = nn.Linear(16*5*5,120)</span></span><br><span class="line">        <span class="comment"># self.fc2 = nn.Linear(120,84)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.tanh(self.conv1(x))</span><br><span class="line">        x = self.pool1(x)</span><br><span class="line">        x = F.tanh(self.conv2(x))</span><br><span class="line">        x = self.pool2(x)</span><br><span class="line">        <span class="comment"># 任何关于fc层的计算也需要被注释掉</span></span><br><span class="line">        <span class="comment"># x = x.view(-1,16*5*5)</span></span><br><span class="line">        <span class="comment"># x = F.tanh(self.fc1(x))</span></span><br><span class="line">        <span class="comment"># output = F.softmax(self.fc2(x),dim=1)</span></span><br><span class="line">        <span class="comment"># output = F.softmax(x.view(-1, 16 * 5 * 5), dim=1)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = LeNet5().cuda()</span><br><span class="line">receptive_field_dict = receptive_field(net, (<span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>))  <span class="comment"># 输入数据的结构注意不要写错</span></span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E6%84%9F%E5%8F%97%E9%87%8E%E8%87%AA%E5%8A%A8%E8%AE%A1%E7%AE%97%E7%BB%93%E6%9E%9C.png" alt=""></p><ul><li><code>torch_receptive_field</code> 只支持 <code>Conv2d</code> 和 <code>MaxPool2d</code> 层，不支持全连接层和平均池化层。使用不支持的层会报错。</li></ul><h3 id="3-平移不变性"><a href="#3-平移不变性" class="headerlink" title="3. 平移不变性"></a>3. 平移不变性</h3><h4 id="3-1-不变性定义"><a href="#3-1-不变性定义" class="headerlink" title="3.1 不变性定义"></a>3.1 不变性定义</h4><p>识别对象不受其在图像中变化的姿态影响。</p><ul><li>池化层虽不能确保完全的平移不变性，从感受野的角度，却能有效减少特征图尺寸和模型参数。</li></ul><h4 id="3-2-基础不变性"><a href="#3-2-基础不变性" class="headerlink" title="3.2 基础不变性"></a>3.2 基础不变性</h4><ul><li><strong>平移不变性</strong>：识别图像中任何位置的对象。</li><li><strong>旋转/视野不变性</strong>：识别任何角度或视角下的对象。</li><li><strong>尺寸不变性</strong>：识别不同大小的相同对象。</li><li><strong>明度不变性</strong>：识别在各种光照条件下的对象。</li><li><strong>镜面不变性</strong>：识别镜像反转后的对象。</li><li><strong>颜色不变性</strong>：识别颜色变化后的对象。</li></ul><h4 id="3-3-CNN中的平移不变性"><a href="#3-3-CNN中的平移不变性" class="headerlink" title="3.3 CNN中的平移不变性"></a>3.3 CNN中的平移不变性</h4><ul><li><p><strong>基本概念</strong>：CNN能识别微小平移后的图像对象。 </p></li><li><p><strong>池化层作用</strong>：筛选出关键信息，提供平移不变性。 </p></li><li><strong>深度与不变性</strong>：更深的网络有更强的平移不变性，但对大范围平移适应性较差。</li><li><strong>结构影响</strong>：卷积与池化的组合增强了平移不变性和模型鲁棒性。</li></ul><h4 id="3-4-数据增强"><a href="#3-4-数据增强" class="headerlink" title="3.4 数据增强"></a>3.4 数据增强</h4><ul><li><strong>手段</strong>：旋转、模糊、调高饱和度、放大/缩小、调高亮度、变形、镜面、镜面旋转、去纹理化、去颜色化、边缘增强、显著边缘化（边缘检测）</li><li><strong>数据增强的目的</strong>：通过生成变化图像扩大训练集，提高模型对“旋转”和“镜像”数据的识别能力。<ul><li>平移不变性通常增强模型性能，但在<code>像素级任务</code>中可能造成<code>灾难</code>。</li></ul></li></ul><h2 id="四、架构对参数量-计算量的影响"><a href="#四、架构对参数量-计算量的影响" class="headerlink" title="四、架构对参数量/计算量的影响"></a>四、架构对参数量/计算量的影响</h2><div class="table-container"><table><thead><tr><th>操作</th><th>参数增加方式</th><th>影响因素</th></tr></thead><tbody><tr><td>全连接层</td><td>直接参数</td><td>权重和偏置</td></tr><tr><td>卷积层</td><td>直接和间接参数</td><td>卷积核尺寸、数量（padding和stride不影响参数量)</td></tr><tr><td>池化</td><td>间接参数</td><td>特征图尺寸变化</td></tr><tr><td>激活函数</td><td>无参数</td><td>-</td></tr><tr><td>Dropout</td><td>无参数</td><td>-</td></tr></tbody></table></div><h3 id="4-1-卷积层"><a href="#4-1-卷积层" class="headerlink" title="4.1 卷积层"></a>4.1 卷积层</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">conv1 = nn.Conv2d(<span class="number">3</span>,<span class="number">6</span>,<span class="number">3</span>) <span class="comment">#(3 * 3 * 3)*6 + 6</span></span><br><span class="line">conv2 = nn.Conv2d(<span class="number">6</span>,<span class="number">4</span>,<span class="number">3</span>) <span class="comment">#(3 * 3 * 6)*4 + 4</span></span><br><span class="line"><span class="comment">#检查一下结果</span></span><br><span class="line">conv1.weight.numel()  <span class="comment"># 162 = (3 * 3 * 3)*6</span></span><br><span class="line">conv1.bias.numel()  <span class="comment"># 6</span></span><br><span class="line">conv2.weight.numel()  <span class="comment"># 216 = (3 * 3 * 6)*4</span></span><br><span class="line">conv2.bias.numel()  <span class="comment"># 4</span></span><br><span class="line"></span><br><span class="line">conv3 = nn.Conv2d(<span class="number">4</span>,<span class="number">16</span>,<span class="number">5</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>)  <span class="comment"># (5*5*4)*16 + 16</span></span><br><span class="line">conv4 = nn.Conv2d(<span class="number">16</span>,<span class="number">3</span>,<span class="number">5</span>,stride=<span class="number">3</span>,padding=<span class="number">2</span>)  <span class="comment"># (5*5*16)*3 + 3</span></span><br><span class="line"><span class="built_in">print</span>(conv3.weight.numel())  <span class="comment"># 1600 = (5*5*4)*16</span></span><br><span class="line"><span class="built_in">print</span>(conv3.bias.numel())  <span class="comment"># 16</span></span><br><span class="line"><span class="built_in">print</span>(conv4.weight.numel())  <span class="comment"># 1200 = (5*5*16)*3</span></span><br><span class="line"><span class="built_in">print</span>(conv4.bias.numel())  <span class="comment"># 3</span></span><br></pre></td></tr></table></figure><ul><li>虽然实际使用的卷积核尺寸较小，但高输出数量使得参数量迅速增加。</li><li>特征图随网络加深而减小，数量会增加，尤其是在池化层后加倍。</li><li>可以减少卷积和池化的组合数量（即减少网络深度），或者在网络初始层使用较少的特征图数量，如32。</li></ul><h3 id="4-2-大尺寸卷积核vs小尺寸卷积核"><a href="#4-2-大尺寸卷积核vs小尺寸卷积核" class="headerlink" title="4.2 大尺寸卷积核vs小尺寸卷积核"></a>4.2 大尺寸卷积核vs小尺寸卷积核</h3><ul><li>2个3x3卷积核=1个5x5卷积核（感受野、feature map的角度）</li></ul><div class="table-container"><table><thead><tr><th>Convolution Layer</th><th>Kernel Size</th><th>Calculation for 3x3 Kernel, 2 Layers</th><th>Calculation for 5x5 Kernel, 1 Layer</th></tr></thead><tbody><tr><td>conv1(64,64,3)</td><td>3x3</td><td>3 <em> 3 </em> 64 * 64 + 64 = 36,928</td><td>5 <em> 5 </em> 64 * 64 + 64 = 102,464</td></tr><tr><td>conv2(64,64,3)</td><td>3x3</td><td>3 <em> 3 </em> 64 * 64 + 64 = 36,928</td><td></td></tr><tr><td><strong>Total</strong></td><td></td><td>73,856</td><td>102,464</td></tr></tbody></table></div><ul><li>选择<code>小卷积核</code>的理由：参数变少了、网络变深了，让提取出的特征信息更“抽象”、更“复杂”。</li></ul><h3 id="4-3-1x1卷积核"><a href="#4-3-1x1卷积核" class="headerlink" title="4.3 1x1卷积核"></a>4.3 1x1卷积核</h3><ul><li>1x1卷积核下的<strong>参数量</strong>为：</li></ul><script type="math/tex; mode=display">N_{parameters} = C_{in} \times C_{out} + C_{out}</script><ul><li><strong>优势</strong><ul><li>1x1卷积核的<strong>参数量大幅减少</strong></li><li><strong>不改变特征图尺寸</strong>的前提下进行特征转换</li><li>能够有效地将特征图中的<strong>“位置信息”</strong>传递到下一层</li></ul></li><li><strong>作用</strong>：加深CNN的深度</li></ul><h3 id="NiN网络—2014迟于VGG"><a href="#NiN网络—2014迟于VGG" class="headerlink" title="NiN网络—2014迟于VGG"></a>NiN网络—2014迟于VGG</h3><ul><li><strong>MLP layer</strong> ： 1x1的卷积层<ul><li>用在卷积层之间，调整通道数，减少参数和计算量，有助于构建更深的网络。</li></ul></li><li>3x3conv+2<em>MLP = block </em>3 = 9 layers</li><li><strong>瓶颈设计</strong>：在核尺寸为1x1的2个卷积层之间包装其他卷积层（右侧）<ul><li>只会出现在超过100层的深度网络中</li><li>几乎不会造成信息损失，带来骤减的参数量</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E7%93%B6%E9%A2%88%E8%AE%BE%E8%AE%A1.png" alt=""></p><div class="table-container"><table><thead><tr><th></th><th>1个3x3卷积，输出256（普通卷积）</th><th></th><th>2个1x1卷积层， 1个3x3卷积层（瓶颈设计）</th></tr></thead><tbody><tr><td>conv(256,256,3)</td><td><code>3 * 3 * 256 * 256 + 256 = 590,080</code></td><td>conv(256,32,1)</td><td>256 * 32 + 32 = 8,224</td></tr><tr><td></td><td></td><td>conv(32,32,3)</td><td>3 <em> 3 </em> 32 * 32 + 32 = 9,248</td></tr><tr><td></td><td></td><td>conv(32,256,1)</td><td>32 * 256 + 256 = 8,448</td></tr><tr><td></td><td>590,080 (59w)</td><td></td><td>25,920 (2w)</td></tr></tbody></table></div><h3 id="4-4-减少参数量：分组卷积与深度分离卷积"><a href="#4-4-减少参数量：分组卷积与深度分离卷积" class="headerlink" title="4.4 减少参数量：分组卷积与深度分离卷积"></a>4.4 减少参数量：分组卷积与深度分离卷积</h3><ul><li><strong>分组卷积</strong></li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E5%88%86%E7%BB%84%E5%8D%B7%E7%A7%AF%E5%9B%BE%E4%BE%8B.png" alt=""></p><script type="math/tex; mode=display">\begin{align*}\text{group1} &= \left( K_H \times K_W \times \frac{C_{in}}{g} \right) \times \frac{C_{out}}{g} + \frac{C_{out}}{g} \\\text{group2} &= \left( K_H \times K_W \times \frac{C_{in}}{g} \right) \times \frac{C_{out}}{g} + \frac{C_{out}}{g} \\\text{total} &= \text{group1} + \text{group2} \\&= \left( \left( K_H \times K_W \times \frac{C_{in}}{g} \right) \times \frac{C_{out}}{g} + \frac{C_{out}}{g} \right) \times g \\&= \frac{1}{g} \left( K_H \times K_W \times C_{in} \times C_{out} \right) + C_{out}\end{align*}</script><blockquote><p> <strong>分组的存在不影响偏置，偏置只与输出的特征图数量有关。</strong></p><ul><li>AlexNet包含groups=2的分组卷积</li><li>map_num一般为偶数，故groups也为偶数</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">conv1 = nn.Conv2d(<span class="number">4</span>,<span class="number">8</span>,<span class="number">3</span>) <span class="comment">#(3 * 3 * 4)*8 + 8 = 296</span></span><br><span class="line">conv1_ = nn.Conv2d(<span class="number">4</span>,<span class="number">8</span>,<span class="number">3</span>,groups=<span class="number">2</span>) <span class="comment"># ((3 * 3 * 4)*8)/2 + 8 = 152</span></span><br><span class="line"><span class="comment">#检查一下结果</span></span><br><span class="line">conv1.weight.numel()  <span class="comment"># 288</span></span><br><span class="line">conv1.bias.numel()  <span class="comment"># 8</span></span><br><span class="line">conv1_.weight.numel()  <span class="comment"># 144</span></span><br><span class="line">conv1_.bias.numel()  <span class="comment"># 8</span></span><br><span class="line"><span class="comment">#如果输入了奇数group呢？</span></span><br><span class="line">conv2 = nn.Conv2d(<span class="number">4</span>,<span class="number">8</span>,<span class="number">3</span>,groups=<span class="number">3</span>)  <span class="comment"># 直接报错</span></span><br></pre></td></tr></table></figure><ul><li><strong>深度卷积(groups = Cin)</strong></li></ul><script type="math/tex; mode=display">\text{parameters} = \frac{1}{g} (K_H \times K_W \times C_{\text{in}} \times C_{\text{out}}) + C_{\text{out}}\text{其中 } g = C_{\text{in}}, \text{则有}:\text{parameters} = K_H \times K_W \times C_{\text{out}} + C_{\text{out}}</script><ul><li><strong>深度可分离卷积(groups = Cin 且 1x1)</strong></li></ul><script type="math/tex; mode=display">一个Block的参数如下：Parameter = K_H \times K_W \times C_{\text{depth\_out}} + C_{\text{pair\_in}} \times C_{\text{pair\_out}}</script><blockquote><p>是谷歌的深度学习模型GoogLeNet进化版中非常关键的block</p><ul><li>帮助卷积层减少参数量</li><li>削弱特征图与特征图之间的 联系来控制过拟合</li></ul></blockquote><hr><h3 id="4-5-全连接层"><a href="#4-5-全连接层" class="headerlink" title="4.5 全连接层"></a>4.5 全连接层</h3><h4 id="4-5-1-全连接层的作用"><a href="#4-5-1-全连接层的作用" class="headerlink" title="4.5.1 全连接层的作用"></a>4.5.1 全连接层的作用</h4><ul><li>作为分类器，实现对数据的分类。</li></ul><blockquote><p>本质上来说，卷积层提供了一系列有意义且稳定的特征值，构成了一个 与输入图像相比维数更少的特征空间，而全连接层负责学习这个空间上的（可能是非线性的）函数关 系，并输出预测结果。</p></blockquote><ul><li>作为整合信息的工具，将特征图中的信息进行整合。</li></ul><blockquote><p>若使用“特征图的不同区域”的信息来进行类别划分，可能会造成 “只有某个区域的数据参与了某个标签的预测”的情况。在进行预测之前，将所有可能的信息充分混合、 进行学习，对预测效果有重大的意义。</p></blockquote><ul><li>全连接层的存在让CNN整体变得更容易过拟合。</li></ul><blockquote><p>dropout、batch normalization现在的CNN架构已经不太容易过拟合了。</p></blockquote><ul><li>令人头疼的参数量问题</li></ul><h4 id="4-5-2-全连接层的问题"><a href="#4-5-2-全连接层的问题" class="headerlink" title="4.5.2 全连接层的问题"></a>4.5.2 全连接层的问题</h4><ul><li>在卷积-FC连接中，通常FC的输出神经元个数不会少于输入的通道数。</li><li><p>如何确定最后一个池化或卷积层输出特征图的尺寸？</p><ul><li><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E6%9E%B6%E6%9E%84%E5%9B%BE%E6%9C%AA%E7%9F%A5%E4%B8%8B%E7%A1%AE%E5%AE%9A%E8%BE%93%E5%87%BA%E5%B0%BA%E5%AF%B8.png" alt=""></p></li><li><p><strong>方法一</strong>：将Model中所有的<strong>线性层都注释掉</strong>，只留下卷积层，然后将model输入<strong>summary</strong>进行计算。</p><ul><li><p><strong>方法二</strong>：<strong>nn.Sequential</strong></p><ul><li>```python<br>import torch<br>import torch.nn as nn<br>import torch.nn.functional as F<br>from torchinfo import summary </li></ul></li></ul></li></ul></li></ul><pre><code>    data = torch.ones(size=(10,3,229,229))    #不使用类，直接将需要串联的网络、函数等信息写在一个“序列”里    #重现上面的4个卷积层、2个池化层的架构    net = nn.Sequential(nn.Conv2d(3,6,3)                       ,nn.ReLU(inplace=True)                       ,nn.Conv2d(6,4,3)                       ,nn.ReLU(inplace=True)                       ,nn.MaxPool2d(2)                       ,nn.Conv2d(4,16,5,stride=2,padding=1)                       ,nn.ReLU(inplace=True)                       ,nn.Conv2d(16,3,5,stride=3,padding=2)                       ,nn.ReLU(inplace=True)                       ,nn.MaxPool2d(2)                       )    #nn.Sequential组成的序列不是类，因此不需要实例化，可以直接输入数据    print(net(data).shape)  # torch.Size([10, 3, 9, 9])    <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  from torch_receptive_field import receptive_field</span><br><span class="line">  </span><br><span class="line">  #net不是类所以不需要实例化,查看感受野</span><br><span class="line">  rfdict = receptive_field(net.cuda(),(3,229,229))</span><br></pre></td></tr></table></figure>- 实例分析：  - ```python    class VGG16(nn.Module):        def __init__(self):            super().__init__()            self.features_ = nn.Sequential(nn.Conv2d(3,64,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.Conv2d(64,64,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.MaxPool2d(2)                                           ,nn.Conv2d(64,128,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.Conv2d(128,128,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.MaxPool2d(2)                                           ,nn.Conv2d(128,256,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.Conv2d(256,256,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.Conv2d(256,256,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.MaxPool2d(2)                                           ,nn.Conv2d(256,512,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.MaxPool2d(2)                                           ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.MaxPool2d(2)                                          )            self.clf_ = nn.Sequential(nn.Dropout(0.5)                                      ,nn.Linear(512*7*7,4096),nn.ReLU(inplace=True)                                      ,nn.Dropout(0.5)                                      ,nn.Linear(4096,4096),nn.ReLU(inplace=True)                                      ,nn.Linear(4096,1000),nn.Softmax(dim=1)                                     )        def forward(self,x):            x = self.features_(x) #用特征提取的架构提取特征            x = x.view(-1,512*7*7) #调整数据结构，拉平数据            output = self.clf_(x)            return output    <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">      - 常常会使用 `nn.Sequential`来**调试卷积层架构**，并**不断查看感受野**的变化。</span><br><span class="line"></span><br><span class="line">#### 4.5.3 代替全连接层：1x1卷积核与全局平均池化（GAP）</span><br><span class="line"></span><br><span class="line">&gt; 在计算机视觉中，不包含全连接层，只有卷积层和池化层的卷积网络被叫做全卷积网络(FCN)</span><br><span class="line"></span><br><span class="line">- CNN中，只要让特征图的尺寸为1x1，卷积核的尺寸也为1x1，就可以实现和普通全连接层一样的计算。</span><br><span class="line">- 特点：</span><br><span class="line">  - 1）使用1x1卷积层代替全连接层**不能减少参数量**</span><br><span class="line">  - 2）输入层对图像尺寸的**解放了限制**</span><br><span class="line"></span><br><span class="line">- 应用：目标检测中的滑窗识别</span><br><span class="line"></span><br><span class="line">- NiN中，用来替代全连接层的是`GAP层`</span><br><span class="line"></span><br><span class="line">  - ```python</span><br><span class="line">    data = torch.ones(10,7,7)</span><br><span class="line">    gap = nn.AvgPool2d(7)</span><br><span class="line">    gap(data).shape  # (10,1,1)</span><br></pre></td></tr></table></figure></code></pre><ul><li>GAP作为池化层，<strong>没有任何需要学习的参数</strong>，这让GAP的<strong>抗过拟合能力更强。</strong></li></ul><h4 id="4-5-4-NiN—2014（VGG之后）"><a href="#4-5-4-NiN—2014（VGG之后）" class="headerlink" title="4.5.4 NiN—2014（VGG之后）"></a>4.5.4 NiN—2014（VGG之后）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NiN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.block1 = nn.Sequential(nn.Conv2d(<span class="number">3</span>,<span class="number">192</span>,<span class="number">5</span>,padding=<span class="number">2</span>),nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                    ,nn.Conv2d(<span class="number">192</span>,<span class="number">160</span>,<span class="number">1</span>),nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                    ,nn.Conv2d(<span class="number">160</span>,<span class="number">96</span>,<span class="number">1</span>),nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                    ,nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>)</span><br><span class="line">                                    ,nn.Dropout(<span class="number">0.25</span>))</span><br><span class="line">        self.block2 = nn.Sequential(nn.Conv2d(<span class="number">96</span>,<span class="number">192</span>,<span class="number">5</span>,padding=<span class="number">2</span>),nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                    ,nn.Conv2d(<span class="number">192</span>,<span class="number">192</span>,<span class="number">1</span>),nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                    ,nn.Conv2d(<span class="number">192</span>,<span class="number">192</span>,<span class="number">1</span>),nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                    ,nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>)</span><br><span class="line">                                    ,nn.Dropout(<span class="number">0.25</span>))</span><br><span class="line">        self.block3 = nn.Sequential(nn.Conv2d(<span class="number">192</span>,<span class="number">192</span>,<span class="number">3</span>,padding=<span class="number">1</span>),nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                    ,nn.Conv2d(<span class="number">192</span>,<span class="number">192</span>,<span class="number">1</span>),nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                    ,nn.Conv2d(<span class="number">192</span>,<span class="number">10</span>,<span class="number">1</span>),nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                    ,nn.AvgPool2d(<span class="number">7</span>,stride=<span class="number">1</span>)</span><br><span class="line">                                    ,nn.Softmax(dim=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        output = self.block3(self.block2(self.block1(x)))</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line">    net= NiN()</span><br><span class="line">    net(data).shape <span class="comment">#10个特征图，每个特征图尺寸是1x1</span></span><br></pre></td></tr></table></figure><ul><li>贡献了1x1卷积层的用途</li><li>GoogLeNet以及ResNet都使用了1x1网络</li></ul><h3 id="五、前沿网络-state-of-the-art-models"><a href="#五、前沿网络-state-of-the-art-models" class="headerlink" title="五、前沿网络 state-of-the-art models"></a>五、前沿网络 state-of-the-art models</h3><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/sota-level.png" alt=""></p><div class="table-container"><table><thead><tr><th>网络模型</th><th>核心特点</th><th>关键创新</th><th></th></tr></thead><tbody><tr><td>LeNet</td><td>早期成功的神经网络，由卷积层块和全连接层块组成，使用卷积层学习空间信息，通过池化层降低敏感度</td><td>卷积层后接最大池化层的基本模板</td><td>输入→(卷积+池化)→(卷积+池化)→(线性x2)→输出</td></tr><tr><td>AlexNet</td><td>更大更深的网络，引入ReLU激活函数和dropout方法控制复杂度，以及数据增强技术<em>**</em></td><td>ReLU、dropout、最大池化、数据增强</td><td>输入→(卷积+池化)→(卷积+池化)→(卷积x3+池化)→(线性x3)→输出</td></tr><tr><td>VGG</td><td>使用重复的卷积层后接一个最大池化层的模板，增加了网络深度<em>**</em></td><td>卷积块的标准化设计</td><td>输入→（卷积x2+池化）x2 →（卷积x3+池化）x3 → FC层x3 →输出</td></tr><tr><td>NiN</td><td>引入1x1卷积核（网络中的网络），使用全局平均池化层直接用于分类</td><td>1x1卷积核，全局平均池化层替代全连接层</td><td>输入→（卷积x3+池化）x3→输出</td></tr><tr><td>GoogLeNet</td><td>含有并行连接的Inception块，解决不同大小卷积核组合的问题</td><td>Inception块，减少参数量同时保持性能</td><td></td></tr><tr><td>ResNet</td><td>引入残差学习框架以训练更深的网络，使用跳跃连接（skip connection）</td><td>残差块，跳跃连接，深度可达千层以上</td></tr></tbody></table></div><h4 id="5-1-GoogLeNet（Inception-V1）基础"><a href="#5-1-GoogLeNet（Inception-V1）基础" class="headerlink" title="5.1 GoogLeNet（Inception V1）基础"></a>5.1 GoogLeNet（Inception V1）基础</h4><ul><li><strong>背景</strong>：<ul><li>1）从2014年的竞赛结果来看，Inception V1的效果只比VGG19好一点点（只比VGG降低了0.6%的错误率），但Inception展现出比VGG强大许多的潜力。</li><li>2）自从LeNet5定下了卷积、池化、线性层串联的基本基调，研究者们在相当长 的一段时间内都在这条道路上探索，最终抵达的终点就是VGG。（参数过多、连接稠密、计算量大、容易过拟合）</li></ul></li><li><p><strong>困难</strong>：</p><ul><li>1）在神经网络由稠密变得稀疏（Sparse）的过程中，网络的<strong>学习能力会波动甚至会下降</strong></li><li>2）现代硬件<strong>不擅长处理在随机或非均匀稀疏的数据</strong>上的计算，并且这种不擅长在矩阵计算上表现得尤其明显。</li></ul></li><li><p><strong>权衡</strong>：</p><ul><li><strong>稠密结构</strong>的学习能力更强，但会因为参数量过于巨大而难以训练。</li><li><strong>稀疏结构</strong>的参数量少，但是学习能力会变得不稳定，并且不能很好地利用现有计算资源。</li></ul></li><li><p><strong>思路</strong>：普通卷积、池化层稠密元素组的块无限逼近一个稀疏架构，构造参数量与稀疏网络相似的稠密网络。</p></li></ul><h4 id="5-2-GoogLeNet（Inception-V1）分析"><a href="#5-2-GoogLeNet（Inception-V1）分析" class="headerlink" title="5.2 GoogLeNet（Inception V1）分析"></a>5.2 GoogLeNet（Inception V1）分析</h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inception-V1.png" alt=""></p><div class="table-container"><table><thead><tr><th>branch</th><th>Dispute</th><th>Advantage</th></tr></thead><tbody><tr><td>branch 1</td><td>conv 1x1</td><td>降低通道数</td></tr><tr><td>branch 2</td><td>conv 1x1 +conv 3x3</td><td>1x1卷积核降低通道数以此来降低参数量和计算量,再使用3x3卷积核进行特征提取</td></tr><tr><td>branch 3</td><td>conv 1x1 +conv 5x5</td><td>基本思路与第二条线路一致</td></tr><tr><td>branch 4</td><td>pool 3x3 +conv 1x1</td><td>池化当做一种特征提取的方式，并在池化后使用1x1卷积层来降低通道数。</td></tr></tbody></table></div><blockquote><p><strong>所有的线路都使用 了巧妙的参数组合，让特征图的尺寸保持不变</strong></p><p>稠密链接，没有分组或dropout造成的空隙</p></blockquote><ul><li><p>1）同时使用多种卷积核确保各种类型和层次的信息都被提取出来</p><ul><li><strong>1x1卷积核</strong>可以最大程度提取像素与像素之间的<strong>位置信息</strong></li><li><strong>尺寸较大的卷积核</strong>可以提取相邻像素之间的<strong>联系信息</strong></li><li><strong>最大池化层</strong>可以提取出<strong>局部中最关键的信息提取特征</strong></li><li>外面的池化层是用来减半特征图的</li></ul></li><li><p>2）并联的卷积池化层计算效率更高。</p></li><li>3）大量使用1x1卷积层来整合信息，既实现了“聚类信息”又实现了大规模降低参数量，让特征图数量实现了前所未有的增长。<ul><li>调整特征图数目变少，信息更加密集</li><li>后都跟ReLu，增加非线性变换</li><li>控制整体参数量、解放了特征图的数量</li></ul></li></ul><blockquote><p>VGG16最大k=3，其一参数为230万个，使得最大通道为512；Inception最大k=5，其一参数为45万个，使得最大通道为1024</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Google-Net.png" alt=""></p><ul><li><strong>总结</strong>：<ul><li>Inception内部是稠密部件的并联，而整个GoogLeNet则是数个 Inception块与传统卷积结构的串联。</li><li>inception前有两个blocks：conv+pool</li><li>Inception实际上取代了传统架构中卷积层的地位,因此网络总体有22层，比VGG19多了三层。</li><li>架构的最后，使用实际上一个用来替代全连接层的全局平均池化层</li><li>GoogLeNet还使用了“辅助分类器”，这两个分类器的输入分别是 inception4a和inception4d的输出结果，他们的结构如下：</li><li><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GoogleNet-%E8%BE%85%E5%8A%A9%E5%88%86%E7%B1%BB%E5%99%A8.png" alt=""><ul><li>加重在训练过程中中层inceptions输出结果的权重</li><li>一个GoogLeNet实际上集成了两个浅层网络和一个深层网络的结果来进 行学习和判断，在一个架构中间增加集成的思想</li></ul></li></ul></li></ul><h4 id="5-3-GoogLeNet（Inception-V1）复现"><a href="#5-3-GoogLeNet（Inception-V1）复现" class="headerlink" title="5.3 GoogLeNet（Inception V1）复现"></a>5.3 GoogLeNet（Inception V1）复现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicConv2d</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_channels, out_channels,**kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#  打包</span></span><br><span class="line">        self.conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, bias=<span class="literal">False</span>, **kwargs)</span><br><span class="line">                                 ,nn.BatchNorm2d(out_channels)</span><br><span class="line">                                 ,nn.ReLU(inplace=<span class="literal">True</span>))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span></span><br><span class="line"><span class="params">                 ,in_channels : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch1x1 : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch3x3red : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch3x3 : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch5x5red : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch5x5 : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,pool_proj : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#1x1</span></span><br><span class="line">        self.branch1 = BasicConv2d(in_channels,ch1x1,kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#1x1 + 3x3</span></span><br><span class="line">        self.branch2 = nn.Sequential(BasicConv2d(in_channels, ch3x3red, kernel_size=<span class="number">1</span>)</span><br><span class="line">                                     ,BasicConv2d(ch3x3red, ch3x3, kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>))</span><br><span class="line">        <span class="comment">#1x1 + 5x5</span></span><br><span class="line">        self.branch3 = nn.Sequential(BasicConv2d(in_channels, ch5x5red, kernel_size=<span class="number">1</span>)</span><br><span class="line">                                     ,BasicConv2d(ch5x5red, ch5x5, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>))</span><br><span class="line">        <span class="comment">#pool + 1x1</span></span><br><span class="line">        self.branch4 = nn.Sequential(nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>, padding=<span class="number">1</span>,ceil_mode=<span class="literal">True</span>)</span><br><span class="line">                                    ,BasicConv2d(in_channels,pool_proj,kernel_size=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        branch1 = self.branch1(x) <span class="comment">#28x28,ch1x1</span></span><br><span class="line">        branch2 = self.branch2(x) <span class="comment">#28x28,ch3x3</span></span><br><span class="line">        branch3 = self.branch3(x) <span class="comment">#28x28,ch5x5</span></span><br><span class="line">        branch4 = self.branch4(x) <span class="comment">#28x28,pool_proj</span></span><br><span class="line">        outputs = [branch1, branch2, branch3, branch4]</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>) <span class="comment">#合并</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AuxClf</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_channels : <span class="built_in">int</span>, num_classes : <span class="built_in">int</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.feature_ = nn.Sequential(nn.AvgPool2d(kernel_size=<span class="number">5</span>,stride=<span class="number">3</span>)</span><br><span class="line">                                     ,BasicConv2d(in_channels,<span class="number">128</span>, kernel_size=<span class="number">1</span>))</span><br><span class="line">        self.clf_ = nn.Sequential(nn.Linear(<span class="number">4</span>*<span class="number">4</span>*<span class="number">128</span>, <span class="number">1024</span>)</span><br><span class="line">                                 ,nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                 ,nn.Dropout(<span class="number">0.7</span>)</span><br><span class="line">                                 ,nn.Linear(<span class="number">1024</span>,num_classes))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.feature_(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>,<span class="number">4</span>*<span class="number">4</span>*<span class="number">128</span>)</span><br><span class="line">        x = self.clf_(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GoogLeNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,num_classes: <span class="built_in">int</span> = <span class="number">1000</span>, blocks = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> blocks <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            blocks = [BasicConv2d, Inception, AuxClf]</span><br><span class="line">        conv_block = blocks[<span class="number">0</span>]</span><br><span class="line">        inception_block = blocks[<span class="number">1</span>]</span><br><span class="line">        aux_clf_block = blocks[<span class="number">2</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block1</span></span><br><span class="line">        self.conv1 = conv_block(<span class="number">3</span>,<span class="number">64</span>,kernel_size=<span class="number">7</span>,stride=<span class="number">2</span>,padding = <span class="number">3</span>)</span><br><span class="line">        self.maxpool1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,ceil_mode = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block2</span></span><br><span class="line">        self.conv2 = conv_block(<span class="number">64</span>,<span class="number">64</span>,kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = conv_block(<span class="number">64</span>,<span class="number">192</span>,kernel_size=<span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        self.maxpool2 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,ceil_mode = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block3</span></span><br><span class="line">        self.inception3a = inception_block(<span class="number">192</span>,<span class="number">64</span>,<span class="number">96</span>,<span class="number">128</span>,<span class="number">16</span>,<span class="number">32</span>,<span class="number">32</span>)</span><br><span class="line">        self.inception3b = inception_block(<span class="number">256</span>,<span class="number">128</span>,<span class="number">128</span>,<span class="number">192</span>,<span class="number">32</span>,<span class="number">96</span>,<span class="number">64</span>)</span><br><span class="line">        self.maxpool3 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,ceil_mode = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block4 </span></span><br><span class="line">        self.inception4a = inception_block(<span class="number">480</span>,<span class="number">192</span>,<span class="number">96</span>,<span class="number">208</span>,<span class="number">16</span>,<span class="number">48</span>,<span class="number">64</span>)</span><br><span class="line">        self.inception4b = inception_block(<span class="number">512</span>,<span class="number">160</span>,<span class="number">112</span>,<span class="number">224</span>,<span class="number">24</span>,<span class="number">64</span>,<span class="number">64</span>)</span><br><span class="line">        self.inception4c = inception_block(<span class="number">512</span>,<span class="number">128</span>,<span class="number">128</span>,<span class="number">256</span>,<span class="number">24</span>,<span class="number">64</span>,<span class="number">64</span>)</span><br><span class="line">        self.inception4d = inception_block(<span class="number">512</span>,<span class="number">112</span>,<span class="number">144</span>,<span class="number">288</span>,<span class="number">32</span>,<span class="number">64</span>,<span class="number">64</span>)</span><br><span class="line">        self.inception4e = inception_block(<span class="number">528</span>,<span class="number">256</span>,<span class="number">150</span>,<span class="number">320</span>,<span class="number">32</span>,<span class="number">128</span>,<span class="number">128</span>)</span><br><span class="line">        self.maxpool4 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,ceil_mode = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block5</span></span><br><span class="line">        self.inception5a = inception_block(<span class="number">832</span>,<span class="number">256</span>,<span class="number">160</span>,<span class="number">320</span>,<span class="number">32</span>,<span class="number">128</span>,<span class="number">128</span>)</span><br><span class="line">        self.inception5b = inception_block(<span class="number">832</span>,<span class="number">384</span>,<span class="number">192</span>,<span class="number">384</span>,<span class="number">48</span>,<span class="number">128</span>,<span class="number">128</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#clf</span></span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)) <span class="comment">#我需要的输出的特征图尺寸是多少</span></span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.4</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">1024</span>,num_classes)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#auxclf</span></span><br><span class="line">        self.aux1 = aux_clf_block(<span class="number">512</span>, num_classes) <span class="comment">#4a</span></span><br><span class="line">        self.aux2 = aux_clf_block(<span class="number">528</span>, num_classes) <span class="comment">#4d</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="comment">#block1</span></span><br><span class="line">        x = self.maxpool1(self.conv1(x))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block2</span></span><br><span class="line">        x = self.maxpool2(self.conv3(self.conv2(x)))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block3</span></span><br><span class="line">        x = self.inception3a(x)</span><br><span class="line">        x = self.inception3b(x)</span><br><span class="line">        x = self.maxpool3(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block4</span></span><br><span class="line">        x = self.inception4a(x)</span><br><span class="line">        aux1 = self.aux1(x)</span><br><span class="line">        </span><br><span class="line">        x = self.inception4b(x)</span><br><span class="line">        x = self.inception4c(x)</span><br><span class="line">        x = self.inception4d(x)</span><br><span class="line">        aux2 = self.aux2(x)</span><br><span class="line">        </span><br><span class="line">        x = self.inception4e(x)</span><br><span class="line">        x = self.maxpool4(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block5</span></span><br><span class="line">        x = self.inception5a(x)</span><br><span class="line">        x = self.inception5b(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#clf</span></span><br><span class="line">        x = self.avgpool(x) <span class="comment">#在这个全局平均池化之后，特征图尺寸就变成了1x1</span></span><br><span class="line">        x = torch.flatten(x,<span class="number">1</span>)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x, aux2, aux1</span><br><span class="line">data = torch.ones(<span class="number">10</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">net = GoogLeNet(num_classes=<span class="number">1000</span>)</span><br><span class="line">fc2, fc1, fc0 = net(data)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [fc2, fc1, fc0]:</span><br><span class="line">    <span class="built_in">print</span>(i.shape)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">torch.Size([10, 1000])</span></span><br><span class="line"><span class="string">torch.Size([10, 1000])</span></span><br><span class="line"><span class="string">torch.Size([10, 1000])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">summary(net,(<span class="number">10</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>),device=<span class="string">&quot;cpu&quot;</span>,depth=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="5-4-ResNet—2015"><a href="#5-4-ResNet—2015" class="headerlink" title="5.4 ResNet—2015"></a>5.4 ResNet—2015</h4><ul><li><p><strong>困难</strong>：</p><ul><li>网络能够达到的<strong>最大深度依然很浅</strong></li><li>深度网络的<strong>训练难度太大</strong>，深度网络会出现<strong>精度平缓甚至退化</strong>的现象。深层网络中的函数关系本质上就比浅层网络中的函数关系更复杂、更难拟合（fit）</li></ul></li><li><p><strong>思路</strong>：</p><ul><li>残差F(x)为0，此时x=H(x)</li><li><code>拟合0与x的关系</code>，比拟合一个<code>未知的函数H(x)与x的关系</code>容易。</li></ul></li></ul><ul><li><strong>残差块H(x)</strong>：快捷连接/跳跃连接x+普通卷积层F(x) </li><li><strong>残差网络</strong>：普通卷积层和全局平均池化层中间插入数个残差单元的网络<ul><li><strong>五种</strong>：18、34、50、101、152</li><li><strong>模式</strong>：7x7开头、3x3(s=2)的重叠池化层、数个重复的残差单元、全局平均池化层、线性层、Softmax</li><li><strong>规定</strong>：对于34层以下的残差网络，每个残差单元至少有两个卷积层；50层以上的残差网络，每个残差单元有三个卷积层（瓶颈架构—-为降低参数量而存在）</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/ResNet%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt=""></p><ul><li><p>padding</p><p>残差单元中，7/3、5/2、3/1、1/0保持特征图尺寸不变，并Conv+Batch Normalization+ReLu为一体。</p></li><li><p>stride</p><p>与GoogleNet一样，残差网络的输入是224x224时，特征图总共折半了5次，最终尺寸是7x7</p><p>我们把这5部分称为Layers，每个Layers包含的残差单元是一个个Block</p><p>只有最初出现了池化层来降低尺寸，说明5次降维认为中的其他4次都是由步长为2的卷积层完成</p><p>conv3_x conv4_x conv5_x这三个Layers，指的是第一个残差单元或瓶颈结构的第一个卷积层</p></li><li><p>跳跃连接上的卷积层</p><p>为了实现残差单元中的加和，在跳跃连接上叶加入核为1x1，步长为2的卷积层缩小x尺寸</p><p>在瓶颈架构中，输出的特征图数量依次是middle_out、middle_out、middle_out*4</p><p>故在跳跃连接上的卷积层要保证被整理为输出的相同结构</p></li><li><p>BN层与ReLU激活函数在哪里？</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/ResNet-BN%E5%92%8CReLu%E4%BD%8D%E7%BD%AE.png" alt=""></p></li><li><p>参数初始化在哪里实现？</p><p>将残差单元中最后一个卷积层后的BN层上的γ=0，就可以让F(x)的输出结果为0了</p></li></ul><script type="math/tex; mode=display">\text{output} = \frac{x - E[x]}{\sqrt{Var[x] + \epsilon}} * \gamma + \beta</script><ul><li><p>特征图数量繁多、多样地变化</p><p>我们必须使用具有一定通用性的代码来实现不同的层</p><p>保持不变。conv1与conv2_x连接时，一个瓶颈架构内嵌两个卷积层连接时，特征图数量保持不变</p><p>上层输出数量的4倍。内嵌三个卷积层的瓶颈架构中，第三个1x1卷积层的输出特征图数量是4倍</p><p>上层输出数量的1/4倍。一个Layers内，下一个瓶颈架构是上一个瓶颈架构连接的1/4倍</p><p>上层输出数量的1/2倍。不同Layers之间连接时，下一个瓶颈架构是上一个瓶颈架构连接变的1/2倍</p></li></ul><h4 id="5-5-复现ResNet"><a href="#5-5-复现ResNet" class="headerlink" title="5.5 复现ResNet"></a>5.5 复现ResNet</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#basicconv - conv2d + BN + ReLU (→ conv3x3, conv1x1)</span></span><br><span class="line"><span class="comment">#Residual Unit, Bottleneck</span></span><br><span class="line"><span class="comment">#导入需要的库</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Type</span>, <span class="type">Union</span>, <span class="type">List</span>, <span class="type">Optional</span></span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br></pre></td></tr></table></figure><ul><li>架构中的最小单元“3x3卷积层”，以此来构建残差单元和瓶颈架构</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把ReLu写在外面，保证相加后可以ReLu</span></span><br><span class="line"><span class="comment"># 步长不能写死,有缩减有保留，但大部分为1，故写默认为1</span></span><br><span class="line"><span class="comment"># 输入与输出不能写死</span></span><br><span class="line"><span class="comment"># 最后一层的gama和belta为0就可以了</span></span><br><span class="line"><span class="comment"># bn类和conv2d类都不存在的参数：initialzero参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv3x3</span>(<span class="params">in_, out_, stride=<span class="number">1</span>, initialzero = <span class="literal">False</span></span>):</span><br><span class="line">    bn = nn.BatchNorm2d(out_)</span><br><span class="line">    <span class="comment">#需要进行判断：要对BN进行0初始化吗？</span></span><br><span class="line">    <span class="comment">#最后一层就初始化,不是最后一层就不改变gamma和beta</span></span><br><span class="line">    <span class="keyword">if</span> initialzero == <span class="literal">True</span>:</span><br><span class="line">        nn.init.constant_(bn.weight, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Conv2d(in_, out_</span><br><span class="line">                            , kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>, stride = stride</span><br><span class="line">                            , bias = <span class="literal">False</span>)</span><br><span class="line">                         ,bn)</span><br></pre></td></tr></table></figure><ul><li>架构中的最小单元“1x1卷积层”，以此来构建残差单元和瓶颈架构</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">conv1x1</span>(<span class="params">in_, out_, stride=<span class="number">1</span>, initialzero = <span class="literal">False</span></span>):</span><br><span class="line">    bn = nn.BatchNorm2d(out_)</span><br><span class="line">    <span class="comment">#需要进行判断：要对BN进行0初始化吗？</span></span><br><span class="line">    <span class="comment">#最后一层就初始化,不是最后一层就不改变gamma和beta</span></span><br><span class="line">    <span class="keyword">if</span> initialzero == <span class="literal">True</span>:</span><br><span class="line">        nn.init.constant_(bn.weight, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Conv2d(in_, out_</span><br><span class="line">                            , kernel_size=<span class="number">1</span>,padding=<span class="number">0</span>, stride = stride</span><br><span class="line">                            , bias = <span class="literal">False</span>)</span><br><span class="line">                         ,bn)</span><br></pre></td></tr></table></figure><ul><li>每个残差单元最后一个卷积层的bn层上，初始化为0</li><li>特征图减半发生在layers与layers之间，且发生在每个layers的第一个残差单元的第一个卷积层上</li><li>相加问题，跳跃连接不是随时都要，只有当需要调整特征图尺寸时候才要，在残差单元中也只有Layers与Layers相连才需要调整尺寸，故forward中要加if，且注意forward中变量要变成属性才能调用</li><li>特征图数量变化，stride=2时，in是out的一半，stride=1时，in和out相等</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualUnit</span>(nn.Module):</span><br><span class="line">    <span class="comment">#这是残差单元类</span></span><br><span class="line">    <span class="comment">#stride1是否等于2呢？如果等于2 - 特征图尺寸会发生变化</span></span><br><span class="line">    <span class="comment">#需要在跳跃链接上增加1x1卷积层来调整特征图尺寸</span></span><br><span class="line">    <span class="comment">#如果stride1等于1，则什么也不需要做</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,out_: <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,stride1: <span class="built_in">int</span> = <span class="number">1</span> <span class="comment">#定义该参数的类型，并且定义默认值</span></span></span><br><span class="line"><span class="params">                 ,in_ : <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span></span><br><span class="line"><span class="params">                </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.stride1 = stride1  <span class="comment"># 不定义属性的话捕捉不到stride1参数，</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#当特征图尺寸需要缩小时，卷积层的输出特征图数量out_等于输入特征图数量in_的2被</span></span><br><span class="line">        <span class="comment">#当特征图尺寸不需要缩小时，out_ == in_</span></span><br><span class="line">        <span class="keyword">if</span> stride1 !=<span class="number">1</span>:</span><br><span class="line">            in_ = <span class="built_in">int</span>(out_/<span class="number">2</span>)  <span class="comment"># 仅仅三次</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            in_ = out_</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#拟合部分，输出F(x)</span></span><br><span class="line">        self.fit_ = nn.Sequential(conv3x3(in_,out_,stride=stride1)</span><br><span class="line">                                 ,nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                 ,conv3x3(out_,out_,initialzero=<span class="literal">True</span>)</span><br><span class="line">                                 <span class="comment"># 单纯拟合，还没加和 故没ReLu</span></span><br><span class="line">                                 )</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#跳跃链接，输出x(1x1卷积核之后的x)</span></span><br><span class="line">        self.skipconv = conv1x1(in_,out_,stride = stride1)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#单独定义放在H(x)之后来使用的激活函数ReLU</span></span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        fx = self.fit_(x) <span class="comment">#拟合结果</span></span><br><span class="line">        <span class="keyword">if</span> self.stride1 != <span class="number">1</span>:</span><br><span class="line">            x = self.skipconv(x) <span class="comment">#跳跃链接</span></span><br><span class="line">        hx = self.relu(fx + x)</span><br><span class="line">        <span class="keyword">return</span> hx</span><br></pre></td></tr></table></figure><ul><li>测试残差单元</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data = torch.ones(<span class="number">10</span>,<span class="number">64</span>,<span class="number">56</span>,<span class="number">56</span>) <span class="comment">#特征图尺寸64x64，输入特征图数量64</span></span><br><span class="line">conv3_x_18_0 = ResidualUnit(<span class="number">128</span>,stride1=<span class="number">2</span>) <span class="comment">#特征图尺寸折半，特征图数量加倍</span></span><br><span class="line">conv3_x_18_0(data).shape  <span class="comment"># (10,128,28,28)</span></span><br><span class="line"></span><br><span class="line">conv2_x_18_0 = ResidualUnit(<span class="number">64</span>) <span class="comment">#特征图尺寸不变，特征图数量也不变</span></span><br><span class="line">conv2_x_18_0(data).shape  <span class="comment"># (10,64,56,56)</span></span><br></pre></td></tr></table></figure><ul><li>特征图数量：<ul><li>layers与layers相连时，需要缩小特征图，上层的in等于middle的两倍（stride=2）</li><li>瓶颈与瓶颈相连时，需要缩小特征图，上层的in等于middle的4倍（stride=1)</li><li>conv1与conv2连接时，上层的in等于middle的值(in不是None的时候)</li></ul></li><li>用middle来调整out和in</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    <span class="comment">#是需要将特征图尺寸缩小的场合吗？</span></span><br><span class="line">    <span class="comment">#conv2_x - conv3_x - conv4_x - conv5_x 相互链接的时候</span></span><br><span class="line">    <span class="comment">#每次都需要将特征图尺寸折半，同时卷积层上的middle_out = 1/2in_</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, middle_out</span></span><br><span class="line"><span class="params">                 , stride1: <span class="built_in">int</span> = <span class="number">1</span></span></span><br><span class="line"><span class="params">                 , in_: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        out_ = <span class="number">4</span> * middle_out</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#我希望使用选填参数in_来帮助我们区别，这个架构是不是在conv1的后面</span></span><br><span class="line">        <span class="comment">#如果这个架构不是紧跟在conv1后，就不填写in_</span></span><br><span class="line">        <span class="comment">#如果是跟在conv1后，就填写in_ = 64</span></span><br><span class="line">        <span class="keyword">if</span> in_ == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> stride1 !=<span class="number">1</span>: <span class="comment">#缩小特征图的场合，即这个瓶颈结构是每个layers的第一个瓶颈结构</span></span><br><span class="line">                in_ = middle_out * <span class="number">2</span></span><br><span class="line">                <span class="comment">#不缩小特征图的场合，即这个瓶颈结构不是这个layers的第一个瓶颈结构</span></span><br><span class="line">                <span class="comment">#而是跟在第一个瓶颈结构后的重复的结构</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                in_ = middle_out * <span class="number">4</span></span><br><span class="line">        </span><br><span class="line">        self.fit_ = nn.Sequential(conv1x1(in_,middle_out,stride=stride1)</span><br><span class="line">                                 ,nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                 ,conv3x3(middle_out,middle_out)</span><br><span class="line">                                 ,nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                 ,conv1x1(middle_out,out_,initialzero=<span class="literal">True</span>))</span><br><span class="line">        </span><br><span class="line">        self.skipconv = conv1x1(in_, out_, stride=stride1)</span><br><span class="line">        </span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        fx = self.fit_(x)</span><br><span class="line">        <span class="comment">#跳跃链接</span></span><br><span class="line">        x = self.skipconv(x)</span><br><span class="line">        hx = self.relu(fx + x)</span><br><span class="line">        <span class="keyword">return</span> hx</span><br></pre></td></tr></table></figure><ul><li>测试瓶颈架构</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">data1 = torch.ones(<span class="number">10</span>,<span class="number">64</span>,<span class="number">56</span>,<span class="number">56</span>) <span class="comment">#特征图尺寸56x56，特征图数量64</span></span><br><span class="line"><span class="comment">#  是conv1后紧跟的第一个瓶颈结构</span></span><br><span class="line">conv2_x_101_0 = Bottleneck(in_ = <span class="number">64</span>, middle_out = <span class="number">64</span>)</span><br><span class="line">conv2_x_101_0(data1).shape  <span class="comment"># (10,256,56,56)</span></span><br><span class="line"></span><br><span class="line">data2 = torch.ones(<span class="number">10</span>,<span class="number">256</span>,<span class="number">56</span>,<span class="number">56</span>)</span><br><span class="line"><span class="comment">#不是conv1后紧跟的第一个瓶颈结构，但是需要缩小特征图尺寸</span></span><br><span class="line">conv3_x_101_0 = Bottleneck(middle_out = <span class="number">128</span>, stride1=<span class="number">2</span>)</span><br><span class="line">conv3_x_101_0(data2).shape <span class="comment">#输出翻2倍并缩小特征图尺寸至一半 (10,512,28,28)</span></span><br><span class="line"></span><br><span class="line">data3 = torch.ones(<span class="number">10</span>,<span class="number">512</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="comment">#不是conv1后紧跟的第一个瓶颈结构，也不需要缩小特征图尺寸</span></span><br><span class="line">conv3_x_101_1 = Bottleneck(<span class="number">128</span>)</span><br><span class="line">conv3_x_101_1(data3).shape <span class="comment">#输出不变，特征图尺寸也不变 (10,512,128,128)</span></span><br></pre></td></tr></table></figure><ul><li>合并两个类、</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_layers</span>(<span class="params">block: <span class="type">Type</span>[<span class="type">Union</span>[ResidualUnit, Bottleneck]]</span></span><br><span class="line"><span class="params">                ,middle_out: <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                ,num_blocks: <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                ,afterconv1: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span><br><span class="line">    </span><br><span class="line">    layers = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> afterconv1 == <span class="literal">True</span>:</span><br><span class="line">        layers.append(block(middle_out, in_ = <span class="number">64</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        layers.append(block(middle_out, stride1 = <span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_blocks-<span class="number">1</span>):</span><br><span class="line">        layers.append(block(middle_out))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure><ul><li>测试</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#测试 - 需要分别对残差块和瓶颈架构进行测试，并且需要对conv1后的首个架构，以及中间的架构进行测试</span></span><br><span class="line"><span class="comment">#注意检查：输入的数据结构是否正确，网络能否允许正确的数据结构输入，输入后产出的结构是否正确，包括</span></span><br><span class="line"><span class="comment">#特征图尺寸是否变化、特征图数量是否变化，以及一个layers中所包含的blocks数量是否正确</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#18层网络，紧跟在conv1之后的conv2_x，不改变特征图尺寸，且每层的输出都是64</span></span><br><span class="line">datashape = (<span class="number">10</span>,<span class="number">64</span>,<span class="number">56</span>,<span class="number">56</span>) <span class="comment">#特征图尺寸56x56，特征图数量64</span></span><br><span class="line">conv2_x_18 = make_layers(ResidualUnit, middle_out = <span class="number">64</span>, blocks=<span class="number">2</span>, afterconv1=<span class="literal">True</span>)</span><br><span class="line">summary(conv2_x_18,datashape,depth=<span class="number">1</span>,device=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#34层网络，conv4_x，缩小特征图尺寸，且每层的输出翻倍</span></span><br><span class="line">datashape2 = (<span class="number">10</span>,<span class="number">128</span>,<span class="number">14</span>,<span class="number">14</span>)</span><br><span class="line">conv2_x_34 = make_layers(ResidualUnit, middle_out = <span class="number">256</span>, blocks=<span class="number">6</span>,afterconv1=<span class="literal">False</span>)</span><br><span class="line">summary(conv2_x_34,datashape2,depth=<span class="number">1</span>,device=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#101层网络，紧跟在conv1之后的conv2_x，不改变特征图尺寸，1x1和3x3卷积层的输出是64，整个瓶颈架构的输出是256</span></span><br><span class="line">conv2_x_101 = make_layers(Bottleneck, middle_out = <span class="number">64</span>, blocks=<span class="number">3</span>, afterconv1=<span class="literal">True</span>)</span><br><span class="line">summary(conv2_x_101,datashape,depth=<span class="number">3</span>,device=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#101层网络，conv4_x，缩小特征图尺寸，且每层的输出翻4倍</span></span><br><span class="line">datashape3 = (<span class="number">10</span>,<span class="number">512</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">conv4_x_101 = make_layers(Bottleneck, middle_out = <span class="number">256</span>, blocks=<span class="number">23</span>, afterconv1=<span class="literal">False</span>)</span><br><span class="line">summary(conv4_x_101,datashape3,depth=<span class="number">1</span>,device=<span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>构建残差网络</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,block: <span class="type">Type</span>[<span class="type">Union</span>[ResidualUnit, Bottleneck]]</span></span><br><span class="line"><span class="params">                ,layers: <span class="type">List</span>[<span class="built_in">int</span>]</span></span><br><span class="line"><span class="params">                ,num_classes : <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        block：要使用的用来加深深度的基本架构是？可以选择残差单元或瓶颈结构，两种都带有skip connection</span></span><br><span class="line"><span class="string">        layers：列表，每个层里具体有多少个块呢？可参考网络架构图。例如，34层的残差网络的layers = [3,4,6,3]</span></span><br><span class="line"><span class="string">        num_classes：真实标签含有多少个类别？</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#layer1:卷积+池化的组合</span></span><br><span class="line">        self.layer1 = nn.Sequential(nn.Conv2d(<span class="number">3</span>,<span class="number">64</span></span><br><span class="line">                                              ,kernel_size=<span class="number">7</span>,stride=<span class="number">2</span></span><br><span class="line">                                              ,padding=<span class="number">3</span>,bias = <span class="literal">False</span>)</span><br><span class="line">                                   ,nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">                                   ,nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                   ,nn.MaxPool2d(kernel_size=<span class="number">3</span></span><br><span class="line">                                                 ,stride=<span class="number">2</span></span><br><span class="line">                                                 ,ceil_mode = <span class="literal">True</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#layer2 - layer5:残差块/瓶颈结构</span></span><br><span class="line">        self.layer2_x = make_layers(block,<span class="number">64</span>,layers[<span class="number">0</span>],afterconv1=<span class="literal">True</span>)</span><br><span class="line">        self.layer3_x = make_layers(block,<span class="number">128</span>,layers[<span class="number">1</span>])</span><br><span class="line">        self.layer4_x = make_layers(block,<span class="number">256</span>,layers[<span class="number">2</span>])</span><br><span class="line">        self.layer5_x = make_layers(block,<span class="number">512</span>,layers[<span class="number">3</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#全局平均池化</span></span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#分类</span></span><br><span class="line">        <span class="keyword">if</span> block == ResidualUnit:</span><br><span class="line">            self.fc = nn.Linear(<span class="number">512</span>,num_classes)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.fc = nn.Linear(<span class="number">2048</span>,num_classes)</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.layer1(x) <span class="comment">#layer1，普通卷积+池化的输出</span></span><br><span class="line">        x = self.layer5_x(self.layer4_x(self.layer3_x(self.layer2_x(x))))</span><br><span class="line">        x = self.avgpool(x) <span class="comment">#特征图尺寸1x1 (n_samples, fc, 1, 1)</span></span><br><span class="line">        x = torch.flatten(x,<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br></pre></td></tr></table></figure><ul><li>测试残差网络</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">datashape = (<span class="number">10</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line"></span><br><span class="line">res34 = ResNet(ResidualUnit, layers=[<span class="number">3</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">3</span>], num_classes=<span class="number">1000</span>)</span><br><span class="line">summary(res34,datashape,depth=<span class="number">2</span>,device=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">res101 = ResNet(Bottleneck, layers=[<span class="number">3</span>,<span class="number">4</span>,<span class="number">23</span>,<span class="number">3</span>], num_classes=<span class="number">1000</span>)</span><br><span class="line">summary(res101,datashape,depth=<span class="number">2</span>,device=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment">#你能数出有多少层吗？一个ResidualUnit中含有2个卷积层，一个Bottleneck中含有3个卷积层，卷积层</span></span><br><span class="line"><span class="comment">#的数目 + 开头的conv1和结尾的线性层，就是整个网络的深度</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(7)-欢迎来到深度学习的世界</title>
      <link href="/Pytorch-7/"/>
      <url>/Pytorch-7/</url>
      
        <content type="html"><![CDATA[<h4 id="1-神经网络-深度学习-机器学习三者关系"><a href="#1-神经网络-深度学习-机器学习三者关系" class="headerlink" title="1. 神经网络/深度学习/机器学习三者关系"></a>1. 神经网络/深度学习/机器学习三者关系</h4><ul><li><p>数学家提出<strong>算法</strong></p></li><li><p>计算机科学家想到让其规模化，提出<strong>机器学习</strong></p></li><li><p>哲学家想到像人类一样学习，提出<strong>人工神经网络</strong></p></li><li><p>神经网络有致命的<strong>弱点</strong>：</p><ul><li>预测效果差(最初只能解决线性问题)</li><li>数据需求大(需要大量的数据去喂养)</li><li>计算时间长</li></ul></li></ul><p>导致其<strong>根本无法到达人类智力水平</strong></p><ul><li>同时期 KNN,决策树等提出，<strong>机器学习其他算法繁荣</strong>，神经网络无人问津。</li><li>新世纪，<strong>算法进步</strong>、<strong>数据量激增</strong>、<strong>算力提升</strong>三者释放了神经网络的潜力。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Performance-Data.png" alt="image-20240103191554847"></p><ul><li>Hinton 提出为神经网络的相关研究提出<strong>深度学习</strong>一词</li></ul><h4 id="2-机器学习中的基本概念"><a href="#2-机器学习中的基本概念" class="headerlink" title="2. 机器学习中的基本概念"></a>2. 机器学习中的基本概念</h4><ul><li><strong>样本、特征、标签</strong></li><li>图片中 3 维以外即<strong>索引</strong></li><li><strong>分类</strong>是离散的，<strong>回归</strong>是连续的</li><li>有标签的任务称为<strong>有监督学习</strong>(KNN,SVM,线性回归,逻辑回归)；</li><li>无标签的任务称为<strong>无监督学习</strong>(聚类分析,协同分析,自编码器)，<strong>辅助</strong>作用</li><li>其他还有<strong>半监督学习</strong>，<strong>强化学习</strong>等</li></ul><h4 id="3-模型评判标准"><a href="#3-模型评判标准" class="headerlink" title="3. 模型评判标准"></a>3. 模型评判标准</h4><ul><li>模型预测效果</li><li>运算速度</li><li>可解释性</li><li>服务于业务</li></ul><h4 id="4-Pytorch-优势"><a href="#4-Pytorch-优势" class="headerlink" title="4. Pytorch 优势"></a>4. Pytorch 优势</h4><ul><li>承受大数据</li><li>调整 NN 架构</li><li>API 简化</li><li><p>服务工业 jit</p></li><li><p>组成</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Pytorch%E7%BB%84%E6%88%901.png" alt="Pytorch组成1"></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Pytorch%E7%BB%84%E6%88%902.png" alt="Pytorch组成2"></p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Git指令</title>
      <link href="/Git/"/>
      <url>/Git/</url>
      
        <content type="html"><![CDATA[<h4 id="1-查看文件"><a href="#1-查看文件" class="headerlink" title="1. 查看文件"></a>1. 查看文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">dir</span></span><br></pre></td></tr></table></figure><blockquote><p>可在文件夹中查看所有的文件名</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Git%E6%9F%A5%E7%9C%8B%E6%96%87%E4%BB%B6" alt="image-20240103135110537"></p><h4 id="2-修改文件"><a href="#2-修改文件" class="headerlink" title="2. 修改文件"></a>2. 修改文件</h4><h5 id="2-1-拉取最新的代码"><a href="#2-1-拉取最新的代码" class="headerlink" title="2.1 拉取最新的代码"></a>2.1 拉取最新的代码</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull</span><br></pre></td></tr></table></figure><h5 id="2-2-重命名文件"><a href="#2-2-重命名文件" class="headerlink" title="2.2 重命名文件"></a>2.2 重命名文件</h5><h6 id="命令行方法"><a href="#命令行方法" class="headerlink" title="命令行方法"></a>命令行方法</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> A.png B.png  <span class="comment"># A改为B</span></span><br></pre></td></tr></table></figure><h6 id="本地文件夹"><a href="#本地文件夹" class="headerlink" title="本地文件夹"></a>本地文件夹</h6><p>直接重命名文件即可。</p><h5 id="2-3-将改动添加到暂存区"><a href="#2-3-将改动添加到暂存区" class="headerlink" title="2.3 将改动添加到暂存区"></a>2.3 将改动添加到暂存区</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add -A</span><br></pre></td></tr></table></figure><h5 id="2-4-提交更改"><a href="#2-4-提交更改" class="headerlink" title="2.4 提交更改"></a>2.4 提交更改</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m <span class="string">&quot;Renamed image file&quot;</span></span><br></pre></td></tr></table></figure><h5 id="2-5-推送到远程仓库"><a href="#2-5-推送到远程仓库" class="headerlink" title="2.5 推送到远程仓库"></a>2.5 推送到远程仓库</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>查看版本</title>
      <link href="/Related%20versions/"/>
      <url>/Related%20versions/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Pytorch版本"><a href="#1-Pytorch版本" class="headerlink" title="1. Pytorch版本"></a>1. Pytorch版本</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch   </span><br><span class="line"><span class="built_in">print</span>(torch.__version__)  <span class="comment"># 注意是双下划线</span></span><br></pre></td></tr></table></figure><h4 id="2-torchvision版本"><a href="#2-torchvision版本" class="headerlink" title="2. torchvision版本"></a>2. torchvision版本</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="built_in">print</span>(torchvision.__version__)</span><br></pre></td></tr></table></figure><h4 id="3-cuda目前版本"><a href="#3-cuda目前版本" class="headerlink" title="3. cuda目前版本"></a>3. cuda目前版本</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda)  <span class="comment">#注意是双下划线</span></span><br></pre></td></tr></table></figure><h4 id="4-cuda最高版本"><a href="#4-cuda最高版本" class="headerlink" title="4. cuda最高版本"></a>4. cuda最高版本</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/查看cuda最高版本.png" alt="查看cuda最高版本"></p><h4 id="5-cudnn版本"><a href="#5-cudnn版本" class="headerlink" title="5. cudnn版本"></a>5. cudnn版本</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.backends.cudnn.version())  <span class="comment">#注意是双下划线</span></span><br></pre></td></tr></table></figure><h4 id="6-查看显卡"><a href="#6-查看显卡" class="headerlink" title="6. 查看显卡"></a>6. 查看显卡</h4><h5 id="6-1-查看显卡数量"><a href="#6-1-查看显卡数量" class="headerlink" title="6.1 查看显卡数量"></a>6.1 查看显卡数量</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.device_count()  <span class="comment"># 查看GPU数量</span></span><br></pre></td></tr></table></figure><h5 id="6-2-查看是否有可用GPU"><a href="#6-2-查看是否有可用GPU" class="headerlink" title="6.2 查看是否有可用GPU"></a>6.2 查看是否有可用GPU</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.is_available()  <span class="comment"># 查看是否有可用GPU</span></span><br></pre></td></tr></table></figure><h5 id="6-3-查看GPU算力"><a href="#6-3-查看GPU算力" class="headerlink" title="6.3 查看GPU算力"></a>6.3 查看GPU算力</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.get_device_capability(device)  <span class="comment"># 查看指定GPU算力，参数device是数字，从0开始</span></span><br><span class="line"><span class="comment">#例如有两张显卡</span></span><br><span class="line">torch.cuda.get_device_capability(<span class="number">0</span>)  <span class="comment"># 查看第0张显卡</span></span><br><span class="line">torch.cuda.get_device_capability(<span class="number">1</span>)  <span class="comment"># 查看第1张显卡</span></span><br></pre></td></tr></table></figure><h5 id="6-4-查看GPU名称"><a href="#6-4-查看GPU名称" class="headerlink" title="6.4 查看GPU名称"></a>6.4 查看GPU名称</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.get_device_name(device)  <span class="comment"># 查看指定GPU名称，参数device是数字，从0开始</span></span><br><span class="line"><span class="comment">#例如有两张显卡</span></span><br><span class="line">torch.cuda.get_device_name(<span class="number">0</span>)  <span class="comment"># 查看第0张显卡名称</span></span><br><span class="line">torch.cuda.get_device_name(<span class="number">1</span>)  <span class="comment"># 查看第1张显卡名称</span></span><br></pre></td></tr></table></figure><h5 id="6-5-查看当前GPU"><a href="#6-5-查看当前GPU" class="headerlink" title="6.5 查看当前GPU"></a>6.5 查看当前GPU</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看当前使用的显卡，返回的是显卡的序号</span></span><br><span class="line">torch.cuda.current_device()</span><br></pre></td></tr></table></figure><h5 id="6-6-其他命令"><a href="#6-6-其他命令" class="headerlink" title="6.6 其他命令"></a>6.6 其他命令</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.empty_cache() <span class="comment"># 清空程序占用的GPU资源</span></span><br><span class="line"></span><br><span class="line">torch.cuda.manual_seed(seed) <span class="comment"># 设置随机种子</span></span><br><span class="line"></span><br><span class="line">torch.cuda.manual_seed_all(seed) <span class="comment"># 设置随机种子</span></span><br></pre></td></tr></table></figure><h4 id="7-指定显卡"><a href="#7-指定显卡" class="headerlink" title="7. 指定显卡"></a>7. 指定显卡</h4><h5 id="7-1在Pytorch程序里面指定显卡"><a href="#7-1在Pytorch程序里面指定显卡" class="headerlink" title="7.1在Pytorch程序里面指定显卡"></a>7.1在Pytorch程序里面指定显卡</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置torch的调用显卡（有效）</span></span><br><span class="line">torch.cuda.set_device(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出当前卡的信息</span></span><br><span class="line">A=torch.arange(<span class="number">12</span>,dtype=torch.float32)</span><br><span class="line">B=A.cuda()</span><br><span class="line"><span class="built_in">print</span>(B.device)  <span class="comment"># cuda:0</span></span><br></pre></td></tr></table></figure><h5 id="7-2在os里面指定显卡"><a href="#7-2在os里面指定显卡" class="headerlink" title="7.2在os里面指定显卡"></a>7.2在os里面指定显卡</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&quot;2,1,3,4&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;torch.cuda.device_count() &#123;&#125;&quot;</span>.<span class="built_in">format</span>(torch.cuda.device_count()))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(11)-优化SGD</title>
      <link href="/Pytorch-11/"/>
      <url>/Pytorch-11/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br></pre></td></tr></table></figure><h3 id="1-神经网络的复杂性与优化"><a href="#1-神经网络的复杂性与优化" class="headerlink" title="1. 神经网络的复杂性与优化"></a>1. 神经网络的复杂性与优化</h3><ul><li>神经网络的<strong>函数复杂度</strong>使得求导过程复杂。</li><li>面对<strong>成千上万的权重 (w)</strong>，逐个计算梯度不现实。</li><li>优化算法核心：<strong>逐步迭代</strong>至最小值。</li></ul><h3 id="2-梯度下降的方向与距离"><a href="#2-梯度下降的方向与距离" class="headerlink" title="2. 梯度下降的方向与距离"></a>2. 梯度下降的方向与距离</h3><ul><li><strong>方向</strong>：梯度下降的反方向。</li><li><strong>距离</strong>：步长乘以梯度向量的大小。</li><li>每个坐标点的梯度方向独一无二。</li></ul><p>梯度下降公式：</p><script type="math/tex; mode=display">    w_{(t+1)} = w_{(t)} - \eta \frac{\partial L}{\partial w}</script><ul><li>梯度下降的反方向总比损失函数低一个维度。</li></ul><h3 id="3-反向传播的实现"><a href="#3-反向传播的实现" class="headerlink" title="3. 反向传播的实现"></a>3. 反向传播的实现</h3><ul><li>在 PyTorch 中实现反向传播。</li><li>输出 <code>y</code> 需为向量形式。</li><li>当使用交叉熵损失 (<code>CrossEntropyLoss</code>) 类时：<ul><li>该类已内置 <code>sigmoid</code> 功能。</li><li>从 <code>forward</code> 函数的输出层中去除 <code>sigmoid</code>。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">X = torch.rand((<span class="number">500</span>,<span class="number">20</span>),dtype=torch.float32) * <span class="number">100</span></span><br><span class="line">y = torch.randint(low=<span class="number">0</span>,high=<span class="number">3</span>,size=(<span class="number">500</span>,),dtype=torch.float32)  <span class="comment"># 向量形式</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_features=<span class="number">10</span>,out_features=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__() <span class="comment">#super(请查找这个类的父类，请使用找到的父类替换现在的类)</span></span><br><span class="line">        self.linear1 = nn.Linear(in_features,<span class="number">13</span>,bias=<span class="literal">True</span>) <span class="comment">#输入层不用写，这里是隐藏层的第一层</span></span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">13</span>,<span class="number">8</span>,bias=<span class="literal">True</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">8</span>,out_features,bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        z1 = self.linear1(x)  <span class="comment"># Hidden1-1</span></span><br><span class="line">        sigma1 = torch.relu(z1)  <span class="comment"># Hidden1-2</span></span><br><span class="line">        z2 = self.linear2(sigma1)  <span class="comment"># Hidden2-1</span></span><br><span class="line">        sigma2 = torch.sigmoid(z2)  <span class="comment"># Hidden2-1</span></span><br><span class="line">        z3 = self.output(sigma2)  <span class="comment"># Output</span></span><br><span class="line">        <span class="comment">#sigma3 = F.softmax(z3,dim=1)  </span></span><br><span class="line">        <span class="keyword">return</span> z3</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算数据大小</span></span><br><span class="line">input_ = X.shape[<span class="number">1</span>] <span class="comment">#特征的数目</span></span><br><span class="line">output_ = <span class="built_in">len</span>(y.unique()) <span class="comment">#分类的数目</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#实例化神经网络类</span></span><br><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">net = Model(in_features=input_, out_features=output_)</span><br><span class="line"></span><br><span class="line"><span class="comment">#前向传播</span></span><br><span class="line">zhat = net.forward(X)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义损失函数</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment">#对打包好的CorssEnrtopyLoss而言，只需要输入zhat</span></span><br><span class="line">loss = criterion(zhat,y.long())</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br><span class="line">loss.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#net.linear1.weight.grad 可打印梯度</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor(1.1057, grad_fn=&lt;NllLossBackward0&gt;)</code></pre><h3 id="4-移动坐标点"><a href="#4-移动坐标点" class="headerlink" title="4.移动坐标点"></a>4.移动坐标点</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">dw = net.linear1.weight.grad</span><br><span class="line">w = net.linear1.weight.data</span><br><span class="line"></span><br><span class="line"><span class="comment">#对任意w可以有</span></span><br><span class="line">w-=lr*dw</span><br></pre></td></tr></table></figure><h3 id="5-动量法Momentum"><a href="#5-动量法Momentum" class="headerlink" title="5.动量法Momentum"></a>5.动量法Momentum</h3><script type="math/tex; mode=display">v(t) = \gamma v(t-1) - \eta \frac{\partial L}{\partial w}</script><script type="math/tex; mode=display">w(t+1) = w(t) + v(t)</script><script type="math/tex; mode=display">\begin{align*}t &= 0 \quad v(0) = 0 \\w(1)=initiate \\t &= 1 \quad v(1) = r v(0) - (\eta \frac{\partial L}{\partial w})_0 \\w(2) &= w(1) + v(1) = w(1) - (\eta \frac{\partial L}{\partial w})_0 \\t &= 2 \quad v(2) = r v(1) - (\eta \frac{\partial L}{\partial w})_1 = - (\eta \frac{\partial L}{\partial w})_1 - r(\eta \frac{\partial L}{\partial w})_0 \\w(3) &= w(2) + v(2) = w(2) - (\eta \frac{\partial L}{\partial w})_1 - r(\eta \frac{\partial L}{\partial w})_0 \\t &= 3 \quad v(3) = r v(2) - (\eta \frac{\partial L}{\partial w})_2 = - (\eta \frac{\partial L}{\partial w})_2 - r(\eta \frac{\partial L}{\partial w})_1 - r^2(\eta \frac{\partial L}{\partial w})_0 \\w(4) &= w(3) + v(3) = w(3) - (\eta \frac{\partial L}{\partial w})_2 - r(\eta \frac{\partial L}{\partial w})_1 - r^2(\eta \frac{\partial L}{\partial w})_0 \\\end{align*}</script><ul><li>梯度<code>下降的方向</code>有了<code>惯性</code></li><li>上一步的梯度向量与现在这一点的梯度向量以加权的方式求和，求解出真实梯度向量</li></ul><h4 id="tensor实现动量法"><a href="#tensor实现动量法" class="headerlink" title="tensor实现动量法"></a>tensor实现动量法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#恢复小步长</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line"></span><br><span class="line">dw = net.linear1.weight.grad</span><br><span class="line">w = net.linear1.weight.data</span><br><span class="line">v = torch.zeros(dw.shape[<span class="number">0</span>],dw.shape[<span class="number">1</span>]) <span class="comment"># v和w的形状相同</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#==========分割cell，不然重复运行的时候w会每次都被覆盖掉=============</span></span><br><span class="line"><span class="comment">#对任意w可以有</span></span><br><span class="line">v = gamma * v - lr * dw</span><br><span class="line">w += v</span><br><span class="line"></span><br><span class="line"><span class="comment">#不难发现，当加入gamma之后，即便是较小的步长，也可以让w发生变化</span></span><br></pre></td></tr></table></figure><h4 id="torch-optim实现动量法"><a href="#torch-optim实现动量法" class="headerlink" title="torch.optim实现动量法"></a>torch.optim<code>实现</code>动量法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#确定数据、确定优先需要设置的值</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">X = torch.rand((<span class="number">500</span>,<span class="number">20</span>),dtype=torch.float32) * <span class="number">100</span></span><br><span class="line">y = torch.randint(low=<span class="number">0</span>,high=<span class="number">3</span>,size=(<span class="number">500</span>,<span class="number">1</span>),dtype=torch.float32)</span><br><span class="line">input_ = X.shape[<span class="number">1</span>] <span class="comment">#特征的数目</span></span><br><span class="line">output_ = <span class="built_in">len</span>(y.unique()) <span class="comment">#分类的数目</span></span><br><span class="line"><span class="comment">#定义神经网路的架构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_features=<span class="number">10</span>,out_features=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Model,self).__init__() <span class="comment">#super(请查找这个类的父类，请使用找到的父类替换现在的类)</span></span><br><span class="line">        self.linear1 = nn.Linear(in_features,<span class="number">13</span>,bias=<span class="literal">True</span>) <span class="comment">#输入层不用写，这里是隐藏层的第一层</span></span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">13</span>,<span class="number">8</span>,bias=<span class="literal">True</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">8</span>,out_features,bias=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        z1 = self.linear1(x)</span><br><span class="line">        sigma1 = torch.relu(z1)</span><br><span class="line">        z2 = self.linear2(sigma1)</span><br><span class="line">        sigma2 = torch.sigmoid(z2)</span><br><span class="line">        z3 = self.output(sigma2)</span><br><span class="line">        <span class="comment">#sigma3 = F.softmax(z3,dim=1)</span></span><br><span class="line">        <span class="keyword">return</span> z3</span><br><span class="line"><span class="comment">#实例化神经网络，调用优化算法需要的参数</span></span><br><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">net = Model(in_features=input_, out_features=output_)</span><br><span class="line">net.parameters()</span><br><span class="line">    </span><br><span class="line"><span class="comment">#定义损失函数</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment">#定义优化算法</span></span><br><span class="line">opt = optim.SGD(net.parameters() <span class="comment">#要优化的参数是哪些？</span></span><br><span class="line">               , lr=lr <span class="comment">#学习率</span></span><br><span class="line">               , momentum = gamma <span class="comment">#动量参数</span></span><br><span class="line">               )</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="神经网络迭代过程简述"><a href="#神经网络迭代过程简述" class="headerlink" title="神经网络迭代过程简述"></a>神经网络迭代过程简述</h4><ul><li><p><strong>迭代流程</strong>包括以下步骤：</p><ol><li><strong>正向传播</strong>：计算网络的输出。</li><li><strong>计算损失</strong>：根据输出和真实标签计算损失函数。</li><li><strong>反向传播</strong>：通过网络传递损失的梯度。</li><li><strong>更新权重 (w)</strong>：根据梯度调整网络参数。</li><li><strong>清除梯度</strong>：为下一次迭代准备。</li></ol></li><li><p>总结：<strong>“正损反更清”</strong>，即正向传播、计算损失、反向传播、更新权重和清除梯度。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">zhat = net.forward(X) <span class="comment">#向前传播</span></span><br><span class="line">loss = criterion(zhat,y.reshape(<span class="number">500</span>).long()) <span class="comment">#损失函数值</span></span><br><span class="line">loss.backward(retain_graph=<span class="literal">True</span>) <span class="comment">#反向传播</span></span><br><span class="line">opt.step() <span class="comment">#更新权重w，从这一瞬间开始，坐标点就发生了变化，所有的梯度必须重新计算</span></span><br><span class="line">opt.zero_grad() <span class="comment">#清除原来储存好的，基于上一个坐标点计算的梯度，为下一次计算梯度腾出空间</span></span><br><span class="line"><span class="built_in">print</span>(loss)</span><br><span class="line"><span class="built_in">print</span>(net.linear1.weight.data[<span class="number">0</span>][:<span class="number">10</span>])</span><br><span class="line"><span class="comment"># 这样每执行依次loss变小</span></span><br></pre></td></tr></table></figure><pre><code>tensor(1.1057, grad_fn=&lt;NllLossBackward0&gt;)tensor([ 0.1365, -0.1346,  0.2128, -0.1776, -0.0682, -0.1541,  0.1724,  0.0839,        -0.1115, -0.1729])</code></pre><h3 id="6-Batch-Size-与-Epoches-在迭代中的作用"><a href="#6-Batch-Size-与-Epoches-在迭代中的作用" class="headerlink" title="6.Batch Size 与 Epoches 在迭代中的作用"></a>6.Batch Size 与 Epoches 在迭代中的作用</h3><ul><li><p><strong>小批量梯度下降的必要性</strong></p><ul><li><strong>正向传播</strong>：大量数据在正向传播中导致计算速度缓慢。</li><li><strong>反向传播</strong>：采用动量法加速，但若数据量大，仍影响效率。</li><li><strong>Mini-batch</strong>：更易找到全局最小值，提升训练效率。</li></ul></li><li><p><strong>不同梯度下降方法的对比</strong></p><ul><li><strong>传统梯度下降</strong>：相同数据集，仅小范围内权重更新。快速接近最小值，但易陷入局部最优。</li><li><strong>随机梯度下降</strong>：每次一个样本，高随机性，计算不稳定。</li><li><strong>Mini-batch SGD</strong>：结合两者优点，数据集和权重均有变化，路径曲折但有效。</li></ul></li><li><p><strong>优势</strong></p><ul><li>减少计算开销，提高训练效率。</li><li>在寻找最优解过程中，避免局部最优的困扰。</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/SGD.png" alt="SGD"></p><h3 id="7-使用-TensorDataset-与-DataLoader-处理数据"><a href="#7-使用-TensorDataset-与-DataLoader-处理数据" class="headerlink" title="7. 使用 TensorDataset 与 DataLoader 处理数据"></a>7. 使用 TensorDataset 与 DataLoader 处理数据</h3><ul><li><p><strong>TensorDataset 用于数据打包</strong></p><ul><li>功能：将数据打包成元组格式。</li><li>要求：数据的首维度大小需保持一致。</li><li>使用示例：通过 <code>td[0]</code> 访问数据。</li></ul></li><li><p><strong>DataLoader 用于数据分批处理</strong></p><ul><li>功能：切分数据为小批量，便于训练。</li><li>特点：支持任意形式的张量或数组。</li><li>主要参数：<ul><li><code>dataset</code>：包含所有数据的列表，通过 <code>dl.dataset[0]</code> 访问。</li><li><code>batch_size</code>（简写为 <code>bs</code>）：定义每批数据的大小。</li><li>通过 <code>len(dl)</code> 计算得到总批次（即 <code>m/bs</code>）。</li></ul></li></ul></li><li><p><strong>特别说明</strong></p><ul><li>若数据集已经以特征张量和标签的形式组合在一起，可跳过此步骤。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">TensorDataset:打包操作</span></span><br><span class="line"><span class="string">DataLoader：切割操作</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.randn(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">c = torch.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> TensorDataset(a,b,c):<span class="comment"># 元组</span></span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line"><span class="keyword">for</span> x,y,z <span class="keyword">in</span> DataLoader(TensorDataset(a,b,c),batch_size=<span class="number">2</span>,drop_last=<span class="literal">True</span>): <span class="comment"># 列表且升维</span></span><br><span class="line">    <span class="built_in">print</span>(x)</span><br><span class="line">    <span class="built_in">print</span>(x.shape)</span><br><span class="line">    <span class="built_in">print</span>(y.shape)</span><br><span class="line">    <span class="built_in">print</span>(z.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorDataset是元组！</span></span><br><span class="line"><span class="comment"># DataLoader转换为列表！</span></span><br><span class="line"><span class="comment"># batch_size是在每一个列表上升维！</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># c = torch.randn(300,1)</span></span><br><span class="line"><span class="comment"># TensorDataset(a,c)[0] </span></span><br><span class="line"><span class="comment"># 报错</span></span><br></pre></td></tr></table></figure><pre><code>(tensor([[-0.3513, -0.3595,  0.9324],        [-1.1048, -0.8469,  1.4757]]), tensor([[ 0.4467,  1.2014, -0.5548,  0.7919],        [ 0.1874, -0.7003,  0.4888, -0.8637]]), tensor([0.1421]))(tensor([[ 1.8659, -1.3644, -0.3450],        [ 1.7672,  1.4598, -0.3842]]), tensor([[-0.4296, -0.5561, -1.4006,  0.3309],        [-0.6369, -0.2273, -0.6020, -0.3844]]), tensor([-0.1834]))tensor([[[-0.3513, -0.3595,  0.9324],         [-1.1048, -0.8469,  1.4757]],        [[ 1.8659, -1.3644, -0.3450],         [ 1.7672,  1.4598, -0.3842]]])torch.Size([2, 2, 3])torch.Size([2, 2, 4])torch.Size([2, 1])</code></pre><ul><li>对于mini-batch SGD,我们一般这样使用TensorDataset和DataLoader</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">X = torch.rand((<span class="number">50000</span>,<span class="number">20</span>),dtype=torch.float32) * <span class="number">100</span> <span class="comment">#要进行迭代了，增加样本数量</span></span><br><span class="line">y = torch.randint(low=<span class="number">0</span>,high=<span class="number">3</span>,size=(<span class="number">50000</span>,<span class="number">1</span>),dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">4</span></span><br><span class="line">bs = <span class="number">4000</span></span><br><span class="line"></span><br><span class="line">data = TensorDataset(X,y)</span><br><span class="line">batchdata = DataLoader(data, batch_size=bs, shuffle = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">len</span>(batchdata) <span class="comment">#查看具体被分了多少个batch 50000 / 14 = 13(12.5)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#可以使用.datasets查看数据集相关的属性</span></span><br><span class="line"><span class="built_in">print</span>(batchdata.dataset[<span class="number">49999</span>]) <span class="comment">#查看其中一个样本 列表[tensor([500,20]),tensor([1])]</span></span><br><span class="line"><span class="built_in">print</span>(batchdata.batch_size) <span class="comment">#查看其中一个样本 列表[tensor([500,20]),tensor([1])]</span></span><br><span class="line"><span class="keyword">for</span> batch_idx, (x,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(batchdata):</span><br><span class="line">    <span class="comment">#sigma = net(x)</span></span><br><span class="line">    <span class="comment">#loss = lossfn(sigma, y)</span></span><br><span class="line">    <span class="comment">#loss.backward()</span></span><br><span class="line">    <span class="comment">#opt.step()</span></span><br><span class="line">    <span class="comment">#opt.zero_grad()</span></span><br><span class="line">    <span class="built_in">print</span>(batch_idx)</span><br><span class="line">    <span class="built_in">print</span>(x.shape)</span><br><span class="line">    <span class="built_in">print</span>(y.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># print(x,y)</span></span><br><span class="line">    <span class="keyword">if</span> batch_idx == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">break</span> <span class="comment">#为了演示用，所以打断，在正常的循环里是不会打断的</span></span><br></pre></td></tr></table></figure><pre><code>(tensor([61.8071, 94.8606, 77.5624,  4.6598, 50.3617, 10.5395, 37.8427, 88.2321,        55.0742, 11.2930, 35.5145, 37.8230, 83.2974, 16.3466, 91.3999, 76.7965,        71.2780, 36.9534,  1.4939, 31.9203]), tensor([1.]))40000torch.Size([4000, 20])torch.Size([4000, 1])1torch.Size([4000, 20])torch.Size([4000, 1])</code></pre><h3 id="8-在Mini-Fashion上实现神经网络的学习流程"><a href="#8-在Mini-Fashion上实现神经网络的学习流程" class="headerlink" title="8.在Mini-Fashion上实现神经网络的学习流程"></a>8.在Mini-Fashion上实现神经网络的学习流程</h3><ul><li>在<code>Linux/UNIX</code>中,推荐使用<code>/</code>,<code>Python解释器</code>也推荐使用<code>/</code></li><li>在Windows中,推荐使用\</li><li>以下三个表示等价<ul><li>root=<code>&#39;../lesson 11/&#39;</code></li><li>root=<code>&#39;..\\lesson 11\\&#39;</code></li><li>root=<code>r&#39;..\lesson 11\\&#39;</code><ul><li><code>./</code>表示当前py文件的<code>子目录</code>下</li><li><code>../</code>表示当前py文件的<code>子目录的父目录</code>下</li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入库，设置各种初始值</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment">#确定数据、确定优先需要设置的值</span></span><br><span class="line">lr = <span class="number">0.15</span></span><br><span class="line">gamma = <span class="number">0.8</span></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line">bs = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#导入数据</span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line">mnist = torchvision.datasets.FashionMNIST(root=<span class="string">&#x27;../lesson 11/&#x27;</span>,</span><br><span class="line">                                          train=<span class="literal">True</span>,</span><br><span class="line">                                          download=<span class="literal">True</span>,</span><br><span class="line">                                          transform=transforms.ToTensor())</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1. 将一个 PIL 图片或者一个 NumPy ndarray 转换为 FloatTensor。</span></span><br><span class="line"><span class="string">2. 把图片的像素值从 [0, 255] 范围线性缩放到 [0.0, 1.0] 范围。即原先的整数类型像素值被缩放到浮点数，并且归一化到0到1之间的范围。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 这里mnist是一个Dataset的对象</span></span><br></pre></td></tr></table></figure><ul><li><code>mnist</code> 数据集是 <code>Dataset</code> 类的一个实例，主要包含以下特点：<ul><li>数据结构：每个元素是一个元组，包含特征和标签。</li><li><code>data</code> 属性：特征张量的形状为 <code>torch.Size([60000, 28, 28])</code>，表示有 60000 个样本，每个样本为 28x28 的图像。</li><li><code>targets</code> 属性：标签张量的形状为 <code>torch.Size([60000])</code>，表示有 60000 个标签。</li><li><code>classes</code> 属性：类别列表，包括 <code>[&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;]</code>，共 10 类。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看特征张量</span></span><br><span class="line"><span class="built_in">print</span>(mnist.data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看标签</span></span><br><span class="line"><span class="built_in">print</span>(mnist.targets.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看标签的类别</span></span><br><span class="line"><span class="built_in">print</span>(mnist.classes)</span><br></pre></td></tr></table></figure><pre><code>torch.Size([60000, 28, 28])torch.Size([60000])[&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;]</code></pre><ul><li>查看图像的模样</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.imshow(mnist[<span class="number">1</span>][<span class="number">0</span>].view((<span class="number">28</span>, <span class="number">28</span>)).numpy());</span><br><span class="line"><span class="comment"># mnist[1][0]表示第一个元组的特征张量(1,28,28) </span></span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/shirt.png" alt="shirt"></p><ul><li>分割batch</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">batchdata = DataLoader(mnist,batch_size=bs, shuffle = <span class="literal">True</span>)</span><br><span class="line"><span class="comment">#总共多少个batch?</span></span><br><span class="line"><span class="built_in">len</span>(batchdata) <span class="comment"># 60000/128=469(ceil)</span></span><br><span class="line"><span class="comment">#查看会放入进行迭代的数据结构</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> batchdata:</span><br><span class="line">    <span class="built_in">print</span>(x.shape)  <span class="comment"># torch.Size([128, 1, 28, 28])</span></span><br><span class="line">    <span class="built_in">print</span>(y.shape)  <span class="comment"># torch.Size([128])</span></span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">input_ = mnist.data[<span class="number">0</span>].numel() <span class="comment">#特征的数目，一般是第一维之外的所有维度相乘的数</span></span><br><span class="line"><span class="comment"># mnist[0][0].numel()</span></span><br><span class="line"><span class="comment">#.data相当于第二个[0]把特征给跑去了</span></span><br><span class="line"><span class="comment">#[0]相当于第一个[0]即元组的第一个部分</span></span><br><span class="line">output_ = <span class="built_in">len</span>(mnist.targets.unique()) <span class="comment">#分类的数目</span></span><br></pre></td></tr></table></figure><pre><code>torch.Size([128, 1, 28, 28])torch.Size([128])</code></pre><ul><li>定义神经网络架构</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_features=<span class="number">10</span>,out_features=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        <span class="comment"># self.normalize = nn.BatchNorm2d(num_features=1)</span></span><br><span class="line">        self.linear1 = nn.Linear(in_features,<span class="number">128</span>,bias=<span class="literal">False</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">128</span>,out_features,bias=<span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x = self.normalize(x)</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">        <span class="comment">#需要对数据的结构进行一个改变，这里的“-1”代表，我不想算，请pytorch帮我计算</span></span><br><span class="line">        sigma1 = torch.relu(self.linear1(x))</span><br><span class="line">        z2 = self.output(sigma1)</span><br><span class="line">        sigma2 = F.log_softmax(z2,dim=<span class="number">1</span>)  <span class="comment"># 为了得准确率</span></span><br><span class="line">        <span class="keyword">return</span> sigma2</span><br></pre></td></tr></table></figure><ul><li>定义训练函数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">net,batchdata,lr=<span class="number">0.01</span>,epochs=<span class="number">5</span>,gamma=<span class="number">0</span></span>):</span><br><span class="line">    criterion = nn.NLLLoss() <span class="comment">#定义损失函数</span></span><br><span class="line">    opt = optim.SGD(net.parameters(), lr=lr,momentum=gamma) <span class="comment">#定义优化算法</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    samples = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> batch_idx, (x,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(batchdata):</span><br><span class="line">            y = y.view(x.shape[<span class="number">0</span>])</span><br><span class="line">            sigma = net.forward(x)  <span class="comment"># 正</span></span><br><span class="line">            loss = criterion(sigma,y)  <span class="comment"># 损</span></span><br><span class="line">            loss.backward()  <span class="comment"># 反</span></span><br><span class="line">            opt.step()  <span class="comment"># 更</span></span><br><span class="line">            opt.zero_grad()  <span class="comment"># 清</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">#求解准确率</span></span><br><span class="line">            yhat = torch.<span class="built_in">max</span>(sigma,dim=<span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># 取下标即预测标签</span></span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            (values, indices) = ([0.7], [2]) 最大值和索引</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            correct += torch.<span class="built_in">sum</span>(yhat == y)</span><br><span class="line">            samples += x.shape[<span class="number">0</span>]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> (batch_idx+<span class="number">1</span>) % <span class="number">125</span> == <span class="number">0</span> <span class="keyword">or</span> batch_idx == <span class="built_in">len</span>(batchdata)-<span class="number">1</span>:</span><br><span class="line">                <span class="comment"># 468没被125整除,,要让m/m等于1加上batch_idx == len(batchdata)-1</span></span><br><span class="line">                <span class="comment"># 125 250 375 468故每个epoch打印四次</span></span><br><span class="line">                <span class="comment"># 最后一下没够125，所以没打印100%，但是确实是训练了，只是没够125</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Epoch&#123;&#125;:[&#123;&#125;/&#123;&#125;(&#123;:.0f&#125;%)]\tLoss:&#123;:.6f&#125;\t Accuracy:&#123;:.3f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                    epoch+<span class="number">1</span></span><br><span class="line">                   ,samples</span><br><span class="line">                   ,<span class="built_in">len</span>(batchdata.dataset)*epochs</span><br><span class="line">                   ,<span class="number">100</span>*samples/(<span class="built_in">len</span>(batchdata.dataset)*epochs)</span><br><span class="line">                   ,loss.data.item()</span><br><span class="line">                   ,<span class="built_in">float</span>(correct*<span class="number">100</span>)/samples))</span><br></pre></td></tr></table></figure><ul><li>进行训练和评估</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#实例化神经网络，调用优化算法需要的参数</span></span><br><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">net = Model(in_features=input_, out_features=output_)</span><br><span class="line">fit(net,batchdata,lr=lr,epochs=epochs,gamma=gamma)</span><br></pre></td></tr></table></figure><pre><code>Epoch1:[16000/600000(3%)]    Loss:0.445854     Accuracy:71.844Epoch1:[32000/600000(5%)]    Loss:0.541044     Accuracy:76.766Epoch1:[48000/600000(8%)]    Loss:0.420771     Accuracy:78.817Epoch1:[60000/600000(10%)]    Loss:0.456890     Accuracy:79.743Epoch2:[76000/600000(13%)]    Loss:0.520517     Accuracy:80.801Epoch2:[92000/600000(15%)]    Loss:0.335469     Accuracy:81.486Epoch2:[108000/600000(18%)]    Loss:0.390427     Accuracy:82.020Epoch2:[120000/600000(20%)]    Loss:0.464855     Accuracy:82.355Epoch3:[136000/600000(23%)]    Loss:0.382425     Accuracy:82.770Epoch3:[152000/600000(25%)]    Loss:0.388381     Accuracy:83.201Epoch3:[168000/600000(28%)]    Loss:0.456149     Accuracy:83.518Epoch3:[180000/600000(30%)]    Loss:0.336247     Accuracy:83.714Epoch4:[196000/600000(33%)]    Loss:0.400932     Accuracy:84.001Epoch4:[212000/600000(35%)]    Loss:0.280075     Accuracy:84.251Epoch4:[228000/600000(38%)]    Loss:0.329395     Accuracy:84.461Epoch4:[240000/600000(40%)]    Loss:0.368081     Accuracy:84.579Epoch5:[256000/600000(43%)]    Loss:0.491687     Accuracy:84.777Epoch5:[272000/600000(45%)]    Loss:0.201454     Accuracy:84.953Epoch5:[288000/600000(48%)]    Loss:0.199246     Accuracy:85.079Epoch5:[300000/600000(50%)]    Loss:0.222263     Accuracy:85.179Epoch6:[316000/600000(53%)]    Loss:0.357817     Accuracy:85.341Epoch6:[332000/600000(55%)]    Loss:0.386327     Accuracy:85.465Epoch6:[348000/600000(58%)]    Loss:0.365660     Accuracy:85.593Epoch6:[360000/600000(60%)]    Loss:0.302036     Accuracy:85.690Epoch7:[376000/600000(63%)]    Loss:0.270010     Accuracy:85.834Epoch7:[392000/600000(65%)]    Loss:0.289248     Accuracy:85.949Epoch7:[408000/600000(68%)]    Loss:0.262059     Accuracy:86.047Epoch7:[420000/600000(70%)]    Loss:0.302930     Accuracy:86.116Epoch8:[436000/600000(73%)]    Loss:0.255372     Accuracy:86.247Epoch8:[452000/600000(75%)]    Loss:0.323171     Accuracy:86.354Epoch8:[468000/600000(78%)]    Loss:0.223748     Accuracy:86.437Epoch8:[480000/600000(80%)]    Loss:0.403298     Accuracy:86.499Epoch9:[496000/600000(83%)]    Loss:0.297387     Accuracy:86.578Epoch9:[512000/600000(85%)]    Loss:0.356909     Accuracy:86.666Epoch9:[528000/600000(88%)]    Loss:0.306189     Accuracy:86.757Epoch9:[540000/600000(90%)]    Loss:0.389048     Accuracy:86.804Epoch10:[556000/600000(93%)]    Loss:0.337781     Accuracy:86.882Epoch10:[572000/600000(95%)]    Loss:0.256883     Accuracy:86.969Epoch10:[588000/600000(98%)]    Loss:0.188146     Accuracy:87.044Epoch10:[600000/600000(100%)]    Loss:0.293160     Accuracy:87.087</code></pre><h3 id="9-数据处理总结"><a href="#9-数据处理总结" class="headerlink" title="9. 数据处理总结"></a>9. 数据处理总结</h3><h4 id="TensorDataset-的使用"><a href="#TensorDataset-的使用" class="headerlink" title="TensorDataset 的使用"></a>TensorDataset 的使用</h4><ul><li>实例化：<code>td = TensorDataset(a, b, c)</code>，形成元组 <code>(tensor(), tensor(), tensor())</code>。</li><li>访问元素：使用 <code>td[0]</code> 可访问每个元组；适用于 <code>for</code> 循环遍历。</li><li>图片数据处理：可通过 <code>td.data</code>、<code>td.classes</code>、<code>td.targets</code> 进行访问。<ul><li>也是minist等data类的类型</li></ul></li></ul><h4 id="DataLoader-的应用"><a href="#DataLoader-的应用" class="headerlink" title="DataLoader 的应用"></a>DataLoader 的应用</h4><ul><li>实例化：<code>dl = DataLoader(td, batch_size)</code>，形成列表。</li><li>遍历数据：<ul><li>使用 <code>for x, y in dl: print(i)</code> 遍历；列表格式 <code>[tensor(), tensor(), tensor()]</code>。</li><li>对于图片数据，<code>x.shape</code> 可能为 <code>[batch_size, 1, 28, 28]</code> 或 <code>[batch_size, 28, 28]</code>。</li></ul></li><li>注意事项：<ul><li>不能直接使用 <code>dl[0]</code> 访问元素，只能<code>dl.dataset[0]</code>。</li><li><code>dl.dataset</code> 包含所有数据列表；<code>len(dl)</code> 表示总批次数（即 <code>m/batch_size</code>）；<code>dl.batch_size</code> 表示每批数据的大小。</li><li><code>batch_size</code> 增加了数据列表的一个维度。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(10)-训练损失</title>
      <link href="/Pytorch-10/"/>
      <url>/Pytorch-10/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br></pre></td></tr></table></figure><h3 id="1-训练目标与流程"><a href="#1-训练目标与流程" class="headerlink" title="1. 训练目标与流程"></a>1. 训练目标与流程</h3><div class="table-container"><table><thead><tr><th>步骤</th><th>描述</th></tr></thead><tbody><tr><td><strong>目标</strong></td><td>找到参数 <code>w</code> 使预测与真实值最接近。</td></tr><tr><td><strong>流程</strong></td><td>模型 (Model) + 损失 (Loss) + 优化器 (Optim) ⇒ 求解 <code>w</code></td></tr><tr><td><strong>损失</strong></td><td><code>L(w)</code></td></tr><tr><td><strong>数学工具</strong></td><td>- 化为凸函数：拉格朗日变换<br>- 最小化 <code>L(w)</code> 时的 <code>w</code>：梯度下降法为代表</td></tr></tbody></table></div><h3 id="2-回归与误差分析"><a href="#2-回归与误差分析" class="headerlink" title="2. 回归与误差分析"></a>2. 回归与误差分析</h3><div class="table-container"><table><thead><tr><th>类型</th><th>描述</th></tr></thead><tbody><tr><td><strong>回归</strong></td><td>误差平方和SSE</td></tr><tr><td><strong>MSE</strong></td><td><code>MSE = SSE/m</code></td></tr><tr><td><strong>顺序</strong></td><td><code>(yhat, y)</code> 的顺序</td></tr><tr><td><strong>reduction 参数</strong></td><td>sum/mean(default)/None</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> MSELoss</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">yhat = torch.randn(size=(<span class="number">50</span>,),dtype=torch.float32)</span><br><span class="line">y = torch.randn(size=(<span class="number">50</span>,),dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">criterion = MSELoss(reduction = <span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">loss = criterion(yhat,y)</span><br><span class="line">loss</span><br></pre></td></tr></table></figure><pre><code>tensor(124.3458)</code></pre><h3 id="3-分类：二分类交叉熵损失函数-对数损失"><a href="#3-分类：二分类交叉熵损失函数-对数损失" class="headerlink" title="3.分类：二分类交叉熵损失函数(对数损失)"></a>3.分类：二分类交叉熵损失函数(对数损失)</h3><ul><li>除非特别声明，不然提到<strong>交叉熵损失</strong>均为<code>多分类</code></li><li><p>极大似然估计：寻找相应的权重w，使得目标事件的发生概率最大，就是极大似然估计的基本方法。</p><ul><li><code>构筑对数似然函数:</code>用于评估目标事件发生的概率，该函数<strong>被设计成</strong>目标事件发生时，概率最大。</li><li><code>对整体取对数:</code>叫做对数似然函数</li><li><code>在对数似然函数上对w求导:</code>并使导数为0，对权重求解。</li></ul></li><li><p>由对数似然函数的概念，可将其定义为：</p></li></ul><script type="math/tex; mode=display">P(\hat{y}_i | x_i, w) = P_1^{y_i} * P_0^{1-y_i}</script><ul><li><strong>为了达成让模型拟合好，损失小的目的，我们每时每刻都希望其为1，也就是说每时每刻都在追求其最大值</strong></li><li>对于多样本而言，推导如下：</li></ul><script type="math/tex; mode=display">\begin{align*}\ln P & = \ln \prod_{i=1}^{m} (\sigma_i^{y_i} \cdot (1 - \sigma_i)^{1-y_i}) \\      & = \sum_{i=1}^{m} \ln(\sigma_i^{y_i} \cdot (1 - \sigma_i)^{1-y_i}) \\      & = \sum_{i=1}^{m} (\ln \sigma_i^{y_i} + \ln(1 - \sigma_i)^{1-y_i}) \\      & = \sum_{i=1}^{m} (y_i \cdot \ln(\sigma_i) + (1 - y_i) \cdot \ln(1 - \sigma_i))\end{align*}</script><ul><li>故对多样本二分类而言，损失函数如下：</li></ul><script type="math/tex; mode=display">L(w) = -\sum_{i=1}^{m} \left( y_i \cdot \ln(\sigma_i) + (1 - y_i) \cdot \ln(1 - \sigma_i) \right)</script><h4 id="3-1-tensor实现二分类交叉熵损失函数"><a href="#3-1-tensor实现二分类交叉熵损失函数" class="headerlink" title="3.1 tensor实现二分类交叉熵损失函数"></a>3.1 tensor实现二分类交叉熵损失函数</h4><ul><li>除了<code>普通的加减乘除</code>，其余<code>均用 torch</code> 运算，<code>比 python 快！</code><ul><li>即sum函数用torch.sum，不要用普通sum</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">m = <span class="number">3</span>*<span class="built_in">pow</span>(<span class="number">10</span>,<span class="number">3</span>)</span><br><span class="line">torch.random.manual_seed(<span class="number">420</span>)</span><br><span class="line">X = torch.rand((m,<span class="number">4</span>),dtype=torch.float32)  <span class="comment"># Input</span></span><br><span class="line">w = torch.rand((<span class="number">4</span>,<span class="number">1</span>),dtype=torch.float32)</span><br><span class="line">y = torch.randint(low=<span class="number">0</span>,high=<span class="number">2</span>,size=(m,<span class="number">1</span>),dtype=torch.float32)</span><br><span class="line">zhat = torch.mm(X,w)  <span class="comment"># Hidden1-1</span></span><br><span class="line">sigma = torch.sigmoid(zhat)  <span class="comment"># Hidden1-2</span></span><br><span class="line">loss = -(<span class="number">1</span>/m)*torch.<span class="built_in">sum</span>(y*torch.log(sigma) + (<span class="number">1</span>-y)*torch.log(<span class="number">1</span>-sigma))</span><br><span class="line">loss</span><br></pre></td></tr></table></figure><pre><code>tensor([0.7962])</code></pre><h4 id="3-2-torch实现二分类交叉熵损失函数"><a href="#3-2-torch实现二分类交叉熵损失函数" class="headerlink" title="3.2 torch实现二分类交叉熵损失函数"></a>3.2 torch实现二分类交叉熵损失函数</h4><div class="table-container"><table><thead><tr><th>类别</th><th>描述</th></tr></thead><tbody><tr><td><code>BCEWithLogitsLoss</code></td><td>- 输入：<code>zhat</code>, <code>y</code><br> - 目的：缩小精度误差</td></tr><tr><td><code>BCELoss</code></td><td>- 输入：<code>sigma</code>, <code>y</code><br> - 目的：监控准确率</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.BCEWithLogitsLoss(reduction = <span class="string">&quot;mean&quot;</span>)</span><br><span class="line">loss = criterion(zhat, y)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br><span class="line">criterion2 = nn.BCELoss(reduction = <span class="string">&quot;mean&quot;</span>) <span class="comment">#实例化</span></span><br><span class="line">loss = criterion2(sigma,y)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br></pre></td></tr></table></figure><pre><code>tensor(0.7962)tensor(0.7962)</code></pre><h3 id="4-分类任务中的多分类交叉熵损失函数"><a href="#4-分类任务中的多分类交叉熵损失函数" class="headerlink" title="4. 分类任务中的多分类交叉熵损失函数"></a>4. 分类任务中的多分类交叉熵损失函数</h3><p>在 PyTorch 中处理多分类问题时，常用的方法是结合 <code>nn.LogSoftmax(dim=1)</code> 和 <code>nn.NLLLoss()</code> 来计算交叉熵损失。</p><ul><li><strong>关键步骤</strong>:<ol><li><strong>LogSoftmax 应用</strong>:<ul><li>应用于模型输出，使用 <code>nn.LogSoftmax(dim=1)</code>。</li><li><code>dim=1</code> 确保沿着正确的维度（特征维度）应用 Softmax。</li></ul></li><li><strong>负对数似然函数（Negative Log Likelihood function）</strong>:<ul><li>使用 <code>nn.NLLLoss()</code> 计算损失。</li><li>确保标签张量 <code>y</code> 使用 <code>.long()</code> 进行类型转换，以匹配损失函数的要求。</li></ul></li><li>即，在计算损失函数时，我们<strong>不再需要使用单独的softmax函数</strong>了。</li><li>推导过程：</li></ol></li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/BCELoss.png" alt="Alt text"></p><h4 id="4-1-PyTorch-nn-LogSoftmax-dim-1-和nn-NLLLoss"><a href="#4-1-PyTorch-nn-LogSoftmax-dim-1-和nn-NLLLoss" class="headerlink" title="4.1 PyTorch(nn.LogSoftmax(dim=1)和nn.NLLLoss())"></a>4.1 PyTorch(<code>nn.LogSoftmax(dim=1)</code>和<code>nn.NLLLoss()</code>)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">N = <span class="number">3</span>*<span class="built_in">pow</span>(<span class="number">10</span>,<span class="number">2</span>) </span><br><span class="line">torch.random.manual_seed(<span class="number">420</span>) </span><br><span class="line">X = torch.rand((N,<span class="number">4</span>),dtype=torch.float32) </span><br><span class="line">w = torch.rand((<span class="number">4</span>,<span class="number">3</span>),dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = torch.randint(low=<span class="number">0</span>,high=<span class="number">3</span>,size=(N,),dtype=torch.float32)</span><br><span class="line">zhat = torch.mm(X,w)</span><br><span class="line"><span class="comment">#从这里开始调用 softmax 和 NLLLoss</span></span><br><span class="line">logsm = nn.LogSoftmax(dim=<span class="number">1</span>) <span class="comment">#实例化</span></span><br><span class="line">logsigma = logsm(zhat)</span><br><span class="line">criterion = nn.NLLLoss() <span class="comment">#实例化</span></span><br><span class="line"><span class="comment">#由于交叉熵损失需要将标签转化为独热形式，因此不接受浮点数作为标签的输入</span></span><br><span class="line"><span class="comment">#对 NLLLoss 而言，需要输入 logsigma</span></span><br><span class="line">criterion(logsigma,y.long())</span><br></pre></td></tr></table></figure><pre><code>tensor(1.1591, grad_fn=&lt;NllLossBackward0&gt;)</code></pre><h4 id="4-2-PyTorch-nn-CrossEntropyLoss"><a href="#4-2-PyTorch-nn-CrossEntropyLoss" class="headerlink" title="4.2 PyTorch(nn.CrossEntropyLoss())"></a>4.2 PyTorch(<code>nn.CrossEntropyLoss()</code>)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">criterion(zhat,y.long())</span><br></pre></td></tr></table></figure><pre><code>tensor(1.1591, grad_fn=&lt;NllLossBackward0&gt;)</code></pre><h3 id="5-PyTorch-中的分类损失函数选择"><a href="#5-PyTorch-中的分类损失函数选择" class="headerlink" title="5. PyTorch 中的分类损失函数选择"></a>5. PyTorch 中的分类损失函数选择</h3><p>在 PyTorch 中，根据分类任务的类型（二分类或多分类），有不同的损失函数选择：</p><ul><li><p><strong>二分类任务</strong>:</p><ul><li><strong>不含激活函数的输出</strong>:<ul><li>使用 <code>BCELoss()</code>。</li><li>应用场景：模型输入 <code>sigma</code>, 真实标签 <code>y</code>。</li></ul></li><li><strong>含激活函数的输出</strong>:<ul><li>使用 <code>BCEWithLogitsLoss()</code>。</li><li>应用场景：模型输入 <code>zhat</code>, 真实标签 <code>y</code>。</li></ul></li></ul></li><li><p><strong>多分类任务</strong>:</p><ul><li><strong>含激活函数</strong>:<ul><li>使用 <code>CrossEntropyLoss()</code>。</li><li>应用场景：模型输出 <code>zhat</code>, 真实标签 <code>y.long()</code>（确保标签为长整型）。</li></ul></li><li><strong>含对数激活函数</strong>:<ul><li>首先应用 <code>LogSoftmax(dim=1)</code> 于模型输出 <code>logsigma</code>。</li><li>然后使用 <code>NLLLoss(logsigma,y.long())</code> 计算损失。</li><li>应用场景：处理后的 <code>logsigma</code>, 真实标签 <code>y.long()</code>。</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(8)-单层神经网络</title>
      <link href="/Pytorch-8/"/>
      <url>/Pytorch-8/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure><h3 id="1-单层线性回归"><a href="#1-单层线性回归" class="headerlink" title="1.单层线性回归"></a>1.单层线性回归</h3><ul><li><code>tensor</code>实现神经网络的正向传播<ul><li><code>特征张量x</code>最好设置为<code>float32</code>规避bug</li><li><code>标签张量z</code>最好设置为<code>二维</code>，当<code>有bug再view</code><ul><li>pytorch中的nn.层中最后输出的是二维结果</li></ul></li><li><code>规避精度</code>问题：<code>torch.allclose()</code></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32) </span><br><span class="line">z=torch.tensor([[-<span class="number">0.2</span>],[-<span class="number">0.05</span>],[-<span class="number">0.05</span>],[<span class="number">0.1</span>]])</span><br><span class="line">w=torch.tensor([-<span class="number">0.2</span>,<span class="number">0.15</span>,<span class="number">0.15</span>])</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">LinearR</span>(<span class="params">X,w</span>):</span><br><span class="line">    zhat=torch.mv(X,w)</span><br><span class="line">    <span class="keyword">return</span> zhat</span><br><span class="line">zhat =LinearR(x,w)</span><br><span class="line"><span class="built_in">print</span>(zhat)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(zhat,z.view(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><pre><code>tensor([-0.2000, -0.0500, -0.0500,  0.1000])True</code></pre><ul><li><code>torch.nn.Linear</code>实现神经网络的正向传播<ul><li>torch.nn.Linear(<code>上一层神经元的个数</code>,<code>这一层神经元的个数</code>)</li><li>所有<code>nn.module的层</code>都会实例化<code>带梯度的w和b</code></li><li><code>zhat</code>计算结果为<code>二维</code></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line">torch.manual_seed(<span class="number">420</span>)  <span class="comment"># 控制随机性</span></span><br><span class="line">output = torch.nn.Linear(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 不要bias：`output=torch.nn.Linear(2,1,bias=False)`</span></span><br><span class="line">zhat = output(X)</span><br><span class="line"><span class="built_in">print</span>(zhat)</span><br><span class="line"><span class="built_in">print</span>(output.weight)</span><br><span class="line"><span class="built_in">print</span>(output.bias)</span><br></pre></td></tr></table></figure><pre><code>tensor([[0.6730],        [1.1048],        [0.2473],        [0.6792]], grad_fn=&lt;AddmmBackward0&gt;)Parameter containing:tensor([[ 0.4318, -0.4256]], requires_grad=True)Parameter containing:tensor([0.6730], requires_grad=True)</code></pre><h3 id="2-单层逻辑回归"><a href="#2-单层逻辑回归" class="headerlink" title="2.单层逻辑回归"></a>2.单层逻辑回归</h3><ul><li><p><code>sigmoid函数</code>：对数几率回归</p><ul><li>设置<code>阈值</code>为0.5(默认),可得<code>预测</code>为0/1</li><li>化<code>回归</code>为<code>分类</code></li></ul></li><li><p><code>tensor</code>实现二分类神经网络的正向传播</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)  <span class="comment"># Input</span></span><br><span class="line">w=torch.tensor([-<span class="number">0.2</span>,<span class="number">0.15</span>,<span class="number">0.15</span>])</span><br><span class="line">andgate=torch.tensor([[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">LogisticR</span>(<span class="params">x,W</span>): </span><br><span class="line">    zhat=torch.mv(x,W)  <span class="comment"># Hidden1-1</span></span><br><span class="line">    sigma=torch.sigmoid(zhat)  <span class="comment"># Hidden1-2</span></span><br><span class="line">    andhat=torch.tensor([<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> sigma &gt;=<span class="number">0.5</span>],dtype=torch.float32)  <span class="comment"># Output</span></span><br><span class="line">    <span class="keyword">return</span> sigma,andhat</span><br><span class="line">sigma,andhat=LogisticR(x,w)</span><br><span class="line"><span class="built_in">print</span>(sigma)</span><br><span class="line"><span class="built_in">print</span>(andgate)</span><br></pre></td></tr></table></figure><pre><code>tensor([0.4502, 0.4875, 0.4875, 0.5250])tensor([[0.],        [0.],        [0.],        [1.]])</code></pre><ul><li><code>torch.functional</code>实现<code>二分类神经网络</code>的<code>正向传播</code><ul><li><code>relu</code>/<code>sigmoid</code>重要所以<code>F有</code>，<code>本质</code>还是<code>torch.</code></li><li><code>tanh</code>/<code>sign</code>还行所以<code>torch.</code></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">X=torch.tensor([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)   <span class="comment"># Input</span></span><br><span class="line">torch.random.manual_seed(<span class="number">420</span>)</span><br><span class="line">output=torch.nn.Linear(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">zhat=output(X)   <span class="comment"># Hidden1-1</span></span><br><span class="line">sigma=F.sigmoid(zhat)  <span class="comment"># Hidden1-2</span></span><br><span class="line">andhat = [<span class="built_in">int</span> (x) <span class="keyword">for</span> x <span class="keyword">in</span> sigma &gt; <span class="number">0.5</span>]  <span class="comment"># Output</span></span><br><span class="line"><span class="built_in">print</span>(zhat)</span><br><span class="line"><span class="built_in">print</span>(sigma)</span><br><span class="line"><span class="built_in">print</span>(andhat)</span><br></pre></td></tr></table></figure><pre><code>tensor([[0.6730],        [1.1048],        [0.2473],        [0.6792]], grad_fn=&lt;AddmmBackward0&gt;)tensor([[0.6622],        [0.7512],        [0.5615],        [0.6636]], grad_fn=&lt;SigmoidBackward0&gt;)[1, 1, 1, 1]</code></pre><h3 id="3-Softmax回归"><a href="#3-Softmax回归" class="headerlink" title="3.Softmax回归"></a>3.Softmax回归</h3><ul><li>假设<code>三分类</code>，故Linear(2,<code>3</code>)</li><li>一般在<code>输出层</code>使用</li><li>本质也是<code>torch.</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X=torch.tensor([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)   <span class="comment"># Input</span></span><br><span class="line">torch.random.manual_seed(<span class="number">420</span>)</span><br><span class="line">output=torch.nn.Linear(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">zhat=output(X)   <span class="comment"># Hidden1-1</span></span><br><span class="line">sigma = F.softmax(zhat,dim=<span class="number">1</span>)  <span class="comment"># Output</span></span><br><span class="line"><span class="built_in">print</span>(zhat)</span><br><span class="line"><span class="built_in">print</span>(sigma)</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.5453,  0.2653, -0.3527],        [ 0.9772,  0.9382, -0.5684],        [ 0.1197, -0.2964, -0.8400],        [ 0.5516,  0.3765, -1.0557]], grad_fn=&lt;AddmmBackward0&gt;)tensor([[0.4623, 0.3494, 0.1883],        [0.4598, 0.4422, 0.0980],        [0.4896, 0.3229, 0.1875],        [0.4902, 0.4115, 0.0983]], grad_fn=&lt;SoftmaxBackward0&gt;)</code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(9)-单层到多层</title>
      <link href="/Pytorch-9/"/>
      <url>/Pytorch-9/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure><h3 id="1-数据划分与阶跃函数的关系"><a href="#1-数据划分与阶跃函数的关系" class="headerlink" title="1. 数据划分与阶跃函数的关系"></a>1. 数据划分与阶跃函数的关系</h3><p>在构建神经网络的 <strong>“与门”</strong> 逻辑时，我们通过应用 <code>sigmoid(wx+b)</code> 函数并设置阈值为 <code>0.5</code> 来决定输出 <code>yhat</code>。这实质上引入了阶跃函数的概念，即：</p><ul><li>当 <code>wx+b</code> 大于 0 时，输出为 <code>1</code>；小于 0 时，输出为 <code>0</code>。这样的映射关系等价于阶跃函数。</li><li><code>wx+b=0</code> 的解析线定义了数据的<strong>二元分类界限</strong>，即数据划分线，可由 <code>x2=(-b-w1x1)/w2</code> 表示。</li><li>总结而言，<strong>阶跃函数等同于使用 <code>sigmoid(wx+b)</code> 以 <code>0.5</code> 为阈值进行数据划分</strong>。</li></ul><h3 id="2-或门"><a href="#2-或门" class="headerlink" title="2.或门"></a>2.或门</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">OR</span>(<span class="params">X</span>):</span><br><span class="line">    w = torch.tensor([-<span class="number">0.08</span>, <span class="number">0.15</span>,<span class="number">0.15</span>], dtype = torch.float32)</span><br><span class="line">    zhat = torch.mv(X,w)</span><br><span class="line">    <span class="comment"># sigma  = F.sigmoid(zhat)</span></span><br><span class="line">    yhat = torch.tensor([<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> zhat &gt; <span class="number">0</span>], dtype=torch.float32) <span class="comment">#化sigmoid和0.5为阶跃</span></span><br><span class="line">    <span class="keyword">return</span> yhat</span><br><span class="line">OR(X)</span><br></pre></td></tr></table></figure><pre><code>tensor([0., 1., 1., 1.])</code></pre><h3 id="3-非与门"><a href="#3-非与门" class="headerlink" title="3.非与门"></a>3.非与门</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">NAND</span>(<span class="params">X</span>):</span><br><span class="line">    w = torch.tensor([<span class="number">0.23</span>,-<span class="number">0.15</span>,-<span class="number">0.15</span>], dtype = torch.float32) <span class="comment">#和与门、或门都不同的权重</span></span><br><span class="line">    zhat = torch.mv(X,w)</span><br><span class="line">    yhat = torch.tensor([<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> zhat &gt;= <span class="number">0</span>],dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> yhat</span><br><span class="line">NAND(X)</span><br></pre></td></tr></table></figure><pre><code>tensor([1., 1., 1., 0.])</code></pre><h3 id="4-与门、或门、非与门的总结"><a href="#4-与门、或门、非与门的总结" class="headerlink" title="4. 与门、或门、非与门的总结"></a>4. 与门、或门、非与门的总结</h3><ul><li><p><strong>直线拟合</strong>：</p><ul><li>与门、或门、非与门的逻辑可以通过线性回归模型中的直线进行拟合，而异或门由于其输出不是线性可分的，不能仅使用直线来拟合。</li></ul></li><li><p><strong>从直线到曲线</strong>：</p><ul><li>当我们不能用直线来划分数据时，决策边界需要从<code>直线演变为曲线</code>。这通常意味着需要从<code>单层网络转变为多层网络</code>来实现更复杂的决策边界。</li></ul></li><li><p><strong>逻辑门总结</strong>：</p><ul><li><strong>与门</strong> (<code>AND gate</code>)：可以通过 <code>sigmoid</code> 函数配合 <code>0.5</code> 的阈值来实现。</li><li><strong>或门</strong> (<code>OR gate</code>)：可以通过 <code>sign</code> 函数来实现，其输出取决于输入值的符号。</li><li><strong>非与门</strong> (<code>NAND gate</code>)：同样可以通过 <code>sign</code> 函数来实现，其输出是与门的逆逻辑。</li><li>输出层的激活函数 <code>g(z)</code> 对网络的性能并不产生影响。</li></ul></li></ul><h3 id="5-异或门"><a href="#5-异或门" class="headerlink" title="5. 异或门"></a>5. 异或门</h3><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/NOR.png" alt="NOR"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">AND</span>(<span class="params">X</span>):</span><br><span class="line">    w=torch.tensor([-<span class="number">0.2</span>,<span class="number">0.15</span>,<span class="number">0.15</span>])</span><br><span class="line">    zhat = torch.mv(X,w)</span><br><span class="line">    <span class="comment"># yhat = F.sigmoid(zhat) #原始AND的h(z)是sigmoid</span></span><br><span class="line">    yhat = torch.tensor([<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> zhat&gt;<span class="number">0</span>],dtype=torch.float32) <span class="comment">#换阶跃效果不变，其与下面等效</span></span><br><span class="line">    <span class="comment"># yhat = torch.tensor([int(x) for x in F.sigmoid(zhat)&gt;0.5],dtype=torch.float32)与上面等效</span></span><br><span class="line">    <span class="keyword">return</span> yhat</span><br><span class="line">X = torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">XOR</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="comment">#输入层：</span></span><br><span class="line">    input_1 = X</span><br><span class="line">    <span class="comment">#中间层：</span></span><br><span class="line">    sigma_nand = NAND(input_1)</span><br><span class="line">    sigma_or = OR(input_1)</span><br><span class="line">    x0 = torch.tensor([[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line">    <span class="comment">#输出层：</span></span><br><span class="line">    input_2 = torch.cat((x0,sigma_nand.view(<span class="number">4</span>,<span class="number">1</span>),sigma_or.view(<span class="number">4</span>,<span class="number">1</span>)),dim=<span class="number">1</span>)</span><br><span class="line">    y_and = AND(input_2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_and</span><br><span class="line">XOR(X)</span><br></pre></td></tr></table></figure><pre><code>tensor([0., 1., 1., 0.])</code></pre><h3 id="6-神经网络的正向传播"><a href="#6-神经网络的正向传播" class="headerlink" title="6.神经网络的正向传播"></a>6.神经网络的正向传播</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调用必要库</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 确定数据</span></span><br><span class="line">torch.random.manual_seed(<span class="number">420</span>)</span><br><span class="line">X = torch.rand((<span class="number">500</span>,<span class="number">20</span>), dtype=torch.float32)</span><br><span class="line">y = torch.randint(low=<span class="number">0</span>,high=<span class="number">3</span>,size=(<span class="number">500</span>,<span class="number">1</span>), dtype=torch.float32)</span><br><span class="line"><span class="comment"># 确定网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_features, out_features</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model,self).__init__()</span><br><span class="line">        self.linear1 = nn.Linear(in_features,<span class="number">13</span>,bias=<span class="literal">True</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">13</span>,<span class="number">8</span>,bias=<span class="literal">True</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">8</span>,out_features,bias=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>): <span class="comment">#神经网络的向前传播</span></span><br><span class="line">        z1 = self.linear1(x)</span><br><span class="line">        sigma1 = torch.relu(z1)</span><br><span class="line">        z2 = self.linear2(sigma1)</span><br><span class="line">        sigma2 = torch.sigmoid(z2)</span><br><span class="line">        z3 = self.output(sigma2)</span><br><span class="line">        sigma3 = F.softmax(z3,dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> sigma3</span><br><span class="line"><span class="comment">#确定特征数目和分类数目</span></span><br><span class="line">input_ = X.shape[<span class="number">1</span>]</span><br><span class="line">output_ = <span class="built_in">len</span>(y.unique())</span><br><span class="line"><span class="comment">#实例化神经网络</span></span><br><span class="line">torch.random.manual_seed(<span class="number">420</span>)</span><br><span class="line">net = Model(in_features = input_, out_features = output_)</span><br><span class="line"><span class="comment">#前向传播</span></span><br><span class="line">net.forward(X) <span class="comment">#向前传播</span></span><br><span class="line"><span class="comment"># net(X) #或者这样 执行init以下的所有函数</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[0.4140, 0.3496, 0.2365],        [0.4210, 0.3454, 0.2336],        [0.4011, 0.3635, 0.2355],        ...,        [0.4196, 0.3452, 0.2352],        [0.4153, 0.3455, 0.2392],        [0.4153, 0.3442, 0.2405]], grad_fn=&lt;SoftmaxBackward0&gt;)</code></pre><p>在 PyTorch 中处理矩阵维度转换的逻辑通常涉及以下步骤：</p><ol><li><p><strong>权重矩阵转置</strong>：</p><ul><li>如果权重矩阵 <code>W</code> 的初始维度是 <code>(20, 13)</code>，为了进行矩阵乘法 <code>WX</code>，我们需要将其转置为 <code>(13, 20)</code>。</li></ul></li><li><p><strong>特征矩阵转置</strong>：</p><ul><li>同样地，如果特征矩阵 <code>X</code> 的初始维度是 <code>(500, 20)</code>，为了与转置后的权重矩阵 <code>W</code> 相乘，我们需要将 <code>X</code> 转置为 <code>(20, 500)</code>。</li></ul></li><li><p><strong>输出层处理</strong>：</p><ul><li>假设输出层 <code>output</code> 的维度是 <code>(3, 500)</code>，通过 <code>softmax</code> 函数处理后，我们得到一个 <code>(500, 3)</code> 的矩阵，其中每一行代表一个样本的类别概率。</li></ul></li><li><p><strong>偏差向量维度</strong>：</p><ul><li>偏差 <code>bias</code> 的维度大小通常与权重矩阵的输出层维度一致，例如 <code>tensor([13])</code>。</li></ul></li></ol><p>注意点：</p><ul><li><strong>权重矩阵的转置</strong>：<ul><li>在矩阵乘法中，权重矩阵的行和列需要与相应的特征矩阵的维度匹配。这通常意味着要进行转置操作。</li></ul></li></ul><h3 id="7-从nn-module中调用的方法"><a href="#7-从nn-module中调用的方法" class="headerlink" title="7.从nn.module中调用的方法"></a>7.从nn.module中调用的方法</h3><p>当操作 PyTorch 中的神经网络时，有几个常用的方法可以管理网络的训练和资源分配：</p><div class="table-container"><table><thead><tr><th>方法</th><th>描述</th></tr></thead><tbody><tr><td><code>net.training</code></td><td>检查网络是否处于训练模式。</td></tr><tr><td><code>net.cuda()</code></td><td>将整个网络迁移到 GPU 上，以利用其计算资源。</td></tr><tr><td><code>net.cpu()</code></td><td>将整个网络迁移到 CPU 上，通常用于推理或当 GPU 资源不可用时。</td></tr><tr><td><code>net.apply()</code></td><td>对神经网络中的所有层进行一致的操作，例如初始化权重。</td></tr><tr><td><code>net.parameters()</code></td><td>用于迭代网络中所有的参数，常用于优化过程中更新权重。</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.train()</span><br></pre></td></tr></table></figure><pre><code>Model(  (linear1): Linear(in_features=20, out_features=13, bias=True)  (linear2): Linear(in_features=13, out_features=8, bias=True)  (output): Linear(in_features=8, out_features=3, bias=True))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.cuda()</span><br></pre></td></tr></table></figure><pre><code>Model(  (linear1): Linear(in_features=20, out_features=13, bias=True)  (linear2): Linear(in_features=13, out_features=8, bias=True)  (output): Linear(in_features=8, out_features=3, bias=True))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.cpu()</span><br></pre></td></tr></table></figure><pre><code>Model(  (linear1): Linear(in_features=20, out_features=13, bias=True)  (linear2): Linear(in_features=13, out_features=8, bias=True)  (output): Linear(in_features=8, out_features=3, bias=True))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">initial_0</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">      m.weight.data.fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">net.apply(initial_0)</span><br></pre></td></tr></table></figure><pre><code>Model(  (linear1): Linear(in_features=20, out_features=13, bias=True)  (linear2): Linear(in_features=13, out_features=8, bias=True)  (output): Linear(in_features=8, out_features=3, bias=True))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for param in net.parameters():</span></span><br><span class="line"><span class="comment">#  print(param)</span></span><br><span class="line"><span class="comment">#  将每层的参数都打印出来</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(6)-张量可微性</title>
      <link href="/Pytorch-6/"/>
      <url>/Pytorch-6/</url>
      
        <content type="html"><![CDATA[<h3 id="1-可微分性相关属性"><a href="#1-可微分性相关属性" class="headerlink" title="1.可微分性相关属性"></a>1.可微分性相关属性</h3><ul><li>张量<code>y</code>具有一个<code>grad_fn</code>属性，可以<code>查看该属性</code>:<code>&lt;PowBackward0&gt;</code><ul><li>保存了一种<code>y-x</code>的函数关系<code>pow</code></li><li>由<code>可微分张量</code>创建而来</li></ul></li><li>张量<code>y</code>还包含<code>由x计算</code>得出的<code>结果</code>:<code>4.</code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建可微分张量x</span></span><br><span class="line">x = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>) </span><br><span class="line"><span class="built_in">print</span>(x)  <span class="comment">## tensor(2., requires_grad=True)</span></span><br><span class="line"><span class="comment"># 构建函数关系</span></span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)  <span class="comment">## tensor(4., grad_fn=&lt;PowBackward0&gt;)</span></span><br><span class="line"><span class="comment"># 查看grad_fn属性</span></span><br><span class="line"><span class="built_in">print</span>(y.grad_fn)  <span class="comment">## &lt;PowBackward0 object at 0x00000154FAE1E160&gt;</span></span><br></pre></td></tr></table></figure></li><li><code>x</code>作为<code>初始张量</code>，<code>没有</code>grad_fn属性</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.grad_fn   <span class="comment"># 不返回任何东西                                              </span></span><br></pre></td></tr></table></figure><ul><li>可微分张量具有<code>传递性</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.requires_grad</span><br></pre></td></tr></table></figure><pre><code>True</code></pre><ul><li><strong>注意</strong><code>grad_fn</code>属性是<code>保存直接y-x</code>的关系:比如这里的<code>add</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">z = y + <span class="number">1</span></span><br><span class="line">z </span><br></pre></td></tr></table></figure><pre><code>tensor(5., grad_fn=&lt;AddBackward0&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br><span class="line"><span class="built_in">print</span>(z.grad_fn)</span><br></pre></td></tr></table></figure><pre><code>True&lt;AddBackward0 object at 0x00000154FAE595B0&gt;</code></pre><h3 id="2-反向传播与梯度计算"><a href="#2-反向传播与梯度计算" class="headerlink" title="2. 反向传播与梯度计算"></a>2. 反向传播与梯度计算</h3><ul><li>某个可微分张量的<code>导数值</code>，存在其<code>grad属性</code>中</li><li>执行<code>反向传播</code>才可查看<code>grad</code>，不然<code>不返回</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.grad</span><br></pre></td></tr></table></figure><pre><code>不返回</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">z.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure><pre><code>tensor(4.)</code></pre><ul><li><code>一张计算图</code>反向传播<code>仅能计算一次</code>，<code>backward</code>再次调用将<code>报错</code>！</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#z.backward() # 报错</span></span><br></pre></td></tr></table></figure><ul><li>可以用<code>retain_graph=True</code>可<code>不报错</code>，但会<code>累加梯度</code>(这段代码没累加)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">z = y+<span class="number">1</span></span><br><span class="line">z.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure><pre><code>tensor(4.)</code></pre><ul><li><code>y</code>上也可<code>反向传播</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">z = y+<span class="number">1</span></span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure><pre><code>tensor(4.)</code></pre><ul><li>无论何时，<strong>仅能计算叶节点的导数值</strong></li><li><code>中间节点不会保存梯度</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#y.grad() # 报错！</span></span><br></pre></td></tr></table></figure><ul><li>若<code>想保存</code>，可以使用<code>retain_grad()</code>方法</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">y.retain_grad()</span><br><span class="line">z = y+<span class="number">1</span></span><br><span class="line">z.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="built_in">print</span>(y.grad)</span><br></pre></td></tr></table></figure><pre><code>tensor(4.)tensor(1.)</code></pre><h3 id="3-阻止计算图跟踪"><a href="#3-阻止计算图跟踪" class="headerlink" title="3.阻止计算图跟踪"></a>3.阻止计算图跟踪</h3><ul><li><code>with torch.no_grad()</code>:阻止计算图跟踪</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    z = y+<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br></pre></td></tr></table></figure><pre><code>tensor(5.)False</code></pre><h3 id="4-创建一个不可导张量"><a href="#4-创建一个不可导张量" class="headerlink" title="4.创建一个不可导张量"></a>4.创建一个不可导张量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y1 = x ** <span class="number">2</span></span><br><span class="line">y2 = y1.detach()</span><br><span class="line">z = y2**<span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y1)</span><br><span class="line"><span class="built_in">print</span>(y2)</span><br><span class="line"><span class="built_in">print</span>(z)</span><br></pre></td></tr></table></figure><pre><code>tensor(4., grad_fn=&lt;PowBackward0&gt;)tensor(4.)tensor(16.)</code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>python</title>
      <link href="/2023-12-18-python/"/>
      <url>/2023-12-18-python/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(1)-基础与张量转换</title>
      <link href="/Pytorch-1/"/>
      <url>/Pytorch-1/</url>
      
        <content type="html"><![CDATA[<h3 id="1-导入库和版本"><a href="#1-导入库和版本" class="headerlink" title="1.导入库和版本"></a>1.导入库和版本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)</span><br></pre></td></tr></table></figure><p>   2.1.1+cu121</p><blockquote><p>cu121表示 PyTorch 是为了与 CUDA 12.1 版本兼容而编译的</p></blockquote><h3 id="2-张量的创建方法"><a href="#2-张量的创建方法" class="headerlink" title="2.张量的创建方法"></a>2.张量的创建方法</h3><ul><li>包括<code>列表</code>、<code>元组</code>、<code>数组</code>的创建</li><li><code>列表/元组</code>的创建方法默认是<strong>int64/float32</strong></li><li><code>数组</code>的创建方法默认是<strong>int32/float64</strong>，且后面会附带上dtype的类型</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">t1=torch.tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">t2=torch.tensor((<span class="number">1</span>,<span class="number">2</span>)) </span><br><span class="line">t3=torch.tensor(<span class="number">1</span>+<span class="number">2j</span>)</span><br><span class="line">t4=torch.tensor([<span class="literal">True</span>,<span class="literal">True</span>])</span><br><span class="line">t5=torch.tensor(np.array([<span class="number">1</span>,<span class="number">2</span>]))  <span class="comment"># 后面会附带类型</span></span><br><span class="line"><span class="built_in">print</span>(t1)  <span class="comment"># tensor([1, 2])</span></span><br><span class="line"><span class="built_in">print</span>(t2)  <span class="comment"># tensor([1, 2])</span></span><br><span class="line"><span class="built_in">print</span>(t3)  <span class="comment"># tensor(1.+2.j)</span></span><br><span class="line"><span class="built_in">print</span>(t4)  <span class="comment"># tensor([True, True])</span></span><br><span class="line"><span class="built_in">print</span>(t5)  <span class="comment"># tensor([1, 2], dtype=torch.int32)</span></span><br></pre></td></tr></table></figure><h3 id="3-torch数据类型大全——10种"><a href="#3-torch数据类型大全——10种" class="headerlink" title="3. torch数据类型大全——10种"></a>3. torch数据类型大全——10种</h3><div class="table-container"><table><thead><tr><th style="text-align:center">Torch Type</th><th style="text-align:center">Alias(显示转换指令)</th></tr></thead><tbody><tr><td style="text-align:center"><strong><code>torch.float64</code></strong></td><td style="text-align:center"><strong><code>torch.double</code></strong></td></tr><tr><td style="text-align:center"><code>torch.float32</code></td><td style="text-align:center"><code>torch.float</code></td></tr><tr><td style="text-align:center"><code>torch.float16</code></td><td style="text-align:center"><code>torch.half</code></td></tr><tr><td style="text-align:center"><strong><code>torch.int64</code></strong></td><td style="text-align:center"><strong><code>torch.long</code></strong></td></tr><tr><td style="text-align:center"><code>torch.int32</code></td><td style="text-align:center"><code>torch.int</code></td></tr><tr><td style="text-align:center"><code>torch.int16</code></td><td style="text-align:center"><code>torch.short</code></td></tr><tr><td style="text-align:center"><code>torch.uint8</code></td><td style="text-align:center"></td></tr><tr><td style="text-align:center"><code>torch.int8</code></td><td style="text-align:center"></td></tr><tr><td style="text-align:center"><strong><code>torch.bool</code></strong></td><td style="text-align:center"></td></tr><tr><td style="text-align:center"><code>torch.complex64</code></td></tr></tbody></table></div><h3 id="4-隐式转换"><a href="#4-隐式转换" class="headerlink" title="4. 隐式转换"></a>4. 隐式转换</h3><ul><li>浮点型&gt;整数型&gt;布尔型</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.tensor([<span class="number">1.1</span>,<span class="number">2.7</span>],dtype = torch.uint8))  <span class="comment"># tensor([1, 2], dtype=torch.uint8)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.tensor([<span class="number">1.1</span>,<span class="number">2</span>]).dtype)   <span class="comment"># torch.float32</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor([<span class="number">2</span>,<span class="literal">True</span>]).dtype)  <span class="comment"># torch.int64</span></span><br></pre></td></tr></table></figure><p>​    </p><h3 id="5-显示转换（方法）"><a href="#5-显示转换（方法）" class="headerlink" title="5. 显示转换（方法）"></a>5. 显示转换（方法）</h3><ul><li><code>不改变</code>原数据类型</li><li>检查类型用<code>dtype</code></li><li><code>half</code>/<code>float</code>/<code>double</code>/<code>short</code>/<code>int</code>/<code>long</code>指令</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(t.<span class="built_in">float</span>())  <span class="comment"># tensor([1., 2.])</span></span><br><span class="line"><span class="built_in">print</span>(t.dtype)    <span class="comment"># torch.int64</span></span><br></pre></td></tr></table></figure><p>​    </p><h3 id="6-一维度情况的维度形变"><a href="#6-一维度情况的维度形变" class="headerlink" title="6. 一维度情况的维度形变"></a>6. 一维度情况的维度形变</h3><p>在PyTorch中，<code>shape</code>与<code>size()</code>用于查看张量的<code>形状</code>。以下是一些常用的属性和方法：</p><div class="table-container"><table><thead><tr><th style="text-align:center">属性/方法</th><th style="text-align:center">作用</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center"><code>ndim</code></td><td style="text-align:center">维度</td><td style="text-align:center">查看张量的维度数</td></tr><tr><td style="text-align:center"><code>shape</code></td><td style="text-align:center">形状</td><td style="text-align:center">查看张量的形状，与<code>size()</code>相同</td></tr><tr><td style="text-align:center"><code>size()</code></td><td style="text-align:center">形状</td><td style="text-align:center">查看张量的形状，与<code>shape</code>相同</td></tr><tr><td style="text-align:center"><code>numel()</code></td><td style="text-align:center">元素数量</td><td style="text-align:center">查看张量的元素总数</td></tr><tr><td style="text-align:center"><code>len()</code></td><td style="text-align:center">元素数量</td><td style="text-align:center">在一维中与<code>numel()</code>相同</td></tr></tbody></table></div><ul><li><strong>注意</strong>：在一维张量中，<code>numel()</code> 和 <code>len()</code> 返回相同的结果，都表示张量的元素数量。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;ndim:\t&#x27;</span>,torch.tensor([<span class="number">1</span>,<span class="number">2</span>]).ndim)          <span class="comment">#   ndim: 1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;shape:\t&#x27;</span>,torch.tensor([<span class="number">1</span>,<span class="number">2</span>]).shape)        <span class="comment">#   shape: torch.Size([2])</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;size():\t&#x27;</span>,torch.tensor([<span class="number">1</span>,<span class="number">2</span>]).size())      <span class="comment">#   size(): torch.Size([2])</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;numel():&#x27;</span>,torch.tensor([<span class="number">1</span>,<span class="number">2</span>]).numel())      <span class="comment">#   numel(): 2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;len():\t&#x27;</span>,<span class="built_in">len</span>(torch.tensor([<span class="number">1</span>,<span class="number">2</span>])))         <span class="comment">#   len(): 2    </span></span><br></pre></td></tr></table></figure><h3 id="7-二维度情况的维度形变"><a href="#7-二维度情况的维度形变" class="headerlink" title="7. 二维度情况的维度形变"></a>7. 二维度情况的维度形变</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;ndim:\t&#x27;</span>,torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).ndim)          <span class="comment">#   ndim: 2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;shape:\t&#x27;</span>,torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).shape)        <span class="comment">#   shape: torch.Size([2, 2]) </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;size():\t&#x27;</span>,torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).size())      <span class="comment">#   size(): torch.Size([2, 2])</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;numel():&#x27;</span>,torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).numel())      <span class="comment">#   numel(): 4</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;len():\t&#x27;</span>,<span class="built_in">len</span>(torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])))         <span class="comment">#   len(): 2</span></span><br></pre></td></tr></table></figure><h3 id="8-三维度情况的维度形变"><a href="#8-三维度情况的维度形变" class="headerlink" title="8. 三维度情况的维度形变"></a>8. 三维度情况的维度形变</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;ndim:\t&#x27;</span>,torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>]],[[<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>]]]).ndim)     <span class="comment"># ndim:3</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;shape:\t&#x27;</span>,torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>]],[[<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>]]]).shape)   <span class="comment"># shape:torch.Size([2, 2, 3]) </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;size():\t&#x27;</span>,torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>]],[[<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>]]]).size()) <span class="comment"># size():torch.Size([2, 2, 3])</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;numel():&#x27;</span>,torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>]],[[<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>]]]).numel()) <span class="comment"># numel():12</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;len():\t&#x27;</span>,<span class="built_in">len</span>(torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>]],[[<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>]]])))    <span class="comment"># len():2</span></span><br></pre></td></tr></table></figure><h3 id="9-零维张量"><a href="#9-零维张量" class="headerlink" title="9. 零维张量"></a>9. 零维张量</h3><ul><li><strong>零维张量</strong>：PyTorch中的单个数值，具有张量属性（如 <code>torch.tensor(1)</code>）。</li><li><strong>标量</strong>：Python中的单个数值，不具备张量属性。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;ndim:\t&#x27;</span>,torch.tensor(<span class="number">1</span>).ndim)     <span class="comment">#  ndim: 0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;shape:\t&#x27;</span>,torch.tensor(<span class="number">1</span>).shape)   <span class="comment">#  shape: torch.Size([])</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;size():\t&#x27;</span>,torch.tensor(<span class="number">1</span>).size()) <span class="comment">#  size(): torch.Size([]) </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;numel():&#x27;</span>,torch.tensor(<span class="number">1</span>).numel()) <span class="comment">#  numel(): 1</span></span><br><span class="line"><span class="comment"># print(&#x27;len():\t&#x27;,len(torch.tensor(1))) 报错，零维张量不能len()</span></span><br></pre></td></tr></table></figure><h3 id="10-张量的形变"><a href="#10-张量的形变" class="headerlink" title="10.张量的形变"></a>10.张量的形变</h3><ul><li><code>N维、0维</code>flatten()后被变为<code>1维</code>，保证只有一个<code>[]</code></li><li>flatten()和reshape()均<code>不会对原张量造成影响</code></li><li>reshape(4),reshape(4,)<code>均为1维</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).flatten())     <span class="comment">#  tensor([1, 2, 3, 4])</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1</span>).flatten())                 <span class="comment">#  tensor([1])</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).reshape(<span class="number">4</span>))    <span class="comment">#  tensor([1, 2, 3, 4])</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).reshape(<span class="number">4</span>,))   <span class="comment">#  tensor([1, 2, 3, 4])</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).reshape(<span class="number">1</span>,<span class="number">4</span>))  <span class="comment">#  tensor([[1, 2, 3, 4]])</span></span><br></pre></td></tr></table></figure><h3 id="11-特殊张量的创建"><a href="#11-特殊张量的创建" class="headerlink" title="11.特殊张量的创建"></a>11.特殊张量的创建</h3><ul><li><code>1/0/empty</code>只填size参数时,无括号/圆括号/方括号均可</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.zeros(<span class="number">2</span>,<span class="number">3</span>))   <span class="comment"># tensor([[0., 0., 0.],[0., 0., 0.]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.ones((<span class="number">2</span>,<span class="number">3</span>)))  <span class="comment"># tensor([[1., 1., 1.],[1., 1., 1.]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.empty([<span class="number">2</span>,<span class="number">3</span>])) <span class="comment"># tensor([[1., 1., 1.],[1., 1., 1.]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.full((<span class="number">2</span>,<span class="number">3</span>),<span class="number">1</span>))<span class="comment"># tensor([[1, 1, 1],[1, 1, 1]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.eye(<span class="number">3</span>))       <span class="comment"># tensor([[1., 0., 0.],[0., 1., 0.],[0., 0., 1.]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.diag(torch.tensor([<span class="number">1</span>,<span class="number">2</span>])))  <span class="comment"># tensor([[1, 0], [0, 2]])</span></span><br></pre></td></tr></table></figure><h3 id="12-随机张量的创建"><a href="#12-随机张量的创建" class="headerlink" title="12.随机张量的创建"></a>12.随机张量的创建</h3><div class="table-container"><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center"><code>rand()</code></td><td style="text-align:center">0-1均匀分布</td></tr><tr><td style="text-align:center"><code>randn()</code></td><td style="text-align:center">标准正态分布</td></tr><tr><td style="text-align:center"><code>normal()</code></td><td style="text-align:center">服从指定正态分布</td></tr><tr><td style="text-align:center"><code>randint()</code></td><td style="text-align:center">整数随机采样</td></tr><tr><td style="text-align:center"><code>arange</code></td><td style="text-align:center">生成数列(间隔)</td></tr><tr><td style="text-align:center"><code>linspace</code></td><td style="text-align:center">生成数列(数量)</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.rand(<span class="number">2</span>,<span class="number">3</span>))  <span class="comment"># tensor([[0.7878, 0.6142, 0.6865], [0.1155, 0.4820, 0.1763]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.randn(<span class="number">2</span>,<span class="number">3</span>)) <span class="comment"># tensor([[ 0.0174,  1.1351, -0.1324],[-0.2343, -0.0504,  0.1417]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.normal(<span class="number">2</span>,<span class="number">3</span>,size=(<span class="number">2</span>,<span class="number">2</span>))) <span class="comment"># tensor([[1.1347, 1.6557], [3.1312, 2.9998]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.randint(<span class="number">1</span>,<span class="number">10</span>,size=[<span class="number">2</span>,<span class="number">4</span>])) <span class="comment"># tensor([[5, 2, 3, 8],[2, 8, 8, 5]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.arange(<span class="number">0</span>,<span class="number">5</span>,<span class="number">3</span>))   <span class="comment"># tensor([0, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.linspace(<span class="number">0</span>,<span class="number">5</span>,<span class="number">4</span>)) <span class="comment">#   tensor([0.0000, 1.6667, 3.3333, 5.0000])</span></span><br></pre></td></tr></table></figure><h3 id="13-tensor-numpy-list相互转换"><a href="#13-tensor-numpy-list相互转换" class="headerlink" title="13.tensor/numpy/list相互转换"></a>13.tensor/numpy/list相互转换</h3><ul><li><code>list(t)</code>是转换为<code>零维张量</code></li><li><code>item()</code>要求<strong>单元素张量（tensor）</strong>中提取其数值</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(t.numpy())   <span class="comment"># [1]</span></span><br><span class="line"><span class="built_in">print</span>(np.array(t)) <span class="comment"># [1] </span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(t.tolist())  <span class="comment"># [1]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(t))     <span class="comment"># [tensor(1)]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(t.item())  <span class="comment">#  1</span></span><br></pre></td></tr></table></figure><h3 id="14-张量的浅拷贝与深拷贝"><a href="#14-张量的浅拷贝与深拷贝" class="headerlink" title="14. 张量的浅拷贝与深拷贝"></a>14. 张量的浅拷贝与深拷贝</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">t2 = t1  <span class="comment"># 赋值/浅拷贝，动一个则变另一个</span></span><br><span class="line">t3=t1.clone()  <span class="comment"># 深拷贝，动一个则另一个不变</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 标签1 </tag>
            
            <tag> 标签A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第三篇文章</title>
      <link href="/2023-12-17-3/"/>
      <url>/2023-12-17-3/</url>
      
        <content type="html"><![CDATA[<h2 id="这是我的第三篇文章"><a href="#这是我的第三篇文章" class="headerlink" title="这是我的第三篇文章"></a>这是我的第三篇文章</h2>]]></content>
      
      
      <categories>
          
          <category> Embedded </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 标签3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第二篇文章</title>
      <link href="/2023-12-17-2/"/>
      <url>/2023-12-17-2/</url>
      
        <content type="html"><![CDATA[<h2 id="这是我的第二篇文章"><a href="#这是我的第二篇文章" class="headerlink" title="这是我的第二篇文章"></a>这是我的第二篇文章</h2>]]></content>
      
      
      <categories>
          
          <category> C </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 标签2 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>关于</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<!-- GitCalendar容器 --><div id="gitZone"></div>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>友链</title>
      <link href="/link/index.html"/>
      <url>/link/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>分类</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/css/custom.css"/>
      <url>/css/custom.css</url>
      
        <content type="html"><![CDATA[:root {  --trans-light: rgba(255, 255, 255, 0.88);  --trans-dark: rgba(25, 25, 25, 0.88);  --border-style: 1px solid rgb(169, 169, 169);  --backdrop-filter: blur(5px) saturate(150%);}/* 页脚与头图透明 */#footer {    background: transparent !important;  }  #page-header {    background: transparent !important;  }    /* 白天模式遮罩透明 */  #footer::before {    background: transparent !important;  }  #page-header::before {    background: transparent !important;  }    /* 夜间模式遮罩透明 */  [data-theme="dark"] #footer::before {    background: transparent !important;  }  [data-theme="dark"] #page-header::before {    background: transparent !important;  }/* 小冰分类分类磁铁黑夜模式适配 *//* 一般状态 */[data-theme="dark"] .magnet_link_context {  background: #1e1e1e;  color: antiquewhite;}/* 鼠标悬浮状态 */[data-theme="dark"] .magnet_link_context:hover {  background: #3ecdf1;  color: #f2f2f2;}@font-face {  /* 为载入的字体取名字(随意) */  font-family: 'YSHST';  /* 字体文件地址(相对或者绝对路径都可以) */  src: url(/font/poppins-black-webfont.woff2);  /* 定义加粗样式(加粗多少) */  font-weight: normal;  /* 定义字体样式(斜体/非斜体) */  font-style: normal;  /* 定义显示样式 */  font-display: block;}/* 翻页按钮居中 */#pagination {  width: 100%;  margin: auto;}/* 一级菜单居中 */#nav .menus_items {  position: absolute !important;  width: fit-content !important;  left: 50% !important;  transform: translateX(-50%) !important;}/* 子菜单横向展示 */#nav .menus_items .menus_item:hover .menus_item_child {  display: flex !important;}/* 这里的2是代表导航栏的第2个元素，即有子菜单的元素，可以按自己需求修改 */.menus_items .menus_item:nth-child(5) .menus_item_child {  left: -38px;}/* 夜间模式菜单栏发光字 */[data-theme="dark"] #nav .site-page,[data-theme="dark"] #nav .menus_items .menus_item .menus_item_child li a {  text-shadow: 0 0 2px var(--theme-color) !important;}/* 手机端适配 */[data-theme="dark"] #sidebar #sidebar-menus .menus_items .site-page {  text-shadow: 0 0 2px var(--theme-color) !important;}/* 闪烁变动颜色连续渐变 *//* 日间模式不生效 */[data-theme="light"] #site-name,[data-theme="light"] #site-title,[data-theme="light"] #site-subtitle,[data-theme="light"] #post-info {  animation: none;}/* 夜间模式生效 */[data-theme="dark"] #site-name,[data-theme="dark"] #site-title {  animation: light_15px 10s linear infinite;}[data-theme="dark"] #site-subtitle {  animation: light_10px 10s linear infinite;}[data-theme="dark"] #post-info {  animation: light_5px 10s linear infinite;}/* 关键帧描述 */@keyframes light_15px {  0% {    text-shadow: #5636ed 0 0 15px;  }  12.5% {    text-shadow: #11ee5e 0 0 15px;  }  25% {    text-shadow: #f14747 0 0 15px;  }  37.5% {    text-shadow: #f1a247 0 0 15px;  }  50% {    text-shadow: #f1ee47 0 0 15px;  }  50% {    text-shadow: #b347f1 0 0 15px;  }  62.5% {    text-shadow: #002afa 0 0 15px;  }  75% {    text-shadow: #ed709b 0 0 15px;  }  87.5% {    text-shadow: #39c5bb 0 0 15px;  }  100% {    text-shadow: #5636ed 0 0 15px;  }}@keyframes light_10px {  0% {    text-shadow: #5636ed 0 0 10px;  }  12.5% {    text-shadow: #11ee5e 0 0 10px;  }  25% {    text-shadow: #f14747 0 0 10px;  }  37.5% {    text-shadow: #f1a247 0 0 10px;  }  50% {    text-shadow: #f1ee47 0 0 10px;  }  50% {    text-shadow: #b347f1 0 0 10px;  }  62.5% {    text-shadow: #002afa 0 0 10px;  }  75% {    text-shadow: #ed709b 0 0 10px;  }  87.5% {    text-shadow: #39c5bb 0 0 10px;  }  100% {    text-shadow: #5636ed 0 0 10px;  }}@keyframes light_5px {  0% {    text-shadow: #5636ed 0 0 5px;  }  12.5% {    text-shadow: #11ee5e 0 0 5px;  }  25% {    text-shadow: #f14747 0 0 5px;  }  37.5% {    text-shadow: #f1a247 0 0 15px;  }  50% {    text-shadow: #f1ee47 0 0 5px;  }  50% {    text-shadow: #b347f1 0 0 5px;  }  62.5% {    text-shadow: #002afa 0 0 5px;  }  75% {    text-shadow: #ed709b 0 0 5px;  }  87.5% {    text-shadow: #39c5bb 0 0 5px;  }  100% {    text-shadow: #5636ed 0 0 5px;  }}/* 背景宇宙星光  */#universe{  display: block;  position: fixed;  margin: 0;  padding: 0;  border: 0;  outline: 0;  left: 0;  top: 0;  width: 100%;  height: 100%;  pointer-events: none;  /* 这个是调置顶的优先级的，-1在文章页下面，背景上面，个人推荐这种 */  z-index: -1;}/* 侧边栏个人信息卡片动态渐变色 */#aside-content > .card-widget.card-info {  background: linear-gradient(    -45deg,    #e8d8b9,    #eccec5,    #a3e9eb,      #bdbdf0,    #eec1ea  );  box-shadow: 0 0 5px rgb(66, 68, 68);  position: relative;  background-size: 400% 400%;  -webkit-animation: Gradient 10s ease infinite;  -moz-animation: Gradient 10s ease infinite;  animation: Gradient 10s ease infinite !important;}@-webkit-keyframes Gradient {  0% {    background-position: 0% 50%;  }  50% {    background-position: 100% 50%;  }  100% {    background-position: 0% 50%;  }}@-moz-keyframes Gradient {  0% {    background-position: 0% 50%;  }  50% {    background-position: 100% 50%;  }  100% {    background-position: 0% 50%;  }}@keyframes Gradient {  0% {    background-position: 0% 50%;  }  50% {    background-position: 100% 50%;  }  100% {    background-position: 0% 50%;  }}/* 黑夜模式适配 */[data-theme="dark"] #aside-content > .card-widget.card-info {  background: #191919ee;}/* 个人信息Follow me按钮 */#aside-content > .card-widget.card-info > #card-info-btn {  background-color: #3eb8be;  border-radius: 8px;}/* 鼠标样式 */#cursor {  position: fixed;  width: 16px;  height: 16px;  /* 这里改变跟随的底色 */  background: rgb(101, 153, 245);  border-radius: 8px;  opacity: 0.25;  z-index: 10086;  pointer-events: none;  transition: 0.2s ease-in-out;  transition-property: background, opacity, transform;}#cursor.hidden {  opacity: 0;}#cursor.hover {  opacity: 0.1;  transform: scale(2.5);  -webkit-transform: scale(2.5);  -moz-transform: scale(2.5);  -ms-transform: scale(2.5);  -o-transform: scale(2.5);}#cursor.active {  opacity: 0.5;  transform: scale(0.5);  -webkit-transform: scale(0.5);  -moz-transform: scale(0.5);  -ms-transform: scale(0.5);  -o-transform: scale(0.5);}/* 首页文章卡片 */#recent-posts > .recent-post-item {  background: var(--trans-light);  backdrop-filter: var(--backdrop-filter);  border-radius: 25px;  border: var(--border-style);}/* 首页侧栏卡片 */#aside-content .card-widget {  background: var(--trans-light);  backdrop-filter: var(--backdrop-filter);  border-radius: 18px;  border: var(--border-style);}/* 文章页、归档页、普通页面 */div#post,div#page,div#archive {  background: var(--trans-light);  backdrop-filter: var(--backdrop-filter);  border: var(--border-style);  border-radius: 20px;}/* 导航栏 */#page-header.nav-fixed #nav {  background: rgba(255, 255, 255, 0.75);  backdrop-filter: var(--backdrop-filter);}[data-theme="dark"] #page-header.nav-fixed #nav {  background: rgba(0, 0, 0, 0.7) !important;}/* 夜间模式遮罩 */[data-theme="dark"] #recent-posts > .recent-post-item,[data-theme="dark"] #aside-content .card-widget,[data-theme="dark"] div#post,[data-theme="dark"] div#archive,[data-theme="dark"] div#page {  background: var(--trans-dark);}/* 夜间模式页脚页头遮罩透明 */[data-theme="dark"] #footer::before {  background: transparent !important;}[data-theme="dark"] #page-header::before {  background: transparent !important;}/* 阅读模式 */.read-mode #aside-content .card-widget {  background: rgba(158, 204, 171, 0.5) !important;}.read-mode div#post {  background: rgba(158, 204, 171, 0.5) !important;}/* 夜间模式下的阅读模式 */[data-theme="dark"] .read-mode #aside-content .card-widget {  background: rgba(25, 25, 25, 0.9) !important;  color: #ffffff;}[data-theme="dark"] .read-mode div#post {  background: rgba(25, 25, 25, 0.9) !important;  color: #ffffff;}/* 文章页H1-H6图标样式效果 *//* 控制风车转动速度 4s那里可以自己调节快慢 */h1::before,h2::before,h3::before,h4::before,h5::before,h6::before {  -webkit-animation: ccc 4s linear infinite;  animation: ccc 4s linear infinite;}/* 控制风车转动方向 -1turn 为逆时针转动，1turn 为顺时针转动，相同数字部分记得统一修改 */@-webkit-keyframes ccc {  0% {    -webkit-transform: rotate(0deg);    transform: rotate(0deg);  }  to {    -webkit-transform: rotate(-1turn);    transform: rotate(-1turn);  }}@keyframes ccc {  0% {    -webkit-transform: rotate(0deg);    transform: rotate(0deg);  }  to {    -webkit-transform: rotate(-1turn);    transform: rotate(-1turn);  }}/* 设置风车颜色 */#content-inner.layout h1::before {  color: #ef50a8;  margin-left: -1.55rem;  font-size: 1.3rem;  margin-top: -0.23rem;}#content-inner.layout h2::before {  color: #fb7061;  margin-left: -1.35rem;  font-size: 1.1rem;  margin-top: -0.12rem;}#content-inner.layout h3::before {  color: #ffbf00;  margin-left: -1.22rem;  font-size: 0.95rem;  margin-top: -0.09rem;}#content-inner.layout h4::before {  color: #a9e000;  margin-left: -1.05rem;  font-size: 0.8rem;  margin-top: -0.09rem;}#content-inner.layout h5::before {  color: #57c850;  margin-left: -0.9rem;  font-size: 0.7rem;  margin-top: 0rem;}#content-inner.layout h6::before {  color: #5ec1e0;  margin-left: -0.9rem;  font-size: 0.66rem;  margin-top: 0rem;}/* s设置风车hover动效 6s那里可以自己调节快慢*/#content-inner.layout h1:hover,#content-inner.layout h2:hover,#content-inner.layout h3:hover,#content-inner.layout h4:hover,#content-inner.layout h5:hover,#content-inner.layout h6:hover {  color: var(--theme-color);}#content-inner.layout h1:hover::before,#content-inner.layout h2:hover::before,#content-inner.layout h3:hover::before,#content-inner.layout h4:hover::before,#content-inner.layout h5:hover::before,#content-inner.layout h6:hover::before {  color: var(--theme-color);  -webkit-animation: ccc 6s linear infinite;  animation: ccc 6s linear infinite;}]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/cursor.js"/>
      <url>/js/cursor.js</url>
      
        <content type="html"><![CDATA[var CURSOR;Math.lerp = (a, b, n) => (1 - n) * a + n * b;const getStyle = (el, attr) => {    try {        return window.getComputedStyle            ? window.getComputedStyle(el)[attr]            : el.currentStyle[attr];    } catch (e) {}    return "";};class Cursor {    constructor() {        this.pos = {curr: null, prev: null};        this.pt = [];        this.create();        this.init();        this.render();    }    move(left, top) {        this.cursor.style["left"] = `${left}px`;        this.cursor.style["top"] = `${top}px`;    }    create() {        if (!this.cursor) {            this.cursor = document.createElement("div");            this.cursor.id = "cursor";            this.cursor.classList.add("hidden");            document.body.append(this.cursor);        }        var el = document.getElementsByTagName('*');        for (let i = 0; i < el.length; i++)            if (getStyle(el[i], "cursor") == "pointer")                this.pt.push(el[i].outerHTML);        document.body.appendChild((this.scr = document.createElement("style")));        // 这里改变鼠标指针的颜色 由svg生成        this.scr.innerHTML = `* {cursor: url("data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 8 8' width='8px' height='8px'><circle cx='4' cy='4' r='4' opacity='1.0' fill='rgb(0, 0, 255)'/></svg>") 4 4, auto}`;    }    refresh() {        this.scr.remove();        this.cursor.classList.remove("hover");        this.cursor.classList.remove("active");        this.pos = {curr: null, prev: null};        this.pt = [];        this.create();        this.init();        this.render();    }    init() {        document.onmouseover  = e => this.pt.includes(e.target.outerHTML) && this.cursor.classList.add("hover");        document.onmouseout   = e => this.pt.includes(e.target.outerHTML) && this.cursor.classList.remove("hover");        document.onmousemove  = e => {(this.pos.curr == null) && this.move(e.clientX - 8, e.clientY - 8); this.pos.curr = {x: e.clientX - 8, y: e.clientY - 8}; this.cursor.classList.remove("hidden");};        document.onmouseenter = e => this.cursor.classList.remove("hidden");        document.onmouseleave = e => this.cursor.classList.add("hidden");        document.onmousedown  = e => this.cursor.classList.add("active");        document.onmouseup    = e => this.cursor.classList.remove("active");    }    render() {        if (this.pos.prev) {            this.pos.prev.x = Math.lerp(this.pos.prev.x, this.pos.curr.x, 0.15);            this.pos.prev.y = Math.lerp(this.pos.prev.y, this.pos.curr.y, 0.15);            this.move(this.pos.prev.x, this.pos.prev.y);        } else {            this.pos.prev = this.pos.curr;        }        requestAnimationFrame(() => this.render());    }}(() => {    CURSOR = new Cursor();    // 需要重新获取列表时，使用 CURSOR.refresh()})();]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/runtime.js"/>
      <url>/js/runtime.js</url>
      
        <content type="html"><![CDATA[function createtime() {    var now = new Date(); // 获取当前时间    var startDate = new Date("12/17/2023 21:00:00"); // 网站开始运行的日期，按需更改    var elapsed = now - startDate; // 计算流逝的时间    var seconds = Math.floor(elapsed / 1000);    var minutes = Math.floor(seconds / 60);    var hours = Math.floor(minutes / 60);    var days = Math.floor(hours / 24);    seconds %= 60; // 剩余秒数    minutes %= 60; // 剩余分钟数    hours %= 24; // 剩余小时数    // 确保时间是两位数字格式    var secondsStr = seconds < 10 ? "0" + seconds : seconds;    var minutesStr = minutes < 10 ? "0" + minutes : minutes;    var hoursStr = hours < 10 ? "0" + hours : hours;    // 更新网站内容    var c = `<div style="font-size:13px;font-weight:bold">本站居然运行了 ${days} 天 ${hoursStr} 小时 ${minutesStr} 分 ${secondsStr} 秒</div>`;    document.getElementById("workboard").innerHTML = c;}setInterval(createtime, 1000); // 每秒更新一次]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>添加我的QQ</title>
      <link href="/qq/index.html"/>
      <url>/qq/index.html</url>
      
        <content type="html"><![CDATA[<p><img src="../img/QQ.jpg" alt="我的QQ二维码"></p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/universe.js"/>
      <url>/js/universe.js</url>
      
        <content type="html"><![CDATA[function dark() {window.requestAnimationFrame=window.requestAnimationFrame||window.mozRequestAnimationFrame||window.webkitRequestAnimationFrame||window.msRequestAnimationFrame;var n,e,i,h,t=.05,s=document.getElementById("universe"),o=!0,a="180,184,240",r="226,225,142",d="226,225,224",c=[];function f(){n=window.innerWidth,e=window.innerHeight,i=.216*n,s.setAttribute("width",n),s.setAttribute("height",e)}function u(){h.clearRect(0,0,n,e);for(var t=c.length,i=0;i<t;i++){var s=c[i];s.move(),s.fadeIn(),s.fadeOut(),s.draw()}}function y(){this.reset=function(){this.giant=m(3),this.comet=!this.giant&&!o&&m(10),this.x=l(0,n-10),this.y=l(0,e),this.r=l(1.1,2.6),this.dx=l(t,6*t)+(this.comet+1-1)*t*l(50,120)+2*t,this.dy=-l(t,6*t)-(this.comet+1-1)*t*l(50,120),this.fadingOut=null,this.fadingIn=!0,this.opacity=0,this.opacityTresh=l(.2,1-.4*(this.comet+1-1)),this.do=l(5e-4,.002)+.001*(this.comet+1-1)},this.fadeIn=function(){this.fadingIn&&(this.fadingIn=!(this.opacity>this.opacityTresh),this.opacity+=this.do)},this.fadeOut=function(){this.fadingOut&&(this.fadingOut=!(this.opacity<0),this.opacity-=this.do/2,(this.x>n||this.y<0)&&(this.fadingOut=!1,this.reset()))},this.draw=function(){if(h.beginPath(),this.giant)h.fillStyle="rgba("+a+","+this.opacity+")",h.arc(this.x,this.y,2,0,2*Math.PI,!1);else if(this.comet){h.fillStyle="rgba("+d+","+this.opacity+")",h.arc(this.x,this.y,1.5,0,2*Math.PI,!1);for(var t=0;t<30;t++)h.fillStyle="rgba("+d+","+(this.opacity-this.opacity/20*t)+")",h.rect(this.x-this.dx/4*t,this.y-this.dy/4*t-2,2,2),h.fill()}else h.fillStyle="rgba("+r+","+this.opacity+")",h.rect(this.x,this.y,this.r,this.r);h.closePath(),h.fill()},this.move=function(){this.x+=this.dx,this.y+=this.dy,!1===this.fadingOut&&this.reset(),(this.x>n-n/4||this.y<0)&&(this.fadingOut=!0)},setTimeout(function(){o=!1},50)}function m(t){return Math.floor(1e3*Math.random())+1<10*t}function l(t,i){return Math.random()*(i-t)+t}f(),window.addEventListener("resize",f,!1),function(){h=s.getContext("2d");for(var t=0;t<i;t++)c[t]=new y,c[t].reset();u()}(),function t(){document.getElementsByTagName('html')[0].getAttribute('data-theme')=='dark'&&u(),window.requestAnimationFrame(t)}()};dark()]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/light.js"/>
      <url>/js/light.js</url>
      
        <content type="html"><![CDATA[// 霓虹灯效果// 颜色数组var arr = ["#39c5bb", "#f14747", "#f1a247", "#f1ee47", "#b347f1", "#1edbff", "#ed709b", "#5636ed"];// 颜色索引var idx = 0;// 切换颜色function changeColor() {    // 仅夜间模式才启用    if (document.getElementsByTagName('html')[0].getAttribute('data-theme') == 'dark') {        if (document.getElementById("site-name"))            document.getElementById("site-name").style.textShadow = arr[idx] + " 0 0 15px";        if (document.getElementById("site-title"))            document.getElementById("site-title").style.textShadow = arr[idx] + " 0 0 15px";        if (document.getElementById("site-subtitle"))            document.getElementById("site-subtitle").style.textShadow = arr[idx] + " 0 0 10px";        if (document.getElementById("post-info"))            document.getElementById("post-info").style.textShadow = arr[idx] + " 0 0 5px";        try {            document.getElementsByClassName("author-info__name")[0].style.textShadow = arr[idx] + " 0 0 12px";            document.getElementsByClassName("author-info__description")[0].style.textShadow = arr[idx] + " 0 0 12px";        } catch {                    }        idx++;        if (idx == 8) {            idx = 0;        }    } else {        // 白天模式恢复默认        if (document.getElementById("site-name"))            document.getElementById("site-name").style.textShadow = "#1e1e1ee0 1px 1px 1px";        if (document.getElementById("site-title"))            document.getElementById("site-title").style.textShadow = "#1e1e1ee0 1px 1px 1px";        if (document.getElementById("site-subtitle"))            document.getElementById("site-subtitle").style.textShadow = "#1e1e1ee0 1px 1px 1px";        if (document.getElementById("post-info"))            document.getElementById("post-info").style.textShadow = "#1e1e1ee0 1px 1px 1px";        try {            document.getElementsByClassName("author-info__name")[0].style.textShadow = "";            document.getElementsByClassName("author-info__description")[0].style.textShadow = "";        } catch {                    }    }}// 开启计时器window.onload = setInterval(changeColor, 1200);]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>音乐</title>
      <link href="/music/index.html"/>
      <url>/music/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>电影</title>
      <link href="/movies/index.html"/>
      <url>/movies/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
