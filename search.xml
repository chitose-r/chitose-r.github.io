<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Pytorch(6)-优化SGD</title>
      <link href="/Pytorch(6)/"/>
      <url>/Pytorch(6)/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br></pre></td></tr></table></figure><h3 id="1-神经网络的复杂性与优化"><a href="#1-神经网络的复杂性与优化" class="headerlink" title="1. 神经网络的复杂性与优化"></a>1. 神经网络的复杂性与优化</h3><ul><li>神经网络的<strong>函数复杂度</strong>使得求导过程复杂。</li><li>面对<strong>成千上万的权重 (w)</strong>，逐个计算梯度不现实。</li><li>优化算法核心：<strong>逐步迭代</strong>至最小值。</li></ul><h3 id="2-梯度下降的方向与距离"><a href="#2-梯度下降的方向与距离" class="headerlink" title="2. 梯度下降的方向与距离"></a>2. 梯度下降的方向与距离</h3><ul><li><strong>方向</strong>：梯度下降的反方向。</li><li><strong>距离</strong>：步长乘以梯度向量的大小。</li><li>每个坐标点的梯度方向独一无二。</li></ul><p>梯度下降公式：</p><pre><code>$$w_&#123;(t+1)&#125; = w_&#123;(t)&#125; - \eta \frac&#123;\partial L&#125;&#123;\partial w&#125;$$</code></pre><ul><li>梯度下降的反方向总比损失函数低一个维度。</li></ul><h3 id="3-反向传播的实现"><a href="#3-反向传播的实现" class="headerlink" title="3. 反向传播的实现"></a>3. 反向传播的实现</h3><ul><li>在 PyTorch 中实现反向传播。</li><li>输出 <code>y</code> 需为向量形式。</li><li>当使用交叉熵损失 (<code>CrossEntropyLoss</code>) 类时：<ul><li>该类已内置 <code>sigmoid</code> 功能。</li><li>从 <code>forward</code> 函数的输出层中去除 <code>sigmoid</code>。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">X = torch.rand((<span class="number">500</span>,<span class="number">20</span>),dtype=torch.float32) * <span class="number">100</span></span><br><span class="line">y = torch.randint(low=<span class="number">0</span>,high=<span class="number">3</span>,size=(<span class="number">500</span>,),dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_features=<span class="number">10</span>,out_features=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Model,self).__init__() <span class="comment">#super(请查找这个类的父类，请使用找到的父类替换现在的类)</span></span><br><span class="line">        self.linear1 = nn.Linear(in_features,<span class="number">13</span>,bias=<span class="literal">True</span>) <span class="comment">#输入层不用写，这里是隐藏层的第一层</span></span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">13</span>,<span class="number">8</span>,bias=<span class="literal">True</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">8</span>,out_features,bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        z1 = self.linear1(x)</span><br><span class="line">        sigma1 = torch.relu(z1)</span><br><span class="line">        z2 = self.linear2(sigma1)</span><br><span class="line">        sigma2 = torch.sigmoid(z2)</span><br><span class="line">        z3 = self.output(sigma2)</span><br><span class="line">        <span class="comment">#sigma3 = F.softmax(z3,dim=1)</span></span><br><span class="line">        <span class="keyword">return</span> z3</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算数据大小</span></span><br><span class="line">input_ = X.shape[<span class="number">1</span>] <span class="comment">#特征的数目</span></span><br><span class="line">output_ = <span class="built_in">len</span>(y.unique()) <span class="comment">#分类的数目</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#实例化神经网络类</span></span><br><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">net = Model(in_features=input_, out_features=output_)</span><br><span class="line"></span><br><span class="line"><span class="comment">#前向传播</span></span><br><span class="line">zhat = net.forward(X)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义损失函数</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment">#对打包好的CorssEnrtopyLoss而言，只需要输入zhat</span></span><br><span class="line">loss = criterion(zhat,y.long())</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br><span class="line">loss.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#net.linear1.weight.grad 可打印梯度</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor(1.1057, grad_fn=&lt;NllLossBackward0&gt;)</code></pre><h3 id="4-移动坐标点"><a href="#4-移动坐标点" class="headerlink" title="4.移动坐标点"></a>4.移动坐标点</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">dw = net.linear1.weight.grad</span><br><span class="line">w = net.linear1.weight.data</span><br><span class="line"></span><br><span class="line"><span class="comment">#对任意w可以有</span></span><br><span class="line">w-=lr*dw</span><br></pre></td></tr></table></figure><h3 id="5-动量法Momentum"><a href="#5-动量法Momentum" class="headerlink" title="5.动量法Momentum"></a>5.动量法Momentum</h3><script type="math/tex; mode=display">v(t) = \gamma v(t-1) - \eta \frac{\partial L}{\partial w}</script><script type="math/tex; mode=display">w(t+1) = w(t) + v(t)</script><script type="math/tex; mode=display">\begin{align*}t &= 0 \quad v(0) = 0 \\w(1) \\t &= 1 \quad v(1) = r v(0) - (\eta \frac{\partial L}{\partial w})_0 \\w(2) &= w(1) + v(1) = w(1) - (\eta \frac{\partial L}{\partial w})_0 \\t &= 2 \quad v(2) = r v(1) - (\eta \frac{\partial L}{\partial w})_1 = - (\eta \frac{\partial L}{\partial w})_1 - r(\eta \frac{\partial L}{\partial w})_0 \\w(3) &= w(2) + v(2) = w(2) - (\eta \frac{\partial L}{\partial w})_1 - r(\eta \frac{\partial L}{\partial w})_0 \\t &= 3 \quad v(3) = r v(2) - (\eta \frac{\partial L}{\partial w})_2 = - (\eta \frac{\partial L}{\partial w})_2 - r(\eta \frac{\partial L}{\partial w})_1 - r^2(\eta \frac{\partial L}{\partial w})_0 \\w(4) &= w(3) + v(3) = w(3) - (\eta \frac{\partial L}{\partial w})_2 - r(\eta \frac{\partial L}{\partial w})_1 - r^2(\eta \frac{\partial L}{\partial w})_0 \\\end{align*}</script><ul><li>梯度<code>下降的方向</code>有了<code>惯性</code></li></ul><h4 id="tensor实现动量法"><a href="#tensor实现动量法" class="headerlink" title="tensor实现动量法"></a>tensor实现动量法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#恢复小步长</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line"></span><br><span class="line">dw = net.linear1.weight.grad</span><br><span class="line">w = net.linear1.weight.data</span><br><span class="line">v = torch.zeros(dw.shape[<span class="number">0</span>],dw.shape[<span class="number">1</span>]) <span class="comment"># v和w的形状相同</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#==========分割cell，不然重复运行的时候w会每次都被覆盖掉=============</span></span><br><span class="line"><span class="comment">#对任意w可以有</span></span><br><span class="line">v = gamma * v - lr * dw</span><br><span class="line">w += v</span><br><span class="line"></span><br><span class="line"><span class="comment">#不难发现，当加入gamma之后，即便是较小的步长，也可以让w发生变化</span></span><br></pre></td></tr></table></figure><h4 id="torch-optim实现动量法"><a href="#torch-optim实现动量法" class="headerlink" title="torch.optim实现动量法"></a>torch.optim<code>实现</code>动量法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#确定数据、确定优先需要设置的值</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">X = torch.rand((<span class="number">500</span>,<span class="number">20</span>),dtype=torch.float32) * <span class="number">100</span></span><br><span class="line">y = torch.randint(low=<span class="number">0</span>,high=<span class="number">3</span>,size=(<span class="number">500</span>,<span class="number">1</span>),dtype=torch.float32)</span><br><span class="line">input_ = X.shape[<span class="number">1</span>] <span class="comment">#特征的数目</span></span><br><span class="line">output_ = <span class="built_in">len</span>(y.unique()) <span class="comment">#分类的数目</span></span><br><span class="line"><span class="comment">#定义神经网路的架构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_features=<span class="number">10</span>,out_features=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Model,self).__init__() <span class="comment">#super(请查找这个类的父类，请使用找到的父类替换现在的类)</span></span><br><span class="line">        self.linear1 = nn.Linear(in_features,<span class="number">13</span>,bias=<span class="literal">True</span>) <span class="comment">#输入层不用写，这里是隐藏层的第一层</span></span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">13</span>,<span class="number">8</span>,bias=<span class="literal">True</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">8</span>,out_features,bias=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        z1 = self.linear1(x)</span><br><span class="line">        sigma1 = torch.relu(z1)</span><br><span class="line">        z2 = self.linear2(sigma1)</span><br><span class="line">        sigma2 = torch.sigmoid(z2)</span><br><span class="line">        z3 = self.output(sigma2)</span><br><span class="line">        <span class="comment">#sigma3 = F.softmax(z3,dim=1)</span></span><br><span class="line">        <span class="keyword">return</span> z3</span><br><span class="line"><span class="comment">#实例化神经网络，调用优化算法需要的参数</span></span><br><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">net = Model(in_features=input_, out_features=output_)</span><br><span class="line">net.parameters()</span><br><span class="line">    </span><br><span class="line"><span class="comment">#定义损失函数</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment">#定义优化算法</span></span><br><span class="line">opt = optim.SGD(net.parameters() <span class="comment">#要优化的参数是哪些？</span></span><br><span class="line">               , lr=lr <span class="comment">#学习率</span></span><br><span class="line">               , momentum = gamma <span class="comment">#动量参数</span></span><br><span class="line">               )</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="神经网络迭代过程简述"><a href="#神经网络迭代过程简述" class="headerlink" title="神经网络迭代过程简述"></a>神经网络迭代过程简述</h4><ul><li><p><strong>迭代流程</strong>包括以下步骤：</p><ol><li><strong>正向传播</strong>：计算网络的输出。</li><li><strong>计算损失</strong>：根据输出和真实标签计算损失函数。</li><li><strong>反向传播</strong>：通过网络传递损失的梯度。</li><li><strong>更新权重 (w)</strong>：根据梯度调整网络参数。</li><li><strong>清除梯度</strong>：为下一次迭代准备。</li></ol></li><li><p>总结：<strong>“正损反更清”</strong>，即正向传播、计算损失、反向传播、更新权重和清除梯度。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">zhat = net.forward(X) <span class="comment">#向前传播</span></span><br><span class="line">loss = criterion(zhat,y.reshape(<span class="number">500</span>).long()) <span class="comment">#损失函数值</span></span><br><span class="line">loss.backward(retain_graph=<span class="literal">True</span>) <span class="comment">#反向传播</span></span><br><span class="line">opt.step() <span class="comment">#更新权重w，从这一瞬间开始，坐标点就发生了变化，所有的梯度必须重新计算</span></span><br><span class="line">opt.zero_grad() <span class="comment">#清除原来储存好的，基于上一个坐标点计算的梯度，为下一次计算梯度腾出空间</span></span><br><span class="line"><span class="built_in">print</span>(loss)</span><br><span class="line"><span class="built_in">print</span>(net.linear1.weight.data[<span class="number">0</span>][:<span class="number">10</span>])</span><br><span class="line"><span class="comment"># 这样每执行依次loss变小</span></span><br></pre></td></tr></table></figure><pre><code>tensor(1.1057, grad_fn=&lt;NllLossBackward0&gt;)tensor([ 0.1365, -0.1346,  0.2128, -0.1776, -0.0682, -0.1541,  0.1724,  0.0839,        -0.1115, -0.1729])</code></pre><h3 id="6-Batch-Size-与-Epoches-在迭代中的作用"><a href="#6-Batch-Size-与-Epoches-在迭代中的作用" class="headerlink" title="6.Batch Size 与 Epoches 在迭代中的作用"></a>6.Batch Size 与 Epoches 在迭代中的作用</h3><ul><li><p><strong>小批量梯度下降的必要性</strong></p><ul><li><strong>正向传播</strong>：大量数据在正向传播中导致计算速度缓慢。</li><li><strong>反向传播</strong>：采用动量法加速，但若数据量大，仍影响效率。</li><li><strong>Mini-batch</strong>：更易找到全局最小值，提升训练效率。</li></ul></li><li><p><strong>不同梯度下降方法的对比</strong></p><ul><li><strong>传统梯度下降</strong>：相同数据集，仅小范围内权重更新。快速接近最小值，但易陷入局部最优。</li><li><strong>随机梯度下降</strong>：每次一个样本，高随机性，计算不稳定。</li><li><strong>Mini-batch SGD</strong>：结合两者优点，数据集和权重均有变化，路径曲折但有效。</li></ul></li><li><p><strong>优势</strong></p><ul><li>减少计算开销，提高训练效率。</li><li>在寻找最优解过程中，避免局部最优的困扰。</li></ul></li></ul><p><img src="/img/SGD.png" alt="ddd"></p><h3 id="7-使用-TensorDataset-与-DataLoader-处理数据"><a href="#7-使用-TensorDataset-与-DataLoader-处理数据" class="headerlink" title="7. 使用 TensorDataset 与 DataLoader 处理数据"></a>7. 使用 TensorDataset 与 DataLoader 处理数据</h3><ul><li><p><strong>TensorDataset 用于数据打包</strong></p><ul><li>功能：将数据打包成元组格式。</li><li>要求：数据的首维度大小需保持一致。</li><li>使用示例：通过 <code>td[0]</code> 访问数据。</li></ul></li><li><p><strong>DataLoader 用于数据分批处理</strong></p><ul><li>功能：切分数据为小批量，便于训练。</li><li>特点：支持任意形式的张量或数组。</li><li>主要参数：<ul><li><code>dataset</code>：包含所有数据的列表，通过 <code>dl.dataset[0]</code> 访问。</li><li><code>batch_size</code>（简写为 <code>bs</code>）：定义每批数据的大小。</li><li>通过 <code>len(dl)</code> 计算得到总批次（即 <code>m/bs</code>）。</li></ul></li></ul></li><li><p><strong>特别说明</strong></p><ul><li>若数据集已经以特征张量和标签的形式组合在一起，可跳过此步骤。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">TensorDataset:打包操作</span></span><br><span class="line"><span class="string">DataLoader：切割操作</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.randn(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">c = torch.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> TensorDataset(a,b,c):<span class="comment"># 元组</span></span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line"><span class="keyword">for</span> x,y,z <span class="keyword">in</span> DataLoader(TensorDataset(a,b,c),batch_size=<span class="number">2</span>,drop_last=<span class="literal">True</span>): <span class="comment"># 列表且升维</span></span><br><span class="line">    <span class="built_in">print</span>(x)</span><br><span class="line">    <span class="built_in">print</span>(x.shape)</span><br><span class="line">    <span class="built_in">print</span>(y.shape)</span><br><span class="line">    <span class="built_in">print</span>(z.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorDataset是元组！</span></span><br><span class="line"><span class="comment"># DataLoader转换为列表！</span></span><br><span class="line"><span class="comment"># batch_size是在每一个列表上升维！</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># c = torch.randn(300,1)</span></span><br><span class="line"><span class="comment"># TensorDataset(a,c)[0] </span></span><br><span class="line"><span class="comment"># 报错</span></span><br></pre></td></tr></table></figure><pre><code>(tensor([[-0.3513, -0.3595,  0.9324],        [-1.1048, -0.8469,  1.4757]]), tensor([[ 0.4467,  1.2014, -0.5548,  0.7919],        [ 0.1874, -0.7003,  0.4888, -0.8637]]), tensor([0.1421]))(tensor([[ 1.8659, -1.3644, -0.3450],        [ 1.7672,  1.4598, -0.3842]]), tensor([[-0.4296, -0.5561, -1.4006,  0.3309],        [-0.6369, -0.2273, -0.6020, -0.3844]]), tensor([-0.1834]))tensor([[[-0.3513, -0.3595,  0.9324],         [-1.1048, -0.8469,  1.4757]],        [[ 1.8659, -1.3644, -0.3450],         [ 1.7672,  1.4598, -0.3842]]])torch.Size([2, 2, 3])torch.Size([2, 2, 4])torch.Size([2, 1])</code></pre><ul><li>对于mini-batch SGD,我们一般这样使用TensorDataset和DataLoader</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">X = torch.rand((<span class="number">50000</span>,<span class="number">20</span>),dtype=torch.float32) * <span class="number">100</span> <span class="comment">#要进行迭代了，增加样本数量</span></span><br><span class="line">y = torch.randint(low=<span class="number">0</span>,high=<span class="number">3</span>,size=(<span class="number">50000</span>,<span class="number">1</span>),dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">4</span></span><br><span class="line">bs = <span class="number">4000</span></span><br><span class="line"></span><br><span class="line">data = TensorDataset(X,y)</span><br><span class="line">batchdata = DataLoader(data, batch_size=bs, shuffle = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">len</span>(batchdata) <span class="comment">#查看具体被分了多少个batch 13</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#可以使用.datasets查看数据集相关的属性</span></span><br><span class="line"><span class="built_in">print</span>(batchdata.dataset[<span class="number">49999</span>]) <span class="comment">#查看其中一个样本 列表[tensor([500,20]),tensor([1])]</span></span><br><span class="line"><span class="built_in">print</span>(batchdata.batch_size) <span class="comment">#查看其中一个样本 列表[tensor([500,20]),tensor([1])]</span></span><br><span class="line"><span class="keyword">for</span> batch_idx, (x,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(batchdata):</span><br><span class="line">    <span class="comment">#sigma = net(x)</span></span><br><span class="line">    <span class="comment">#loss = lossfn(sigma, y)</span></span><br><span class="line">    <span class="comment">#loss.backward()</span></span><br><span class="line">    <span class="comment">#opt.step()</span></span><br><span class="line">    <span class="comment">#opt.zero_grad()</span></span><br><span class="line">    <span class="built_in">print</span>(batch_idx)</span><br><span class="line">    <span class="built_in">print</span>(x.shape)</span><br><span class="line">    <span class="built_in">print</span>(y.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># print(x,y)</span></span><br><span class="line">    <span class="keyword">if</span> batch_idx == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">break</span> <span class="comment">#为了演示用，所以打断，在正常的循环里是不会打断的</span></span><br></pre></td></tr></table></figure><pre><code>(tensor([61.8071, 94.8606, 77.5624,  4.6598, 50.3617, 10.5395, 37.8427, 88.2321,        55.0742, 11.2930, 35.5145, 37.8230, 83.2974, 16.3466, 91.3999, 76.7965,        71.2780, 36.9534,  1.4939, 31.9203]), tensor([1.]))40000torch.Size([4000, 20])torch.Size([4000, 1])1torch.Size([4000, 20])torch.Size([4000, 1])</code></pre><h3 id="8-在Mini-Fashion上实现神经网络的学习流程"><a href="#8-在Mini-Fashion上实现神经网络的学习流程" class="headerlink" title="8.在Mini-Fashion上实现神经网络的学习流程"></a>8.在Mini-Fashion上实现神经网络的学习流程</h3><ul><li>在<code>Linux/UNIX</code>中,推荐使用<code>/</code>,<code>Python解释器</code>也推荐使用<code>/</code></li><li>在Windows中,推荐使用\</li><li>以下三个表示等价<ul><li>root=<code>&#39;../lesson 11/&#39;</code></li><li>root=<code>&#39;..\\lesson 11\\&#39;</code></li><li>root=<code>r&#39;..\lesson 11\\&#39;</code></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入库，设置各种初始值</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment">#确定数据、确定优先需要设置的值</span></span><br><span class="line">lr = <span class="number">0.15</span></span><br><span class="line">gamma = <span class="number">0.8</span></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line">bs = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#导入数据</span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line">mnist = torchvision.datasets.FashionMNIST(root=<span class="string">&#x27;../lesson 11/&#x27;</span>,</span><br><span class="line">                                          train=<span class="literal">True</span>,</span><br><span class="line">                                          download=<span class="literal">True</span>,</span><br><span class="line">                                          transform=transforms.ToTensor())</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1. 将一个 PIL 图片或者一个 NumPy ndarray 转换为 FloatTensor。</span></span><br><span class="line"><span class="string">2. 把图片的像素值从 [0, 255] 范围线性缩放到 [0.0, 1.0] 范围。即原先的整数类型像素值被缩放到浮点数，并且归一化到0到1之间的范围。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 这里mnist是一个Dataset的对象</span></span><br></pre></td></tr></table></figure><pre><code>&#39;\n1. 将一个 PIL 图片或者一个 NumPy ndarray 转换为 FloatTensor。\n2. 把图片的像素值从 [0, 255] 范围线性缩放到 [0.0, 1.0] 范围。即原先的整数类型像素值被缩放到浮点数，并且归一化到0到1之间的范围。\n&#39;</code></pre><ul><li><code>mnist</code> 数据集是 <code>Dataset</code> 类的一个实例，主要包含以下特点：<ul><li>数据结构：每个元素是一个元组，包含特征和标签。</li><li><code>data</code> 属性：特征张量的形状为 <code>torch.Size([60000, 28, 28])</code>，表示有 60000 个样本，每个样本为 28x28 的图像。</li><li><code>targets</code> 属性：标签张量的形状为 <code>torch.Size([60000])</code>，表示有 60000 个标签。</li><li><code>classes</code> 属性：类别列表，包括 <code>[&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;]</code>，共 10 类。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看特征张量</span></span><br><span class="line"><span class="built_in">print</span>(mnist.data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看标签</span></span><br><span class="line"><span class="built_in">print</span>(mnist.targets.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看标签的类别</span></span><br><span class="line"><span class="built_in">print</span>(mnist.classes)</span><br></pre></td></tr></table></figure><pre><code>torch.Size([60000, 28, 28])torch.Size([60000])[&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">TensorDataset:打包操作</span></span><br><span class="line"><span class="string">DataLoader：切割操作</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.randn(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">c = torch.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> TensorDataset(a,b,c):<span class="comment"># 元组</span></span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> DataLoader(TensorDataset(a,b,c),batch_size=<span class="number">2</span>,drop_last=<span class="literal">True</span>): <span class="comment"># 列表且升维</span></span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorDataset是元组！</span></span><br><span class="line"><span class="comment"># DataLoader转换为列表！</span></span><br><span class="line"><span class="comment"># batch_size是在每一个列表上升维！</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># c = torch.randn(300,1)</span></span><br><span class="line"><span class="comment"># TensorDataset(a,c)[0] </span></span><br><span class="line"><span class="comment"># 报错</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>(tensor([[ 0.1381, -1.2224,  2.7075],        [ 0.3480, -0.5253, -0.4556]]), tensor([[-1.7098,  0.9470,  0.4756, -0.6225],        [ 0.6651,  0.4215,  1.4153, -0.8847]]), tensor([0.0968]))(tensor([[-0.4337,  0.3605,  0.8836],        [-0.3691,  1.5067,  0.3661]]), tensor([[-0.9452,  0.7593,  1.0918,  0.7972],        [-0.8260,  0.8221, -0.0790,  0.8462]]), tensor([-1.4553]))[tensor([[[ 0.1381, -1.2224,  2.7075],         [ 0.3480, -0.5253, -0.4556]],        [[-0.4337,  0.3605,  0.8836],         [-0.3691,  1.5067,  0.3661]]]), tensor([[[-1.7098,  0.9470,  0.4756, -0.6225],         [ 0.6651,  0.4215,  1.4153, -0.8847]],        [[-0.9452,  0.7593,  1.0918,  0.7972],         [-0.8260,  0.8221, -0.0790,  0.8462]]]), tensor([[ 0.0968],        [-1.4553]])]</code></pre><ul><li>查看图像的模样</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.imshow(mnist[<span class="number">1</span>][<span class="number">0</span>].view((<span class="number">28</span>, <span class="number">28</span>)).numpy());</span><br><span class="line"><span class="comment"># mnist[1][0]表示第一个元组的特征张量(1,28,28) </span></span><br></pre></td></tr></table></figure><p><img src="/img/shirt.png" alt="png"></p><ul><li>分割batch</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">batchdata = DataLoader(mnist,batch_size=bs, shuffle = <span class="literal">True</span>)</span><br><span class="line"><span class="comment">#总共多少个batch?</span></span><br><span class="line"><span class="built_in">len</span>(batchdata) <span class="comment"># 60000/128=469(ceil)</span></span><br><span class="line"><span class="comment">#查看会放入进行迭代的数据结构</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> batchdata:</span><br><span class="line">    <span class="built_in">print</span>(x.shape)  <span class="comment"># torch.Size([128, 1, 28, 28])</span></span><br><span class="line">    <span class="built_in">print</span>(y.shape)  <span class="comment"># torch.Size([128])</span></span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">input_ = mnist.data[<span class="number">0</span>].numel() <span class="comment">#特征的数目，一般是第一维之外的所有维度相乘的数</span></span><br><span class="line"><span class="comment"># mnist[0][0].numel()</span></span><br><span class="line"><span class="comment">#.data相当于第二个[0]把特征给跑去了</span></span><br><span class="line"><span class="comment">#[0]相当于第一个[0]即第一个元组</span></span><br><span class="line">output_ = <span class="built_in">len</span>(mnist.targets.unique()) <span class="comment">#分类的数目</span></span><br></pre></td></tr></table></figure><pre><code>torch.Size([128, 1, 28, 28])torch.Size([128])</code></pre><ul><li>定义神经网络架构</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_features=<span class="number">10</span>,out_features=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        <span class="comment"># self.normalize = nn.BatchNorm2d(num_features=1)</span></span><br><span class="line">        self.linear1 = nn.Linear(in_features,<span class="number">128</span>,bias=<span class="literal">False</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">128</span>,out_features,bias=<span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x = self.normalize(x)</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">        <span class="comment">#需要对数据的结构进行一个改变，这里的“-1”代表，我不想算，请pytorch帮我计算</span></span><br><span class="line">        sigma1 = torch.relu(self.linear1(x))</span><br><span class="line">        z2 = self.output(sigma1)</span><br><span class="line">        sigma2 = F.log_softmax(z2,dim=<span class="number">1</span>)  <span class="comment"># 为了得准确率</span></span><br><span class="line">        <span class="keyword">return</span> sigma2</span><br></pre></td></tr></table></figure><ul><li>定义训练函数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">net,batchdata,lr=<span class="number">0.01</span>,epochs=<span class="number">5</span>,gamma=<span class="number">0</span></span>):</span><br><span class="line">    criterion = nn.NLLLoss() <span class="comment">#定义损失函数</span></span><br><span class="line">    opt = optim.SGD(net.parameters(), lr=lr,momentum=gamma) <span class="comment">#定义优化算法</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    samples = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> batch_idx, (x,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(batchdata):</span><br><span class="line">            y = y.view(x.shape[<span class="number">0</span>])</span><br><span class="line">            sigma = net.forward(x)  <span class="comment"># 正</span></span><br><span class="line">            loss = criterion(sigma,y)  <span class="comment"># 损</span></span><br><span class="line">            loss.backward()  <span class="comment"># 反</span></span><br><span class="line">            opt.step()  <span class="comment"># 更</span></span><br><span class="line">            opt.zero_grad()  <span class="comment"># 清</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">#求解准确率</span></span><br><span class="line">            yhat = torch.<span class="built_in">max</span>(sigma,dim=<span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># 取下标即预测标签</span></span><br><span class="line">            correct += torch.<span class="built_in">sum</span>(yhat == y)</span><br><span class="line">            samples += x.shape[<span class="number">0</span>]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> (batch_idx+<span class="number">1</span>) % <span class="number">125</span> == <span class="number">0</span> <span class="keyword">or</span> batch_idx == <span class="built_in">len</span>(batchdata)-<span class="number">1</span>:</span><br><span class="line">                <span class="comment"># 468没被125整除,,要让m/m等于1加上batch_idx == len(batchdata)-1</span></span><br><span class="line">                <span class="comment"># 125 250 375 468故每个epoch打印四次</span></span><br><span class="line">                <span class="comment"># 最后一下没够125，所以没打印100%，但是确实是训练了，只是没够125</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Epoch&#123;&#125;:[&#123;&#125;/&#123;&#125;(&#123;:.0f&#125;%)]\tLoss:&#123;:.6f&#125;\t Accuracy:&#123;:.3f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                    epoch+<span class="number">1</span></span><br><span class="line">                   ,samples</span><br><span class="line">                   ,<span class="built_in">len</span>(batchdata.dataset)*epochs</span><br><span class="line">                   ,<span class="number">100</span>*samples/(<span class="built_in">len</span>(batchdata.dataset)*epochs)</span><br><span class="line">                   ,loss.data.item()</span><br><span class="line">                   ,<span class="built_in">float</span>(correct*<span class="number">100</span>)/samples))</span><br></pre></td></tr></table></figure><ul><li>进行训练和评估</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#实例化神经网络，调用优化算法需要的参数</span></span><br><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">net = Model(in_features=input_, out_features=output_)</span><br><span class="line">fit(net,batchdata,lr=lr,epochs=epochs,gamma=gamma)</span><br></pre></td></tr></table></figure><pre><code>Epoch1:[16000/600000(3%)]    Loss:0.445854     Accuracy:71.844Epoch1:[32000/600000(5%)]    Loss:0.541044     Accuracy:76.766Epoch1:[48000/600000(8%)]    Loss:0.420771     Accuracy:78.817Epoch1:[60000/600000(10%)]    Loss:0.456890     Accuracy:79.743Epoch2:[76000/600000(13%)]    Loss:0.520517     Accuracy:80.801Epoch2:[92000/600000(15%)]    Loss:0.335469     Accuracy:81.486Epoch2:[108000/600000(18%)]    Loss:0.390427     Accuracy:82.020Epoch2:[120000/600000(20%)]    Loss:0.464855     Accuracy:82.355Epoch3:[136000/600000(23%)]    Loss:0.382425     Accuracy:82.770Epoch3:[152000/600000(25%)]    Loss:0.388381     Accuracy:83.201Epoch3:[168000/600000(28%)]    Loss:0.456149     Accuracy:83.518Epoch3:[180000/600000(30%)]    Loss:0.336247     Accuracy:83.714Epoch4:[196000/600000(33%)]    Loss:0.400932     Accuracy:84.001Epoch4:[212000/600000(35%)]    Loss:0.280075     Accuracy:84.251Epoch4:[228000/600000(38%)]    Loss:0.329395     Accuracy:84.461Epoch4:[240000/600000(40%)]    Loss:0.368081     Accuracy:84.579Epoch5:[256000/600000(43%)]    Loss:0.491687     Accuracy:84.777Epoch5:[272000/600000(45%)]    Loss:0.201454     Accuracy:84.953Epoch5:[288000/600000(48%)]    Loss:0.199246     Accuracy:85.079Epoch5:[300000/600000(50%)]    Loss:0.222263     Accuracy:85.179Epoch6:[316000/600000(53%)]    Loss:0.357817     Accuracy:85.341Epoch6:[332000/600000(55%)]    Loss:0.386327     Accuracy:85.465Epoch6:[348000/600000(58%)]    Loss:0.365660     Accuracy:85.593Epoch6:[360000/600000(60%)]    Loss:0.302036     Accuracy:85.690Epoch7:[376000/600000(63%)]    Loss:0.270010     Accuracy:85.834Epoch7:[392000/600000(65%)]    Loss:0.289248     Accuracy:85.949Epoch7:[408000/600000(68%)]    Loss:0.262059     Accuracy:86.047Epoch7:[420000/600000(70%)]    Loss:0.302930     Accuracy:86.116Epoch8:[436000/600000(73%)]    Loss:0.255372     Accuracy:86.247Epoch8:[452000/600000(75%)]    Loss:0.323171     Accuracy:86.354Epoch8:[468000/600000(78%)]    Loss:0.223748     Accuracy:86.437Epoch8:[480000/600000(80%)]    Loss:0.403298     Accuracy:86.499Epoch9:[496000/600000(83%)]    Loss:0.297387     Accuracy:86.578Epoch9:[512000/600000(85%)]    Loss:0.356909     Accuracy:86.666Epoch9:[528000/600000(88%)]    Loss:0.306189     Accuracy:86.757Epoch9:[540000/600000(90%)]    Loss:0.389048     Accuracy:86.804Epoch10:[556000/600000(93%)]    Loss:0.337781     Accuracy:86.882Epoch10:[572000/600000(95%)]    Loss:0.256883     Accuracy:86.969Epoch10:[588000/600000(98%)]    Loss:0.188146     Accuracy:87.044Epoch10:[600000/600000(100%)]    Loss:0.293160     Accuracy:87.087</code></pre><h3 id="9-数据处理总结"><a href="#9-数据处理总结" class="headerlink" title="9. 数据处理总结"></a>9. 数据处理总结</h3><h4 id="TensorDataset-的使用"><a href="#TensorDataset-的使用" class="headerlink" title="TensorDataset 的使用"></a>TensorDataset 的使用</h4><ul><li>实例化：<code>td = TensorDataset(a, b, c)</code>，形成元组 <code>(tensor(), tensor(), tensor())</code>。</li><li>访问元素：使用 <code>td[0]</code> 可访问每个元组；适用于 <code>for</code> 循环遍历。</li><li>图片数据处理：可通过 <code>td.data</code>、<code>td.classes</code>、<code>td.targets</code> 进行访问。</li></ul><h4 id="DataLoader-的应用"><a href="#DataLoader-的应用" class="headerlink" title="DataLoader 的应用"></a>DataLoader 的应用</h4><ul><li>实例化：<code>dl = DataLoader(td, batch_size)</code>，形成列表。</li><li>遍历数据：<ul><li>使用 <code>for x, y in dl: print(i)</code> 遍历；列表格式 <code>[tensor(), tensor(), tensor()]</code>。</li><li>对于图片数据，<code>x.shape</code> 可能为 <code>[batch_size, 1, 28, 28]</code> 或 <code>[batch_size, 28, 28]</code>。</li></ul></li><li>注意事项：<ul><li>不能直接使用 <code>dl[0]</code> 访问元素。</li><li><code>dl.dataset</code> 包含所有数据列表；<code>len(dl)</code> 表示总批次数（即 <code>m/batch_size</code>）；<code>dl.batch_size</code> 表示每批数据的大小。</li><li><code>batch_size</code> 增加了数据列表的一个维度。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(5)-训练损失</title>
      <link href="/Pytorch(5)/"/>
      <url>/Pytorch(5)/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br></pre></td></tr></table></figure><h3 id="1-训练目标与流程"><a href="#1-训练目标与流程" class="headerlink" title="1. 训练目标与流程"></a>1. 训练目标与流程</h3><div class="table-container"><table><thead><tr><th>步骤</th><th>描述</th></tr></thead><tbody><tr><td><strong>目标</strong></td><td>找到参数 <code>w</code> 使预测与真实值最接近。</td></tr><tr><td><strong>流程</strong></td><td>模型 (Model) + 损失 (Loss) + 优化器 (Optim) ⇒ 求解 <code>w</code></td></tr><tr><td><strong>损失</strong></td><td><code>L(w)</code></td></tr><tr><td><strong>数学工具</strong></td><td>- 凸函数：拉格朗日变换<br>- 最小化 <code>L(w)</code> 时的 <code>w</code>：梯度下降法</td></tr></tbody></table></div><h3 id="2-回归与误差分析"><a href="#2-回归与误差分析" class="headerlink" title="2. 回归与误差分析"></a>2. 回归与误差分析</h3><div class="table-container"><table><thead><tr><th>类型</th><th>描述</th></tr></thead><tbody><tr><td><strong>回归</strong></td><td>误差平方和SSE</td></tr><tr><td><strong>MSE</strong></td><td><code>MSE = SSE/m</code></td></tr><tr><td><strong>顺序</strong></td><td><code>(yhat, y)</code> 的顺序</td></tr><tr><td><strong>reduction 参数</strong></td><td>sum/mean(default)/None</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> MSELoss</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">yhat = torch.randn(size=(<span class="number">50</span>,),dtype=torch.float32)</span><br><span class="line">y = torch.randn(size=(<span class="number">50</span>,),dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">criterion = MSELoss(reduction = <span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">loss = criterion(yhat,y)</span><br><span class="line">loss</span><br></pre></td></tr></table></figure><pre><code>tensor(124.3458)</code></pre><h3 id="3-分类：二分类交叉熵损失函数-对数函数"><a href="#3-分类：二分类交叉熵损失函数-对数函数" class="headerlink" title="3.分类：二分类交叉熵损失函数(对数函数)"></a>3.分类：<code>二分类</code>交叉熵损失函数(对数函数)</h3><ul><li>除非特别声明，不然提到<code>交叉熵损失均为多分类</code></li><li><code>步骤</code>:似然函数、对数似然函数、取负值</li></ul><p><img src="/img/BCE.png" alt="image.png"></p><h4 id="tensor实现二分类交叉熵损失函数"><a href="#tensor实现二分类交叉熵损失函数" class="headerlink" title="tensor实现二分类交叉熵损失函数"></a>tensor实现二分类交叉熵损失函数</h4><ul><li>除了<code>普通的加减乘除</code>，其余<code>均用 torch</code> 运算，<code>比 python 快！</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">m = <span class="number">3</span>*<span class="built_in">pow</span>(<span class="number">10</span>,<span class="number">3</span>)</span><br><span class="line">torch.random.manual_seed(<span class="number">420</span>)</span><br><span class="line">X = torch.rand((m,<span class="number">4</span>),dtype=torch.float32)</span><br><span class="line">w = torch.rand((<span class="number">4</span>,<span class="number">1</span>),dtype=torch.float32)</span><br><span class="line">y = torch.randint(low=<span class="number">0</span>,high=<span class="number">2</span>,size=(m,<span class="number">1</span>),dtype=torch.float32)</span><br><span class="line">zhat = torch.mm(X,w)</span><br><span class="line">sigma = torch.sigmoid(zhat)</span><br><span class="line">loss = -(<span class="number">1</span>/m)*<span class="built_in">sum</span>(y*torch.log(sigma) + (<span class="number">1</span>-y)*torch.log(<span class="number">1</span>-sigma))</span><br><span class="line">loss</span><br></pre></td></tr></table></figure><pre><code>tensor([0.7962])</code></pre><h4 id="torch实现二分类交叉熵损失函数"><a href="#torch实现二分类交叉熵损失函数" class="headerlink" title="torch实现二分类交叉熵损失函数"></a>torch实现二分类交叉熵损失函数</h4><h4 id="nn-模块中的损失类别"><a href="#nn-模块中的损失类别" class="headerlink" title="nn 模块中的损失类别"></a>nn 模块中的损失类别</h4><div class="table-container"><table><thead><tr><th>类别</th><th>描述</th></tr></thead><tbody><tr><td><code>BCEWithLogitsLoss</code></td><td>- 输入：<code>zhat</code>, <code>y</code><br> - 目的：缩小精度误差</td></tr><tr><td><code>BCELoss</code></td><td>- 输入：<code>sigma</code>, <code>y</code><br> - 目的：监控准确率</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">criterion2 = nn.BCEWithLogitsLoss(reduction = <span class="string">&quot;mean&quot;</span>)</span><br><span class="line">loss = criterion2(zhat, y)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br><span class="line">criterion = nn.BCELoss(reduction = <span class="string">&quot;mean&quot;</span>) <span class="comment">#实例化</span></span><br><span class="line">loss = criterion(sigma,y)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br></pre></td></tr></table></figure><pre><code>tensor(0.7962)tensor(0.7962)</code></pre><h3 id="4-分类任务中的多分类交叉熵损失函数"><a href="#4-分类任务中的多分类交叉熵损失函数" class="headerlink" title="4. 分类任务中的多分类交叉熵损失函数"></a>4. 分类任务中的多分类交叉熵损失函数</h3><p>在 PyTorch 中处理多分类问题时，常用的方法是结合 <code>nn.LogSoftmax(dim=1)</code> 和 <code>nn.NLLLoss()</code> 来计算交叉熵损失。</p><ul><li><strong>关键步骤</strong>:<ol><li><strong>LogSoftmax 应用</strong>:<ul><li>应用于模型输出，使用 <code>nn.LogSoftmax(dim=1)</code>。</li><li><code>dim=1</code> 确保沿着正确的维度（特征维度）应用 Softmax。</li></ul></li><li><strong>损失计算</strong>:<ul><li>使用 <code>nn.NLLLoss()</code> 计算损失。</li><li>确保标签张量 <code>y</code> 使用 <code>.long()</code> 进行类型转换，以匹配损失函数的要求。</li></ul></li></ol></li></ul><h4 id="PyTorch-nn-LogSoftmax-dim-1-和nn-NLLLoss"><a href="#PyTorch-nn-LogSoftmax-dim-1-和nn-NLLLoss" class="headerlink" title="PyTorch(nn.LogSoftmax(dim=1)和nn.NLLLoss())"></a>PyTorch(<code>nn.LogSoftmax(dim=1)</code>和<code>nn.NLLLoss()</code>)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">N = <span class="number">3</span>*<span class="built_in">pow</span>(<span class="number">10</span>,<span class="number">2</span>) </span><br><span class="line">torch.random.manual_seed(<span class="number">420</span>) </span><br><span class="line">X = torch.rand((N,<span class="number">4</span>),dtype=torch.float32) </span><br><span class="line">w = torch.rand((<span class="number">4</span>,<span class="number">3</span>),dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = torch.randint(low=<span class="number">0</span>,high=<span class="number">3</span>,size=(N,),dtype=torch.float32)</span><br><span class="line">zhat = torch.mm(X,w)</span><br><span class="line"><span class="comment">#从这里开始调用 softmax 和 NLLLoss</span></span><br><span class="line">logsm = nn.LogSoftmax(dim=<span class="number">1</span>) <span class="comment">#实例化</span></span><br><span class="line">logsigma = logsm(zhat)</span><br><span class="line">criterion = nn.NLLLoss() <span class="comment">#实例化</span></span><br><span class="line"><span class="comment">#由于交叉熵损失需要将标签转化为独热形式，因此不接受浮点数作为标签的输入</span></span><br><span class="line"><span class="comment">#对 NLLLoss 而言，需要输入 logsigma</span></span><br><span class="line">criterion(logsigma,y.long())</span><br></pre></td></tr></table></figure><pre><code>tensor(1.1591, grad_fn=&lt;NllLossBackward0&gt;)</code></pre><h4 id="PyTorch-nn-CrossEntropyLoss"><a href="#PyTorch-nn-CrossEntropyLoss" class="headerlink" title="PyTorch(nn.CrossEntropyLoss())"></a>PyTorch(<code>nn.CrossEntropyLoss()</code>)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">criterion(zhat,y.long())</span><br></pre></td></tr></table></figure><pre><code>tensor(1.1591, grad_fn=&lt;NllLossBackward0&gt;)</code></pre><h3 id="5-PyTorch-中的分类损失函数选择"><a href="#5-PyTorch-中的分类损失函数选择" class="headerlink" title="5. PyTorch 中的分类损失函数选择"></a>5. PyTorch 中的分类损失函数选择</h3><p>在 PyTorch 中，根据分类任务的类型（二分类或多分类），有不同的损失函数选择：</p><ul><li><p><strong>二分类任务</strong>:</p><ul><li><strong>不含激活函数的输出</strong>:<ul><li>使用 <code>BCELoss()</code>。</li><li>应用场景：模型输出 <code>sigma</code>, 真实标签 <code>y</code>。</li></ul></li><li><strong>含激活函数的输出</strong>:<ul><li>使用 <code>BCEWithLogitsLoss()</code>。</li><li>应用场景：模型输出 <code>zhat</code>, 真实标签 <code>y</code>。</li></ul></li></ul></li><li><p><strong>多分类任务</strong>:</p><ul><li><strong>含集成激活函数</strong>:<ul><li>使用 <code>CrossEntropyLoss()</code>。</li><li>应用场景：模型输出 <code>zhat</code>, 真实标签 <code>y.long()</code>（确保标签为长整型）。</li></ul></li><li><strong>含对数激活函数</strong>:<ul><li>首先应用 <code>LogSoftmax(dim=1)</code> 于模型输出 <code>zhat</code>。</li><li>然后使用 <code>NLLLoss()</code> 计算损失。</li><li>应用场景：处理后的 <code>logsigma</code>, 真实标签 <code>y.long()</code>。</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(4)-单层到多层</title>
      <link href="/Pytorch(4)/"/>
      <url>/Pytorch(4)/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure><h3 id="1-数据划分与阶跃函数的关系"><a href="#1-数据划分与阶跃函数的关系" class="headerlink" title="1. 数据划分与阶跃函数的关系"></a>1. 数据划分与阶跃函数的关系</h3><p>在构建神经网络的 <strong>“与门”</strong> 逻辑时，我们通过应用 <code>sigmoid(wx+b)</code> 函数并设置阈值为 <code>0.5</code> 来决定输出 <code>yhat</code>。这实质上引入了阶跃函数的概念，即：</p><ul><li>当 <code>wx+b</code> 大于 0 时，输出为 <code>1</code>；小于 0 时，输出为 <code>0</code>。这样的映射关系等价于阶跃函数。</li><li><code>wx+b=0</code> 的解析线定义了数据的<strong>二元分类界限</strong>，即数据划分线，可由 <code>x2=(-b-w1x1)/w2</code> 表示。</li><li>总结而言，<strong>阶跃函数等同于使用 <code>sigmoid(wx+b)</code> 以 <code>0.5</code> 为阈值进行数据划分</strong>。</li></ul><h3 id="2-或门"><a href="#2-或门" class="headerlink" title="2.或门"></a>2.或门</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">OR</span>(<span class="params">X</span>):</span><br><span class="line">    w = torch.tensor([-<span class="number">0.08</span>, <span class="number">0.15</span>,<span class="number">0.15</span>], dtype = torch.float32)</span><br><span class="line">    zhat = torch.mv(X,w)</span><br><span class="line">    <span class="comment"># sigma  = F.sigmoid(zhat)</span></span><br><span class="line">    yhat = torch.tensor([<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> zhat &gt; <span class="number">0</span>], dtype=torch.float32) <span class="comment">#化sigmoid和0.5为阶跃</span></span><br><span class="line">    <span class="keyword">return</span> yhat</span><br><span class="line">OR(X)</span><br></pre></td></tr></table></figure><pre><code>tensor([0., 1., 1., 1.])</code></pre><h3 id="3-非与门"><a href="#3-非与门" class="headerlink" title="3.非与门"></a>3.非与门</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">NAND</span>(<span class="params">X</span>):</span><br><span class="line">    w = torch.tensor([<span class="number">0.23</span>,-<span class="number">0.15</span>,-<span class="number">0.15</span>], dtype = torch.float32) <span class="comment">#和与门、或门都不同的权重</span></span><br><span class="line">    zhat = torch.mv(X,w)</span><br><span class="line">    yhat = torch.tensor([<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> zhat &gt;= <span class="number">0</span>],dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> yhat</span><br><span class="line">NAND(X)</span><br></pre></td></tr></table></figure><pre><code>tensor([1., 1., 1., 0.])</code></pre><h3 id="4-与门、或门、非与门的总结"><a href="#4-与门、或门、非与门的总结" class="headerlink" title="4. 与门、或门、非与门的总结"></a>4. 与门、或门、非与门的总结</h3><ul><li><p><strong>直线拟合</strong>：</p><ul><li>与门、或门、非与门的逻辑可以通过线性回归模型中的直线进行拟合，而异或门由于其输出不是线性可分的，不能仅使用直线来拟合。</li></ul></li><li><p><strong>从直线到曲线</strong>：</p><ul><li>当我们不能用直线来划分数据时，决策边界需要从<code>直线演变为曲线</code>。这通常意味着需要从<code>单层网络转变为多层网络</code>来实现更复杂的决策边界。</li></ul></li><li><p><strong>逻辑门总结</strong>：</p><ul><li><strong>与门</strong> (<code>AND gate</code>)：可以通过 <code>sigmoid</code> 函数配合 <code>0.5</code> 的阈值来实现。</li><li><strong>或门</strong> (<code>OR gate</code>)：可以通过 <code>sign</code> 函数来实现，其输出取决于输入值的符号。</li><li><strong>非与门</strong> (<code>NAND gate</code>)：同样可以通过 <code>sign</code> 函数来实现，其输出是与门的逆逻辑。</li><li>输出层的激活函数 <code>g(z)</code> 对网络的性能并不产生影响。</li></ul></li></ul><h3 id="5-异或门"><a href="#5-异或门" class="headerlink" title="5. 异或门"></a>5. 异或门</h3><p><img src="/img/NOR.png" alt="image.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">AND</span>(<span class="params">X</span>):</span><br><span class="line">    w=torch.tensor([-<span class="number">0.2</span>,<span class="number">0.15</span>,<span class="number">0.15</span>])</span><br><span class="line">    zhat = torch.mv(X,w)</span><br><span class="line">    <span class="comment"># yhat = F.sigmoid(zhat) #原始AND的h(z)是sigmoid</span></span><br><span class="line">    yhat = torch.tensor([<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> zhat&gt;<span class="number">0</span>],dtype=torch.float32) <span class="comment">#换阶跃效果不变，其与下面等效</span></span><br><span class="line">    <span class="comment"># yhat = torch.tensor([int(x) for x in F.sigmoid(zhat)&gt;0.5],dtype=torch.float32)与上面等效</span></span><br><span class="line">    <span class="keyword">return</span> yhat</span><br><span class="line">X = torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">XOR</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="comment">#输入层：</span></span><br><span class="line">    input_1 = X</span><br><span class="line">    <span class="comment">#中间层：</span></span><br><span class="line">    sigma_nand = NAND(input_1)</span><br><span class="line">    sigma_or = OR(input_1)</span><br><span class="line">    x0 = torch.tensor([[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line">    <span class="comment">#输出层：</span></span><br><span class="line">    input_2 = torch.cat((x0,sigma_nand.view(<span class="number">4</span>,<span class="number">1</span>),sigma_or.view(<span class="number">4</span>,<span class="number">1</span>)),dim=<span class="number">1</span>)</span><br><span class="line">    y_and = AND(input_2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_and</span><br><span class="line">XOR(X)</span><br></pre></td></tr></table></figure><pre><code>tensor([0., 1., 1., 0.])</code></pre><h3 id="6-神经网络的正向传播"><a href="#6-神经网络的正向传播" class="headerlink" title="6.神经网络的正向传播"></a>6.神经网络的正向传播</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调用必要库</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 确定数据</span></span><br><span class="line">torch.random.manual_seed(<span class="number">420</span>)</span><br><span class="line">X = torch.rand((<span class="number">500</span>,<span class="number">20</span>), dtype=torch.float32)</span><br><span class="line">y = torch.randint(low=<span class="number">0</span>,high=<span class="number">3</span>,size=(<span class="number">500</span>,<span class="number">1</span>), dtype=torch.float32)</span><br><span class="line"><span class="comment"># 确定网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_features, out_features</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model,self).__init__()</span><br><span class="line">        self.linear1 = nn.Linear(in_features,<span class="number">13</span>,bias=<span class="literal">True</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">13</span>,<span class="number">8</span>,bias=<span class="literal">True</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">8</span>,out_features,bias=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>): <span class="comment">#神经网络的向前传播</span></span><br><span class="line">        z1 = self.linear1(x)</span><br><span class="line">        sigma1 = torch.relu(z1)</span><br><span class="line">        z2 = self.linear2(sigma1)</span><br><span class="line">        sigma2 = torch.sigmoid(z2)</span><br><span class="line">        z3 = self.output(sigma2)</span><br><span class="line">        sigma3 = F.softmax(z3,dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> sigma3</span><br><span class="line"><span class="comment">#确定特征数目和分类数目</span></span><br><span class="line">input_ = X.shape[<span class="number">1</span>]</span><br><span class="line">output_ = <span class="built_in">len</span>(y.unique())</span><br><span class="line"><span class="comment">#实例化神经网络</span></span><br><span class="line">torch.random.manual_seed(<span class="number">420</span>)</span><br><span class="line">net = Model(in_features = input_, out_features = output_)</span><br><span class="line"><span class="comment">#前向传播</span></span><br><span class="line">net.forward(X) <span class="comment">#向前传播</span></span><br><span class="line"><span class="comment"># net(X) #或者这样 执行init以下的所有函数</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[0.4140, 0.3496, 0.2365],        [0.4210, 0.3454, 0.2336],        [0.4011, 0.3635, 0.2355],        ...,        [0.4196, 0.3452, 0.2352],        [0.4153, 0.3455, 0.2392],        [0.4153, 0.3442, 0.2405]], grad_fn=&lt;SoftmaxBackward0&gt;)</code></pre><p>在 PyTorch 中处理矩阵维度转换的逻辑通常涉及以下步骤：</p><ol><li><p><strong>权重矩阵转置</strong>：</p><ul><li>如果权重矩阵 <code>W</code> 的初始维度是 <code>(20, 13)</code>，为了进行矩阵乘法 <code>WX</code>，我们需要将其转置为 <code>(13, 20)</code>。</li></ul></li><li><p><strong>特征矩阵转置</strong>：</p><ul><li>同样地，如果特征矩阵 <code>X</code> 的初始维度是 <code>(500, 20)</code>，为了与转置后的权重矩阵 <code>W</code> 相乘，我们需要将 <code>X</code> 转置为 <code>(20, 500)</code>。</li></ul></li><li><p><strong>输出层处理</strong>：</p><ul><li>假设输出层 <code>output</code> 的维度是 <code>(3, 500)</code>，通过 <code>softmax</code> 函数处理后，我们得到一个 <code>(500, 3)</code> 的矩阵，其中每一行代表一个样本的类别概率。</li></ul></li><li><p><strong>偏差向量维度</strong>：</p><ul><li>偏差 <code>bias</code> 的维度大小通常与权重矩阵的输出层维度一致，例如 <code>tensor([13])</code>。</li></ul></li></ol><p>注意点：</p><ul><li><strong>权重矩阵的转置</strong>：<ul><li>在矩阵乘法中，权重矩阵的行和列需要与相应的特征矩阵的维度匹配。这通常意味着要进行转置操作。</li></ul></li></ul><h3 id="7-从nn-module中调用的方法"><a href="#7-从nn-module中调用的方法" class="headerlink" title="7.从nn.module中调用的方法"></a>7.从nn.module中调用的方法</h3><p>当操作 PyTorch 中的神经网络时，有几个常用的方法可以管理网络的训练和资源分配：</p><div class="table-container"><table><thead><tr><th>方法</th><th>描述</th></tr></thead><tbody><tr><td><code>net.training</code></td><td>检查网络是否处于训练模式。</td></tr><tr><td><code>net.cuda()</code></td><td>将整个网络迁移到 GPU 上，以利用其计算资源。</td></tr><tr><td><code>net.cpu()</code></td><td>将整个网络迁移到 CPU 上，通常用于推理或当 GPU 资源不可用时。</td></tr><tr><td><code>net.apply()</code></td><td>对神经网络中的所有层进行一致的操作，例如初始化权重。</td></tr><tr><td><code>net.parameters()</code></td><td>用于迭代网络中所有的参数，常用于优化过程中更新权重。</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.train()</span><br></pre></td></tr></table></figure><pre><code>Model(  (linear1): Linear(in_features=20, out_features=13, bias=True)  (linear2): Linear(in_features=13, out_features=8, bias=True)  (output): Linear(in_features=8, out_features=3, bias=True))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.cuda()</span><br></pre></td></tr></table></figure><pre><code>Model(  (linear1): Linear(in_features=20, out_features=13, bias=True)  (linear2): Linear(in_features=13, out_features=8, bias=True)  (output): Linear(in_features=8, out_features=3, bias=True))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.cpu()</span><br></pre></td></tr></table></figure><pre><code>Model(  (linear1): Linear(in_features=20, out_features=13, bias=True)  (linear2): Linear(in_features=13, out_features=8, bias=True)  (output): Linear(in_features=8, out_features=3, bias=True))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">initial_0</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">      m.weight.data.fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">net.apply(initial_0)</span><br></pre></td></tr></table></figure><pre><code>Model(  (linear1): Linear(in_features=20, out_features=13, bias=True)  (linear2): Linear(in_features=13, out_features=8, bias=True)  (output): Linear(in_features=8, out_features=3, bias=True))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for param in net.parameters():</span></span><br><span class="line"><span class="comment">#  print(param)</span></span><br><span class="line"><span class="comment">#  将每层的参数都打印出来</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(3)-线性、逻辑、Softmax回归</title>
      <link href="/Pytorch(3)/"/>
      <url>/Pytorch(3)/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure><h3 id="1-单层线性回归"><a href="#1-单层线性回归" class="headerlink" title="1.单层线性回归"></a>1.单层线性回归</h3><ul><li><p><strong>特征张量 (Feature Tensor) <code>x</code></strong></p><ul><li>推荐设置为 <code>float32</code> 类型，以规避潜在的 bug。</li></ul></li><li><p><strong>标签张量 (Label Tensor) <code>z</code></strong></p><ul><li>建议设置为<code>二维张量</code>。</li><li>如果遇到 bug，尝试使用 <code>.view()</code> 方法进行调整。</li></ul></li><li><p><strong>规避精度问题</strong></p><ul><li>使用 <code>torch.allclose()</code> 来处理精度问题，确保计算的准确性。</li></ul></li></ul><h4 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32) </span><br><span class="line">z=torch.tensor([[-<span class="number">0.2</span>],[-<span class="number">0.05</span>],[-<span class="number">0.05</span>],[<span class="number">0.1</span>]])</span><br><span class="line">w=torch.tensor([-<span class="number">0.2</span>,<span class="number">0.15</span>,<span class="number">0.15</span>])</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">LinearR</span>(<span class="params">X,w</span>):</span><br><span class="line">    zhat=torch.mv(X,w)</span><br><span class="line">    <span class="keyword">return</span> zhat</span><br><span class="line">zhat =LinearR(x,w)</span><br><span class="line"><span class="built_in">print</span>(zhat)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(zhat,z.view(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><pre><code>tensor([-0.2000, -0.0500, -0.0500,  0.1000])True</code></pre><ul><li><code>torch.nn.Linear</code>实现<code>神经网络</code>的<code>正向传播</code><ul><li>torch.nn.Linear(<code>上一层神经元的个数</code>,<code>这一层神经元的个数</code>)</li><li>所有<code>nn.module的层</code>都会实例化<code>带梯度的w和b</code></li><li><code>zhat</code>计算结果为<code>二维</code></li></ul></li></ul><h4 id="torch-functional"><a href="#torch-functional" class="headerlink" title="torch.functional"></a>torch.functional</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line">torch.manual_seed(<span class="number">420</span>)  <span class="comment"># 控制随机性</span></span><br><span class="line">output = torch.nn.Linear(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 不要bias：`output=torch.nn.Linear(2,1,bias=False)`</span></span><br><span class="line">zhat = output(X)</span><br><span class="line"><span class="built_in">print</span>(zhat)</span><br><span class="line"><span class="built_in">print</span>(output.weight)</span><br><span class="line"><span class="built_in">print</span>(output.bias)</span><br></pre></td></tr></table></figure><pre><code>tensor([[0.6730],        [1.1048],        [0.2473],        [0.6792]], grad_fn=&lt;AddmmBackward0&gt;)Parameter containing:tensor([[ 0.4318, -0.4256]], requires_grad=True)Parameter containing:tensor([0.6730], requires_grad=True)</code></pre><h3 id="2-单层逻辑回归"><a href="#2-单层逻辑回归" class="headerlink" title="2.单层逻辑回归"></a>2.单层逻辑回归</h3><h4 id="Sigmoid-函数在对数几率回归中的应用"><a href="#Sigmoid-函数在对数几率回归中的应用" class="headerlink" title="Sigmoid 函数在对数几率回归中的应用"></a>Sigmoid 函数在对数几率回归中的应用</h4><ul><li><p><strong>设置阈值</strong></p><ul><li>默认阈值设置为 <code>0.5</code>。</li><li>依据阈值，可将预测结果划分为 <code>0</code> 或 <code>1</code>。</li></ul></li><li><p><strong>回归转分类</strong></p><ul><li>使用 Sigmoid 函数将回归问题转换为分类问题。</li></ul></li></ul><h4 id="Tensor-1"><a href="#Tensor-1" class="headerlink" title="Tensor"></a>Tensor</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32) </span><br><span class="line">andgate=torch.tensor([[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line">w=torch.tensor([-<span class="number">0.2</span>,<span class="number">0.15</span>,<span class="number">0.15</span>])</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">LogisticR</span>(<span class="params">x,W</span>): </span><br><span class="line">    zhat=torch.mv(x,W)</span><br><span class="line">    sigma=torch.sigmoid(zhat)</span><br><span class="line">    andhat=torch.tensor([<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> sigma &gt;=<span class="number">0.5</span>],dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> sigma,andhat</span><br><span class="line">sigma,andhat=LogisticR(x,w)</span><br><span class="line"><span class="built_in">print</span>(sigma)</span><br><span class="line"><span class="built_in">print</span>(andgate)</span><br></pre></td></tr></table></figure><pre><code>tensor([0.4502, 0.4875, 0.4875, 0.5250])tensor([[0.],        [0.],        [0.],        [1.]])</code></pre><h4 id="torch-functional-1"><a href="#torch-functional-1" class="headerlink" title="torch.functional"></a>torch.functional</h4><ul><li><code>relu</code>/<code>sigmoid</code>重要所以<code>F有</code>，<code>本质</code>还是<code>torch.</code></li><li><code>tanh</code>/<code>sign</code>不是很重要所以<code>torch.</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">X=torch.tensor([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32) </span><br><span class="line">torch.random.manual_seed(<span class="number">420</span>)</span><br><span class="line">output=torch.nn.Linear(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">zhat=output(X) </span><br><span class="line">sigma=F.sigmoid(zhat)</span><br><span class="line">andhat = [<span class="built_in">int</span> (x) <span class="keyword">for</span> x <span class="keyword">in</span> sigma &gt; <span class="number">0.5</span>]</span><br><span class="line"><span class="built_in">print</span>(zhat)</span><br><span class="line"><span class="built_in">print</span>(sigma)</span><br><span class="line"><span class="built_in">print</span>(andhat)</span><br></pre></td></tr></table></figure><pre><code>tensor([[0.6730],        [1.1048],        [0.2473],        [0.6792]], grad_fn=&lt;AddmmBackward0&gt;)tensor([[0.6622],        [0.7512],        [0.5615],        [0.6636]], grad_fn=&lt;SigmoidBackward0&gt;)[1, 1, 1, 1]</code></pre><h3 id="3-Softmax回归"><a href="#3-Softmax回归" class="headerlink" title="3.Softmax回归"></a>3.Softmax回归</h3><ul><li>假设<code>三分类</code>，故Linear(2,<strong>3</strong>)</li><li>一般在<code>输出层</code>使用</li><li>本质也是<code>torch.</code></li></ul><h4 id="torch-functional-2"><a href="#torch-functional-2" class="headerlink" title="torch.functional"></a>torch.functional</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X=torch.tensor([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line">torch.random.manual_seed(<span class="number">420</span>)</span><br><span class="line">output=torch.nn.Linear(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">zhat=output(X)</span><br><span class="line">sigma = F.softmax(zhat,dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(zhat)</span><br><span class="line"><span class="built_in">print</span>(sigma)</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.5453,  0.2653, -0.3527],        [ 0.9772,  0.9382, -0.5684],        [ 0.1197, -0.2964, -0.8400],        [ 0.5516,  0.3765, -1.0557]], grad_fn=&lt;AddmmBackward0&gt;)tensor([[0.4623, 0.3494, 0.1883],        [0.4598, 0.4422, 0.0980],        [0.4896, 0.3229, 0.1875],        [0.4902, 0.4115, 0.0983]], grad_fn=&lt;SoftmaxBackward0&gt;)</code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(2)-张量可微性</title>
      <link href="/Pytorch(2)/"/>
      <url>/Pytorch(2)/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><h3 id="1-可微分性相关属性"><a href="#1-可微分性相关属性" class="headerlink" title="1.可微分性相关属性"></a>1.可微分性相关属性</h3><ul><li>张量<code>y</code>具有一个<code>grad_fn</code>属性，可以<code>查看该属性</code>:<code>&lt;PowBackward0&gt;</code><ul><li>保存了一种<code>y-x</code>的函数关系<code>pow</code></li><li>由<code>可微分张量</code>创建而来</li></ul></li><li>张量<code>y</code>还包含<code>由x计算</code>得出的<code>结果</code>:<code>1.</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建可微分张量x</span></span><br><span class="line">x = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># 构建函数关系</span></span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="comment"># 查看grad_fn属性</span></span><br><span class="line"><span class="built_in">print</span>(y.grad_fn)</span><br></pre></td></tr></table></figure><pre><code>tensor(2., requires_grad=True)tensor(4., grad_fn=&lt;PowBackward0&gt;)&lt;PowBackward0 object at 0x00000154FAE1E160&gt;</code></pre><ul><li><code>x</code>作为<code>初始张量</code>，<code>没有</code>grad_fn属性</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.grad_fn                                                                                               </span><br></pre></td></tr></table></figure><pre><code>不返回</code></pre><ul><li>可微分张量具有<code>传递性</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.requires_grad</span><br></pre></td></tr></table></figure><pre><code>True</code></pre><ul><li><strong>注意</strong><code>grad_fn</code>属性是<code>保存直接y-x</code>的关系:比如这里的<code>add</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">z = y + <span class="number">1</span></span><br><span class="line">z</span><br></pre></td></tr></table></figure><pre><code>tensor(5., grad_fn=&lt;AddBackward0&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br><span class="line"><span class="built_in">print</span>(z.grad_fn)</span><br></pre></td></tr></table></figure><pre><code>True&lt;AddBackward0 object at 0x00000154FAE595B0&gt;</code></pre><h3 id="2-反向传播与梯度计算"><a href="#2-反向传播与梯度计算" class="headerlink" title="2. 反向传播与梯度计算"></a>2. 反向传播与梯度计算</h3><ul><li>某个可微分张量的<code>导数值</code>，存在其<code>grad属性</code>中</li><li>执行<code>反向传播</code>才可查看<code>grad</code>，不然<code>不返回</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.grad</span><br></pre></td></tr></table></figure><pre><code>不返回</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">z.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure><pre><code>tensor(4.)</code></pre><ul><li><code>一张计算图</code>反向传播<code>仅能计算一次</code>，<code>backward</code>再次调用将<code>报错</code>！</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#z.backward() # 报错</span></span><br></pre></td></tr></table></figure><ul><li>可以用<code>retain_graph=True</code>可<code>不报错</code>，但会<code>累加梯度</code>(这段代码没累加)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">z = y+<span class="number">1</span></span><br><span class="line">z.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure><pre><code>tensor(4.)</code></pre><ul><li><code>y</code>上也可<code>反向传播</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">z = y+<span class="number">1</span></span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure><pre><code>tensor(4.)</code></pre><ul><li>无论何时，<code>仅能</code>计算<code>叶节点的导数值</code></li><li><code>中间节点不会保存梯度</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#y.grad() # 报错！</span></span><br></pre></td></tr></table></figure><ul><li>若<code>想保存</code>，可以使用<code>retain_grad()</code>方法</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">y.retain_grad()</span><br><span class="line">z = y+<span class="number">1</span></span><br><span class="line">z.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="built_in">print</span>(y.grad)</span><br></pre></td></tr></table></figure><pre><code>tensor(4.)tensor(1.)</code></pre><h3 id="3-阻止计算图跟踪"><a href="#3-阻止计算图跟踪" class="headerlink" title="3.阻止计算图跟踪"></a>3.阻止计算图跟踪</h3><ul><li><code>with torch.no_grad()</code>:阻止计算图跟踪</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    z = y+<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br></pre></td></tr></table></figure><pre><code>tensor(5.)False</code></pre><h3 id="4-创建一个不可导张量"><a href="#4-创建一个不可导张量" class="headerlink" title="4.创建一个不可导张量"></a>4.创建一个不可导张量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y1 = x ** <span class="number">2</span></span><br><span class="line">y2 = y1.detach()</span><br><span class="line">z = y2**<span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y1)</span><br><span class="line"><span class="built_in">print</span>(y2)</span><br><span class="line"><span class="built_in">print</span>(z)</span><br></pre></td></tr></table></figure><pre><code>tensor(4., grad_fn=&lt;PowBackward0&gt;)tensor(4.)tensor(16.)</code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>python</title>
      <link href="/2023-12-18-python/"/>
      <url>/2023-12-18-python/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(1)-基础与张量转换</title>
      <link href="/Pytorch(1)/"/>
      <url>/Pytorch(1)/</url>
      
        <content type="html"><![CDATA[<h3 id="1-导入库和版本"><a href="#1-导入库和版本" class="headerlink" title="1.导入库和版本"></a>1.导入库和版本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)</span><br></pre></td></tr></table></figure><pre><code>2.1.0</code></pre><h3 id="2-张量的创建方法"><a href="#2-张量的创建方法" class="headerlink" title="2.张量的创建方法"></a>2.张量的创建方法</h3><ul><li>包括<code>列表</code>、<code>元组</code>、<code>数组</code>的创建</li><li><code>列表/元组</code>默认是<strong>int64/float32</strong></li><li><code>数组</code>默认是<strong>int32/float64</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">t1=torch.tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">t2=torch.tensor((<span class="number">1</span>,<span class="number">2</span>)) </span><br><span class="line">t3=torch.tensor(<span class="number">1</span>+<span class="number">2j</span>)</span><br><span class="line">t4=torch.tensor([<span class="literal">True</span>,<span class="literal">True</span>])</span><br><span class="line">t5=torch.tensor(np.array([<span class="number">1</span>,<span class="number">2</span>]))</span><br><span class="line"><span class="built_in">print</span>(t1)</span><br><span class="line"><span class="built_in">print</span>(t2)</span><br><span class="line"><span class="built_in">print</span>(t3)</span><br><span class="line"><span class="built_in">print</span>(t4)</span><br><span class="line"><span class="built_in">print</span>(t5)</span><br></pre></td></tr></table></figure><pre><code>tensor([1, 2])tensor([1, 2])tensor(1.+2.j)tensor([True, True])tensor([1, 2], dtype=torch.int32)</code></pre><h3 id="3-torch数据类型大全"><a href="#3-torch数据类型大全" class="headerlink" title="3. torch数据类型大全"></a>3. torch数据类型大全</h3><div class="table-container"><table><thead><tr><th>Torch Type</th><th>Alias</th></tr></thead><tbody><tr><td><strong><code>torch.float64</code></strong></td><td><strong><code>torch.double</code></strong></td></tr><tr><td><code>torch.float32</code></td><td><code>torch.float</code></td></tr><tr><td><code>torch.float16</code></td><td><code>torch.half</code></td></tr><tr><td><strong><code>torch.int64</code></strong></td><td><strong><code>torch.long</code></strong></td></tr><tr><td><code>torch.int32</code></td><td><code>torch.int</code></td></tr><tr><td><code>torch.int16</code></td><td><code>torch.short</code></td></tr><tr><td><code>torch.uint8</code></td><td></td></tr><tr><td><code>torch.int8</code></td><td></td></tr><tr><td><strong><code>torch.bool</code></strong></td><td></td></tr><tr><td><code>torch.complex64</code></td></tr></tbody></table></div><h3 id="4-隐式转换"><a href="#4-隐式转换" class="headerlink" title="4. 隐式转换"></a>4. 隐式转换</h3><ul><li>浮点型&gt;整数型&gt;布尔型</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.tensor([<span class="number">1.1</span>,<span class="number">2.7</span>],dtype = torch.uint8))</span><br></pre></td></tr></table></figure><pre><code>tensor([1, 2], dtype=torch.uint8)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.tensor([<span class="number">1.1</span>,<span class="number">2</span>]).dtype)</span><br><span class="line"><span class="built_in">print</span>(torch.tensor([<span class="number">2</span>,<span class="literal">True</span>]).dtype)</span><br></pre></td></tr></table></figure><pre><code>torch.float32torch.int64</code></pre><h3 id="5-显示转换（方法）"><a href="#5-显示转换（方法）" class="headerlink" title="5. 显示转换（方法）"></a>5. 显示转换（方法）</h3><ul><li><code>不改变</code>原数据类型</li><li>检查类型用<code>dtype</code></li><li><code>half</code>/<code>float</code>/<code>double</code>/<code>short</code>/<code>int</code>/<code>long</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(t.<span class="built_in">float</span>())</span><br><span class="line"><span class="built_in">print</span>(t.dtype)</span><br></pre></td></tr></table></figure><pre><code>tensor([1., 2.])torch.int64</code></pre><h3 id="6-一维度情况的维度形变"><a href="#6-一维度情况的维度形变" class="headerlink" title="6. 一维度情况的维度形变"></a>6. 一维度情况的维度形变</h3><p>在PyTorch中，<code>shape</code>与<code>size()</code>用于查看张量的<code>形状</code>。以下是一些常用的属性和方法：</p><div class="table-container"><table><thead><tr><th>属性/方法</th><th>作用</th><th>说明</th></tr></thead><tbody><tr><td><code>ndim</code></td><td>维度</td><td>查看张量的维度数</td></tr><tr><td><code>shape</code></td><td>形状</td><td>查看张量的形状，与<code>size()</code>相同</td></tr><tr><td><code>size()</code></td><td>形状</td><td>查看张量的形状，与<code>shape</code>相同</td></tr><tr><td><code>numel()</code></td><td>元素数量</td><td>查看张量的元素总数</td></tr><tr><td><code>len()</code></td><td>元素数量</td><td>在一维中与<code>numel()</code>相同</td></tr></tbody></table></div><ul><li><strong>注意</strong>：在一维张量中，<code>numel()</code> 和 <code>len()</code> 返回相同的结果，都表示张量的元素数量。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;ndim:\t&#x27;</span>,torch.tensor([<span class="number">1</span>,<span class="number">2</span>]).ndim) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;shape:\t&#x27;</span>,torch.tensor([<span class="number">1</span>,<span class="number">2</span>]).shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;size():\t&#x27;</span>,torch.tensor([<span class="number">1</span>,<span class="number">2</span>]).size())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;numel():&#x27;</span>,torch.tensor([<span class="number">1</span>,<span class="number">2</span>]).numel())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;len():\t&#x27;</span>,<span class="built_in">len</span>(torch.tensor([<span class="number">1</span>,<span class="number">2</span>])))</span><br></pre></td></tr></table></figure><pre><code>ndim:       1shape:      torch.Size([2])size():     torch.Size([2])numel():    2len():      2</code></pre><h3 id="7-二维度情况的维度形变"><a href="#7-二维度情况的维度形变" class="headerlink" title="7. 二维度情况的维度形变"></a>7. 二维度情况的维度形变</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;ndim:\t&#x27;</span>,torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).ndim)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;shape:\t&#x27;</span>,torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;size():\t&#x27;</span>,torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).size()) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;numel():&#x27;</span>,torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).numel()) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;len():\t&#x27;</span>,<span class="built_in">len</span>(torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])))</span><br></pre></td></tr></table></figure><pre><code>ndim:       2shape:      torch.Size([2, 2])size():     torch.Size([2, 2])numel():    4len():      2</code></pre><h3 id="8-三维度情况的维度形变"><a href="#8-三维度情况的维度形变" class="headerlink" title="8. 三维度情况的维度形变"></a>8. 三维度情况的维度形变</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;ndim:\t&#x27;</span>,torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>]],[[<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>]]]).ndim)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;shape:\t&#x27;</span>,torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>]],[[<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>]]]).shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;size():\t&#x27;</span>,torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>]],[[<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>]]]).size()) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;numel():&#x27;</span>,torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>]],[[<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>]]]).numel()) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;len():\t&#x27;</span>,<span class="built_in">len</span>(torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>]],[[<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>]]])))</span><br></pre></td></tr></table></figure><pre><code>ndim:       3shape:      torch.Size([2, 2, 3])size():     torch.Size([2, 2, 3])numel():    12len():      2</code></pre><h3 id="9-零维张量"><a href="#9-零维张量" class="headerlink" title="9. 零维张量"></a>9. 零维张量</h3><ul><li><strong>零维张量</strong>：PyTorch中的单个数值，具有张量属性（如 <code>torch.tensor(1)</code>）。</li><li><strong>标量</strong>：Python中的单个数值，不具备张量属性。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;ndim:\t&#x27;</span>,torch.tensor(<span class="number">1</span>).ndim)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;shape:\t&#x27;</span>,torch.tensor(<span class="number">1</span>).shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;size():\t&#x27;</span>,torch.tensor(<span class="number">1</span>).size()) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;numel():&#x27;</span>,torch.tensor(<span class="number">1</span>).numel()) </span><br><span class="line"><span class="comment"># print(&#x27;len():\t&#x27;,len(torch.tensor(1))) 报错，零维张量不能len()</span></span><br></pre></td></tr></table></figure><pre><code>ndim:    0shape:   torch.Size([])size():  torch.Size([])numel(): 1</code></pre><h3 id="10-张量的形变"><a href="#10-张量的形变" class="headerlink" title="10.张量的形变"></a>10.张量的形变</h3><ul><li><code>N维、0维</code>flatten()后被变为<code>1维</code></li><li>flatten()和reshape()均<code>不会对原张量造成影响</code></li><li>reshape(4),reshape(4,)<code>均为1维</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).flatten())</span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1</span>).flatten())</span><br><span class="line"><span class="built_in">print</span>(torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).reshape(<span class="number">4</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).reshape(<span class="number">4</span>,))</span><br><span class="line"><span class="built_in">print</span>(torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).reshape(<span class="number">1</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure><pre><code>tensor([1, 2, 3, 4])tensor([1])tensor([1, 2, 3, 4])tensor([1, 2, 3, 4])tensor([[1, 2, 3, 4]])</code></pre><h3 id="11-特殊张量的创建"><a href="#11-特殊张量的创建" class="headerlink" title="11.特殊张量的创建"></a>11.特殊张量的创建</h3><ul><li><code>1/0/empty</code>只填size参数时,无括号/圆括号/方括号均可</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.zeros(<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.ones((<span class="number">2</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="built_in">print</span>(torch.empty([<span class="number">2</span>,<span class="number">3</span>]))</span><br><span class="line"><span class="built_in">print</span>(torch.full((<span class="number">2</span>,<span class="number">3</span>),<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.eye(<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.diag(torch.tensor([<span class="number">1</span>,<span class="number">2</span>])))</span><br></pre></td></tr></table></figure><pre><code>tensor([[0., 0., 0.],        [0., 0., 0.]])tensor([[1., 1., 1.],        [1., 1., 1.]])tensor([[1., 1., 1.],        [1., 1., 1.]])tensor([[1, 1, 1],        [1, 1, 1]])tensor([[1., 0., 0.],        [0., 1., 0.],        [0., 0., 1.]])tensor([[1, 0],        [0, 2]])</code></pre><h3 id="12-随机张量的创建"><a href="#12-随机张量的创建" class="headerlink" title="12.随机张量的创建"></a>12.随机张量的创建</h3><div class="table-container"><table><thead><tr><th>函数</th><th>描述</th></tr></thead><tbody><tr><td><code>rand()</code></td><td>0-1均匀分布</td></tr><tr><td><code>randn()</code></td><td>标准正态分布</td></tr><tr><td><code>normal()</code></td><td>服从指定正态分布</td></tr><tr><td><code>randint()</code></td><td>整数随机采样</td></tr><tr><td><code>arange</code>/<code>linspace</code></td><td>生成数列</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.rand(<span class="number">2</span>,<span class="number">3</span>)) </span><br><span class="line"><span class="built_in">print</span>(torch.randn(<span class="number">2</span>,<span class="number">3</span>))  </span><br><span class="line"><span class="built_in">print</span>(torch.normal(<span class="number">2</span>,<span class="number">3</span>,size=(<span class="number">2</span>,<span class="number">2</span>))) </span><br><span class="line"><span class="built_in">print</span>(torch.randint(<span class="number">1</span>,<span class="number">10</span>,size=[<span class="number">2</span>,<span class="number">4</span>])) </span><br><span class="line"><span class="built_in">print</span>(torch.arange(<span class="number">0</span>,<span class="number">5</span>,<span class="number">3</span>)) </span><br><span class="line"><span class="built_in">print</span>(torch.linspace(<span class="number">0</span>,<span class="number">5</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure><pre><code>tensor([[0.7878, 0.6142, 0.6865],        [0.1155, 0.4820, 0.1763]])tensor([[ 0.0174,  1.1351, -0.1324],        [-0.2343, -0.0504,  0.1417]])tensor([[1.1347, 1.6557],        [3.1312, 2.9998]])tensor([[5, 2, 3, 8],        [2, 8, 8, 5]])tensor([0, 3])tensor([0.0000, 1.6667, 3.3333, 5.0000])</code></pre><h3 id="13-tensor-numpy-list相互转换"><a href="#13-tensor-numpy-list相互转换" class="headerlink" title="13.tensor/numpy/list相互转换"></a>13.tensor/numpy/list相互转换</h3><ul><li><code>list(t)</code>是转换为<code>零维张量</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(t.numpy())</span><br><span class="line"><span class="built_in">print</span>(np.array(t))</span><br><span class="line"><span class="built_in">print</span>(t.tolist())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(t))</span><br><span class="line"><span class="built_in">print</span>(t.item())</span><br></pre></td></tr></table></figure><pre><code>[1][1][1][tensor(1)]1</code></pre><h3 id="14-张量的浅拷贝与深拷贝"><a href="#14-张量的浅拷贝与深拷贝" class="headerlink" title="14. 张量的浅拷贝与深拷贝"></a>14. 张量的浅拷贝与深拷贝</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">t2 = t1  <span class="comment"># 浅拷贝，动一个则变另一个</span></span><br><span class="line">t3=t1.clone()  <span class="comment"># 深拷贝，动一个则另一个不变</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 标签1 </tag>
            
            <tag> 标签A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第三篇文章</title>
      <link href="/2023-12-17-3/"/>
      <url>/2023-12-17-3/</url>
      
        <content type="html"><![CDATA[<h2 id="这是我的第三篇文章"><a href="#这是我的第三篇文章" class="headerlink" title="这是我的第三篇文章"></a>这是我的第三篇文章</h2>]]></content>
      
      
      <categories>
          
          <category> Embedded </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 标签3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第二篇文章</title>
      <link href="/2023-12-17-2/"/>
      <url>/2023-12-17-2/</url>
      
        <content type="html"><![CDATA[<h2 id="这是我的第二篇文章"><a href="#这是我的第二篇文章" class="headerlink" title="这是我的第二篇文章"></a>这是我的第二篇文章</h2>]]></content>
      
      
      <categories>
          
          <category> C </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 标签2 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title></title>
      <link href="/css/custom.css"/>
      <url>/css/custom.css</url>
      
        <content type="html"><![CDATA[:root {  --trans-light: rgba(255, 255, 255, 0.88);  --trans-dark: rgba(25, 25, 25, 0.88);  --border-style: 1px solid rgb(169, 169, 169);  --backdrop-filter: blur(5px) saturate(150%);}/* 页脚与头图透明 */#footer {    background: transparent !important;  }  #page-header {    background: transparent !important;  }    /* 白天模式遮罩透明 */  #footer::before {    background: transparent !important;  }  #page-header::before {    background: transparent !important;  }    /* 夜间模式遮罩透明 */  [data-theme="dark"] #footer::before {    background: transparent !important;  }  [data-theme="dark"] #page-header::before {    background: transparent !important;  }/* 小冰分类分类磁铁黑夜模式适配 *//* 一般状态 */[data-theme="dark"] .magnet_link_context {  background: #1e1e1e;  color: antiquewhite;}/* 鼠标悬浮状态 */[data-theme="dark"] .magnet_link_context:hover {  background: #3ecdf1;  color: #f2f2f2;}@font-face {  /* 为载入的字体取名字(随意) */  font-family: 'YSHST';  /* 字体文件地址(相对或者绝对路径都可以) */  src: url(/font/poppins-black-webfont.woff2);  /* 定义加粗样式(加粗多少) */  font-weight: normal;  /* 定义字体样式(斜体/非斜体) */  font-style: normal;  /* 定义显示样式 */  font-display: block;}/* 翻页按钮居中 */#pagination {  width: 100%;  margin: auto;}/* 一级菜单居中 */#nav .menus_items {  position: absolute !important;  width: fit-content !important;  left: 50% !important;  transform: translateX(-50%) !important;}/* 子菜单横向展示 */#nav .menus_items .menus_item:hover .menus_item_child {  display: flex !important;}/* 这里的2是代表导航栏的第2个元素，即有子菜单的元素，可以按自己需求修改 */.menus_items .menus_item:nth-child(5) .menus_item_child {  left: -38px;}/* 夜间模式菜单栏发光字 */[data-theme="dark"] #nav .site-page,[data-theme="dark"] #nav .menus_items .menus_item .menus_item_child li a {  text-shadow: 0 0 2px var(--theme-color) !important;}/* 手机端适配 */[data-theme="dark"] #sidebar #sidebar-menus .menus_items .site-page {  text-shadow: 0 0 2px var(--theme-color) !important;}/* 闪烁变动颜色连续渐变 *//* 日间模式不生效 */[data-theme="light"] #site-name,[data-theme="light"] #site-title,[data-theme="light"] #site-subtitle,[data-theme="light"] #post-info {  animation: none;}/* 夜间模式生效 */[data-theme="dark"] #site-name,[data-theme="dark"] #site-title {  animation: light_15px 10s linear infinite;}[data-theme="dark"] #site-subtitle {  animation: light_10px 10s linear infinite;}[data-theme="dark"] #post-info {  animation: light_5px 10s linear infinite;}/* 关键帧描述 */@keyframes light_15px {  0% {    text-shadow: #5636ed 0 0 15px;  }  12.5% {    text-shadow: #11ee5e 0 0 15px;  }  25% {    text-shadow: #f14747 0 0 15px;  }  37.5% {    text-shadow: #f1a247 0 0 15px;  }  50% {    text-shadow: #f1ee47 0 0 15px;  }  50% {    text-shadow: #b347f1 0 0 15px;  }  62.5% {    text-shadow: #002afa 0 0 15px;  }  75% {    text-shadow: #ed709b 0 0 15px;  }  87.5% {    text-shadow: #39c5bb 0 0 15px;  }  100% {    text-shadow: #5636ed 0 0 15px;  }}@keyframes light_10px {  0% {    text-shadow: #5636ed 0 0 10px;  }  12.5% {    text-shadow: #11ee5e 0 0 10px;  }  25% {    text-shadow: #f14747 0 0 10px;  }  37.5% {    text-shadow: #f1a247 0 0 10px;  }  50% {    text-shadow: #f1ee47 0 0 10px;  }  50% {    text-shadow: #b347f1 0 0 10px;  }  62.5% {    text-shadow: #002afa 0 0 10px;  }  75% {    text-shadow: #ed709b 0 0 10px;  }  87.5% {    text-shadow: #39c5bb 0 0 10px;  }  100% {    text-shadow: #5636ed 0 0 10px;  }}@keyframes light_5px {  0% {    text-shadow: #5636ed 0 0 5px;  }  12.5% {    text-shadow: #11ee5e 0 0 5px;  }  25% {    text-shadow: #f14747 0 0 5px;  }  37.5% {    text-shadow: #f1a247 0 0 15px;  }  50% {    text-shadow: #f1ee47 0 0 5px;  }  50% {    text-shadow: #b347f1 0 0 5px;  }  62.5% {    text-shadow: #002afa 0 0 5px;  }  75% {    text-shadow: #ed709b 0 0 5px;  }  87.5% {    text-shadow: #39c5bb 0 0 5px;  }  100% {    text-shadow: #5636ed 0 0 5px;  }}/* 背景宇宙星光  */#universe{  display: block;  position: fixed;  margin: 0;  padding: 0;  border: 0;  outline: 0;  left: 0;  top: 0;  width: 100%;  height: 100%;  pointer-events: none;  /* 这个是调置顶的优先级的，-1在文章页下面，背景上面，个人推荐这种 */  z-index: -1;}/* 侧边栏个人信息卡片动态渐变色 */#aside-content > .card-widget.card-info {  background: linear-gradient(    -45deg,    #e8d8b9,    #eccec5,    #a3e9eb,      #bdbdf0,    #eec1ea  );  box-shadow: 0 0 5px rgb(66, 68, 68);  position: relative;  background-size: 400% 400%;  -webkit-animation: Gradient 10s ease infinite;  -moz-animation: Gradient 10s ease infinite;  animation: Gradient 10s ease infinite !important;}@-webkit-keyframes Gradient {  0% {    background-position: 0% 50%;  }  50% {    background-position: 100% 50%;  }  100% {    background-position: 0% 50%;  }}@-moz-keyframes Gradient {  0% {    background-position: 0% 50%;  }  50% {    background-position: 100% 50%;  }  100% {    background-position: 0% 50%;  }}@keyframes Gradient {  0% {    background-position: 0% 50%;  }  50% {    background-position: 100% 50%;  }  100% {    background-position: 0% 50%;  }}/* 黑夜模式适配 */[data-theme="dark"] #aside-content > .card-widget.card-info {  background: #191919ee;}/* 个人信息Follow me按钮 */#aside-content > .card-widget.card-info > #card-info-btn {  background-color: #3eb8be;  border-radius: 8px;}/* 鼠标样式 */#cursor {  position: fixed;  width: 16px;  height: 16px;  /* 这里改变跟随的底色 */  background: rgb(101, 153, 245);  border-radius: 8px;  opacity: 0.25;  z-index: 10086;  pointer-events: none;  transition: 0.2s ease-in-out;  transition-property: background, opacity, transform;}#cursor.hidden {  opacity: 0;}#cursor.hover {  opacity: 0.1;  transform: scale(2.5);  -webkit-transform: scale(2.5);  -moz-transform: scale(2.5);  -ms-transform: scale(2.5);  -o-transform: scale(2.5);}#cursor.active {  opacity: 0.5;  transform: scale(0.5);  -webkit-transform: scale(0.5);  -moz-transform: scale(0.5);  -ms-transform: scale(0.5);  -o-transform: scale(0.5);}/* 首页文章卡片 */#recent-posts > .recent-post-item {  background: var(--trans-light);  backdrop-filter: var(--backdrop-filter);  border-radius: 25px;  border: var(--border-style);}/* 首页侧栏卡片 */#aside-content .card-widget {  background: var(--trans-light);  backdrop-filter: var(--backdrop-filter);  border-radius: 18px;  border: var(--border-style);}/* 文章页、归档页、普通页面 */div#post,div#page,div#archive {  background: var(--trans-light);  backdrop-filter: var(--backdrop-filter);  border: var(--border-style);  border-radius: 20px;}/* 导航栏 */#page-header.nav-fixed #nav {  background: rgba(255, 255, 255, 0.75);  backdrop-filter: var(--backdrop-filter);}[data-theme="dark"] #page-header.nav-fixed #nav {  background: rgba(0, 0, 0, 0.7) !important;}/* 夜间模式遮罩 */[data-theme="dark"] #recent-posts > .recent-post-item,[data-theme="dark"] #aside-content .card-widget,[data-theme="dark"] div#post,[data-theme="dark"] div#archive,[data-theme="dark"] div#page {  background: var(--trans-dark);}/* 夜间模式页脚页头遮罩透明 */[data-theme="dark"] #footer::before {  background: transparent !important;}[data-theme="dark"] #page-header::before {  background: transparent !important;}/* 阅读模式 */.read-mode #aside-content .card-widget {  background: rgba(158, 204, 171, 0.5) !important;}.read-mode div#post {  background: rgba(158, 204, 171, 0.5) !important;}/* 夜间模式下的阅读模式 */[data-theme="dark"] .read-mode #aside-content .card-widget {  background: rgba(25, 25, 25, 0.9) !important;  color: #ffffff;}[data-theme="dark"] .read-mode div#post {  background: rgba(25, 25, 25, 0.9) !important;  color: #ffffff;}]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>关于</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<!-- GitCalendar容器 --><div id="gitZone"></div>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>分类</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/cursor.js"/>
      <url>/js/cursor.js</url>
      
        <content type="html"><![CDATA[var CURSOR;Math.lerp = (a, b, n) => (1 - n) * a + n * b;const getStyle = (el, attr) => {    try {        return window.getComputedStyle            ? window.getComputedStyle(el)[attr]            : el.currentStyle[attr];    } catch (e) {}    return "";};class Cursor {    constructor() {        this.pos = {curr: null, prev: null};        this.pt = [];        this.create();        this.init();        this.render();    }    move(left, top) {        this.cursor.style["left"] = `${left}px`;        this.cursor.style["top"] = `${top}px`;    }    create() {        if (!this.cursor) {            this.cursor = document.createElement("div");            this.cursor.id = "cursor";            this.cursor.classList.add("hidden");            document.body.append(this.cursor);        }        var el = document.getElementsByTagName('*');        for (let i = 0; i < el.length; i++)            if (getStyle(el[i], "cursor") == "pointer")                this.pt.push(el[i].outerHTML);        document.body.appendChild((this.scr = document.createElement("style")));        // 这里改变鼠标指针的颜色 由svg生成        this.scr.innerHTML = `* {cursor: url("data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 8 8' width='8px' height='8px'><circle cx='4' cy='4' r='4' opacity='1.0' fill='rgb(0, 0, 255)'/></svg>") 4 4, auto}`;    }    refresh() {        this.scr.remove();        this.cursor.classList.remove("hover");        this.cursor.classList.remove("active");        this.pos = {curr: null, prev: null};        this.pt = [];        this.create();        this.init();        this.render();    }    init() {        document.onmouseover  = e => this.pt.includes(e.target.outerHTML) && this.cursor.classList.add("hover");        document.onmouseout   = e => this.pt.includes(e.target.outerHTML) && this.cursor.classList.remove("hover");        document.onmousemove  = e => {(this.pos.curr == null) && this.move(e.clientX - 8, e.clientY - 8); this.pos.curr = {x: e.clientX - 8, y: e.clientY - 8}; this.cursor.classList.remove("hidden");};        document.onmouseenter = e => this.cursor.classList.remove("hidden");        document.onmouseleave = e => this.cursor.classList.add("hidden");        document.onmousedown  = e => this.cursor.classList.add("active");        document.onmouseup    = e => this.cursor.classList.remove("active");    }    render() {        if (this.pos.prev) {            this.pos.prev.x = Math.lerp(this.pos.prev.x, this.pos.curr.x, 0.15);            this.pos.prev.y = Math.lerp(this.pos.prev.y, this.pos.curr.y, 0.15);            this.move(this.pos.prev.x, this.pos.prev.y);        } else {            this.pos.prev = this.pos.curr;        }        requestAnimationFrame(() => this.render());    }}(() => {    CURSOR = new Cursor();    // 需要重新获取列表时，使用 CURSOR.refresh()})();]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/light.js"/>
      <url>/js/light.js</url>
      
        <content type="html"><![CDATA[// 霓虹灯效果// 颜色数组var arr = ["#39c5bb", "#f14747", "#f1a247", "#f1ee47", "#b347f1", "#1edbff", "#ed709b", "#5636ed"];// 颜色索引var idx = 0;// 切换颜色function changeColor() {    // 仅夜间模式才启用    if (document.getElementsByTagName('html')[0].getAttribute('data-theme') == 'dark') {        if (document.getElementById("site-name"))            document.getElementById("site-name").style.textShadow = arr[idx] + " 0 0 15px";        if (document.getElementById("site-title"))            document.getElementById("site-title").style.textShadow = arr[idx] + " 0 0 15px";        if (document.getElementById("site-subtitle"))            document.getElementById("site-subtitle").style.textShadow = arr[idx] + " 0 0 10px";        if (document.getElementById("post-info"))            document.getElementById("post-info").style.textShadow = arr[idx] + " 0 0 5px";        try {            document.getElementsByClassName("author-info__name")[0].style.textShadow = arr[idx] + " 0 0 12px";            document.getElementsByClassName("author-info__description")[0].style.textShadow = arr[idx] + " 0 0 12px";        } catch {                    }        idx++;        if (idx == 8) {            idx = 0;        }    } else {        // 白天模式恢复默认        if (document.getElementById("site-name"))            document.getElementById("site-name").style.textShadow = "#1e1e1ee0 1px 1px 1px";        if (document.getElementById("site-title"))            document.getElementById("site-title").style.textShadow = "#1e1e1ee0 1px 1px 1px";        if (document.getElementById("site-subtitle"))            document.getElementById("site-subtitle").style.textShadow = "#1e1e1ee0 1px 1px 1px";        if (document.getElementById("post-info"))            document.getElementById("post-info").style.textShadow = "#1e1e1ee0 1px 1px 1px";        try {            document.getElementsByClassName("author-info__name")[0].style.textShadow = "";            document.getElementsByClassName("author-info__description")[0].style.textShadow = "";        } catch {                    }    }}// 开启计时器window.onload = setInterval(changeColor, 1200);]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>友链</title>
      <link href="/link/index.html"/>
      <url>/link/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/runtime.js"/>
      <url>/js/runtime.js</url>
      
        <content type="html"><![CDATA[function createtime() {    var now = new Date(); // 获取当前时间    var startDate = new Date("12/17/2023 21:00:00"); // 网站开始运行的日期，按需更改    var elapsed = now - startDate; // 计算流逝的时间    var seconds = Math.floor(elapsed / 1000);    var minutes = Math.floor(seconds / 60);    var hours = Math.floor(minutes / 60);    var days = Math.floor(hours / 24);    seconds %= 60; // 剩余秒数    minutes %= 60; // 剩余分钟数    hours %= 24; // 剩余小时数    // 确保时间是两位数字格式    var secondsStr = seconds < 10 ? "0" + seconds : seconds;    var minutesStr = minutes < 10 ? "0" + minutes : minutes;    var hoursStr = hours < 10 ? "0" + hours : hours;    // 更新网站内容    var c = `<div style="font-size:13px;font-weight:bold">本站居然运行了 ${days} 天 ${hoursStr} 小时 ${minutesStr} 分 ${secondsStr} 秒</div>`;    document.getElementById("workboard").innerHTML = c;}setInterval(createtime, 1000); // 每秒更新一次]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>添加我的QQ</title>
      <link href="/qq/index.html"/>
      <url>/qq/index.html</url>
      
        <content type="html"><![CDATA[<p><img src="../img/QQ.jpg" alt="我的QQ二维码"></p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>音乐</title>
      <link href="/music/index.html"/>
      <url>/music/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/universe.js"/>
      <url>/js/universe.js</url>
      
        <content type="html"><![CDATA[function dark() {window.requestAnimationFrame=window.requestAnimationFrame||window.mozRequestAnimationFrame||window.webkitRequestAnimationFrame||window.msRequestAnimationFrame;var n,e,i,h,t=.05,s=document.getElementById("universe"),o=!0,a="180,184,240",r="226,225,142",d="226,225,224",c=[];function f(){n=window.innerWidth,e=window.innerHeight,i=.216*n,s.setAttribute("width",n),s.setAttribute("height",e)}function u(){h.clearRect(0,0,n,e);for(var t=c.length,i=0;i<t;i++){var s=c[i];s.move(),s.fadeIn(),s.fadeOut(),s.draw()}}function y(){this.reset=function(){this.giant=m(3),this.comet=!this.giant&&!o&&m(10),this.x=l(0,n-10),this.y=l(0,e),this.r=l(1.1,2.6),this.dx=l(t,6*t)+(this.comet+1-1)*t*l(50,120)+2*t,this.dy=-l(t,6*t)-(this.comet+1-1)*t*l(50,120),this.fadingOut=null,this.fadingIn=!0,this.opacity=0,this.opacityTresh=l(.2,1-.4*(this.comet+1-1)),this.do=l(5e-4,.002)+.001*(this.comet+1-1)},this.fadeIn=function(){this.fadingIn&&(this.fadingIn=!(this.opacity>this.opacityTresh),this.opacity+=this.do)},this.fadeOut=function(){this.fadingOut&&(this.fadingOut=!(this.opacity<0),this.opacity-=this.do/2,(this.x>n||this.y<0)&&(this.fadingOut=!1,this.reset()))},this.draw=function(){if(h.beginPath(),this.giant)h.fillStyle="rgba("+a+","+this.opacity+")",h.arc(this.x,this.y,2,0,2*Math.PI,!1);else if(this.comet){h.fillStyle="rgba("+d+","+this.opacity+")",h.arc(this.x,this.y,1.5,0,2*Math.PI,!1);for(var t=0;t<30;t++)h.fillStyle="rgba("+d+","+(this.opacity-this.opacity/20*t)+")",h.rect(this.x-this.dx/4*t,this.y-this.dy/4*t-2,2,2),h.fill()}else h.fillStyle="rgba("+r+","+this.opacity+")",h.rect(this.x,this.y,this.r,this.r);h.closePath(),h.fill()},this.move=function(){this.x+=this.dx,this.y+=this.dy,!1===this.fadingOut&&this.reset(),(this.x>n-n/4||this.y<0)&&(this.fadingOut=!0)},setTimeout(function(){o=!1},50)}function m(t){return Math.floor(1e3*Math.random())+1<10*t}function l(t,i){return Math.random()*(i-t)+t}f(),window.addEventListener("resize",f,!1),function(){h=s.getContext("2d");for(var t=0;t<i;t++)c[t]=new y,c[t].reset();u()}(),function t(){document.getElementsByTagName('html')[0].getAttribute('data-theme')=='dark'&&u(),window.requestAnimationFrame(t)}()};dark()]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>电影</title>
      <link href="/movies/index.html"/>
      <url>/movies/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
