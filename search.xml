<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>yolov7-paper</title>
      <link href="/yolov7-paper/"/>
      <url>/yolov7-paper/</url>
      
        <content type="html"><![CDATA[<h1 id="YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors"><a href="#YOLOv7-Trainable-bag-of-freebies-sets-new-state-of-the-art-for-real-time-object-detectors" class="headerlink" title="YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors"></a><strong>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</strong></h1><h2 id="Abstract—摘要"><a href="#Abstract—摘要" class="headerlink" title="Abstract—摘要"></a><strong>Abstract—摘要</strong></h2><p>YOLOv7在5FPS到 160 FPS 范围内的速度和准确度都超过了所有已知的物体检测器，YOLOv7 在 5 FPS 到 160 FPS 范围内的速度和准确度都超过了所有已知的目标检测器，并且在 GPU V100 上 30 FPS 或更高的所有已知实时目标检测器中具有最高的准确度 56.8% AP。YOLOv7-E6 目标检测器（56 FPS V100，55.9% AP）比基于transformer-based的检测器 SWINL Cascade-Mask R-CNN（9.2 FPS A100，53.9% AP）的速度和准确度分别高出 509% 和 2%，以及基于卷积的检测器 ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2% AP) 速度提高 551%，准确率提高 0.7%，以及 YOLOv7 的表现优于：YOLOR、YOLOX、Scaled-YOLOv4、YOLOv5、DETR、Deformable DETR  , DINO-5scale-R50, ViT-Adapter-B 和许多其他物体探测器在速度和准确度上。 此外，我们只在 MS COCO 数据集上从头开始训练 YOLOv7，而不使用任何其他数据集或预训练的权重。  源码发布在： GitHub - WongKinYiu/yolov7: Implementation of paper - YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</p><blockquote><h4 id="YOLOv7的成就"><a href="#YOLOv7的成就" class="headerlink" title="YOLOv7的成就"></a>YOLOv7的成就</h4><p>在5FPS到160FPS的范围内，在速度和精度上都超过了所有已知的物体检测器，在GPU V100上以30 FPS或更高的速度在所有已知 的实时物体检测器中具有最高的精度56.8%AP</p><h4 id="YOLOv7-E6在速度和精度上优于"><a href="#YOLOv7-E6在速度和精度上优于" class="headerlink" title="YOLOv7-E6在速度和精度上优于"></a>YOLOv7-E6在速度和精度上优于</h4><ul><li>基于transformer的检测器SWINL Cascade-Mask R-CNN</li><li>基于卷积的检测器ConvNeXt XL级联掩码R-CNN</li></ul><h4 id="YOLOv7优于"><a href="#YOLOv7优于" class="headerlink" title="YOLOv7优于"></a>YOLOv7优于</h4><p>YOLOR、YOLOX、Scaled-YOLOv4、YOLOv5、DETR、可变形DETR、DINO-5scale-R50、ViT-Adapter-B和许多其他物体检测器的速度和精度。</p><p><strong>训练方面：</strong>作者只在COCO数据集上从0开始训练YOLOv7，而不使用任何其他数据集或预先训练的权重。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Figure1.png" alt=""></p></blockquote><h2 id="一、-Introduction—-简介"><a href="#一、-Introduction—-简介" class="headerlink" title="一、 *\*Introduction—****简介"></a>一、 <strong>*\</strong>*Introduction—**<em>**</em>简介</h2><p>实时对象检测是计算机视觉中非常重要的主题，因为它通常是计算机视觉系统中的必要组件。 例如，多目标跟踪[94, 93]，自动驾驶[40, 18]，机器人[35, 58]，医学图像分析[34, 46]等。执行实时目标检测的计算设备通常是一些移动CPU或GPU，以及各大厂商开发的各种神经处理单元（NPU）。 比如苹果神经引擎（Apple）、神经计算棒（Intel）、Jetson AI边缘设备（Nvidia）、边缘TPU（谷歌）、神经处理引擎（Qualcomm）、AI处理单元（MediaTek）、 和 AI SoC（Kneron）都是 NPU。 上面提到的一些边缘设备专注于加速不同的操作，例如普通卷积、深度卷积或 MLP 操作。 在本文中，我们提出的实时目标检测器主要希望它能够同时支持移动 GPU 和从边缘到云端的 GPU 设备。</p><p>近年来，实时目标检测器仍在针对不同的边缘设备进行开发。例如，开发MCUNet [49, 48] 和 NanoDet [54] 的运营专注于生产低功耗单芯片并提高边缘 CPU 的推理速度。 至于 YOLOX [21] 和 YOLOR [81] 等方法，他们专注于提高各种 GPU 的推理速度。 最近，实时目标检测器的发展集中在高效架构的设计上。 至于可以在 CPU [54, 88, 84, 83] 上使用的实时目标检测器，他们的设计主要基于 MobileNet [28, 66, 27]、ShuffleNet [92, 55] 或 GhostNet [25]  . 另一个主流的实时目标检测器是为 GPU [81, 21, 97] 开发的，它们大多使用 ResNet [26]、DarkNet [63] 或 DLA [87]，然后使用 CSPNet [80] 策略来优化架构。 本文提出的方法的发展方向与当前主流的实时目标检测器不同。 除了架构优化之外，我们提出的方法将专注于训练过程的优化。 我们的重点将放在一些优化的模块和优化方法上，它们可能会增加训练成本以提高目标检测的准确性，但不会增加推理成本。我们将提出的模块和优化方法称为可训练的bag-of-freebies。</p><p>最近，模型重新参数化[13,12,29]和动态标签分配[20,17,42]已成为网络训练和目标检测的重要课题。 主要是在上述新概念提出之后，物体检测器的训练演变出了很多新的问题。 在本文中，我们将介绍我们发现的一些新问题，并设计解决这些问题的有效方法。对于模型重参数化，我们用梯度传播路径的概念分析了适用于不同网络层的模型重参数化策略，并提出了有计划的重参数化模型。 此外，当我们发现使用动态标签分配技术时，具有多个输出层的模型的训练会产生新的问题。即：“如何为不同分支的输出分配动态目标？” 针对这个问题，我们提出了一种新的标签分配方法，称为coarse-to-fine引导式标签分配</p><p>本文的贡献总结如下：（1）我们设计了几种可训练的bag-of-freebies方法，使得实时目标检测可以在不增加推理成本的情况下大大提高检测精度；(2) 对于目标检测方法的演进，我们发现了两个新问题，即重新参数化的模块如何替换原始模块，以及动态标签分配策略如何处理分配给不同输出层的问题。 此外，我们还提出了解决这些问题所带来的困难的方法；(3) 我们提出了实时目标检测器的“扩展”和“复合缩放”方法，可以有效地利用参数和计算；  (4) 我们提出的方法可以有效减少最先进实时目标检测器约40%的参数和50%的计算量，并具有更快的推理速度和更高的检测精度。</p><blockquote><h3 id="本文主要工作"><a href="#本文主要工作" class="headerlink" title="本文主要工作"></a>本文主要工作</h3><ul><li>提出了一个实时对象检测器，主要是希望它能够从边缘到云端同时支持移动GPU和GPU设备</li><li>优化了架构，专注于优化训练过程。重点放在优化模块和优化方法上，称为可训练的<strong>Bag of freebies</strong></li></ul><blockquote><p>其实关于<strong>Bag of freebies</strong>和<strong>Bag of specials</strong>我们在YOLOv4就见过，现在来回顾一下：</p><p>Bag of freebies：字面意思就是“免费赠品”。在这里就是指用一些比较有用的训练技巧来训练模型，  <strong>只会改变训练策略或只会增加训练成本(不增加推理成本)的方法。</strong>从而使得模型获得更好的准确率但<strong>不增加模型的复杂度</strong>，也就<strong>不会增加推理的计算量。</strong><br>Bag of specials：指一些插件模块(plugin modules)和后处理方法(post-processing methods)，  <strong>它们只稍微增加了推理成本，但可以极大的提高目标检测的准确度。</strong>  一般来说，这些插件用来提高一个模型中的特定属性。比如<strong>增加感受野( SPP、  ASPP、   RFB)，引入注意力机制(spatial attention、channel attention)，提高特征整合的能力(FPN、ASFF、BiFPN)。</strong></p></blockquote><ul><li>对于模型重参数化问题，本文使用梯度传播路径的概念分析了<strong>适用于不同网络中的层的模型重参数化策略，并提出了有计划的重参数化模型。</strong></li><li>对于动态标签分配问题，本文提出了<strong>一种新的标签分配方法，称为由粗到细引导标签分配</strong></li></ul><h4 id="本文主要贡献"><a href="#本文主要贡献" class="headerlink" title="本文主要贡献"></a>本文主要贡献</h4><p>(1)提出了几种可用于训练的方法，这些方法<strong>仅仅会增加训练上的负担用于提升model性能，而不会增加推理负担</strong></p><p>(2)对于目标检测方法的发展，作者发现了两个新问题： </p><ul><li><strong>①重参数化模块如何替换原始模块</strong></li><li><strong>②动态标签分配策略如何处理对不同输出层的分配</strong></li></ul></blockquote><p>不过本文提出了解决这俩问题的方法</p><p>(3)作者针对目标检测可以更有效的利用参数和计算问题，提出了<strong>“扩展”(extend)和“**</strong>复合缩放”(compound scaling)**</p><p>(4)提出的方法可以有效的减少40%参数量和50%计算量，高精度高速度</p><h2 id="二、Related-work—相关工作"><a href="#二、Related-work—相关工作" class="headerlink" title="二、Related work—相关工作"></a>二、Related work—相关工作</h2><h3 id="2-1-Real-time-object-detectors—实时物体检测器"><a href="#2-1-Real-time-object-detectors—实时物体检测器" class="headerlink" title="2.1 Real-time object detectors—实时物体检测器"></a>2.1 Real-time object detectors—实时物体检测器</h3><p>目前最先进的实时目标检测器主要基于 YOLO [61, 62, 63] 和 FCOS [76, 77]，分别是 [3, 79, 81, 21, 54, 85, 23]  . 能够成为最先进的实时目标检测器通常需要以下特性：（1）更快更强的网络架构；  (2) 更有效的特征整合方法[22, 97, 37, 74, 59, 30, 9, 45]； (3) 更准确的检测方法 [76, 77, 69];  (4) 更稳健的损失函数 [96, 64, 6, 56, 95, 57]；  (5) 一种更有效的标签分配方法 [99, 20, 17, 82, 42]；  (6) 更有效的训练方法。 在本文中，我们不打算探索需要额外数据或大型模型的自我监督学习或知识蒸馏方法。 相反，我们将针对与上述 (4)、(5) 和 (6) 相关的最先进方法衍生的问题设计新的可训练bag-of-freebies方法。</p><blockquote><p>先进的网络应该具有以下特性：</p><p>(1)更快更有效的<strong>网络</strong></p><p>(2)更有效的<strong>特征集成方法</strong></p><p>(3)更准确的<strong>检测方法</strong></p><p>(4)更有鲁棒性的<strong>损失函数</strong></p><p>(5)更有效的<strong>标签匹配方法</strong></p><p>(6)更有效的<strong>训练方法</strong></p><p>本文中主要针对(4)、(5)、(6)。</p></blockquote><h3 id="2-2-Model-re-parameterization—模型重新参数化"><a href="#2-2-Model-re-parameterization—模型重新参数化" class="headerlink" title="2.2 Model re-parameterization—模型重新参数化"></a>2.2 Model re-parameterization—模型重新参数化</h3><p>模型重新参数化技术 [71、31、75、19、33、11、4、24、13、12、10、29、14、78] 在推理阶段将多个计算模块合并为一个。 模型重参数化技术可以看作是一种集成技术，我们可以将其分为两类，即模块级集成和模型级集成。 模型级别的重新参数化有两种常见的做法来获得最终的推理模型。 一种是用不同的训练数据训练多个相同的模型，然后对多个训练模型的权重进行平均。 另一种是对不同迭代次数的模型权重进行加权平均。 模块级重新参数化是最近比较热门的研究问题。这种方法在训练时将一个模块拆分为多个相同或不同的模块分支，在推理时将多个分支模块整合为一个完全等效的模块。然而，并非所有提出的重新参数化模块都可以完美地应用于不同的架构。 考虑到这一点，我们开发了新的重新参数化模块，并为各种架构设计了相关的应用策略。</p><blockquote><h4 id="模型重新参数化的介绍"><a href="#模型重新参数化的介绍" class="headerlink" title="模型重新参数化的介绍"></a>模型重新参数化的介绍</h4><p>在<strong>训练</strong>时<strong>将一个模块拆分为多个相同或不同的模块分支</strong>；在<strong>推理</strong>时<strong>将多个分支模块整合为一个完全等效的模块</strong>。   </p><p>有两种方法：即模块级集成和模型级集成</p><h4 id="获得最终推理模型的两种方法"><a href="#获得最终推理模型的两种方法" class="headerlink" title="获得最终推理模型的两种方法"></a>获得最终推理模型的两种方法</h4><p>(1)用不同的训练数据训练多个相同的模型，然后对多个训练模型的权重进行平均</p><p>(2)对不同迭代次数的模型权重进行加权平均</p><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul><li>训练时，采用多分支的网络使模型获取更好的特征表达</li><li>推理时，将并行融合成串行，从而降低计算量和参数量，提升速度(融合后理论上和融合前识别效果一样，实际基本都是稍微降低一点点)</li></ul><h4 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h4><p>并不是所有提出的重新参数化模块都可以完美地应用于不同的架构。</p></blockquote><h3 id="2-3-Model-scaling—模型缩放"><a href="#2-3-Model-scaling—模型缩放" class="headerlink" title="2.3 Model scaling—模型缩放"></a>2.3 Model scaling—模型缩放</h3><p>模型缩放 [72, 60, 74, 73, 15, 16, 2, 51] 是一种放大或缩小已设计模型并使其适合不同计算设备的方法。 模型缩放方法通常使用不同的缩放因子，例如分辨率（输入图像的大小）、深度（层数）、宽度（通道数）和阶段（特征金字塔的数量），以实现良好的权衡 -off 表示网络参数的数量、计算量、推理速度和准确性。 网络架构搜索（NAS）是常用的模型缩放方法之一。  NAS 可以自动从搜索空间中搜索到合适的缩放因子，而无需定义过于复杂的规则。  NAS 的缺点是需要非常昂贵的计算来完成对模型缩放因子的搜索。 在[15]中，研究人员分析了缩放因子与参数数量和操作量之间的关系，试图直接估计一些规则，从而获得模型缩放所需的缩放因子。 查阅文献，我们发现几乎所有模型缩放方法都独立分析单个缩放因子，甚至复合缩放类别中的方法也独立优化缩放因子。 这样做的原因是因为大多数流行的 NAS 架构处理的比例因子不是很相关。 我们观察到，所有基于连接的模型，例如 DenseNet [32] 或 VoVNet [39]，都会在缩放此类模型的深度时改变某些层的输入宽度。 由于提出的架构是基于串联的我们必须为此模型设计一种新的复合缩放方法。</p><blockquote><h4 id="常采用的缩放因子"><a href="#常采用的缩放因子" class="headerlink" title="常采用的缩放因子"></a>常采用的缩放因子</h4><ul><li>分辨率(输入图像的大小)</li><li>深度(层数)<br>宽度(通道数)</li><li>阶段(特征金字塔的数量)</li></ul><h4 id="常用方法：NAS"><a href="#常用方法：NAS" class="headerlink" title="常用方法：NAS"></a>常用方法：NAS</h4><p><strong>介绍：</strong>NAS即模型搜索，其主要思路就是不需要人为去设计特定的网络，而是让模型自己去选择</p><p><strong>注意问题：</strong>    </p><pre><code>(1)怎么定义候选空间</code></pre><p>(2)加速训练   </p><p><strong>不足：</strong>消耗大量的时间和资源</p></blockquote><h2 id="三、Architecture—网络结构"><a href="#三、Architecture—网络结构" class="headerlink" title="三、Architecture—网络结构"></a>三、Architecture—网络结构</h2><h3 id="3-1-Extended-efficient-layer-aggregation-networks—扩展的高效层聚合网络"><a href="#3-1-Extended-efficient-layer-aggregation-networks—扩展的高效层聚合网络" class="headerlink" title="3.1 Extended efficient layer aggregation networks—扩展的高效层聚合网络"></a><strong>3.1 Extended efficient layer aggregation networks—扩展的高效层聚合网络</strong></h3><p>在大多数关于设计高效架构的文献中，主要考虑因素不超过参数的数量、计算量和计算密度。Ma 等人还从内存访问成本的特点出发，分析了输入/输出通道比、架构的分支数量以及 element-wise 操作对网络推理速度的影响。多尔阿尔等人在执行模型缩放时还考虑了激活，即更多地考虑卷积层输出张量中的元素数量。 图 2（b）中 CSPVoVNet [79] 的设计是 VoVNet [39] 的变体。 除了考虑上述基本设计问题外，CSPVoVNet [79] 的架构还分析了梯度路径，以使不同层的权重能够学习更多不同的特征。上述梯度分析方法使推理更快、更准确。 图 2 (c) 中的 ELAN [1] 考虑了以下设计策略——“如何设计一个高效的网络？”。 他们得出了一个结论：通过控制最短最长的梯度路径，更深的网络可以有效地学习和收敛。 在本文中，我们提出了基于 ELAN 的扩展 ELAN（E-ELAN），其主要架构如图 2（d）所示。</p><p>无论梯度路径长度和大规模 ELAN 中计算块的堆叠数量如何，它都达到了稳定状态。 如果无限堆叠更多的计算块，可能会破坏这种稳定状态，参数利用率会降低。 提出的E-ELAN使用expand、shuffle、merge cardinality来实现在不破坏原有梯度路径的情况下不断增强网络学习能力的能力。在架构方面，E-ELAN 只改变了计算块的架构，而过渡层的架构完全没有改变。 我们的策略是使用组卷积来扩展计算块的通道和基数。 我们将对计算层的所有计算块应用相同的组参数和通道乘数。 然后，每个计算块计算出的特征图会根据设置的组参数g被打乱成g个组，然后将它们连接在一起。 此时，每组特征图的通道数将与原始架构中的通道数相同。 最后，我们添加 g 组特征图来执行合并基数。 除了保持原有的 ELAN 设计架构，E-ELAN 还可以引导不同组的计算块学习更多样化的特征。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Figure2.png" alt=""></p><blockquote><h4 id="（a）VoVNet"><a href="#（a）VoVNet" class="headerlink" title="（a）VoVNet"></a><strong>（a）VoVNet</strong></h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-VoVNet.png" alt=""></p><p><strong>VoVNet</strong>是一个基于连接的模型，由OSA组成，将 DenseNet 改进的更高效，区别于常见的plain结构和残差结构。</p><p>这种结构<strong>不仅继承了 DenseNet 的多感受野表示多种特征的优点</strong>，也解决了<strong>密集连接效率低下的问题</strong>。</p><h4 id="（b）CSPVoVNet"><a href="#（b）CSPVoVNet" class="headerlink" title="（b）CSPVoVNet"></a><strong>（b）CSPVoVNet</strong></h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-CSPVoVNet.png" alt=""></p><p><strong>(b)是(a)的CSP变体</strong>，CSPVoVNet除了考虑到参数量、计算量、计算密度、ShuffleNet v2提出的内存访问成本(输入输出通道比，架构分支数量，element-wise等)，还分析了梯度路径，可以让不同层的权重学习到更具有区分性的特征。</p><h4 id="（c）ELAN"><a href="#（c）ELAN" class="headerlink" title="（c）ELAN"></a><strong>（c）ELAN</strong></h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-ELAN.png" alt=""></p><p>(c)就<strong>“如何设计一个高效的网络”</strong>得出结论：<strong>通过控制最短最长的梯度路径，更深的网络可以更有效地学习和收敛。</strong></p><blockquote><p><strong>梯度路径设计的优点与缺点</strong></p><p>  <strong>梯度路径设计策略总共有3个优点：</strong></p><ol><li><strong>可以有效地使用网络参数</strong> ，在这一部分中提出通过调整梯度传播路径，不同计算单元的权重可以学习各种信息，从而实现更高的参数利用效率</li><li><p><strong>具有稳定的模型学习能力，</strong>由于梯度路径设计策略直接确定并传播信息以更新权重到每个计算单元，因此所设计的架构可以避免训练期间的退化</p><ol><li><strong>具有高效的推理速度</strong>，梯度路径设计策略使得参数利用非常有效，因此网络可以在不增加额外复杂架构的情况下实现更高的精度。<br>由于上述原因，所设计的网络在架构上可以更轻、更简单。</li></ol><p><strong>梯度路径设计策略有1个缺点：</strong></p></li><li><p>当梯度更新路径<strong>不是网络的简单反向前馈路径</strong>时，编程的难度将大大增加。</p></li></ol></blockquote><h4 id="d-E-ELAN"><a href="#d-E-ELAN" class="headerlink" title="(d) E-ELAN"></a>(d) E-ELAN</h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-E-ELAN.png" alt=""></p><p>(d)是作者提出的 <strong>Extended-ELAN (E-ELAN)</strong> ，E-ELAN 使用了 <strong>expand、shuffle、merge cardinality</strong>来实现对 ELAN 网络的增强。</p><p>在架构方面，E-ELAN <strong>只改变了计算块(computation blocks)的架构，而过渡层(transition layer)的架构完全没有改变</strong>。 </p><p><strong>YOLOv7 的策略是使用组卷积来扩展计算块的通道和基数</strong>。</p><p><strong>实现方法</strong></p><ul><li>首先，使用<strong>组卷积</strong>来增大<strong>通道</strong>和<strong>计算块的基数</strong>  (所有计算块使用的组参数及通道乘数都相同)</li><li>接着，将<strong>计算块得到的特征图 shuffle 到 g 个组</strong>，然后 <strong>concat</strong>，这样一来每个组中的特征图通道数和初始结构的通道数是相同的</li><li>最后，将 g 个组的特征都相加</li></ul></blockquote><h3 id="3-2-Model-scaling-for-concatenation-based-models—-基于连接的模型的模型缩放"><a href="#3-2-Model-scaling-for-concatenation-based-models—-基于连接的模型的模型缩放" class="headerlink" title="3.2 Model scaling for concatenation-based models— 基于连接的模型的模型缩放"></a>3.2 Model scaling for concatenation-based models— 基于连接的模型的模型缩放</h3><p>模型缩放的主要目的是调整模型的一些属性，生成不同尺度的模型，以满足不同推理速度的需求。 例如，EfficientNet [72] 的缩放模型考虑了宽度、深度和分辨率。至于 scaled-YOLOv4 [79]，其缩放模型是调整阶段数。 在 [15] 中，Dollar等人。分析了香草卷积和组卷积在进行宽度和深度缩放时对参数量和计算量的影响，并以此设计了相应的模型缩放方法。<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Figure3.png" alt=""></p><p> 以上方法主要用在PlainNet或ResNet等架构中。 这些架构在进行扩容或缩容时，每一层的入度和出度都不会发生变化，因此我们可以独立分析每个缩放因子对参数量和计算量的影响。 然而，如果将这些方法应用于基于连接的架构，我们会发现当对深度进行放大或缩小时，紧接在基于连接的计算块之后的平移层的入度将减小或 增加，如图3（a）和（b）所示。</p><p>从上述现象可以推断，对于基于串联的模型，我们不能单独分析不同的缩放因子，而必须一起考虑。 以scaling-up depth为例，这样的动作会导致transition layer的输入通道和输出通道的比例发生变化，这可能会导致模型的硬件使用率下降。 因此，我们必须为基于级联的模型提出相应的复合模型缩放方法。 当我们缩放一个计算块的深度因子时，我们还必须计算该块的输出通道的变化。 然后，我们将对过渡层进行等量变化的宽度因子缩放，结果如图3（c）所示。 我们提出的复合缩放方法可以保持模型在初始设计时的特性并保持最佳结构。</p><blockquote><h4 id="模型缩放的主要目的"><a href="#模型缩放的主要目的" class="headerlink" title="模型缩放的主要目的"></a>模型缩放的主要目的</h4><p>模型缩放的主要目的是<strong>调整模型的某些属性，并生成不同比例的模型</strong>，以满足不同推理速度的需要【像V5和YOLOX】</p><h4 id="问题引入"><a href="#问题引入" class="headerlink" title="问题引入"></a>问题引入</h4><p>每个比例因子的参数和计算是可以独立分析的。然而，如果将这些方法应用于基于连接的模型架构，我们会发现<strong>当对深度进行放大或缩小时，后续网络层的输入width发生变化，使后续层的输入channel和输出channel的ratio发生变化，紧接在基于连接的计算块之 后的转化层(translation layer)的入度将减小或增加</strong>，从而导致模型的硬件使用率下降如图(a)-&gt;(b)的过程。<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Figure4.png" alt=""></p><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p>作者对于连接模型，提出了一种<strong>复合模型方法</strong>，在考虑计算模块深度因子缩放的同时也考虑过渡层宽度因子做同等量的变化</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Figure5.png" alt=""></p><p>当对连接结构的网络进行尺度缩放时，只缩放<strong>计算块</strong>的深度，转换层的其余部分只进行宽度的缩放</p></blockquote><h2 id="四、Trainable-bag-of-freebies一可训练的bag-of-freebies"><a href="#四、Trainable-bag-of-freebies一可训练的bag-of-freebies" class="headerlink" title="四、Trainable bag-of-freebies一可训练的bag-of-freebies"></a>四、Trainable bag-of-freebies一可训练的bag-of-freebies</h2><h3 id="4-1-Planned-re-parameterized-convolution—卷积重参化"><a href="#4-1-Planned-re-parameterized-convolution—卷积重参化" class="headerlink" title="4.1 Planned re-parameterized convolution—卷积重参化"></a>4.1 Planned re-parameterized convolution—卷积重参化</h3><p>虽然 RepConv [13] 在 VGG [68] 上取得了优异的性能，但当我们直接将其应用于 ResNet [26] 和 DenseNet [32] 等架构时，其准确率会显着降低。 我们使用梯度流传播路径来分析重新参数化的卷积应该如何与不同的网络相结合。我们还相应地设计了计划的重新参数化卷积。</p><p>RepConv 实际上将 3×3 卷积、1×1 卷积和恒等连接组合在一个卷积层中。 在分析了 RepConv 和不同架构的组合和对应性能后，我们发现 RepConv 中的恒等连接破坏了 ResNet 中的残差和 DenseNet 中的连接，为不同的特征图提供了更多的梯度多样性。 基于以上原因，我们使用无身份连接的 RepConv (RepConvN) 来设计计划重参数化卷积的架构。 在我们的想法中，当一个带有残差或连接的卷积层被重新参数化的卷积代替时，应该没有恒等连接。 图 4 显示了我们设计的用于 PlainNet 和 ResNet 的“计划重新参数化卷积”的示例。 至于基于残差模型和基于连接模型的完整计划的重新参数化卷积实验，将在消融研究部分上进行介绍。</p><blockquote><h4 id="回顾RepVGG"><a href="#回顾RepVGG" class="headerlink" title="回顾RepVGG"></a>回顾RepVGG</h4><p>RepVGG是采用了VGG风格进行搭建的，采用了重参数化技术，因此叫RepVGG。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Figure22.png" alt=""></p><p><strong>A)是ResNet结构</strong>，最上面的部分的identity采用了1×1卷积，而RepVGG大体结构与ResNet相似</p><p><strong>(B)图是RepVGG训练时结构</strong>，借鉴了ResNet的residual block的结构，具体包括3×3、1×1、shortcut三个分支。</p><ul><li><strong>当输入输出的维度不一致</strong>，即stride=2时，则只有3×3、1×1两个分支</li><li><strong>当输入输出的维度一致时</strong>，在后三层卷积中不仅有1×1的identity connection，还有一个无卷积的直接进行特征融合的identity connection</li></ul><p><strong>(C)图是RepVGG测试时结构</strong>，会把这些连接全部去掉，就变成了一个单一的VGG结构，这种操作也被称为训练与预测的解耦合</p><h4 id="问题引入-1"><a href="#问题引入-1" class="headerlink" title="问题引入"></a>问题引入</h4><p>RepConv中带有的 identity connection破坏了ResNet中的残差和DenseNet中的concatenation，而我们知道残差和concatenation为不同的特征图提供了更多的梯度多样性，这样一来会导致精度下降。所以当一个带有残差或concatenation的卷积层被重参数的卷积所取代时，应该不存在 identity connection</p><h4 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h4><p>基于此，作者使用无identity connection的RepConv (RepConvN)来设计规划重参化卷积结构，如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Figure44.png" alt=""></p></blockquote><h3 id="4-2-Coarse-for-auxiliary-and-fine-for-lead-loss—辅助训练模块"><a href="#4-2-Coarse-for-auxiliary-and-fine-for-lead-loss—辅助训练模块" class="headerlink" title="4.2 Coarse for auxiliary and fine for lead loss—辅助训练模块"></a>4.2 Coarse for auxiliary and fine for lead loss—辅助训练模块</h3><p>我的个人理解就是：</p><p><strong>Lead head</strong>是负责<strong>最终输出</strong>的 head ；<strong>Auxiliary head</strong>是负责<strong>辅助训练</strong>的 head</p><p>那么这个标题就是在说负责辅助训练的head用粗标签，负责最终输出的head用细标签</p><h4 id="4-2-1-Deep-supervision—深度监督"><a href="#4-2-1-Deep-supervision—深度监督" class="headerlink" title="4.2.1 Deep supervision—深度监督"></a>4.2.1 Deep supervision—深度监督</h4><p>深度监督 [38] 是一种经常用于训练深度网络的技术。 它的主要概念是在网络的中间层添加额外的辅助头，以辅助损失为指导的浅层网络权重。 即使对于 ResNet [26] 和 DenseNet [32] 等通常收敛良好的架构，深度监督 [70, 98, 67, 47, 82, 65, 86, 50] 仍然可以显着提高模型在许多任务上的性能 . 图 5 (a) 和 (b) 分别显示了“没有”和“有”深度监督的目标检测器架构。 在本文中，我们将负责最终输出的head称为lead head，用于辅助训练的head称为辅助head。<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Figure55.png" alt=""></p><blockquote><p>深度监督是一种常用于训练深度网络的技术。其主要概念是在网络的中间层增加额外的Auxiliary head，以及以Auxiliary损失为导向的浅层网络权值。</p><p>下图(A)是无深度监督，  (B)是有深度监督</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-%E7%9B%91%E7%9D%A3.png" alt=""></p><h4 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h4><p>将负责<strong>最终输出的head</strong>为<strong>Lead head</strong>，将用于<strong>辅助训练的head</strong>称为<strong>Auxiliary head</strong>。</p></blockquote><h4 id="4-2-2-Label-assigner—-标签分配器"><a href="#4-2-2-Label-assigner—-标签分配器" class="headerlink" title="4.2.2 Label assigner— 标签分配器"></a>4.2.2 <strong>Label assigner— 标签分配器</strong></h4><p>接下来我们要讨论标签分配的问题。 过去，在深度网络的训练中，标签分配通常直接指的是ground truth，并根据给定的规则生成hard  label。 然而，近年来，如果我们以物体检测为例，研究人员往往会利用网络预测输出的质量和分布，然后与ground truth一起考虑使用一些计算和优化方法来生成可靠的soft label [61、8、36、99、91、44、43、90、20、17、42]。例如，YOLO [61] 使用预测边界框回归和地面实况的 IoU 作为 objectness 的soft label。 在本文中，我们将网络预测结果与基本事实一起考虑，然后分配soft label的机制称为“标签分配器”。</p><p>无论auxiliary head or lead head的情况如何，都需要对目标目标进行深度监督。 在soft label分配器相关技术的过程中，我们无意中发现了一个新的衍生问题，即“如何将soft label分配给auxiliary head and lead head？” 据我们所知，到目前为止，相关文献还没有探讨过这个问题。 目前最流行的方法的结果如图5（c）所示，就是将辅助head和lead head分离出来，然后利用各自的预测结果和ground truth进行标签分配。 本文提出的方法是一种新的标签分配方法，通过前导头预测来指导辅助头和前导头。换句话说，我们使用引导头预测作为指导来生成从coarse-to-fine的分层标签，这些标签分别用于辅助头部和引导头学习。 提出的两种深度监督标签分配策略分别如图 5 (d) 和 (e) 所示。</p><blockquote><h4 id="以前方法"><a href="#以前方法" class="headerlink" title="以前方法"></a>以前方法</h4><p> 过去，在深度网络的训练中，标签分配通常直接参考GT，并根据给定的规则生成hard label</p><h4 id="本文方法-1"><a href="#本文方法-1" class="headerlink" title="本文方法"></a>本文方法</h4><p> 作者提出一个“label assigner(标签分配器)” 机制，该机制将网络预测结果与GT一起考虑，然后分配soft label</p><blockquote><h4 id="关于hard-label和soft-label："><a href="#关于hard-label和soft-label：" class="headerlink" title="关于hard label和soft label："></a>关于hard label和soft label：</h4><ul><li>hard label：有些论文中也称为hard target ，其实这也是借鉴了知识蒸馏的思想，hard字面意思上就可以看出比较强硬，是就是，不是就是不是，标签形式：(1,2,3…)或(0,1,0,0…)【举个栗子：要么是猫要么是狗】</li><li>soft label：soft是以概率的形式来表示。可理解为对标签的平滑也即软化，比如像[0.6,0.4]，【举个栗子：有60%的概率是猫，  40%的概率是狗】，就好像不会给你非常明确的回答。</li></ul><p>现在比较流行的结构中，经常会将网络输出的数据分布通过一定的优化方法等与GT进行匹配生成soft label（其实我们熟悉的经过softmax的或者sigmod输出就是一种soft label）。</p></blockquote><h4 id="问题引入-2"><a href="#问题引入-2" class="headerlink" title="问题引入"></a>问题引入</h4><p>如何为Auxiliary head和Lead head分配soft label？</p><h4 id="目前方法"><a href="#目前方法" class="headerlink" title="目前方法"></a>目前方法</h4><p>如(c)所示，这是独立标签匹配结构，将Auxiliary head和Lead head分离，然后使用它们自己的预测结果和真实标签来进行标签分配。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-IndepA.png" alt=""></p></blockquote><h4 id="4-2-3-Lead-head-guided-label-assigner—-先导头导向标签分配器"><a href="#4-2-3-Lead-head-guided-label-assigner—-先导头导向标签分配器" class="headerlink" title="4.2.3 Lead head guided label assigner— 先导头导向标签分配器"></a>4.2.3 Lead head guided label assigner— 先导头导向标签分配器</h4><p><strong>(Lead head guided label assigner) lead head导向标签分配器</strong> 主要基于lead head的预测结果和GT来计算，并通过优化过程生成soft标签。这组soft标签将用作辅助head和lead head的训练。这样做的原因是因为lead head具有相对较强的学习能力，因此由其生成的soft标签应更能代表源数据和目标之间的分布和相关性。此外，我们可以将这种学习视为一种广义残差学习。通过让较浅的辅助head直接学习lead head已经学习的信息，lead head将更能够专注于学习尚未学习的剩余(residual )信息。</p><blockquote><h4 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h4><p>  主要基于<strong>Lead head的预测结果和GT</strong>来计算，并通过优化过程生成<strong>soft label</strong> 。这组<strong>soft label</strong> 将用作Auxiliary head和Lead head的训练。</p><h4 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h4><ul><li>Lead head 的学习能力更强一些，所以从它当中得到的soft label在数据集的数据概率中更具有代表性。</li><li>Lead head 能够只学习 Auxiliary head 没有学习到的特征。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-LeadGuA.png" alt=""></p></blockquote><h4 id="4-2-4-Coarse-to-fine-lead-head-guided-label-assigner—-由粗到精的-lead-head-指导标签分配器"><a href="#4-2-4-Coarse-to-fine-lead-head-guided-label-assigner—-由粗到精的-lead-head-指导标签分配器" class="headerlink" title="4.2.4 Coarse-to-fine lead head guided label assigner— 由粗到精的 lead head 指导标签分配器"></a>4.2.4 Coarse-to-fine lead head guided label assigner— 由粗到精的 lead head 指导标签分配器</h4><p>(Coarse-to-fine lead head guided label assigner)由粗至细的lead head引导标签分配器 ，也使用lead head的预测结果和GT生成soft标签。然而，在此过程中，生成了两组不同的soft标签，即粗标签和细标签，其中细标签与lead head引导标签分配器生成的soft标签相同，并且通过放松正样本分配过程的约束，允许更多grid被视为正目标来生成粗标签。其原因是辅助head的学习能力不如lead head强，为了避免丢失需要学习的信息，将重点优化目标检测任务中辅助head的召回。对于lead head的输出，可以从高召回率结果中过滤出高精度结果作为最终输出。但是必须注意，如果粗标签的附加权重与细标签很接近，它可能在最终预测时产生不良先验。因此为了使这些超粗正网格具有较小的影响，在解码器中设置了限制，使得超粗正grid不能完美地产生soft标签。上述机制允许在学习过程中动态调整细标签和粗标签的重要性，并且使细标签的优化上界总是高于粗标签。</p><blockquote><h4 id="具体方法-1"><a href="#具体方法-1" class="headerlink" title="具体方法"></a>具体方法</h4><p>在这个过程中生成了两组不同的soft label，即粗标签和细标签</p><ul><li>细标签与Lead head 在标签分配器上生成的soft label相同</li><li>粗标签是通过放宽认定positive target的条件生成的，也就是允许更多的grids作为positive target</li></ul><blockquote><p>粗标签和细标签究竟是什么？<br>首先是细标签，网络最终输出的三个head是Lead head，会将这部分的预测结果与ground truth生成soft label，网络会觉得这个soft label得到是数据分布更接近真实的数据分布，训练得到的内容更加“细致”，<br>再来说说粗标签，Auxiliary head由于是从中间网络部分得到的，它的预测效果肯定是没有深层网络Lead head提取到的数据或者特征更细致，所以Auxiliary head部分的内容是比较“粗糙”的，在训练过程中，会将Lead head与ground truth的soft label当成一个全新的ground truth，然后与Auxiliary head之间建立损失函数，说白了，就是让辅助head的预测结果也“近似”为Lead head<br>—————————以上引用@爱吃肉的鹏 的解读，感谢大佬！</p></blockquote><h4 id="原因-1"><a href="#原因-1" class="headerlink" title="原因"></a>原因</h4><p>Auxiliary head 的学习能力没有Lead head 那么强大，为了避免信息丢失，需要接收更多的信息来学习，在目标检测任务中， Auxiliary head 需要有很高的 recall。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Coarse.png" alt=""></p><p><strong>【注意】</strong>如果粗label非常接近与细label，可能会产生不好的结果</p><p><strong>【解决方法】</strong> 为了让超粗正grids(Auxiliary head 的正标签)影响小一些，在 decoder 中设置了限制条件，所以那些超粗正grids不能产生 soft label，这样就能在训练的过程中，让grids自动的调节粗标签和细标签的重要程度。</p></blockquote><h3 id="4-3-Other-trainable-bag-of-freebies—其他的可训练“工具”"><a href="#4-3-Other-trainable-bag-of-freebies—其他的可训练“工具”" class="headerlink" title="4.3 Other trainable bag-of-freebies—其他的可训练“工具”"></a>4.3 Other trainable bag-of-freebies—其他的可训练“工具”</h3><p>本节中，我们将列出一些可训练的免费赠品包。 这些免费赠品是我们在训练中使用的一些技巧，但最初的概念并不是我们提出的。 这些freebies的训练细节将在附录中详细说明，包括（1）conv-bn-activation topology中的Batch normalization：这部分主要将batch normalization layer直接连接到convolutional layer。这样做的目的是在推理阶段将批归一化的均值和方差整合到卷积层的偏差和权重中。  (2) YOLOR[81]中的隐式知识与卷积特征图相结合的加法和乘法方式：YOLOR中的隐式知识可以在推理阶段通过预计算简化为向量。 该向量可以与前一个或后一个卷积层的偏差和权重相结合。  (3) EMA 模型：EMA 是一种在 mean teacher [75] 中使用的技术，在我们的系统中，我们使用 EMA 模型纯粹作为最终的推理模式</p><blockquote><p>(1) conv-bn-activation 结构组合中的批量归一化</p><p>(2)  YOLOR中的隐式知识与卷积特征图以及加法和乘法方式相结合</p><p>(3)  EMA模型</p><p>这个详细解读大家看上面翻译就好~</p></blockquote><h2 id="五、Experiments—实验"><a href="#五、Experiments—实验" class="headerlink" title="五、Experiments—实验"></a>五、Experiments—实验</h2><h3 id="5-1-Experimental-setup—实验步骤"><a href="#5-1-Experimental-setup—实验步骤" class="headerlink" title="5.1 Experimental setup—实验步骤"></a>5.1 Experimental setup—实验步骤</h3><p>我们使用 Microsoft COCO 数据集进行实验并验证我们的对象检测方法。 我们所有的实验都没有使用预训练模型。 也就是说，所有模型都是从头开始训练的。 在开发过程中，我们使用train 2017 set进行训练，然后使用val 2017 set进行验证和选择超参数。 最后，我们展示了对象检测在 2017 年测试集上的性能，并将其与最先进的对象检测算法进行了比较。 详细的训练参数设置在附录中描述。</p><p>我们设计了边缘 GPU、普通 GPU 和云 GPU 的基本模型，它们分别称为 YOLOv7tiny、YOLOv7 和 YOLOv7-W6。 同时，我们还针对不同的业务需求，使用基础模型进行模型缩放，得到不同类型的模型。 对于YOLOv7，我们对颈部进行stack scaling，并使用提出的复合缩放方法对整个模型的深度和宽度进行缩放，并以此获得YOLOv7-X。 对于 YOLOv7-W6，我们使用新提出的复合缩放方法得到 YOLOv7-E6 和 YOLOv7-D6。 此外，我们将提议的 EELAN 用于 YOLOv7-E6，从而完成了 YOLOv7E6E。 由于 YOLOv7-tiny 是一个面向边缘 GPU 的架构，它会使用leaky ReLU 作为激活函数。 至于其他模型，我们使用 SiLU 作为激活函数。 我们将在附录中详细描述每个模型的比例因子。</p><blockquote><ul><li>数据集： COCO 数据集</li><li>预训练模型： 无，从0开始训练</li><li><p>不同GPU和对应模型：</p><ul><li><p>边缘GPU：YOLOv7-tiny</p></li><li><p>普通GPU：YOLOv7</p></li><li><p>云GPU的基本模型： YOLOv7-W6</p></li></ul></li><li><p>激活函数：</p><ul><li><p>YOLOv7 tiny： leaky ReLU</p></li><li><p>其他模型：SiLU</p></li></ul></li></ul></blockquote><h3 id="5-2-Baselines—基线网络"><a href="#5-2-Baselines—基线网络" class="headerlink" title="5.2 Baselines—基线网络"></a>5.2 Baselines—基线网络</h3><p>我们选择以前版本的 YOLO [3, 79] 和最先进的目标检测器 YOLOR [81] 作为我们的基线。 表 1 显示了我们提出的 YOLOv7 模型与使用相同设置训练的基线的比较。</p><p>从结果我们看到，如果与 YOLOv4 相比，YOLOv7 的参数减少了 75%，计算量减少了 36%，AP 提高了 1.5%。 如果与 state-of-theart YOLOR-CSP 相比，YOLOv7 的参数减少了 43%，计算量减少了 15%，AP 提高了 0.4%。 在tiny模型的性能上，与YOLOv4-tiny-31相比，YOLOv7tiny参数数量减少了39%，计算量减少了49%，但AP保持不变。在云 GPU 模型上，我们的模型仍然可以有更高的 AP，同时减少 19% 的参数数量和 33% 的计算量。</p><blockquote><p>选择以前版本的YOLO[YOLOv4 ，Scaled-YOLOv4]和最先进的目标检测器YOLOR作为基线。表1展示了本文提出的YOLOv7模型与使 用相同设置训练的基线模型的比较。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Table1.png" alt=""></p><p>表1：基线目标探测器的比较</p><p>结论：具体数值就不重复了，通过对比我们可以看出参数量和计算量都有减少，精确度提高了。</p></blockquote><h3 id="5-3-Comparison-with-state-of-the-arts—与其他流行网络的对比"><a href="#5-3-Comparison-with-state-of-the-arts—与其他流行网络的对比" class="headerlink" title="5.3 Comparison with state-of-the-arts—与其他流行网络的对比"></a>5.3 Comparison with state-of-the-arts—与其他流行网络的对比</h3><p>我们将所提出的方法与用于通用 GPU 和移动 GPU 的最先进的目标检测器进行比较，结果如表 2 所示。从表 2 的结果中，我们知道所提出的方法具有最佳的速度-准确度权衡 - 全面关闭。 如果我们将 YOLOv7-tiny-SiLU 与 YOLOv5-N (r6.1) 进行比较，我们的方法在 AP 上的速度提高了 127 fps，准确率提高了 10.7%。 此外，YOLOv7 161 fps 的帧率有 51.4% AP，而相同 AP 的 PPYOLOE-L 只有 78 fps 的帧率。 在参数使用方面，YOLOv7 比 PPYOLOE-L 少 41%。 如果我们将推理速度为 114 fps 的 YOLOv7-X 与推理速度为 99 fps 的 YOLOv5-L (r6.1) 进行比较，YOLOv7-X 可以将 AP 提高 3.9%。 如果将 YOLOv7X 与类似规模的 YOLOv5-X (r6.1) 进行比较，YOLOv7-X 的推理速度快了 31 fps。 此外，在参数量和计算量方面，YOLOv7-X 相比 YOLOv5-X（r6.1）减少了 22% 的参数和 8% 的计算量，但 AP 提升了 2.2%。</p><p>如果我们使用输入分辨率 1280 比较 YOLOv7 和 YOLOR，YOLOv7-W6 的推理速度比 YOLOR-P6 快 8 fps，检测率也提高了 1% AP。 至于YOLOv7-E6和YOLOv5-X6（r6.1）的对比，前者AP增益比后者高0.9%，参数少45%，计算量少63%，推理速度提升47%。YOLOv7-D6 的推理速度与 YOLOR-E6 接近，但 AP 提高了 0.8%。  YOLOv7-E6E 的推理速度与 YOLOR-D6 接近，但 AP 提高了 0.3%。</p><blockquote><p>本文将提出的方法与通用GPU和移动GPU的最先进的对象检测器进行了比较，结果如表2所示：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Table2.png" alt=""></p><p>结论：反正是打败了它们，目前我们最牛就对咯</p></blockquote><h3 id="5-4-Ablation-study—消融研究"><a href="#5-4-Ablation-study—消融研究" class="headerlink" title="5.4 Ablation study—消融研究"></a>5.4 Ablation study—消融研究</h3><h4 id="5-4-1-Proposed-compound-scaling-method—提出的复合-scaling-方法"><a href="#5-4-1-Proposed-compound-scaling-method—提出的复合-scaling-方法" class="headerlink" title="5.4.1 Proposed compound scaling method—提出的复合 scaling 方法"></a>5.4.1 Proposed compound scaling method—提出的复合 scaling 方法</h4><p>显示了使用不同模型缩放策略进行缩放时获得的结果。 其中，我们提出的复合缩放方法是将计算块的深度放大1.5倍，将过渡块的宽度放大1.25倍。 如果我们的方法与仅按比例放大宽度的方法相比，我们的方法可以以更少的参数和计算量将 AP 提高 0.5%。 如果我们的方法与只放大深度的方法相比，我们的方法只需增加2.9%的参数数量和1.2%的计算量，就可以提高0.2%的AP。</p><blockquote><p>表3显示了使用不同模型缩放策略进行缩放时获得的结果：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Table3.png" alt=""></p><p>结论：复合缩放策略可以更有效地利用参数和计算。</p></blockquote><h4 id="5-4-2-Proposed-planned-re-parameterized-model—提出的-planned-re-parameterized-模型"><a href="#5-4-2-Proposed-planned-re-parameterized-model—提出的-planned-re-parameterized-模型" class="headerlink" title="5.4.2 Proposed planned re-parameterized model—提出的 planned re-parameterized 模型"></a>5.4.2 Proposed planned re-parameterized model—提出的 planned re-parameterized 模型</h4><p>为了验证我们提出的平面重参数化模型的普遍性，我们分别将其用于基于连接的模型和基于残差的模型进行验证。 我们选择用于验证的基于连接的模型和基于残差的模型分别是 3-stacked ELAN 和 CSPDarknet。</p><p>在基于 concatenation 的模型实验中，我们将 3-stacked ELAN 中不同位置的 3×3 卷积层替换为 RepConv，详细配置如图 6 所示。从表 4 所示的结果可以看出，所有更高 AP 值存在于我们提出的计划重新参数化模型中。</p><p>在处理基于残差的模型的实验中，由于原始dark block没有 3 × 3 con卷积块符合我们的设计策略，我们额外设计了一个反向dark block进行实验，其架构如图 7 所示。由于具有dark block和反向dark block的 CSPDarknet 具有完全相同的参数和操作量，所以它是 公平比较。 表 5 所示的实验结果充分证实了所提出的计划重新参数化模型在基于残差的模型上同样有效。 我们发现 RepCSPResNet [85] 的设计也符合我们的设计模式</p><blockquote><h4 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h4><p>为了验证提出的 planned re-parameterized 模型的通用性，将其分别用于基于连接的模型和基于残差的模型进行验证。</p><h4 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h4><p>三层ELAN和CSPDarknet</p><p>将3层堆叠ELAN中不同位置的3 × 3卷积层替换为RepConv，详细配置如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-F6T4.png" alt=""></p><p><strong>结论：</strong>从表4所示的结果中，看到所有较高的AP值都出现在提出的Planned RepConcatenation 模型上。</p><p>在处理基于残差模型的实验中，由于原始dark block没有3×3的卷积块，作者另外设计了一种反向dark block，其体系结构如图7所示：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-F7T5.png" alt=""></p><p><strong>结论：</strong>表5所示的实验结果证实了所提出的重参化策略对于residual的模型依旧有效。 RepCSPResNet的设计也符合本文的设计模式</p></blockquote><h4 id="5-4-3-Proposed-assistant-loss-for-auxiliary-head—提出的-auxiliary-head-的-assistant-loss"><a href="#5-4-3-Proposed-assistant-loss-for-auxiliary-head—提出的-auxiliary-head-的-assistant-loss" class="headerlink" title="5.4.3 Proposed assistant loss for auxiliary head—提出的 auxiliary head 的 assistant loss"></a>5.4.3 Proposed assistant loss for auxiliary head—提出的 auxiliary head 的 assistant loss</h4><p>在辅助头部实验的辅助损失中，我们比较了引导头部和辅助头部方法的一般独立标签分配，并且我们还比较了两种提出的引导引导标签分配方法。 我们在表 6 中展示了所有比较结果。从表 6 中列出的结果可以看出，任何增加辅助损失的模型都可以显着提高整体性能。 此外，我们提出的引导式标签分配策略比 AP、AP50 和 AP75 中的一般独立标签分配策略具有更好的性能。 至于我们提出的粗略辅助和精细引导标签分配策略，它在所有情况下都会产生最佳结果。 在图 8 中，我们展示了在辅助头和前导头上通过不同方法预测的对象图。 从图 8 中我们发现，如果辅助头部学习引导引导soft label，它确实会帮助引导头部从一致目标中提取残差信息。</p><p>由于提出的 YOLOv7 使用多个金字塔来联合预测目标检测结果，我们可以直接将辅助头连接到中间层的金字塔进行训练。 这种训练可以弥补下一层金字塔预测中可能丢失的信息。由于上述原因，我们在提议的 E-ELAN 架构中设计了部分辅助头。 我们的方法是在合并基数之前在其中一组特征图之后连接辅助头，这种连接可以使新生成的一组特征图的权重不会被辅助损失直接更新。 我们的设计允许每个铅头金字塔仍然从不同大小的物体中获取信息。 表 8 显示了使用两种不同方法获得的结果，即粗到细导联方法和部分粗到细导联方法。 显然，部分由粗到细的导引法具有更好的辅助效果。</p><blockquote><p>作者比较了lead head和auxiliary head的独立标签分配策略，同时也比较了所提出的引导型标签分配方法，在表6中显示了所有的比较结果：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/YOLOV7-Table6.png" alt=""></p><p><strong>结论：</strong>从表6中列出的结果可以看出，任何增加辅助损失的模型都可以显著提高整体性能。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Figure8.png" alt=""></p><p><strong>结论：</strong>如果auxiliary head学习先导引导soft label，它确实会帮助lead head从一致目标中提取残差信息。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Table7.png" alt=""></p><p><strong>结论：</strong> 从表中的数字来看，通过距离目标中心的大小来约束目标的上界可以获得更好的性能。<br><strong>方法：</strong> E-ELAN 架构中设计了 partial auxiliary head 。在合并基数（ cardinality ）之前，在一组特征图后连接 auxiliary head ，这种连接可以使新生成的特征图集的权值不直接通过辅助损失来更新<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov7-Table8.png" alt=""></p><p><strong>结论：</strong> partial auxiliary head方法具有更好的辅助效果。</p></blockquote><h2 id="六、Conclusions—结论"><a href="#六、Conclusions—结论" class="headerlink" title="六、Conclusions—结论"></a>六、Conclusions—结论</h2><p>在本文中，我们提出了一种新的实时目标检测器架构和相应的模型缩放方法。 此外，我们发现目标检测方法的发展过程产生了新的研究课题。 在研究过程中，我们发现了重新参数化模块的替换问题和动态标签分配的分配问题。 为了解决这个问题，我们提出了可训练的bag-of-freebies方法来提高目标检测的准确性。 基于上述，我们开发了 YOLOv7 系列目标检测系统，该系统获得了 state-of-the-art 的结果。</p><blockquote><p>本文提出了一种新的实时检测器，解决了重参化模块的替换问题和动态标签的分配问题。</p><h4 id="主要工作："><a href="#主要工作：" class="headerlink" title="主要工作："></a>主要工作：</h4><p><strong>（ 1 ）高效聚合网络架构：</strong> YOLOV7 对 ELAN 进行扩展，提出的一个新的网络架构 E-ELAN ，以高效为主<br><strong>（ 2 ）重参数化卷积：</strong> YOLOV7 将模型重参数化引入到网络架构中，重参数化这一思想最早出现于 REPVGG 中<br><strong>（ 3 ）辅助头检测：</strong> YOLOv7 中将 head 部分的浅层特征提取出来作为 Auxiliary head ，深层特征也就是网络的最终输出作为 Lead head<br><strong>（ 4 ）基于连接的模型缩放：</strong> 作者对于连接模型提出了一种复合模型方法，当对连接结构的网络进行尺度缩放时，只缩放计算块的深度，转换层的其余部分只进行宽度的缩放<br><strong>（ 5 ）动态标签分配策略：</strong> Lead head 导向标签分配方法和由粗到精的 lead head 指导标签分配方法</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>yolov6-paper</title>
      <link href="/yolov6-paper/"/>
      <url>/yolov6-paper/</url>
      
        <content type="html"><![CDATA[<h1 id="YOLOv6-A-Single-Stage-Object-Detection-Framework-for-IndustrialApplications"><a href="#YOLOv6-A-Single-Stage-Object-Detection-Framework-for-IndustrialApplications" class="headerlink" title="YOLOv6: A Single-Stage Object Detection Framework for IndustrialApplications"></a>YOLOv6: A Single-Stage Object Detection Framework for IndustrialApplications</h1><h2 id="Abstract—摘要"><a href="#Abstract—摘要" class="headerlink" title="Abstract—摘要"></a>Abstract—摘要</h2><p>多年来，YOLO系列一直是高效物体检测的事实上的工业级标准。YOLO社区以压倒性的优势丰富了它在众多硬件平台和丰富场景中的应用。在这份技术报告中，我们努力把它的极限推到一个新的水平，以坚定不移的心态向行业应用迈进。考虑到现实环境中对速度和准确性的不同要求，我们广泛地研究了来自工业界或学术界的最新的物体检测进展。具体来说，我们大量吸收了最近的网络设计、训练策略、测试技术、量化和优化方法的思想。在此基础上，我们整合了我们的想法和实践，建立了一套不同规模的可部署的网络，以适应多样化的用例。在YOLO作者的慷慨许可下，我们将其命名为YOLOv6。我们也表示热烈欢迎用户和贡献者的进一步改进。对于性能的表现，我们的YOLOv6-N在COCO数据集上达到了35.9%的AP，在NVIDIA Tesla T4 GPU上的吞吐量为1234 FPS。YOLOv6-S以495 FPS的速度达到了43.5%的AP，超过了其他相同规模的主流检测器（YOLOv5-S、YOLOX-S和PPYOLOE-S）。我们的量化版本YOLOv6-S甚至在869 FPS时带来了新的最先进的43.3%AP。此外，YOLOv6-M/L也比其他具有类似推理速度的检测器取得了更好的准确性表现（即49.5%/52.3%）。我们仔细进行了实验来验证每个组件的有效性。</p><blockquote><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a><strong>背景</strong></h4><p>YOLO系列如今已广泛应用在工业方面</p><h4 id="本文主要工作"><a href="#本文主要工作" class="headerlink" title="本文主要工作"></a><strong>本文主要工作</strong></h4><p>YOLOv6大量地吸收了最近的网络设计、训练策略、测试技术、量化和优化方法的想法。（也就是说没有吸睛创新点，主要做的也是缝合和堆砌）</p><h4 id="最终效果"><a href="#最终效果" class="headerlink" title="最终效果"></a><strong>最终效果</strong></h4><ul><li>精度更高：YOLOv6-N在COCO数据集上达到了35.9%的AP，在NVIDIA Tesla T4 GPU上的吞吐量为1234 FPS。</li><li>速度更快：YOLOv6-S以495 FPS的速度达到了43.5%的AP，超过了其他相同规模的主流检测器(YOLOv5-S、YOLOX-S和 PPYOLOE-S)。</li><li>量化版本也有提高：量化版本YOLOv6-S甚至在869 FPS时带来了新的最先进的43.3%AP。</li><li>其余版本：YOLOv6-M/L也比其他具有类似推理速度的检测器取得了更好的准确性表现(即49.5%/52.3%)。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Figure1.png" alt=""></p></blockquote><h2 id="一、Introduction——介绍"><a href="#一、Introduction——介绍" class="headerlink" title="一、Introduction——介绍"></a>一、Introduction——介绍</h2><p>YOLO系列一直是工业应用中最受欢迎的检测框架，因为它在速度和精度之间有很好的平衡。YOLO系列的开创性工作是YOLOv1-3[32-34]，它开辟了one-stage检测器的新道路，并在后来进行了大幅改进。YOLOv4[1]将检测框架重组为几个独立的部分（backbone, neck and head），并在当时验证了bag-of-freebies和bag-of-specials，设计了一个适合在单个GPU上训练的框架。目前，YOLOv5[10]、YOLOX[7]、PPY-OLOE[44]和YOLOv7[42]都是可以部署的高效检测器的竞争对象。不同尺寸的模型通常是通过缩放技术获得的。</p><p>在这份报告中，我们从经验上观察到几个重要的因素，促使我们重新装修YOLO框架：（1）来自RepVGG[3]的重新参数化是一种优越的技术，在检测中还没有得到很好的利用。我们还注意到，RepVGG块的简单模型扩展变得不切实际，为此我们认为小网络和大网络之间网络设计的优雅一致性是不必要的。对于小型网络来说，普通的单路径架构是一个较好的选择，但对于大型模型来说，参数的指数增长和单路径架构的计算成本使其不可行；（2）基于重参数化的检测器的量化也需要细致的处理，否则在训练和推理过程中，由于其异质配置导致的性能下降将是难以处理的。(3) 以前的工作[7, 10, 42, 44]往往不太注意部署，其延迟通常是在V100这样的高成本机器上进行比较。当涉及到真正的服务环境时，存在着硬件差距。通常情况下，像Tesla T4这样的低功耗GPU成本较低，并提供相当好的推理性能。(4) 考虑到架构上的差异，先进的特定领域策略，如标签分配和损失函数设计，需要进一步验证；(5) 对于部署，我们可以容忍训练策略的调整，以提高准确率性能，但不增加推理成本，如知识提炼。</p><p>考虑到上述意见，我们带来了YOLOv6的诞生，它在准确性和速度方面完成了迄今为止的最佳权衡。我们在图1中展示了YOLOv6与其他同行在类似规模下的比较。为了在不降低性能的情况下提高推理速度，我们研究了最先进的量化方法，包括训练后量化（PTQ）和量化感知训练（QAT），并将其纳入YOLOv6，以实现部署就绪的目标。网络的目标。</p><blockquote><h4 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a><strong>背景</strong></h4><p>(1)来自RepVGG的重参数化是一种优越的技术，在检测(已有的yolo版本)中尚未得到很好的利用。同时，作者认为小型网络和大型网络的设计不一样，对大型网络来说，对RepVGG块进行简单地模型缩放不切实际</p><p>(2)基于重参数化的检测器的量化需要细致的处理</p><p>(3)以前的工作往往不太关注部署，其延迟通常在 V100 等高成本机器上进行比较。在实际服务环境方面存在硬件差距</p><p>(4)标签分配和损失函数设计，需要进一步验证考虑架构的差异</p><p>(5)对于部署，训练过程可以使用知识蒸馏等策略，但不增加推理成本</p><h4 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a><strong>贡献</strong></h4><p>(1)为不同场景应用定制不同规模的模型，小模型以普通的单路径主干为特征，而大模型建立在高效的多分支块上    </p><p>(2)加入了自蒸馏策略，同时执行了分类任务和回归任务</p><p>(3)融合了各种先进tricks，如：标签分配检测技术、损失函数和数据增强技术</p><p>(4)在重新优化器和通道蒸馏的帮助下改革了定量检测方案，得到了一个更好的探测器</p></blockquote><h2 id="二、Method——方法"><a href="#二、Method——方法" class="headerlink" title="二、Method——方法"></a>二、Method——方法</h2><p>YOLOv6的重新设计由以下部分组成，网络设计、标签分配、损失函数、数据增强、适合工业界的改进，以及量化和部署：</p><ul><li>网络设计：Backbone：与其他主流架构相比，我们发现RepVGG[3]骨干网络在推理速度相近的情况下，在小型网络中具备更多的特征表示能力，而由于参数和计算成本的爆炸性增长，它很难被扩展以获得更大的模型。在这方面，我们把RepBlock[3]作为我们小型网络的构建模块。对于大型模型，我们修改了一个更有效的CSP[43]块，名为CSPStackRep块。Neck：YOLOv6的颈部采用了YOLOv4和YOLOv5之后的PAN拓扑结构[24]。我们用RepBlocks或CSPStackRep Blocks来增强颈部，以实现Rep-PAN。Head：我们简化了解耦头，使其更加有效，称为高效解耦头。</li><li>标签的分配：我们通过大量的实验评估了最近在YOLOv6上的标签分配策略[5, 7, 18, 48, 51]的进展，结果表明TAL[5]更加有效和 训练友好。<br>损失函数：主流anchor-free目标检测器的损失函数包含分类损失、box回归损失和object损失。对于每个损失，我们用所有可用的技术进行了系统的实验，最后选择VariFocal损失[50]作为我们的分类损失，SIoU[8]/GIoU[35]损失作为我们的回归损失。</li><li>适合工业界的改进：我们引入了更多常见的做法和技巧来提高性能，包括自我蒸馏和更多的训练epoch。对于自蒸馏，分类和回归都分别由教师模型进行监督。回归的蒸馏是由于DFL[20]而实现的。此外，软标签和硬标签的信息比例通过余弦衰减动态下降，这有助于学生在训练过程中的不同阶段选择性地获取知识。此外，我们还遇到了在评估时不添加额外的灰边而导致性能受损的问题，对此我们提供了一些补救措施。我们提供了一些补救措施。</li><li>量化和部署：为了解决基于重新参数化的模型量化时的性能下降问题，我们用RepOptimizer[2]训练YOLOv6，以获得PTQ友好的权重。我们进一步采用QAT与信道精馏[36]和图优化来追求极致的性能。我们的量化YOLOv6-S以42.3%的AP和869 FPS的吞吐量（批次大小=32）达到了一个新的技术水平。</li></ul><h3 id="2-1-Network-Design—-网络设计"><a href="#2-1-Network-Design—-网络设计" class="headerlink" title="2.1 Network Design— 网络设计"></a>2.1 Network Design— 网络设计</h3><p>一个单阶段目标检测器一般由以下部分组成：backbone、neck和head。主干部分主要决定了特征表示能力，同时，由于它承载了很大一部分计算成本，所以它的设计对推理效率有关键影响。neck用于将低层次的物理特征与高层次的语义特征聚合在一起，然后在各个层次建立金字塔特征图。头部由几个卷积层组成，它根据neck集合的多层次特征预测最终的检测结果。从结构上看，它可以分为 anchor-based 和 anchor-free的，或者说是参数耦合头和参数解耦头。</p><p>在YOLOv6中，基于硬件友好型网络设计的原则[3]，我们提出了两个可扩展的可重新参数化的骨干和颈部，以适应不同规模的模型，以及一个高效的混合通道策略的解耦头。</p><blockquote><p>单阶段物体探测器的组成：</p><ul><li><strong>主干网络：</strong>主要决定了特征表示能力 </li><li><strong>颈部：</strong>用于将低级的物理特征与高级的语义特征进行聚合，然后在所有层次上建立金字塔形特征映射</li><li><strong>头部：</strong>由多个卷积层组成，并根据颈部组装的多级特征预测最终检测结果</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Figure2.png" alt=""></p></blockquote><h4 id="2-1-1-Backbone—主干网络"><a href="#2-1-1-Backbone—主干网络" class="headerlink" title="2.1.1 Backbone—主干网络"></a>2.1.1 Backbone—主干网络</h4><p>如上所述，骨干网络的设计对检测模型的有效性和效率有很大影响。以前的研究表明，多分支网络[13, 14, 38, 39]往往能比单路径网络[15, 37]取得更好的分类性能，但往往伴随着并行性的降低，导致推理延迟的增加。相反，像VGG[37]这样的普通单路径网络具有高并行性和较少内存占用的优势，导致更高的推理效率。最近在RepVGG[3]中，提出了一种结构上的重新参数化方法，将训练时的多分支拓扑结构与推理时的普通结构解耦，以实现更好的速度-准确度权衡。<br>受上述工作的启发, 我们设计了一个高效的可重参数化骨干网, 命名为EfficientRep. 对于小型模型, 骨干网的主要组成部分是训练阶段的Rep-Block, 如图3(a)所示. 在推理阶段，每个RepBlock被转换为具有ReLU激活函数的3×3卷积层（表示为RepConv），如图3（b）所示。通常情况下，3×3卷积在主流的GPU和CPU上被高度优化，它享有更高的计算密度。因此，EfficientRep Backbone充分地利用了硬件的计算能力，使推理的延迟大大降低，同时提高了表示能力。</p><p>然而，我们注意到，随着模型容量的进一步扩大，单路径简单网络的计算成本和参数数量呈指数级增长。为了在计算负担和准确性之间实现更好的权衡，我们修改了一个CSPStackRep块来构建大中型网络的主干。如图3（c）所示，CSPStackRep Block由三个1×1的卷积层和一个由两个RepVGG块[3]或RepConv（分别在训练或推理时）组成的堆栈子块与一个剩余连接组成。此外，还采用了跨阶段部分（CSP）连接来提高性能，而没有过多的计算成本。与CSPRepResStage[45]相比，它有一个更简洁的前景，并考虑了准确性和速度之间的平衡。</p><blockquote><h4 id="背景-2"><a href="#背景-2" class="headerlink" title="背景"></a><strong>背景</strong></h4><p>RepVGG 主干在小型网络中具有更强的特征表示能力，但是随着参数和计算成本的爆炸式增长， RepVGG 在大模型中难以获得较高的性能</p><h4 id="改进工作"><a href="#改进工作" class="headerlink" title="改进工作"></a><strong>改进工作</strong></h4><ul><li>设计了一个高效的可重新参数化的骨干，称为<strong>EffificientRep</strong></li><li>在小模型<strong>(n/t/s)</strong>中，使用<strong>RepBlock</strong></li><li>在大模型<strong>(m/l)</strong> 中，使用<strong>CSPStackRep Blocks</strong>  </li></ul><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a><strong>方法</strong></h4><p>（1）将 Backbone 中 stride=2 的普通 Conv 层替换成了 <strong>stride=2 的RepConv层</strong></p><p>（2）将原始的 CSP-Block 都重新设计为 <strong>RepBlock</strong>，其中 <strong>RepBlock 的第一个RepConv会做channel</strong> <strong>维度的变换和对齐</strong> </p><p>（3）将原始的 SPPF 优化设计为更加高效的<strong>SimSPPF</strong></p><p>下图为 EfficientRep Backbone 具体设计结构图</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-EfficientRep.png" alt=""></p><p>下图是网络在不同情况下的结构</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Figure3.png" alt=""></p></blockquote><p>图<strong>(a)</strong>：表示训练的时候，RepVGG block接一个ReLU</p><p>图<strong>(b)</strong>：表示推理的时候，RepVGG块被替换成了RepConv</p><p>图<strong>(c)</strong>：表示CSPStackRep块的结构   ( 3 个 1x1 conv + 2 个 RepVGG(训练) / RepConv(推理) + 1 个Relu。)</p><h4 id="2-1-2-Neck—颈部"><a href="#2-1-2-Neck—颈部" class="headerlink" title="2.1.2 Neck—颈部"></a><strong>2.1.2 Neck—颈部</strong></h4><p>在实践中，多尺度的特征整合已被证明是目标检测的一个关键和有效的部分[9, 21, 24, 40]。我们采用YOLOv4[1]和YOLOv5[10]中修改的PAN拓扑结构[24]作为我们检测颈部的基础。此外，我们用RepBlock（用于小模型）或CSPStackRep Block（用于大模型）取代YOLOv5中使用的CSPBlock，并相应调整宽度和深度。YOLOv6的颈部被表示为Rep-PAN。</p><blockquote><p>参考YOLOv4和v5用的PAN，结合Backbone里的RepBlock或者CSPStackRep，提出了一个Rep-PAN</p><h4 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a><strong>方法</strong></h4><p>Rep-PAN 基于 PAN拓扑方式，用 RepBlock 替换了 YOLOv5 中使用的 CSP-Block，对整体 Neck 中的算子进行了调整</p><h4 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h4><p>在硬件上达到高效推理的同时，保持较好的多尺度特征融合能力</p><p>Rep-PAN 基于PAN拓扑方式，用RepBlock替换了YOLOv5中使用的CSP-Block，同时对整体Neck中的算子进行了调整，目的是在硬件上达到高效推理的同时，保持较好的多尺度特征融合能力</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-RepPAN.png" alt=""></p></blockquote><h3 id="2-1-3-Head—头部"><a href="#2-1-3-Head—头部" class="headerlink" title="2.1.3 Head—头部"></a>2.1.3 Head—头部</h3><h4 id="Efficient-decoupled-head—高效的解耦头"><a href="#Efficient-decoupled-head—高效的解耦头" class="headerlink" title="Efficient decoupled head—高效的解耦头"></a>Efficient decoupled head—高效的解耦头</h4><p>高效的解耦头 YOLOv5的检测头是一个耦合头，其参数在分类和定位分支之间共享，而其在FCOS[41]和YOLOX[7]中的同类产品则将这两个分支解耦，并且在每个分支中引入额外的两个3×3卷积层以提高性能。在YOLOv6中，我们采用混合通道策略来建立一个更有效的解耦头。具体来说，我们将中间的3×3卷积层的数量减少到只有一个。头部的宽度由骨干和颈部的宽度乘数共同缩放。这些修改进一步降低了计算成本，以实现更低的推理延迟。</p><blockquote><h4 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h4><p>（1）将中间的3x3卷积层的数量减少为<strong>1</strong></p><p>（2）Head 的尺度和 backbone 及 neck <strong>同大同小</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-head.png" alt=""></p><h4 id="目的-1"><a href="#目的-1" class="headerlink" title="目的"></a>目的</h4><p>进一步降低了计算成本，以实现更低的推断延迟。</p></blockquote><h4 id="Anchor-free"><a href="#Anchor-free" class="headerlink" title="Anchor-free"></a>Anchor-free</h4><p><strong>Anchor-free</strong>检测器因其更好的泛化能力和解码预测结果的简单性而脱颖而出。其后期处理的时间成本大大降低。有两种类型的Anchor-free检测器：基于锚点[7, 41]和基于关键点[16, 46, 53]。在YOLOv6中，我们采用了基于锚点的范式，其回归分支实际上预测了从锚点到box四边的距离。</p><blockquote><h4 id="不用Anchor-based的原因"><a href="#不用Anchor-based的原因" class="headerlink" title="不用Anchor-based的原因"></a><strong>不用Anchor-based的原因</strong></h4><p>由于 Anchor-based检测器需要在训练之前进行聚类分析以确定最佳 Anchor 集合，这会一定程度提高检测器的复杂度  在一些边缘端的应用中，需要在硬件之间搬运大量检测结果的步骤，也会带来额外的延时</p><h4 id="Anchor-free优点"><a href="#Anchor-free优点" class="headerlink" title="Anchor-free优点"></a>Anchor-free优点</h4><p>Anchor-free方案<strong>不需要预设参数，同时后处理耗时短</strong></p><p><strong>Anchor-free方案有两种</strong></p><ul><li><strong>point-base(FCOS)</strong> ——YOLOv6使用的</li><li>keypoint-based  ( CornerNet)</li></ul></blockquote><h3 id="2-2-Label-Assignment—标签分配"><a href="#2-2-Label-Assignment—标签分配" class="headerlink" title="2.2 Label Assignment—标签分配"></a>2.2 Label Assignment—标签分配</h3><h4 id="SimOTA"><a href="#SimOTA" class="headerlink" title="SimOTA"></a>SimOTA</h4><p>SimOTA OTA[6]认为物体检测中的标签分配是一个最优传输问题。它从全局的角度为每个ground-truth目标定义了正/负的训练样本。SimOTA[7]是OTA[6]的简化版本，它减少了额外的超参数并保持了性能。在YOLOv6的早期版本中，SimOTA被用作标签分配方法。然而，在实践中，我们发现引入SimOTA会减慢训练过程。而且，陷入不稳定的训练也并不罕见。因此，我们渴望有一个替代SimOTA的方法。</p><blockquote><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a><strong>介绍</strong></h4><p>OTA 将目标检测中的标签分配视为最佳传输问题。它从<strong>全局角度为每个真实对象定义了正/负训练样本。</strong></p><p>SimOTA 是 OTA 的简化版本，它<strong>减少了额外的超参数</strong>并保持了性能。</p><h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><p>  ①计算成对预测框与真值框代价，由分类及回归loss构成</p><p>  ②计算真值框与前k个预测框IoU，其和为Dynamic k；因此对于不同真值框，其Dynamic k存在差异</p><p>  ③最后选择代价最小的前Dynamic k个预测框作为正样本</p><h4 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h4><p>SimOTA 会拉慢训练速度，容易导致训练不稳定</p></blockquote><h4 id="Task-alignment-learning—任务对齐学习"><a href="#Task-alignment-learning—任务对齐学习" class="headerlink" title="Task alignment learning—任务对齐学习"></a>Task alignment learning—任务对齐学习</h4><p>任务对齐学习 任务对齐学习（TAL）最早是在TOOD[5]中提出的，其中设计了一个统一的分类分数和预测框质量的指标。IoU被这个度量所取代，用于分配对象标签。在一定程度上，任务（分类和箱体回归）不一致的问题得到了缓解。<br>TOOD的另一个主要贡献是关于任务对齐的头（T-head）。T-head堆叠卷积层以建立交互式特征，在此基础上使用任务对齐预测器（TAP）。PP-YOLOE[45]改进了T-head，用轻量级的ESE注意力取代了T-head中的层注意力，形成ET-head。然而，我们发现ET-head在我们的模型中会降低推理速度，而且没有准确性的提高。</p><p>因此，我们保留了Efficient解耦头的设计。此外，我们观察到TAL比SimOTA能带来更多的性能提升，并且能稳定训练。因此，我们在YOLOv6中采用TAL作为默认的标签分配策略。</p><blockquote><h4 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h4><p>TAL 是在 TOOD 中被提出的，其中设计了一个【分类得分和定位框质量的统一度量标准】，使用该度量结果代替 IoU 来帮助</p><p>分配标签，有助于解决任务不对齐的问题，且更稳定，效果更好</p><h4 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h4><p> ①在各个特征层计算gt与预测框IoU及与分类得分乘积作为score，进行分类检测任务对齐</p><p> ②对于每个gt选择top-k个最大的score对应bbox</p><p> ③选取bbox所使用anchor的中心落在gt内的为正样本</p><p> ④若一个anchor box对应多个gt，则选择gt与预测框IoU最大那个预测框对应anchor负责该gt</p><h4 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h4><p>  TAL比SimOTA带来更多的性能改善，稳定训练</p></blockquote><hr><h3 id="2-3-Loss-Functions—损失函数"><a href="#2-3-Loss-Functions—损失函数" class="headerlink" title="2.3 Loss Functions—损失函数"></a>2.3 Loss Functions—损失函数</h3><h4 id="2-3-1-Classification-Loss—分类损失"><a href="#2-3-1-Classification-Loss—分类损失" class="headerlink" title="2.3.1 Classification Loss—分类损失"></a>2.3.1 Classification Loss—分类损失</h4><p>提高分类器的性能是优化检测器的一个关键部分。Focal Loss[22]修改了传统的交叉熵损失，以解决正负样本之间或难易样本之间类别不平衡的问题。为了解决训练和推理之间质量估计和分类的不一致使用问题，质量Focal Loss（QFL）[20]进一步扩展了Focal Loss，对分类分数和分类监督的定位质量进行联合表示。而VariFocal Loss（VFL）[50]则源于Focal Loss[22]，但它对正负样本的处理是不对称的。通过考虑正负样本的不同重要程度，它平衡了来自两个样本的学习信号。Poly Loss[17]将常用的分类损失分解为一系列的加权多项式基数。它在不同的任务和数据集上调整多项式系数，通过实验证明它比交叉熵损失和Focal Loss更好。</p><p>我们在YOLOv6上评估了所有这些高级分类损失，最终采用了VFL[50]。</p><blockquote><ul><li><strong>VariFocal Loss(VFL) ：</strong>提出了非对称的加权操作。</li><li>针对正负样本有不平衡的问题和正样本中不等权的问题，来发现更多有价值的正样本。因此选择 VariFocal Loss 作为分类损失。</li></ul></blockquote><h4 id="2-3-2-Box-Regression-Loss—-回归框损失"><a href="#2-3-2-Box-Regression-Loss—-回归框损失" class="headerlink" title="2.3.2 Box Regression Loss— 回归框损失"></a><strong>2.3.2 Box Regression Loss— 回归框损失</strong></h4><p>回归损失提供了精确定位box边界的重要学习信号。L1损失是早期工作中最初的回归损失。逐渐地，各种精心设计的回归损失涌现出来，如IoU系列损失[8, 11, 35, 47, 52, 52] 和概率损失[20]。</p><p><strong>IoU-系列损失</strong> IoU损失[47]将预测框的四个边界作为一个整体单位进行回归。它被证明是有效的，因为它与评价指标一致。IoU有很多变体，如GIoU[35]、DIoU[52]、CIoU[52]、α-IoU[11]和SIoU[8]等，形成相关损失函数。在这项工作中，我们用GIoU、CIoU和SIoU进行实验。而SIoU被应用于YOLOv6-N和YOLOv6-T，而其他的则使用GIoU。</p><p><strong>概率损失</strong> Distribution Focal Loss（DFL）[20]将连续分布的box位置简化为离散的概率分布。它考虑了数据的模糊性和不确定性，而没有引入任何其他强的先验因素，这有助于提高box的定位精度，特别是当ground-truth boxes模糊时。在DFL的基础上，DFLv2[19]开发了一个轻量级的子网络，以利用分布统计和实际定位质量之间的密切关联，这进一步提高了检测性能。然而，DFL通常比一般的目标框回归多输出17倍的回归值，导致了大量的开销。额外的计算成本大大阻碍了小模型的训练。而DFLv2由于有了额外的子网络，进一步增加了计算负担。在我们的实验中，DFLv2在我们的模型上带来了与DFL相似的 在我们的模型上，DFLv2带来的性能增益与DFL相似。因此，我们 只在YOLOv6-M/L中采用DFL。实验细节可以在可在第3.3.3节中找到。</p><blockquote><h4 id="IoU-series-Loss-loU系列损失"><a href="#IoU-series-Loss-loU系列损失" class="headerlink" title="IoU-series Loss-loU系列损失"></a><strong>IoU-series Loss-loU系列损失</strong></h4><p>SIoU Loss在小模型上提升明显， GIoU Loss在大模型上提升明显，因此选择SIoU  (for n/t/s)  /GIoU (for m/l) 损失作为回归损失。</p><h4 id="Probability-Loss-概率损失"><a href="#Probability-Loss-概率损失" class="headerlink" title="Probability Loss-概率损失"></a>Probability Loss-概率损失</h4><p>Distribution Focal Loss (DFL) 和 Distribution Focal Loss (DFL) v2可以带来一定的性能提升，但是对效率影响较大，因此弃用。</p></blockquote><h4 id="2-3-3-Object-Loss—-目标损失"><a href="#2-3-3-Object-Loss—-目标损失" class="headerlink" title="2.3.3 Object Loss— 目标损失"></a>2.3.3 Object Loss— 目标损失</h4><p>物体损失最早是在FCOS[41]中提出的，用于降低低质量bounding boxes的得分，以便在后期处理中过滤掉它们。它也被用于YOLOX[7]，以加速收敛并提高网络精度。作为一个像FCOS和YOLOX一样的anchor-free框架，我们已经在YOLOv6中尝试了object loss。不太幸运的是，它并没有带来很多好的效果。详细情况在第3节中给出。</p><blockquote><p>Object loss 首次提出是在 FCOS 中，用于降低 low-quality bbox 的得分，利于在 NMS 中过滤掉， YOLOX 中使用了该 loss 来加 速收敛并提升准确性，但 YOLOv6 中使用同样的方法后并无收益。</p></blockquote><h3 id="2-4-Industry-handy-improvements—工业界处理改进"><a href="#2-4-Industry-handy-improvements—工业界处理改进" class="headerlink" title="2.4. Industry-handy improvements—工业界处理改进"></a>2.4. Industry-handy improvements—工业界处理改进</h3><h4 id="2-4-1-More-training-epochs—更多的训练epoch"><a href="#2-4-1-More-training-epochs—更多的训练epoch" class="headerlink" title="2.4.1 More training epochs—更多的训练epoch"></a>2.4.1 More training epochs—更多的训练epoch</h4><p>经验结果表明，随着训练时间的增加，检测器的性能也在不断进步。我们将训练时间从300 epochs延长到400 epochs，以达到一个更好的收敛性。</p><blockquote><p>将训练从300个epochs延长到400个epochs，以达到更好的收敛性。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table12.png" alt=""></p><p>YOLOv6-N、T、S在较长时期的训练中，使AP分别提高了0.4%、0.6%和0.5%。</p></blockquote><h4 id="2-4-2-Self-distillation—-自蒸馏"><a href="#2-4-2-Self-distillation—-自蒸馏" class="headerlink" title="2.4.2 Self-distillation— 自蒸馏"></a>2.4.2 Self-distillation— 自蒸馏</h4><p>为了进一步提高模型的准确性，同时不引入太多的额外计算成本，我们应用经典的知识蒸馏技术，使教师和学生的预测之间的KL-散度最小。我们把老师限定为学生本身，但进行了预训练，因此我们称之为自我蒸馏。请注意，KL-散度通常被用来衡量数据分布之间的差异。然而，在目标检测中有两个子任务，其中只有分类任务可以直接利用基于KL-散度的知识提炼。由于DFL损失[20]的存在，我们也可以在box回归上执行它。知识提炼损失 可以被表述为：</p><p>L KD = KL(p cls t ||p s ) + KL(p t ||p s ),</p><p>其中p cls t和p s分别是教师模型和学生模型的班级预测，相应地p reg t和p reg是盒式回归预测。现在，总体损失s函数被表述为：</p><p>L total = L det + αL KD ,</p><p>其中L det是用预测和标签计算的检测损失。引入超参数α是为了平衡两种损失。在训练的早期阶段，教师的软标签更容易学习。随着训练的继续，学生的表现将与教师相匹配，因此硬标签将更多地帮助学生。在此基础上，我们对α应用余弦权重衰减来动态调整来自硬标签和来自教师的软标签的信息。我们进行了详细的实验来验证YOLOv6的自我蒸馏的效果，这将在第3节讨论。</p><blockquote><h4 id="背景-3"><a href="#背景-3" class="headerlink" title="背景"></a>背景</h4><p>为了进一步提高模型的准确性，同时不引入太多的额外计算成本，我们应用经典的知识蒸馏技术，使教师和学生的预测之间的KL-散度最小。</p><h4 id="方法-3"><a href="#方法-3" class="headerlink" title="方法"></a>方法</h4><p>作者限制教师模型与学生模型网络结构相同，但经过预训练，因此称为自蒸馏。</p><p>归因于DFL损失，回归分支也可使用知识蒸馏，损失函数如式1所示：</p><script type="math/tex; mode=display">L_{KD} = KL(p_t^{cls} || p_s^{cls}) + KL(p_t^{reg} || p_s^{reg}),</script><h4 id="效果-1"><a href="#效果-1" class="headerlink" title="效果"></a>效果</h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table13.png" alt=""></p><ul><li>仅在分类分支上应用自蒸馏可以提高<strong>0.4%</strong>的AP</li><li>在预测框回归任务上执行自蒸馏可以提高<strong>0.3%</strong>的AP</li><li>权重衰减的引入自蒸馏使模型可以提高<strong>0.6%</strong>的AP</li></ul></blockquote><h4 id="2-4-3-Gray-border-of-images—-图像灰色边界"><a href="#2-4-3-Gray-border-of-images—-图像灰色边界" class="headerlink" title="2.4.3 Gray border of images— 图像灰色边界"></a>2.4.3 Gray border of images— 图像灰色边界</h4><p>我们注意到，在评价YOLOv5[10]和YOLOv7[42]的实现中的模型性能时，每个图像周围都有一个半截灰色的边框。虽然没有增加有用的信息，但它有助于检测图像边缘附近的物体。这个技巧也适用于YOLOv6。然而，额外的灰色像素显然会降低推理速度。没有灰色边界，YOLOv6的性能就会下降，这也是[10, 42]的情况。我们推测，这个问题与Mosaic增强中的灰边填充有关[1, 10]。为了验证，我们进行了在最后一个epoch中关闭马赛克增强的实验[7]（又称淡化策略）。在这方面，我们改变了灰色边框的面积，并将带有灰色边框的图像直接调整为目标图像的大小。结合这两种策略，我们的模型可以在不降低推理速度的情况下保持甚至提高性能。推理速度。</p><blockquote><h4 id="背景-4"><a href="#背景-4" class="headerlink" title="背景"></a>背景</h4><p>半截灰色的边框它有助于检测图像边缘附近的物体</p><h4 id="方法-4"><a href="#方法-4" class="headerlink" title="方法"></a>方法</h4><p>（1）在最后一个epoch中关闭mosaic增强的实验(又称淡化策略)</p><p>（2）改变了灰色边框的面积，并将带有灰色边框的图像直接调整为目标图像的大小</p><h4 id="效果-2"><a href="#效果-2" class="headerlink" title="效果"></a>效果</h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-effection.png" alt=""></p><p>YOLOv6-N/S/M的最终性能更准确，最终图像尺寸从672减少到640。</p></blockquote><h3 id="2-5-Quantization-and-Deployment—量化和部署"><a href="#2-5-Quantization-and-Deployment—量化和部署" class="headerlink" title="2.5. Quantization and Deployment—量化和部署"></a>2.5. Quantization and Deployment—量化和部署</h3><h4 id="2-5-1-Reparameterizing-Optimizer—重新参数化"><a href="#2-5-1-Reparameterizing-Optimizer—重新参数化" class="headerlink" title="2.5.1 Reparameterizing Optimizer—重新参数化"></a>2.5.1 Reparameterizing Optimizer—重新参数化</h4><p>RepOptimizer[2]在每个优化步骤中提出梯度重新参数化。该技术也能很好地解决了基于再参数化的模型的量化问题。因此，我们以这种方式重建了YOLOv6的重新参数化块，并使用重新优化器对其进行训练，以获得对PTQ友好的权值。特征图的分布很窄（如图4，B.1），这大大有利于量化过程，结果见第3.5.1节。<br>（1）RepOptimizer 提出了在每次训练的时候进行梯度重参数化，该方法能够较好的解决基于重参数化的模型。               </p><p>（2）YOLOv6 中就使用了 RepOptimizer 用于获得 PTQ-friendly 的权重，其特征的分布是非常狭窄的，能够有利于量化。<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Figure4.png" alt=""></p><h4 id="2-5-2-Sensitivity-Analysis—敏感度"><a href="#2-5-2-Sensitivity-Analysis—敏感度" class="headerlink" title="2.5.2 Sensitivity Analysis—敏感度"></a>2.5.2 Sensitivity Analysis—敏感度</h4><p>我们通过将量化敏感操作部分转换为浮点计算，进一步提高了PTQ的性能。为了获得灵敏度分布，我们常用了几个指标，即均方误差（MSE）、信噪比（SNR）和余弦相似度。通常，为了进行比较，可以选择输出特征映射（在激活某一层之后）来计算有量化和没有量化的这些度量。作为一种替代方法，它也可以通过开关特定层[29]的量化来计算验证AP。</p><p> 我们在使用重新优化器训练的YOLOv6-S模型上计算所有这些指标，并选择前6个敏感层，以浮动形式运行。敏感性分析的完整图表见B.2。</p><blockquote><p>通过将量化敏感操作部分转换为浮点计算，进一步提高了 PTQ 性能为了得到敏感性分布，作者使用了 mean-square error (MSE), signal-noise ratio (SNR) 和 cosine similarity 。</p></blockquote><h4 id="2-5-3-Quantization-aware-Training-with-Channel-wise-Distillation—基于通道蒸馏的量化感知训练"><a href="#2-5-3-Quantization-aware-Training-with-Channel-wise-Distillation—基于通道蒸馏的量化感知训练" class="headerlink" title="2.5.3 Quantization-aware Training with Channel-wise Distillation—基于通道蒸馏的量化感知训练"></a>2.5.3 Quantization-aware Training with Channel-wise Distillation—基于通道蒸馏的量化感知训练</h4><p> 在PTQ不足的情况下，我们建议涉及量化感知训练（QAT）来提高量化性能。为了解决在训练和推理过程中假量化器的不一致性问题，有必要在重新优化器上建立QAT。此外，在YOLOv6框架内采用了通道蒸馏[36]（后来称为CW蒸馏），如图5所示。这也是一种自蒸馏的方法，其中教师网络是在fp32精度上的学生模型。参见第3.5.1节中的实验。</p><blockquote><p>为防止PTQ不足，作者引入QAT (训练中量化)，保证训练推理一致，作者同样使用RepOptimizer，此外使用channel-wise蒸馏，如图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-PTQ.png" alt=""></p></blockquote><h2 id="三、Experiments—实验"><a href="#三、Experiments—实验" class="headerlink" title="三、Experiments—实验"></a>三、Experiments—实验</h2><h3 id="3-1-Implementation-Details—实施细节"><a href="#3-1-Implementation-Details—实施细节" class="headerlink" title="3.1 Implementation Details—实施细节"></a>3.1 Implementation Details—实施细节</h3><p>我们使用与YOLOv5 [10]相同的优化器和学习时间表，i.即具有动量和学习率余弦衰减的随机梯度下降（SGD）。还利用了预热、分组权重衰减策略和指数移动平均（EMA）。我们采用了两个强数据增强（Mosaic [1，10]和Mixup [49]）[1，7，10]。超参数设置的完整列表可以在我们发布的代码中找到。我们在COCO 2017 [23]训练集上训练我们的模型，并在COCO 2017验证集上评估准确性。我们所有的模型都在8个NVIDIA A100 GPU上进行训练，速度性能在配备TensorRT版本7的NVIDIA Tesla T4 GPU上进行测量。2除非另有说明。还有速度使用其他TensorRT版本或其他设备测量的性能在附录A中演示。</p><blockquote><p>(1)采用了和YOLOv5相同的优化算法和学习机制设置(包括SGD、学习率、预热、分组权重衰减策略和EMA、还有两个数据增强)</p><p>(2)在COCO 2017训练集上训练模型，并在COCO 2017验证集上评估准确性</p><p>(3)所有的模型都在8个NVIDIA A100 GPU上进行训练，速度性能在配备TensorRT版本7的NVIDIA Tesla T4 GPU上进行测量</p></blockquote><h3 id="3-2-Comparisons—对照实验"><a href="#3-2-Comparisons—对照实验" class="headerlink" title="3.2 Comparisons—对照实验"></a>3.2 Comparisons—对照实验</h3><p>考虑到这项工作的目标是为工业应用构建网络，我们主要关注部署后所有模型的速度性能，包括吞吐量（批量大小为1或32的FPS）和GPU延迟，而不是FLOPs或参数数量。我们将YOLOv 6与YOLO系列的其他最先进的探测器进行了比较，包括YOLOv 5 [10]，YOLOX [7]，PPYOLOE [45]和YOLOv 7 [42]。请注意，我们使用TensorRT在相同的Tesla T4 GPU上测试了所有官方型号的FP 16精度的速度性能[28]。YOLOv 7-Tiny的性能根据其开源代码和输入大小为416和640的权重进行重新评估。结果示于表1和图2中。1.与YOLOv 5-N/YOLOv 7-Tiny（输入大小=416）相比，我们的YOLOv 6-N显著提高了7。9%/2.6%。在吞吐量和延迟方面，它还具有最佳的速度性能。与YOLOX-S/PPYOLOE-S相比，YOLOv 6-S可使AP提高3.0%/0.4%，速度更快。我们将YOLOv 5-S和YOLOv 7-Tiny（输入大小=640）与YOLOv 6-T进行比较，我们的方法是2。精度提高9%，批处理大小为1时，速度提高73/25 FPS。YOLOv 6-M的性能比YOLOv 5-M高4倍。2%的AP，在相同的速度下，实现了2.在更高的速度下，AP比YOLOX-M/PPYOLOE-M高7%/0.6%。此外，它比YOLOv 5-L更准确，更快。YOLOv 6-L为2。在相同的延迟限制下，比YOLOX-L/PPYLOE-L准确8%/1.1%。我们还通过用ReLU替换SiLU（表示为YOLOv 6-L-ReLU）来提供YOLOv 6-L的更快版本。达到51。7%AP，延迟为8.8 ms，在精度和速度上均优于YOLOX-L/PPYOLOE-L/YOLOv 7。</p><blockquote><h4 id="和SOTA对比"><a href="#和SOTA对比" class="headerlink" title="和SOTA对比"></a>和SOTA对比</h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-vssota.png" alt=""></p><p>(1)相比 YOLOv5-N/YOLOv7-Tiny (input size=416) ，YOLOv6-N 分别提升了 7.9% 和 2.6% ，也达到了最高的速度</p><p>(2)相比 YOLOX-S/PPYOLOE-S, YOLOv6-S 分别提升了 3.0% 和 0.4%</p><p>(3)相比 YOLOv5-S 和 YOLOv7-Tiny (input size=640) ，YOLOv6-M 在同等速度的情况下高了 4.2% AP</p><p>(4)相比 YOLOX-M/PPYOLOE-M ，YOLOv6-M 更快，且分别高了 2.7% 和 0.6% AP</p><p>(5)相比 YOLOX-L/PPYOLOE-L/YOLOv7 ，YOLOv6-L-Relu 达到了 51.7% AP，超越了前面几个方法</p></blockquote><h3 id="3-3-Ablation-Study—消融实验"><a href="#3-3-Ablation-Study—消融实验" class="headerlink" title="3.3 Ablation Study—消融实验"></a>3.3 Ablation Study—消融实验</h3><h4 id="3-3-1-Network—-网络"><a href="#3-3-1-Network—-网络" class="headerlink" title="3.3.1 Network— 网络"></a><strong>3.3.1 Network— 网络</strong></h4><p><strong>Backbone and neck—骨干网络和颈部</strong></p><p>主干和颈部我们探讨了单路径结构和多分支结构对主干和颈部的影响，以及CSPStackRep Block的信道系数（表示为CC）。本部分描述的所有模型均采用TAL作为标签分配策略，VFL作为分类损失，GIoU和DFL作为回归损失。结果示于表2中。我们发现，在不同规模的模型的最佳网络结构应该拿出不同的解决方案。对于YOLOv 6-N，单路径结构在精度和速度方面优于多分支结构。</p><p>由于相对较低的存储器占用和较高的并行度，因此运行得更快。对于YOLOv 6-S，两种块样式带来相似的性能。当涉及到较大的模型，多分支结构实现更好的性能，在准确性和速度。并且我们最终选择多分支，其中对于YOLOv 6-M具有2/3的信道系数，并且对于YOLOv 6-L具有1/2的信道系数。此外，我们研究了颈部的宽度和深度对YOLOv 6-L的影响。表3中的结果显示细长颈部执行0.2%，比宽浅颈在相同的速度。</p><blockquote><p>作者比较backbone及neck中不同block及CSPStackRep Block中channel系数影响</p><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>不同网络结构适用不同策略</p></blockquote><p><strong>Combinations of convolutional layers and activation functions—卷积层和激活函数组合</strong></p><p>YOLO系列采用了广泛的激活函数，ReLU [27]，LReLU [25]，Swish [31]，SiLU [4]，Mish [26]等。在这些激活函数中，SiLU是使用最多的。一般来说，SiLU具有更好的准确性，并且不会导致太多额外的计算成本。然而，当涉及到工业应用时，特别是部署具有TensorRT [28]加速的模型时，ReLU由于融合到卷积中而具有更大的速度优势。此外，我们进一步验证了RepConv/普通卷积（表示为Conv）和ReLU/SiLU/LReLU在不同规模的网络中的组合的有效性，以实现更好的权衡。如表4所示，具有SiLU的Conv在准确性方面表现最好，而RepConv和ReLU的组合实现了更好的折衷。我们建议用户在延迟敏感的应用程序中采用ReLU的RepConv。我们选择使用RepConv/ReLU组合</p><blockquote><p>YOLO系列中常用激活函数有ReLU、LReLU、Swish、SiLU、Mish等，  SiLU精度最高且最常用，但是部署与TensorRT 加速 的模型时无法与卷积层融合，  ReLU更具有速度优势</p><p>进一步验证了RepConv/普通卷积(记为Conv)和ReLU/SiLU/LReLU组合在不同大小的网络中的有效性<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table3.png" alt=""></p><h4 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h4><p>（1）Conv+SiLU性能最佳，但RepConv+ReLU达到性能与速度均衡</p><p>（2）在YOLOv6-N/T/S/M中使用RepConv/ReLU组合来获得更高的推理速度</p><p>（3）在大型模型YOLOv6-L中使用Conv/SiLU组合来加速训练和提高性能。</p></blockquote><p><strong>Miscellaneous design—其余设计</strong></p><p>我们还对第2节中提到的其他网络部分进行了一系列消融。1基于YOLOv 6-N。我们选择YOLOv 5-N作为基线，并逐步添加其他组件。结果示于表5中。首先，对于解耦头（表示为DH），我们的模型是1。精确度提高4%，时间成本增加5%。其次，我们验证了无锚范式比基于锚的范式快51%，因为它的3倍少的预定义锚，这导致输出的维数更少。此外，表示为EB+RN的骨架（EfficientRep骨架）和颈部（Rep-PAN颈部）的统一修饰带来3。6%的AP改进，运行速度提高21%。最后，优化的解耦头（混合通道，HC）带来0。2%AP和6.FPS的准确性和速度分别提高了8%。</p><blockquote><h4 id="操作及结论"><a href="#操作及结论" class="headerlink" title="操作及结论"></a>操作及结论</h4><p><strong>DH:</strong> 以YOLOv5-N为基线，验证YOLOv6-N中不同部件影响，使用解耦头(DH)性能提升1.4%，耗时增加5%</p><p><strong>AF:</strong> Anchor-free方案耗时降低51%</p><p><strong>EB+RN:</strong> 主干网络EfficientRep +颈部Rep-PAN 使得性能提升3.6%，耗时降低21%</p><p><strong>HC:</strong> Head中混合通道策略，使得性能提升0.2%，耗时降低6.8%</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table5.png" alt=""></p><h4 id="3-3-2-Label-Assignment—标签分配"><a href="#3-3-2-Label-Assignment—标签分配" class="headerlink" title="3.3.2 Label Assignment—标签分配"></a>3.3.2 Label Assignment—标签分配</h4><p>在表6中，我们分析了主流标签分配策略的有效性。在YOLOv6N上进行实验。如预期的，我们观察到SimOTA和TAL是最好的两个策略与ATSS相比，SimOTA可以提高AP 2.0%，TAL带来0。AP比SimOTA高5%。考虑到TAL的稳定训练和更好的准确性能，我们采用TAL作为我们的标签分配策略。此外，TOOD [5]的实现采用ATSS [51]作为早期训练时期的预热标签分配策略。我们还保留了热身策略，并对其进行了进一步的探索。详细信息如表7所示，我们可以发现，在没有预热或通过其他策略预热的情况下（即：例如，SimOTA）也可以实现类似的性能。</p><blockquote><p>对比可知，  <strong>SimOTA</strong>和<strong>TAL</strong>是最好的两种策略。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table6.png" alt=""></p><h4 id="结论-2"><a href="#结论-2" class="headerlink" title="结论"></a>结论</h4><p>考虑到TAL的稳定训练和更好的准确性性能，作者采用TAL作为我们的标签分配策略。</p><p>进一步探索warm-up策略：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table7.png" alt=""></p><h4 id="结论-3"><a href="#结论-3" class="headerlink" title="结论"></a>结论</h4><p>如果没有warm-up或通过其他策略(即SimOTA)进行热身，也可以达到类似的性能。</p></blockquote><h4 id="3-3-3-Loss-functions—损失函数"><a href="#3-3-3-Loss-functions—损失函数" class="headerlink" title="3.3.3 Loss functions—损失函数"></a>3.3.3 Loss functions—损失函数</h4><p><strong>Classification Loss—分类损失</strong></p><p><strong>分类损失：</strong>我们在YOLOv 6-N/S/M上实验了Focal Loss [22]、Poly loss [17]、QFL [20]和VFL [50]。从表8中可以看出，VFL带来0。与局灶性丢失相比，YOLOv 6-N/S/M的AP改善分别为2%/0.3%/0.1%。我们选择VFL作为分类损失函数。</p><blockquote><p>作者对不同分类损失函数进行验证</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table8.png" alt=""></p><h4 id="结论-4"><a href="#结论-4" class="headerlink" title="结论"></a>结论</h4><p>选择VFL作为分类损失函数</p></blockquote><h4 id="Regression-Loss—-回归损失"><a href="#Regression-Loss—-回归损失" class="headerlink" title="Regression Loss— 回归损失"></a>Regression Loss— 回归损失</h4><p>在YOLOv 6-N/S/M上对回归损失IoU序列和概率损失函数进行了实验。YOLOv 6 N/S/M采用了最新的IoU系列损耗。表9中的实验结果显示，对于YOLOv 6-N和YOLOv 6-T，SIoU损失优于其他损失，而CIoU损失在YOLOv 6-M上表现更好。对于概率损失，如表10所列，引入DFL可以获得0。YOLOv 6-N/S/M的性能增益分别为2%/0.1%/0.2%。然而，对于小模型，推理速度受到很大影响。因此，仅在YOLOv 6-M/L中引入DFL。</p><blockquote><p>作者在YOLOv6-N/S/M上实验了IoU系列损失和概率损失函数</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table910.png" alt=""></p><h4 id="结论-5"><a href="#结论-5" class="headerlink" title="结论"></a>结论</h4><p>（1）关于<strong>IoU</strong>系列损失：  YOLOv6-N及YOLOv6-T使用SIoU损失，其余使用GIoU损失</p><p>（2）关于概率损失：  YOLOv6-M/L使用DFL，其余未使用</p></blockquote><p><strong>Object Loss- 目标损失</strong></p><p>如表11所示，还用YOLOv 6实验物体损失。从表11中，我们可以看到对象丢失对YOLOv 6-N/S/M网络有负面影响，其中最大减少为1。YOLOv 6-N上的1% AP。负增益可能来自目标分支与TAL中的其他两个分支之间的冲突。具体来说，在训练阶段，预测框和地面实况框之间的IoU以及分类得分用于联合构建度量作为分配标签的标准。但是，引入的对象分支将要对齐的任务数量从两个扩展到三个，这显然增加了难度。基于实验结果和该分析，然后在YOLOv6中丢弃对象丢失。</p><blockquote><p>使用YOLOv6也进行了Object Loss 实验</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table11.png" alt=""></p><h4 id="结论-6"><a href="#结论-6" class="headerlink" title="结论"></a>结论</h4><p>YOLOv6-N/S/M中目标损失都降低了效果；作者选择丢弃</p><p><strong>原因：</strong>负增益可能来自于TAL中对象分支和其他两个分支之间的冲突，TAL中将IoU与分类联合作为，额外引入一分支导</p><p>致两分支对齐变为三分支，增加对齐难度</p></blockquote><h3 id="3-4-Industry-handy-improvements—工业的便利改进"><a href="#3-4-Industry-handy-improvements—工业的便利改进" class="headerlink" title="3.4 Industry-handy improvements—工业的便利改进"></a>3.4 Industry-handy improvements—工业的便利改进</h3><p>（这一部分内容请看第2部分，就不再重复讲咯~）</p><h3 id="3-5-Quantization-Results—量化结果"><a href="#3-5-Quantization-Results—量化结果" class="headerlink" title="3.5. Quantization Results—量化结果"></a>3.5. Quantization Results—量化结果</h3><h4 id="3-5-1-PTQ"><a href="#3-5-1-PTQ" class="headerlink" title="3.5.1 PTQ"></a>3.5.1 PTQ</h4><p>当使用RepOptimizer训练模型时，平均性能得到了显著提高，请参见表15。RepOptimizer通常更快，几乎相同</p><blockquote><h3 id="使用RepOptimizer训练模型"><a href="#使用RepOptimizer训练模型" class="headerlink" title="使用RepOptimizer训练模型"></a>使用RepOptimizer训练模型</h3><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table15.png" alt=""></p><p><strong>结论：</strong> 表明RepOptimizer带来性能大幅改进</p></blockquote><h4 id="3-5-2-QAT"><a href="#3-5-2-QAT" class="headerlink" title="3.5.2 QAT"></a>3.5.2 QAT</h4><p>对于v1.0，我们将伪量化器应用于从第2节获得的非敏感层。5.2执行量化感知训练并将其称为部分QAT。我们将结果与表16中的完整QAT。部分QAT导致更好的准确性，但吞吐量略有降低。由于v2中的量化敏感层的去除。0版本，我们直接在使用RepOptimizer训练的YOLOv 6-S上使用完整的QAT。我们通过图优化消除插入量化器，以获得更高的精度和更快的速度。我们在表17中比较来自PaddleSlim [30]的基于蒸馏的量化结果。请注意，我们的YOLOv 6-S的量化版本是最快和最准确的，也请参见图1.</p><blockquote><p>对于v1.0，作者将假量化器应用于非敏感层，进行量化感知训练，并称之为部分QAT。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table16.png" alt=""></p><h4 id="结论-7"><a href="#结论-7" class="headerlink" title="结论"></a>结论</h4><p>Partial QAT (只对敏感层进行量化)比full QAT性能更佳，但耗时略增加</p><p>在v2.0版本中删除了量化敏感层，作者直接在使用RepOptimizer训练的YOLOv6-S上使用全QAT。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov6-Table17.png" alt=""></p><h4 id="结论-8"><a href="#结论-8" class="headerlink" title="结论"></a>结论</h4><p>表明作者量化的YOLOv6-S速度快性能佳，其余检测器使用PaddleSlim中基于蒸馏量化方法。</p></blockquote><h2 id="四、-Conclusion—结论"><a href="#四、-Conclusion—结论" class="headerlink" title="四、 Conclusion—结论"></a>四、 Conclusion—结论</h2><p>简而言之，考虑到持续的工业需求，我们提出了YOLOv6的当前形式，仔细研究了迄今为止物体探测器组件的所有进步，同时灌输了我们的思想和实践。其结果在精度和速度上都超过了其他可用的实时检测器。为了方便工业部署，我们还为YOLOv6提供了定制的量化方法，使其成为开箱即用的快速检测器。我们衷心感谢学术界和工业界的杰出想法和努力。未来，我们将继续扩大该项目，以满足更高的标准和更苛刻的场景。</p><blockquote><p>YOLOv6在精度和速度上都超过了其他可用的目标检测器。为了方便工业部署，作者还为YOLOv6提供了定制的量化方法，使其成为开箱 即用的快速检测器。</p><h4 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h4><p>（1）RepVGG提出的结构重参数化方法表现良好，但在此之前没有检测模型使用。作者认为RepVGG的block缩放不合理，小模型和大模型</p><p>（2）没必要保持相似网络结构；小模型使用单路径架构，大模型就不适合在单路径上堆参数量。</p><p>（3）使用重参数化的方法后，检测器的量化也需要重新考虑，否则因为训练和推理时的结构不同，性能可能会退化。</p><p>（4）前期工作很少关注部署。前期工作中，推理是在V100等高配机器完成的，但实际使用时往往用T4等低功耗推理gpu，作者更关注后者</p><p>的性能。</p><p>（5）针对网络结构的变化，重新考虑标签分配和损失函数。</p><p>（6）对于部署，可以调整训练策略，在不增加推理成本的情况下提升性能，如使用知识蒸馏。</p><h4 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h4><p> （1）在不同的工业落地场景下，设计了不同的模型，兼顾精度与速度。其中，小模型为单分支，大模型为多分支。    </p><p> （2）在分类和回归任务上都使用自蒸馏策略，动态调整教师模型和标签，便于学生模型的训练。</p><p> （3）分析了各种标签分配、损失函数和数据增强技术，选择合适的策略进一步提升性能。</p><p> （4） 基于RepOptimizer优化器和通道蒸馏，对量化方式做了改进。</p><h4 id="未来完善"><a href="#未来完善" class="headerlink" title="未来完善"></a>未来完善</h4><ol><li><p>完善 YOLOv6 全系列模型，持续提升检测性能。</p></li><li><p>在多种硬件平台上，设计硬件友好的模型。</p></li><li><p>支持 ARM 平台部署以及量化蒸馏等全链条适配。</p></li><li><p>横向拓展和引入关联技术，如半监督、自监督学习等等。</p></li><li><p>探索 YOLOv6 在更多的未知业务场景上的泛化性能。</p></li></ol></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>yolov4-paper</title>
      <link href="/yolov4-paper/"/>
      <url>/yolov4-paper/</url>
      
        <content type="html"><![CDATA[<h1 id="《YOLOv4：Optimal-Speed-and-Accuracy-of-Object-Detection》"><a href="#《YOLOv4：Optimal-Speed-and-Accuracy-of-Object-Detection》" class="headerlink" title="《YOLOv4：Optimal Speed and Accuracy of Object Detection》"></a>《YOLOv4：Optimal Speed and Accuracy of Object Detection》</h1><h2 id="Abstract—摘要"><a href="#Abstract—摘要" class="headerlink" title="Abstract—摘要"></a><strong>Abstract—摘要</strong></h2><p>大量的特征据说可以提高卷积神经网络(CNN)的精度。需要在大数据集上对这些特征的组合进行实际测试，并对结果进行理论证明。有些特性只适用于某些模型，只适用于某些问题，或仅适用于小规模数据集；而一些特性，如批处理标准化和残差连接，适用于大多数模型、任务和数据集。我们假设这些普遍特征包括加权残差连接(WRC)、跨阶段部分连接(CSP)、交叉小批归一化(CmBN)、自我对抗训练(SAT)和Mish激活。我们使用新功能：WRC，CSP，CmBN，SAT，Mish激活，Mosaic数据增强、CmBN，DropBlock正则化和CIoU损失，并结合其中一些实现最先进的结果：43.5%AP，(65.7%AP50)的实时速度∼65FPS Tesla V100。源代码是在<a href="https://github.com/AlexeyAB/darknet.。">https://github.com/AlexeyAB/darknet.。</a> </p><blockquote><h4 id="提高CNN准确性的方法"><a href="#提高CNN准确性的方法" class="headerlink" title="提高CNN准确性的方法"></a><strong>提高CNN准确性的方法</strong></h4><p>（1）<strong>专用特性：</strong> 一些特征只针对某一模型，某一问题，或仅为小规模数据集</p><p>（2）<strong>通用特性：</strong> 一些特性，如批处理规范化和残差连接，则适用于大多数模型、任务和数据集。这些通用特性包括加权剩余连接(WRC)、跨阶段部分连接(CSP)、跨小批标准化(CmBN)、自反训练(SAT)和Mish 激活函数。</p><h4 id="YOLOv4使用的技巧"><a href="#YOLOv4使用的技巧" class="headerlink" title="YOLOv4使用的技巧"></a>YOLOv4使用的技巧</h4><p><strong>使用新特性：</strong>WRC、CSP、CmBN、SAT、Mish 激活函数、Mosaic数据增强、CmBN、DropBlock正则化、CIoU损失，结合这些技巧实现先进的结果。</p><h4 id="实现结果"><a href="#实现结果" class="headerlink" title="实现结果"></a>实现结果</h4><p>在Tesla V100上，MS COCO数据集以65 FPS的实时速度达到43.5 % AP ( 65.7 % AP50 )。</p></blockquote><h2 id="一、-Introduction—简介"><a href="#一、-Introduction—简介" class="headerlink" title="一、 Introduction—简介"></a><strong>一、 Introduction—简介</strong></h2><p>大多数基于cnn的对象检测器基本上只适用于推荐系统。例如，通过城市摄像机搜索免费停车位是由慢速精确的模型执行的，而汽车碰撞警告与快速不准确的模型有关。为了提高实时目标检测器的精度，不仅可以将它们用于提示生成推荐系统，还可以用于独立的流程管理和减少人工输入。在传统图形处理单元(GPU)上的实时对象检测器操作允许它们以可承受的价格大规模使用。最精确的现代神经网络不能实时运行，需要大量的gpu来进行大的小批量训练。我们通过创建一个在普通的GPU上实时运行的CNN来解决这些问题，而其训练只需要一个普通的GPU。<br>这项工作的主要目标是在生产系统中设计一个目标检测器的快速运行速度，并优化并行计算，而不是低计算体积理论指标(BFLOP)。我们希望所设计的对象能够方便地训练和使用。例如，任何使用普通的GPU进行训练和测试的人都可以实现实时、高质量和令人信服的目标检测结果，如图1所示的YOLOv4结果所示。我们的贡献总结如下：</p><p>1.我们开发了一个高效而强大的目标检测模型。它使每个人都可以使用一个1080 Ti或2080 Ti GPU来训练一个超快和准确的目标探测器。</p><p>2.我们验证了state-of-the-art Bag-of Freebies and Bag-of-Specials对目标检测的影响。</p><p>3.我们修改了最先进的方法，使其更有效，更适合于单一的GPU训练，包括CBN[89]，PAN[49]，SAM[85]等。</p><blockquote><h4 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h4><p>（1）<strong>改进性能：</strong> 大多数基于CNN的目标检测器主要只适用于推荐系统，因此需要提高实时目标探测器的准确性。</p><p>（2）<strong>单GPU训练：</strong> 最精确的现代神经网络不能实时运行，需要大量的GPU来进行大规模的小批量训练。我们通过创建一个在常规GPU上实时运行的CNN来解决这些问题，而训练只需要一个常规GPU。</p><h4 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h4><p>设计生产系统中目标检测器的快速运行速度，优化并行计算，而不是低计算量理论指标 （BFLOP）。</p><h4 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h4><p>（1）开发了一个高效、强大的目标检测模型。使用单个1080 Ti或2080 Ti GPU就能训练一个超级快速和精确的目标探测器。</p><p>（2）验证了在检测器训练过程中，最先进的Bag-of-Freebies 和Bag-of-Specials对目标检测方法的影响。</p><p>（3）修改了最先进的方法，使其更有效，更适合于单GPU训练。</p><blockquote><p><strong>Q： Bag-of-Freebies 和Bag-of-Specials</strong></p><p>Bag-of-Freebies： 指不会显著影响模型测试速度和模型复杂度的技巧，主要就是数据增强操作、标签软化等外在训练方法，即不需要改变网络模型。</p><p>Bag-of-Specials： 是用最新最先进的方法（网络模块）来魔改检测模型。只增加少量推理成本但能显著提高对象检测精度的插件模块和后处理方法，一般来说，这些插件模块都是为了增强模型中的某些属性，如扩大感受野、引入注意力机制或加强特征整合能力等，而后处理是筛选模型预测结果的一种方法。</p></blockquote></blockquote><h2 id="二、Related-work—相关工作"><a href="#二、Related-work—相关工作" class="headerlink" title="二、Related work—相关工作"></a><strong>二、Related work—相关工作</strong></h2><h3 id="2-1-Object-detection-models—目标检测模型"><a href="#2-1-Object-detection-models—目标检测模型" class="headerlink" title="2.1 Object detection models—目标检测模型"></a>2.1 Object detection models—目标检测模型</h3><p>现代探测器通常由两部分组成，一个是在ImageNet上预先训练的主干，另一个是用于预测物体的类和边界框的头部。对于那些运行在GPU平台上的检测器，它们的主干可以是VGG[68]、ResNet[26]、ResNeXt[86]或DenseNet[30]。对于那些运行在CPU平台上的检测器，它们的主干可以是SqueezeNet [31]、MobileNet[28,66,27,74]或ShufflfleNet[97,53]。对于头部部分，通常可分为一级目标探测器和两级目标探测器两类。最具代表性的两级目标探测器是R-CNN[19]系列，包括Fast R-CNN[18]、Faster R-CNN[64]、R-FCN[9]和Libra R-CNN[58].也可以使一个两级目标检测器成为一个无锚点的目标检测器，如反应点[87]。对于单级目标探测器，最具代表性的模型是YOLO[61,62,63]、SSD[50]和RetinaNet[45]。近年来，无锚的单级目标探测器已经发展起来。这类检测器有CenterNet [13]、CornerNet [37,38]、FCOS[78]等。近年来开发的目标探测器经常在主干和头部之间插入一些层，这些层通常用于收集不同阶段的特征图。我们可以称之为物体探测器的颈部。通常，颈部由几条自下向上的路径和几条自上向下的路径组成。配备这种机制的网络包括特征金字塔网络(FPN)[44]、路径聚合网络(PAN)[49]、BiFPN[77]和NAS-FPN[17]。</p><p>除了上述模型外，一些研究人员还强调了直接构建一个新的主干(DetNet[43]，DetNAS[7])或一个新的整体模型(SpineNet[12]，HitDetector[20])用于目标检测。 </p><p>综上所述，一个普通的物体探测器由以下几个部分组成：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4%E7%89%A9%E4%BD%93%E6%8E%A2%E6%B5%8B%E5%99%A8.png" alt=""></p><blockquote><h4 id="现代目标检测器组成"><a href="#现代目标检测器组成" class="headerlink" title="现代目标检测器组成"></a>现代目标检测器组成</h4><p><strong>（1）主干backbone：</strong> 在ImageNet上预先训练的网络用来特征提取。</p><ul><li>在<strong>GPU</strong>平台上运行的检测器，主干可以是VGG、ResNet、ResNeXt或DenseNet。</li><li>在<strong>CPU</strong>平台上运行的检测器，主干可以是SqueezeNet、MobileNet或ShuffleNet。</li></ul><p><strong>（2）头部head：</strong> 对图像特征进行预测，生成边界框和并预测类别。通常分为两类即单阶段目标检测器和两阶段目标检测器。</p><ul><li><strong>two stage：</strong> R-CNN系列，包括fast R-CNN、faster R-CNN、R-FCN和Libra R-CNN。</li><li><strong>one stage：</strong> 最具代表性的模型有YOLO、SSD和RetinaNet。</li></ul><p><strong>（3）颈部neck：</strong> 近年来发展起来的目标检测器常常在主干和头部之间插入一系列混合和组合图像特征的网络层，并将图像特征传递到预测层。称之为目标检测器的颈部neck。</p><p>通常，一个颈部neck由几个自下而上的路径和几个自上而下的路径组成。具有该机制的网络包括特征金字塔网络(FPN)、路径汇聚网络(PAN)、BiFPN和NAS-FPN。</p><p>综上所述，一个普通的物体探测器是由“特征输入、骨干网络、颈部和头部”四部分组成的：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-%E7%89%A9%E4%BD%93%E6%8E%A2%E6%B5%8B%E5%99%A8%E7%BB%84%E6%88%90.png" alt=""></p></blockquote><h3 id="2-2-Bag-of-freebies"><a href="#2-2-Bag-of-freebies" class="headerlink" title="2.2 Bag of freebies"></a>2.2 Bag of freebies</h3><p>通常，一个传统的目标检测器是离线训练的。因此，研究者总是喜欢利用这一优势，开发出更好的训练方法，使目标检测器在不增加推理成本的情况下获得更好的精度。我们把这些只会改变培训策略或只增加培训成本的方法称为“bag of freebies”。目标检测方法经常采用的、满足bag of freebies.定义的是数据增强。数据增强的目的是为了增加输入图像的可变性，从而使所设计的目标检测模型对在不同环境下获得的图像具有更高的鲁棒性。例如，光度畸变和几何畸变是两种常用的数据增强方法，它们肯定有利于目标检测任务。在处理光度失真时，我们会调整图像的亮度、对比度、色调、饱和度和噪声。对于几何失真，我们添加了随机缩放、裁剪、翻转和旋转。 </p><p>上述数据增强方法都是像素级调整，并保留调整区域中的所有原始像素信息。此外，一些从事数据增强工作的研究人员将其重点放在了模拟对象遮挡问题上。它们在图像分类和目标检测方面取得了良好的效果。例如，随机擦除[100]和CutOut[11]可以随机选择图像中的矩形区域，并填充一个随机的或互补的零值。对于hide-and-seek[69]和grid mask[6]，它们随机或均匀地选择一个图像中的多个矩形区域，并将它们替换为所有的零。如果将类似的概念应用于特征映射，则会有DropOut[71]、Drop连接[80]和DropBlock[16]方法。此外，一些研究者提出了使用多个图像一起进行数据增强的方法。例如，MixUp[92]使用两幅图像用不同的系数比进行乘法和叠加，然后用这些叠加的比率来调整标签。 </p><p>CutMix[91]是将裁剪后的图像覆盖到其他图像的矩形区域，并根据混合区域的大小调整标签。除上述方法外，还采用了样式转移GAN[15]进行数据增强，这种使用可以有效地减少CNN学习到的纹理偏差。</p><p>与上面提出的各种方法不同，其他一些bag of freebies都致力于解决数据集中的语义分布可能存在偏差的问题。在处理语义分布偏差问题时，一个非常重要的问题是不同类之间存在数据不平衡的问题，这个问题通常通过两级对象检测器中的硬负例挖掘[72]或在线硬例挖掘[67]来解决。但该示例挖掘方法不适用于单级对象检测器，因为这种检测器属于密集预测体系结构。因此，Lin等人[45]提出了焦点损失来解决不同类之间存在的数据不平衡问题。另一个非常重要的问题是，很难表达不同类别之间的关联程度与单一热硬表示之间的关系。这种表示方案经常用于执行标记。[73]中提出的标签平滑方法是将硬标签转换为软标签进行训练，使模型的鲁棒性更强。为了获得更好的软标签，Islam等人引入了知识精馏的概念来设计标签细化网络</p><p>最后bag of freebies是边界盒(BBox)回归的目标函数。传统的对象检测器通常使用均方误差(MSE)直接对BBox的中心点坐标和高度和宽度进行回归，{,w、h}或左上角点和右下角点。对于基于锚的方法，是估计相应的偏移量，例如和,然而，直接估计BBox中每个点的坐标值是要将这些点作为自变量来处理，但实际上并没有考虑对象本身的完整性。为了更好地处理这一问题，一些研究人员最近提出了IoU损失[90]，它考虑了预测的BBox区域和地面真实BBox区域的覆盖范围。IoU损失计算过程将通过使用地面真相执行IoU，触发BBox的四个坐标点的计算，然后将生成的结果连接到一个整个代码中。由于IoU是一种尺度不变表示，它可以解决传统方法计算{x、y、w、h}的l1或l2损失时，损失会随着尺度的增加而增加的问题。最近，一些研究人员继续改善IoU的损失。例如，GIoU损失[65]除了包括覆盖区域外，还包括物体的形状和方向。他们提出找到能够同时覆盖预测的BBox和地面真实BBox的最小面积的BBox，并使用该BBox作为分母来代替IoU损失中最初使用的分母。对于DIoU损失[99]，它另外考虑了物体中心的距离，而CIoU损失[99]则同时考虑了重叠面积、中心点之间的距离和高宽比。CIoU在BBox回归问题上可以获得更好的收敛速度和精度。</p><blockquote><h4 id="BoF方法一：数据增强"><a href="#BoF方法一：数据增强" class="headerlink" title="BoF方法一：数据增强"></a>BoF方法一：数据增强</h4><p><strong>（1）像素级调整</strong></p><p>①光度失真： brightness(亮度)、contrast(对比度)、hue(色度)、saturation(饱和度)、noise(噪声)</p><p>②几何失真： scaling(缩放尺寸)、cropping(裁剪)、flipping(翻转)、rotating(旋转)</p><p><strong>（2）模拟目标遮挡</strong></p><p>①erase(擦除)、CutOut(剪切)： 随机选择图像的矩形区域，并填充随机或互补的零值</p><p>②hide-and-seek和grid mask： 随机或均匀地选择图像中的多个矩形区域，并将它们替换为全零</p><p>③将上述方式作用于特征图上： DropOut、DropConnect、DropBlock</p><p><strong>（3）将多张图像组合在一起</strong></p><p>①MixUp： 使用两个图像以不同的系数比率相乘后叠加，利用叠加比率调整标签</p><p>②CutMix： 将裁剪的图像覆盖到其他图像的矩形区域，并根据混合区域大小调整标签</p><p><strong>（4）使用style transfer GAN进行数据扩充，有效减少CNN学习到的纹理偏差。</strong></p><h4 id="BoF方法二：解决数据集中语义分布偏差问题"><a href="#BoF方法二：解决数据集中语义分布偏差问题" class="headerlink" title="BoF方法二：解决数据集中语义分布偏差问题"></a>BoF方法二：解决数据集中语义分布偏差问题</h4><p><strong>①两阶段对象检测器：</strong> 使用硬反例挖掘或在线硬例挖掘来解决。不适用于单级目标检测。</p><p><strong>②单阶段目标检测器：</strong> focal损来处理各个类之间存在的数据不平衡问题。</p><h4 id="BoF方法三：边界框-BBox-回归的目标函数"><a href="#BoF方法三：边界框-BBox-回归的目标函数" class="headerlink" title="BoF方法三：边界框(BBox)回归的目标函数"></a>BoF方法三：边界框(BBox)回归的目标函数</h4><p><strong>①IoU损失：</strong> 将预测BBox区域的区域和真实BBox区域考虑在内。由于IoU是尺度不变的表示，它可以解决传统方法在计算{x, y, w, h}的l1或l2损耗时，损耗会随着尺度的增大而增大的问题。</p><p><strong>②GIoU loss：</strong> 除了覆盖区域外，还包括了物体的形状和方向。他们提出寻找能够同时覆盖预测BBox和地面真实BBox的最小面积BBox，并以此BBox作为分母来代替IoU损失中原来使用的分母。</p><p><strong>③DIoU loss：</strong> 它额外考虑了物体中心的距离。</p><p><strong>④CIoU loss ：</strong> 同时考虑了重叠区域、中心点之间的距离和纵横比。对于BBox回归问题，CIoU具有更好的收敛速度和精度。</p></blockquote><h3 id="2-3-Bag-of-specials"><a href="#2-3-Bag-of-specials" class="headerlink" title="2.3 Bag of specials"></a>2.3 Bag of specials</h3><p>对于那些只增加少量推理成本但又能显著提高目标检测精度的插件模块和后处理方法，我们称它们为“bag of specials”。一般来说，这些插件模块是用于增强模型中的某些属性，如扩大接受域、引入注意机制或增强特征整合能力等，而后处理是筛选模型预测结果的一种方法。 </p><p>可用于增强感受野的常见模块是SPP[25]、ASPP[5]和RFB[47]。SPP模块起源于空间金字塔匹配(SPM)[39]，SPMs的原始方法是将特征映射分割成几个d×d相等的块，其中d可以是{1,2,3，…}，从而形成空间金字塔，然后提取bag-of-word特征。SPP将SPM集成到CNN中，使用最大池化操作，而不是bag-of-word操作。由于He等人[25]提出的SPP模块将输出一维特征向量，因此在全卷积网络(FCN)中应用是不可行的。因此，在YOLOv3[63]的设计中，Redmon和Farhadi将SPP模块改进为核大小为k×k，其中k={1,5,9,13}，步幅等于1。在这种设计下，相对较大的最大池有效地增加了主干特征的接受域。 在添加改进版本的SPP模块后，YOLOv3-608在MS COCO目标检测任务上将AP50升级了2.7%，额外计算0.5%。ASPP[5]模块与改进的SPP模块在操作上的差异主要是从原始的k×k核大小，步幅最大池化等于1到多个3×3核大小，扩张比等于k，步幅等于1。RFB模块采用k×k核的多个扩张卷积，扩张比等于k，步幅等于1，以获得比ASPP更全面的空间覆盖。RFB[47]只需要花费7%的额外推理时间，就可以使MS COCO上的SSD的AP50增加5.7%。</p><p>目标检测中常用的注意模块主要分为通道式注意和点态注意，这两种注意模型的代表分别是Squeeze-and-Excitation (SE)[29]和空间注意模块(SAM)[85]。虽然SE模块可以提高ResNet50的力量在ImageNet图像分类任务1%top-1精度的只增加2%计算，但在GPU通常将使推理时间增加约10%，所以它更适合用于移动设备。但对于SAM，它只需要额外支付0.1%的计算量，就可以将ResNet50-SE提高到ImageNet图像分类任务的0.5%的top-1精度。最重要的是，它根本不影响GPU上的推理速度。</p><p>在特征集成方面，早期的实践是使用skip connection[51]或hyper-column[22]将低级物理特征与高级语义特征进行集成。随着FPN等多尺度预测方法越来越流行，人们提出了许多整合不同特征金字塔的轻量级模块。这类模块包括SFAM[98]、ASFF[48]和BiFPN[77]。SFAM的主要思想是利用SE模块在多尺度连接的特征图上执行信道级重加权。对于ASFF，它使用softmax作为点级重新加权，然后添加不同尺度的特征图。在BiFPN中，提出了多输入加权残差连接来进行尺度水平重加权，然后添加不同尺度的特征图。</p><p>在深度学习的研究中，一些人将重点放在寻找良好的激活函数上。一个好的激活函数可以使梯度更有效地传播，同时也不会造成太多的额外计算成本。2010年，Nair和Hinton[56]提出ReLU来实质上解决传统的tanh和s型激活函数中经常遇到的梯度消失问题。随后，提出了LReLU[54]、PReLU[24]、ReLU6[28]、尺度指数线性单位(SELU)[35]、Swish[59]、hard-Swish[27]、Mish[55]等，它们也被用于解决梯度消失问题。LReLU和PReLU的主要目的是解决当输出小于零时，ReLU的梯度为零的问题。对于ReLU6和hard-swish，它们是专门为量化网络设计的。对于神经网络的自归一化，提出了SELU激活函数来满足该目标。需要注意的一点是，Swish和Mish都是连续可区分的激活函数。 </p><p>在基于深度学习的对象检测中常用的后处理方法是NMS，它可以用于过滤那些预测同一对象不好的预测框，并且只保留响应率较高的候框。NMS试图改进的方法与优化目标函数的方法是一致的。NMS提出的原始方法不考虑上下文信息，因此Girshick等[19]在R-CNN中添加分类置信分数作为参考，根据置信分数的顺序，按照高到低的顺序进行greedy NMS。对于soft NMS[1]，它考虑了对象的遮挡在greedy NMS中可能导致置信度分数下降的问题。DIoU NMS[99]开发者的思维方式是在soft NMS的基础上，将中心点距离的信息添加到BBox的筛选过程中。值得一提的是，由于上述所有的后处理方法都没有直接涉及到所捕获的图像特征，因此在后续的无锚定方法的开发中，不再需要后处理。</p><blockquote><h4 id="BoS方法一：插件模块之增强感受野"><a href="#BoS方法一：插件模块之增强感受野" class="headerlink" title="BoS方法一：插件模块之增强感受野"></a>BoS方法一：插件模块之增强感受野</h4><p><strong>①改进的SPP模块</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-SPP.png" alt=""></p><p><strong>②ASPP模块</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-ASPP.png" alt=""></p><p><strong>③RFB模块</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-RFB.png" alt=""></p><h4 id="BoS方法二：插件模块之注意力机制"><a href="#BoS方法二：插件模块之注意力机制" class="headerlink" title="BoS方法二：插件模块之注意力机制"></a>BoS方法二：插件模块之注意力机制</h4><p><strong>①channel-wise注意力：</strong> 代表是Squeeze-and-Excitation挤压激励模块(SE)。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-SE.png" alt=""></p><p><strong>②point-wise注意力：</strong> 代表是Spatial Attention Module空间注意模块(SAM)。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-SAM.png" alt=""></p><h4 id="BoS方法三：插件模块之特征融合"><a href="#BoS方法三：插件模块之特征融合" class="headerlink" title="BoS方法三：插件模块之特征融合"></a>BoS方法三：插件模块之特征融合</h4><p><strong>①SFAM：</strong> 主要思想是利用SE模块在多尺度的拼接特征图上进行信道级重加权。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-SFAM.png" alt=""></p><p><strong>②ASFF：</strong> 使用softmax对多尺度拼接特征图在点维度进行加权。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-ASFF.png" alt=""></p><p><strong>③BiFPN：</strong> 提出了多输入加权剩余连接来执行按比例的水平重加权，然后添加不同比例的特征图。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-BiFPN.png" alt=""></p><h4 id="BoS方法四：激活函数"><a href="#BoS方法四：激活函数" class="headerlink" title="BoS方法四：激活函数"></a>BoS方法四：激活函数</h4><p><strong>①LReLU和PReLU：</strong> 主要目的是解决输出小于0时ReLU的梯度为零的问题。</p><p><strong>②ReLU6和hard-Swish：</strong> 专门为量化网络设计的。</p><p><strong>③SELU：</strong> 针对神经网络的自归一化问题。</p><p><strong>④Swish和Mish：</strong> 都是连续可微的激活函数。</p><h4 id="BoS方法五：后处理"><a href="#BoS方法五：后处理" class="headerlink" title="BoS方法五：后处理"></a>BoS方法五：后处理</h4><p><strong>①NMS：</strong> 目标检测中常用的后处理方法是NMS, NMS可以对预测较差的bbox进行过滤，只保留响应较高的候选bbox。NMS试图改进的方法与优化目标函数的方法是一致的。NMS提出的原始方法没有考虑上下文信息，所以在R-CNN中加入了分类的置信分作为参考，按照置信分的顺序，从高到低依次进行贪心NMS。</p><p><strong>②soft NMS：</strong> 考虑了对象的遮挡可能导致带IoU分数的贪婪NMS的信心分数下降的问题。</p><p><strong>③DIoU NMS：</strong> 在soft NMS的基础上，将中心点距离信息添加到BBox筛选过程中。值得一提的是，由于以上的后处理方法都没有直接引用捕获的图像特征，因此在后续的无锚方法开发中不再需要后处理。</p></blockquote><h2 id="三、Methodology—方法"><a href="#三、Methodology—方法" class="headerlink" title="三、Methodology—方法"></a><strong>三、Methodology—方法</strong></h2><h3 id="3-1-Selection-of-architecture—架构选择"><a href="#3-1-Selection-of-architecture—架构选择" class="headerlink" title="3.1 Selection of architecture—架构选择"></a>3.1 Selection of architecture—架构选择</h3><p>我们的目标是在输入网络分辨率、卷积层数、参数数（滤波器大小2<em>滤波器</em>通道/组）和层输出数（滤波器）之间找到最优的平衡。例如，我们的大量研究表明，在ILSVRC2012(ImageNet)数据集[10]上，CSPResNext50比CSPDarknet53要好得多。然而，相反地，在检测MS COCO数据集[46]上的对象方面，CSPDarknet53比CSPResNext50更好。</p><p>下一个目标是选择额外的块来增加感受野，以及从不同检测器级别的参数聚合的最佳方法：例如FPN、PAN、ASFF、BiFPN。 </p><p>对于分类最优的参考模型对于探测器来说并不总是最优的。与分类器相比，该探测器需要以下条件：</p><ul><li>更高的输入网络大小（分辨率）</li><li>用于检测多个小大小的物体更多的层</li><li>更高的接受域以覆盖增加的输入网络大小更多的参数</li><li>模型更大的能力来检测单一图像中多个不同大小的物体</li></ul><p>假设来说，我们可以假设应该选择一个具有更大的接受场大小（具有更多的卷积层3×3）和更多的参数的模型作为主干。表1显示了CSPResNeXt50、CSPDarknet53和efficientnetB3的信息。CSPResNext50只包含16个卷积层3×3、一个425×425感受野和20.6 M参数，而CSPDarknet53包含29个卷积层3×3、一个725×725感受野和27.6 M参数。这一理论证明，加上我们进行的大量实验，表明CSPDarknet53神经网络是两者作为探测器主干的最佳模型。 </p><p>不同大小的感受野的影响总结如下：</p><ul><li>到对象大小，允许查看整个对象到网络大小</li><li>允许查看对象周围的上下文</li><li>增加图像点和最终激活之间的连接数量 </li></ul><p>我们在CSPDarknet53上添加了SPP块，因为它显著地增加了接受域，分离出了最重要的上下文特征，并且几乎不会导致降低网络运行速度。我们使用PANet作为来自不同检测器级别的不同主干级别的参数聚合的方法，而不是在YOLOv3中使用的FPN。</p><p>最后，我们选择CSPDarknet53主干、SPP附加模块、PANet路径聚合颈和YOLOv3（基于锚点）的头作为YOLOv4的体系结构。</p><p>未来，我们计划显著扩展检测器的f Bag of Freebies(BoF)的内容，理论上可以解决一些问题，提高检测器的精度，并以实验方式依次检查每个特征的影响。<br>我们不使用Cross-GPU批处理归一化(CGBN或SyncBN)或昂贵的专用设备。这允许任何人都可以在传统的图形处理器上再现我们最先进的结果，例如GTX 1080Ti或RTX 2080Ti。 </p><blockquote><h4 id="架构选择目标"><a href="#架构选择目标" class="headerlink" title="架构选择目标"></a>架构选择目标</h4><p><strong>目标一：在输入网络分辨率、卷积层数、参数数(filter size2×filters × channel / groups)和层输出数(filters)之间找到最优平衡。</strong></p><p>检测器需要满足以下条件：</p><p><strong>①更高的输入网络大小(分辨率)：</strong> 用于检测多个小型对象</p><p><strong>②更多的层：</strong> 一个更高的接受域，以覆盖增加的输入网络的大小</p><p><strong>③更多的参数：</strong> 模型有更强大的能力，以检测单个图像中的多个不同大小的对象。</p><p><strong>目标二：选择额外的块来增加感受野</strong></p><p>不同大小的感受野的影响总结如下：</p><p><strong>①对象大小：</strong> 允许查看整个对象</p><p><strong>②网络大小：</strong> 允许查看对象周围的上下文</p><p><strong>③超过网络大小：</strong> 增加图像点和最终激活之间的连接数</p><p><strong>目标三：选择不同的主干层对不同的检测器层(如FPN、PAN、ASFF、BiFPN)进行参数聚合的最佳方法。</strong></p><h4 id="YOLOv4架构"><a href="#YOLOv4架构" class="headerlink" title="YOLOv4架构"></a>YOLOv4架构</h4><p><strong>（1）CSPDarknet53主干（backbone）：</strong> 作者实验对比了CSPResNext50、CSPDarknet53和EfficientNet-B3。从理论与实验角度表明：CSPDarkNet53更适合作为检测模型的Backbone。（还是自家的网络结构好用）</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-backbone.png" alt=""></p><blockquote><p>CSP介绍：</p><p>CSP是可以增强CNN学习能力的新型backbone，论文发表2019年11月份</p><p><strong>主要技巧：</strong>CSPNet将底层的特征映射分为两部分，一部分经过密集块和过渡层，另一部分与传输的特征映射结合到下一阶段。</p></blockquote><p><strong>（2）SPP附加模块增加感受野：</strong> 在CSPDarknet53上添加了SPP块，SPP来源于何恺明大佬的SPP Net因为它显著增加了接受域，分离出了最重要的上下文特性，并且几乎不会降低网络运行速度。</p><p><strong>（3）PANet路径聚合（neck）：</strong> PANet主要是特征融合的改进，使用PANet作为不同检测层的不同主干层的参数聚合方法。而不是YOLOv3中使用的FPN。</p><p><strong>（4）基于锚的YOLOv3头部（head）：</strong> 因为是anchor-base方法，因此分类、回归分支没有改变。</p><p><strong>总结：</strong> YOLOv4模型 = CSPDarkNet53 + SPP + PANet(path-aggregation neck) + YOLOv3-head</p></blockquote><h3 id="3-2-Selection-of-BoF-and-BoS—BoF和BoS的选择"><a href="#3-2-Selection-of-BoF-and-BoS—BoF和BoS的选择" class="headerlink" title="3.2 Selection of BoF and BoS—BoF和BoS的选择"></a>3.2 Selection of BoF and BoS—BoF和BoS的选择</h3><p>为了改进目标检测训练，CNN通常使用以下：</p><ul><li>激活：ReLU, leaky-ReLU, parametric-ReLU,ReLU6, SELU, Swish, or Mish</li><li>边界盒回归损失：MSE，IoU、GIoU、CIoU、DIoU</li><li>数据增强：CutOut, MixUp, CutMix</li><li>正则化方法：DropOut, DropPath，Spatial DropOut [79], or DropBlock</li><li>规范化的网络激活（通过均值和方差）：批标准化(BN)[32]，Cross-GPU Batch Normalization(CGBN或SyncBN)[93]，Filter Response Normalization(FRN)[70]，或交叉迭代批标准化(CBN)[89]</li><li>Skip-connections：Residual connections，加权Residual connections、多输入加权Residual connections或Cross stage partial连接(CSP) </li></ul><p>对于训练激活函数，由于PReLU和SELU更难训练，而且ReLU6是专门为量化网络设计的，因此我们将上述激活函数从候选列表中删除。在需求化方法上，发表DropBlock的人将其方法与其他方法进行了详细的比较，其正则化方法获得了很大的成功。因此，我们毫不犹豫地选择了DropBlock作为我们的正则化方法。至于归一化方法的选择，由于我们关注于只使用一个GPU的训练策略，因此不考虑syncBN。 </p><blockquote><p>为了提高目标检测训练，CNN通常使用以上提到的方法</p><ul><li><p><strong>（1）激活函数：</strong> 由于PReLU和SELU更难训练，我们选择专门为量化网络设计的ReLU6</p></li><li><p><strong>（2）正则化：</strong> 我们选择DropBlock</p></li><li><strong>（3）归一化：</strong> 由于是单GPU，所以没有考虑syncBN</li></ul></blockquote><h3 id="3-3-Additional-improvements—进一步改进"><a href="#3-3-Additional-improvements—进一步改进" class="headerlink" title="3.3 Additional improvements—进一步改进"></a>3.3 Additional improvements—进一步改进</h3><p>为了使设计的探测器更适合训练单GPU上，我们做了额外的设计和改进如下：</p><p> 我们引入了一种新的数据增强Mosic，和自我对抗训练（SAT）<br>我们选择最优超参数而应用遗传算法<br>我们修改一些现有方法使设计适合有效的训练和检测，modifified SAM，modifified PAN，和交叉小批归一化(CmBN) </p><p>Mosaic代表了一种新的数据增强方法，它混合了4个训练图像。因此，混合了4种不同的上下文，而CutMix只混合了2个输入图像。这允许检测其正常上下文之外的对象。此外，批归一化计算每一层上4个不同图像的激活统计信息。这大大减少了对大型小批量处理的需求</p><p>自对抗训练(SAT)也代表了一种新的数据增强技术，可以在2个向前向后的阶段运行。在第一阶段，神经网络改变了原始图像，而不是网络的权值。通过这种方式，神经网络对自己进行敌对性攻击，改变原始图像，以制造出图像上没有想要的目标的欺骗。在第二阶段，神经网络被训练以正常的方式检测修改后的图像上的对象。 </p><blockquote><p><strong>（1）新的数据增强Mosic和自我对抗训练（SAT）</strong><br>①Mosaic： Mosaic代表了一种新的数据增强方法，它混合了4幅训练图像。基于现有数据极大的丰富了样本的多样性，极大程度降低了模型对于多样性学习的难度。</p><p><strong>②自对抗训练（SAT）：</strong></p><p>在第一阶段，神经网络改变原始图像而不是网络权值。通过这种方式，神经网络对自己执行一种对抗性攻击，改变原始图像，以制造图像上没有期望对象的假象。<br>在第二阶段，神经网络以正常的方式对这个修改后的图像进行检测。<br><strong>（2）应用遗传算法选择最优超参数</strong><br><strong>（3）修改现有的方法，使设计适合于有效的训练和检测</strong><br><strong>①修改的SAM：</strong> 将SAM从空间上的注意修改为点态注意</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-SAMpro.png" alt=""></p><p><strong>②修改PAN：</strong> 将PAN的快捷连接替换为shortcut 连接</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-PANpro.png" alt=""></p><p><strong>③交叉小批量标准化(CmBN)：</strong> CmBN表示CBN修改后的版本，如图所示，只在单个批内的小批之间收集统计信息。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-CmBNpro.png" alt=""></p></blockquote><h3 id="3-4-YOLOv4"><a href="#3-4-YOLOv4" class="headerlink" title="3.4 YOLOv4"></a>3.4 YOLOv4</h3><p> 在本节中，我们将详细说明YOLOv4的细节。</p><p>YOLOv4 consists of :<br>• Backbone: CSPDarknet53 [ 81 ]<br>• Neck: SPP [ 25 ], PAN [ 49 ]<br>• Head: YOLOv3 [ 63 ]</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-bag.png" alt=""></p><blockquote><h4 id="YOLOv4包括"><a href="#YOLOv4包括" class="headerlink" title="YOLOv4包括"></a>YOLOv4包括</h4><ul><li><strong>主干(backbone)：</strong> CSPDarknet53</li><li><strong>颈部(neck)：</strong> SPP ， PAN</li><li><strong>头(head)：</strong> YOLOv3</li></ul><p>YOLO v4使用</p><ul><li>Bag of Freebies 外在引入技巧： CutMix和Mosaic数据增强，DropBlock正则化，类标签平滑</li><li>Bag of Specials 网络改进技巧： Mish激活、跨级部分连接(CSP)、多输入加权剩余连接(MiWRC)</li><li>Bag of Freebies 外在检测器引入技巧： CIoU loss, CmBN, DropBlock正则化，Mosaic数据增强，自对抗训练，消除网格敏感性，为一个真值使用多个锚，余弦退火调度，最优超参数，随机训练形状</li><li>Bag of Specials检测器网络改进技巧： Mish激活、SPP-block、SAM-block、PAN路径聚合块、DIoU-NMS</li></ul></blockquote><h2 id="四、Experiments—实验"><a href="#四、Experiments—实验" class="headerlink" title="四、Experiments—实验"></a><strong>四、Experiments—实验</strong></h2><h3 id="4-1-Experimental-setup—实验设置"><a href="#4-1-Experimental-setup—实验设置" class="headerlink" title="4.1 Experimental setup—实验设置"></a>4.1 Experimental setup—实验设置</h3><p>在ImageNet图像分类实验中，默认的超参数如下：训练步骤为8000000；批大小和小批量大小分别为128和32；采用多项式衰减学习率调度策略，初始学习率为0.1；预热步骤为1000；动量衰减和权重衰减分别设置为0.9和0.005。我们所有的BoS实验都使用与默认设置相同的超参数，在BoF实验中，我们额外添加了50%的训练步骤。在BoF实验中，我们验证了MixUp、CutMix、Mosaic、模糊数据增强和标签平滑正则化方法。在BoS实验中，我们比较了LReLU、Swish和Mish激活功能的影响。所有实验均采用1080 Ti或2080TiGPU进行训练。 </p><p>在MS COCO目标检测实验中，默认的超参数如下：训练步长为500,500；采用步长衰减学习率调度策略，初始学习率为0.01，在400000步和450000步时分别乘以0.1倍；动量和权重衰减分别设置为0.9和0.0005。所有架构都使用一个GPU来执行批处理大小为64的多规模训练，而小批处理大小为8或4，这取决于架构和GPU内存限制。除在超参数搜索实验中使用遗传算法外，所有其他实验均使用默认设置。遗传算法使用YOLOv3-SPP对GIoU损失进行训练，并搜索300个时元的最小值5k集。我们采用搜索学习率0.00261，动量0.949，IoU阈值分配地面真值0.213，遗传算法实验采用损失归一化器0.07。我们验证了大量的BoF，包括网格灵敏度消除、Mosaic数据增强、IoU阈值、遗传算法、类标签平滑、交叉小批归一化、自对抗训练、余弦退火调度器、动态小批大小、dropblock、优化锚点、不同类型的IoU损失。我们还在各种BoS上进行了实验，包括Mish、SPP、SAM、RFB、BiFPN和高斯YOLO[8]。对于所有的实验，我们只使用一个GPU来进行训练，因此不使用优化多个GPU的像syncBN这样的技术。</p><blockquote><p>（1）在ImageNet图像分类实验中，默认超参数为：</p><ul><li><strong>训练步骤：</strong> 8,000,000</li><li><strong>批大小和小批大小分别：</strong> 128和32</li><li><strong>初始学习率：</strong> 0.1</li><li><strong>warm-up步长：</strong> 1000</li><li><strong>动量衰减：</strong> 0.9</li><li><strong>权重衰减：</strong> 0.005</li></ul><p>（2）在MS COCO对象检测实验中，默认的超参数为：</p><ul><li><strong>训练步骤：</strong> 500500</li><li><strong>初始学习率：</strong> 0.01</li><li><strong>warm-up步长：</strong> 在400,000步和450,000步分别乘以因子0.1</li><li><strong>动量衰减：</strong> 0.9</li><li><strong>权重衰减：</strong> 0.0005</li><li><strong>GPU数量：</strong> 1个</li><li><strong>批处理大小：</strong> 64</li></ul></blockquote><h3 id="4-2-Influence-of-different-features-on-Classifier-training—不同特征对分类器训练的影响"><a href="#4-2-Influence-of-different-features-on-Classifier-training—不同特征对分类器训练的影响" class="headerlink" title="4.2 Influence of different features on Classifier training—不同特征对分类器训练的影响"></a>4.2 Influence of different features on Classifier training—不同特征对分类器训练的影响</h3><p>首先，我们研究了不同特征对分类器训练的影响；具体来说，类标签平滑的影响，不同数据增强技术的影响，bilateral blurring, MixUp, CutMix and Mosaic，如Fugure7所示，以及不同激活的影响，如Leaky-relu（默认）、Swish和Mish。 </p><p>在我们的实验中，如表2所示，通过引入：CutMix和Mosaic数据增强、类标签平滑和Mish激活等特征，提高了分类器的精度。因此，我们用于分类器训练的BoF backbone(Bag of Freebies)包括以下内容：CutMix和Mosaic数据增强和类标签平滑。此外，我们使用Mish激活作为补充。 </p><blockquote><p>研究了不同特征对分类器训练的影响：类标签平滑的影响，不同数据增强技术的影响，不同的激活的影响。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-augmentation.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-Table23.png" alt=""></p><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>（1）通过引入特征如：CutMix和Mosaic数据增强、类标签平滑、Mish激活等，可以提高分类器的准确率。</p><p>（2）CutMix和Mosaic数据增强和类标签平滑可用于分类器训练的BoF backbone，此外，还可以使用Mish激活作为补充选项。</p></blockquote><h3 id="4-3-Influence-of-different-features-on-Detector-training—不同特征对检测器训练的影响"><a href="#4-3-Influence-of-different-features-on-Detector-training—不同特征对检测器训练的影响" class="headerlink" title="4.3 Influence of different features on Detector training—不同特征对检测器训练的影响"></a><strong>4.3 Influence of different features on Detector training—不同特征对检测器训练的影响</strong></h3><p> 进一步研究了不同的Bag-of Freebies(BoF-detector)对探测器训练精度的影响，如表4所示。通过研究在不影响FPS的情况下提高检测器精度的不同特征，我们显著地扩展了BoF列表： </p><p> S：消除网格灵敏度的公式 其中cx和cy总是整数，在YOLOv3中用于计算对象坐标，因此，对于接近cx或cx+1值的bx值，需要极高的tx绝对值。我们通过将s型矩阵乘以一个超过1.0的因子来解决这个问题，从而消除了对象无法检测到的网格的影响。</p><ul><li>M：Mosaic data-在训练期间使用4张图像的马赛克，而不是单一的图像 </li><li>IT：IoU阈值-使用多个锚作为单一地面真实IoU(truth, anchor) &gt;IoU阈值</li><li>GA：Genetic algorithms-在前10%的时间段内使用遗传算法选择最优超参数</li><li>LS:类标签平滑-使用类标签平滑的s型符号激活 </li><li>CBN：CmBN-使用交叉小批标准化来收集整个批内的统计信息，而不是在单个小批内收集统计数据</li><li>CA:余弦退火调度器-改变正弦波训练过程中的学习速率</li><li>DM：动态小批量大小-在小分辨率训练中，通过使用随机训练形状自动增加小批量大小</li><li>OA：优化的锚-使用优化的锚与512x512网络分辨率进行训练</li><li>GIoU，CIoU，DIoU，MSE-使用不同的损失算法进行边界框回归 </li></ul><p>进一步研究了不同的Bag-of-Specials (bos-检测器)对检测器训练精度的影响，包括PAN、RFB、SAM、高斯YOLO(G)和ASFF，如表5所示。在我们的实验中，检测器在使用SPP、PAN和SAM时性能最好。</p><blockquote><p>进一步的研究关注不同Bag-of-Freebies免费包 (BoF-detector)对检测器训练精度的影响，通过研究在不影响FPS（帧率：每秒传输的帧数）的情况下提高检测器精度的不同特征，我们显著扩展了BoF列表：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-Table4.png" alt=""></p><p>表4：Bag-of-Freebies 的消融研究。( CSPResNeXt50 - PANet - SPP , 512 × 512)。 粗体黑色表示有效</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-Table5.png" alt=""></p><h4 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h4><p>当使用SPP、PAN和SAM时，检测器的性能最佳。</p></blockquote><h3 id="4-4-Influence-of-different-backbones-and-pre-trained-weightings-on-Detector-training—不同的backbone和预先训练权重对检测器训练的影响"><a href="#4-4-Influence-of-different-backbones-and-pre-trained-weightings-on-Detector-training—不同的backbone和预先训练权重对检测器训练的影响" class="headerlink" title="4.4 Influence of different backbones and pre- trained weightings on Detector training—不同的backbone和预先训练权重对检测器训练的影响"></a>4.4 Influence of different backbones and pre- trained weightings on Detector training—不同的backbone和预先训练权重对检测器训练的影响</h3><p>进一步研究了不同主干模型对检测器精度的影响，如表6所示。我们注意到，具有最佳分类精度特征的模型在检测器精度方面并不总是最好的。</p><p>首先，虽然使用不同特征训练的CSPResNeXt-50模型的分类精度高于CSPDarknet53模型，但CSPDarknet53模型在目标检测方面具有更高的精度。</p><p>其次，使用CSPResF和Mish进行50分类器训练可以提高分类精度，但进一步应用这些预先训练的权重用于检测器训练会降低检测器的精度。然而，在CSPDarknet53分类器训练中使用BoF和Mish可以提高分类器和使用该分类器预训练的加权的检测器的准确性。最终的结果是，主干CSPDarknet53比CSPResNeXt50更适合用于检测器。</p><p>我们观察到，CSPDarknet53模型由于各种改进，显示出更大的能力来提高探测器的精度。</p><blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-Table6.png" alt=""></p><p>表6：使用不同的分类器预训练权重进行检测器训练(所有其他训练参数在所有模型中都是相似的)。</p><h4 id="结论-2"><a href="#结论-2" class="headerlink" title="结论"></a>结论</h4><ul><li>具有最佳分类精度的模型在检测器精度方面并不总是最佳的。</li><li>骨干CSPDarknet53比CSPResNeXt50更适合于检测器。</li><li>由于各种改进，CSPDarknet53模型展示了更大的能力来提高检测器的精度。</li></ul></blockquote><h3 id="4-5-Influence-of-different-mini-batch-size-on-Detec-tor-training—不同的小批尺寸对检测器培训的影响"><a href="#4-5-Influence-of-different-mini-batch-size-on-Detec-tor-training—不同的小批尺寸对检测器培训的影响" class="headerlink" title="4.5 Influence of different mini-batch size on Detec- tor training—不同的小批尺寸对检测器培训的影响"></a>4.5 Influence of different mini-batch size on Detec- tor training—不同的小批尺寸对检测器培训的影响</h3><p>最后，我们分析了用不同的小批量训练的模型得到的结果，结果如表7所示。从表7所示的结果中，我们发现在添加BoF和BoS训练策略后，小批量大小对检测器的性能几乎没有影响。这一结果表明，在引入BoF和BoS后，不再需要使用昂贵的gpu进行训练。换句话说，任何人都只能使用一个普通的GPU来训练一个优秀的探测器。</p><blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-Table7.png" alt=""></p><p>表7：使用不同的 mini-batch size 进行检测器训练。</p><h4 id="结论-3"><a href="#结论-3" class="headerlink" title="结论"></a>结论</h4><ul><li>加入BoF和BoS训练策略后，小批量大小对检测器的性能几乎没有影响。</li><li>minibatch越大越好，CSPDarknet53对minibatch不敏感，利于单卡训练。</li><li>在引入BoF和BoS之后，不再需要使用昂贵的GPU进行训练。</li></ul></blockquote><h2 id="五、Results—结果"><a href="#五、Results—结果" class="headerlink" title="五、Results—结果"></a>五、Results—结果</h2><p>与其他最先进的对象检测器所获得的结果的比较如图8所示。我们的YOLOv4位于Pareto最优性曲线上，在速度和精度方面都优于最快和最精确的探测器。 </p><p>由于不同的方法使用不同架构的gpu进行推理时间验证，我们在通常采用的Maxwell、Pascal和Volta架构的gpu上操作YOLOv4，并将它们与其他最先进的方法进行比较。表8列出了使用MaxwellGPU的帧率比较结果，它可以是GTX TitanX（Maxwell）或TeslaM40GPU。表9列出了使用PascalGPU的帧率比较结果，可以是TitanX(Pascal)、TitanXp、GTX 1080Ti或特斯拉P100GPU。如表10所述，它列出了使用VoltaGPU的帧率比较结果，可以是Titan Volta或Tesla V100GPU。 </p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov4-Figure8.png" alt=""></p><p>图8 不同物体探测器的速度和精度比较。(一些文章只针对其中一个GPU : Maxwell / Pascal / Volta ，阐述了它们探测器的FPS)</p><p><strong>结论</strong></p><ul><li>得到的结果与其他最先进的物体探测器的比较如图8所示。我们的YOLOv4位于Pareto最优曲线上，无论是速度还是精度都优于最快最准确的检测器。</li><li>由于不同的方法使用不同架构的gpu进行推理时间验证，我们在Maxwell架构、Pascal架构和Volta架构常用的gpu上运行YOLOv4，并与其他最先进的方法进行比较。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>ET</title>
      <link href="/ET/"/>
      <url>/ET/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>os</title>
      <link href="/os/"/>
      <url>/os/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>yolov1-code</title>
      <link href="/yolov1-code/"/>
      <url>/yolov1-code/</url>
      
        <content type="html"><![CDATA[<h2 id=""><a href="#" class="headerlink" title="#"></a>#</h2><h2 id="writetxt-py"><a href="#writetxt-py" class="headerlink" title="writetxt.py"></a>writetxt.py</h2><h3 id="解析-XML-文件"><a href="#解析-XML-文件" class="headerlink" title="解析 XML 文件"></a>解析 XML 文件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析 XML 文件</span></span><br><span class="line">tree = ET.parse(<span class="string">&#x27;example.xml&#x27;</span>)  <span class="comment"># 假设这个 XML 文件的名称是 &#x27;example.xml&#x27;</span></span><br><span class="line">root = tree.getroot()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历 XML 文件中的 &#x27;object&#x27; 元素</span></span><br><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> root.findall(<span class="string">&#x27;object&#x27;</span>):</span><br><span class="line">    <span class="comment"># 获取对象的类别名称</span></span><br><span class="line">    name = obj.find(<span class="string">&#x27;name&#x27;</span>).text</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取对象的边界框坐标</span></span><br><span class="line">    bndbox = obj.find(<span class="string">&#x27;bndbox&#x27;</span>)</span><br><span class="line">    xmin = <span class="built_in">int</span>(bndbox.find(<span class="string">&#x27;xmin&#x27;</span>).text)</span><br><span class="line">    ymin = <span class="built_in">int</span>(bndbox.find(<span class="string">&#x27;ymin&#x27;</span>).text)</span><br><span class="line">    xmax = <span class="built_in">int</span>(bndbox.find(<span class="string">&#x27;xmax&#x27;</span>).text)</span><br><span class="line">    ymax = <span class="built_in">int</span>(bndbox.find(<span class="string">&#x27;ymax&#x27;</span>).text)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Object: <span class="subst">&#123;name&#125;</span>, Bounding Box: [<span class="subst">&#123;xmin&#125;</span>, <span class="subst">&#123;ymin&#125;</span>, <span class="subst">&#123;xmax&#125;</span>, <span class="subst">&#123;ymax&#125;</span>]&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="root-findall解释"><a href="#root-findall解释" class="headerlink" title="root.findall解释"></a>root.findall解释</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打印出所有部门和所有员工</span></span><br><span class="line">    <span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模拟的 XML 内容</span></span><br><span class="line">    xml_content = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    &lt;company&gt;</span></span><br><span class="line"><span class="string">        &lt;department name=&quot;Development&quot;&gt;</span></span><br><span class="line"><span class="string">            &lt;employee&gt;John&lt;/employee&gt;</span></span><br><span class="line"><span class="string">            &lt;employee&gt;Alice&lt;/employee&gt;</span></span><br><span class="line"><span class="string">        &lt;/department&gt;</span></span><br><span class="line"><span class="string">        &lt;department name=&quot;HR&quot;&gt;</span></span><br><span class="line"><span class="string">            &lt;employee&gt;Bob&lt;/employee&gt;</span></span><br><span class="line"><span class="string">            &lt;employee&gt;Eve&lt;/employee&gt;</span></span><br><span class="line"><span class="string">        &lt;/department&gt;</span></span><br><span class="line"><span class="string">    &lt;/company&gt;</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从字符串加载 XML</span></span><br><span class="line">    tree = ET.ElementTree(ET.fromstring(xml_content))</span><br><span class="line">    root = tree.getroot()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印所有部门</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;部门列表:&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> dept <span class="keyword">in</span> root.findall(<span class="string">&#x27;department&#x27;</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;- <span class="subst">&#123;dept.attrib[<span class="string">&#x27;name&#x27;</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印所有员工</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n员工列表:&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> emp <span class="keyword">in</span> root.findall(<span class="string">&#x27;.//employee&#x27;</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;- <span class="subst">&#123;emp.text&#125;</span>&quot;</span>)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">.//employee : xml语言中，.表示当前层级company，//表示s</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">部门列表:</span><br><span class="line">- Development</span><br><span class="line">- HR</span><br><span class="line"></span><br><span class="line">员工列表:</span><br><span class="line">- John</span><br><span class="line">- Alice</span><br><span class="line">- Bob</span><br><span class="line">- Eve</span><br></pre></td></tr></table></figure><h3 id="XML-文件内容解释"><a href="#XML-文件内容解释" class="headerlink" title="XML 文件内容解释"></a>XML 文件内容解释</h3><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">1. `&lt;annotation&gt;`: 这是根元素，表示整个文件是一个注释文档。</span><br><span class="line">2. `&lt;folder&gt;VOC2012&lt;/folder&gt;`: 图片所在的文件夹名称。</span><br><span class="line">3. `&lt;filename&gt;2007_000027.jpg&lt;/filename&gt;`: 被标注的图片的文件名。</span><br><span class="line">4. `&lt;source&gt;`: 描述图片的来源信息。</span><br><span class="line">    - `&lt;database&gt;The VOC2007 Database&lt;/database&gt;`: 图片来自的数据库。</span><br><span class="line">    - `&lt;annotation&gt;PASCAL VOC2007&lt;/annotation&gt;`: 使用的标注信息。</span><br><span class="line">    - `&lt;image&gt;flickr&lt;/image&gt;`: 图片来源（这里是Flickr）。</span><br><span class="line">5. `&lt;size&gt;`: 图片的尺寸。</span><br><span class="line">    - `&lt;width&gt;486&lt;/width&gt;`: 图片的宽度。</span><br><span class="line">    - `&lt;height&gt;500&lt;/height&gt;`: 图片的高度。</span><br><span class="line">    - `&lt;depth&gt;3&lt;/depth&gt;`: 图片的颜色通道数（这里是3，即 RGB）。</span><br><span class="line">6. `&lt;segmented&gt;0&lt;/segmented&gt;`: 表明图片是否被分割过，0表示没有分割。</span><br><span class="line">7. `&lt;object&gt;`: 标注的对象。</span><br><span class="line">    - `&lt;name&gt;person&lt;/name&gt;`: 对象的类别名称，这里是 &quot;person&quot;。</span><br><span class="line">    - `&lt;pose&gt;Unspecified&lt;/pose&gt;`: 对象的姿态，这里没有特别指定。</span><br><span class="line">    - `&lt;truncated&gt;0&lt;/truncated&gt;`: 表明对象是否被截断，0表示没有被截断。</span><br><span class="line">    - `&lt;difficult&gt;0&lt;/difficult&gt;`: 表明对象是否难以识别，0表示容易识别。</span><br><span class="line">    - `&lt;bndbox&gt;`: 对象的边界框坐标。</span><br><span class="line">        - `&lt;xmin&gt;174&lt;/xmin&gt;`: 边界框左上角的 x 坐标。</span><br><span class="line">        - `&lt;ymin&gt;101&lt;/ymin&gt;`: 边界框左上角的 y 坐标。</span><br><span class="line">        - `&lt;xmax&gt;349&lt;/xmax&gt;`: 边界框右下角的 x 坐标。</span><br><span class="line">        - `&lt;ymax&gt;351&lt;/ymax&gt;`: 边界框右下角的 y 坐标。</span><br><span class="line">    - `&lt;part&gt;`: 描述对象的部分（如果有）。（专注于分割任务）</span><br><span class="line">        - 每个 `&lt;part&gt;` 元素包含部分的名称（如 &quot;head&quot;、&quot;hand&quot;、&quot;foot&quot;）和该部分的边界框坐标。</span><br></pre></td></tr></table></figure><h3 id="打印图片坐标"><a href="#打印图片坐标" class="headerlink" title="打印图片坐标"></a>打印图片坐标</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图像的路径</span></span><br><span class="line">image_path = <span class="string">r&#x27;D:\Desktop\QNJS\Model\Yolo\Yolov1\YOLOV1-pytorch\VOCdevkit\JPEGImages\2007_000027.jpg&#x27;</span>  <span class="comment"># 替换为你的图像文件路径</span></span><br><span class="line">image = cv2.imread(image_path)</span><br><span class="line"><span class="keyword">if</span> image <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;无法加载图像，请检查你的路径。&quot;</span>)</span><br><span class="line">    exit()</span><br><span class="line"></span><br><span class="line">height, width = image.shape[:<span class="number">2</span>]  <span class="comment"># 高、宽、通道</span></span><br><span class="line">right_bottom = (width - <span class="number">1</span>, height - <span class="number">1</span>)  <span class="comment"># x是宽 y是高</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;图像右下角的坐标是: <span class="subst">&#123;right_bottom&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="在图像上画正确框"><a href="#在图像上画正确框" class="headerlink" title="在图像上画正确框"></a>在图像上画正确框</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 图像的路径</span></span><br><span class="line">image_path = <span class="string">&#x27;path/to/your/image.jpg&#x27;</span>  <span class="comment"># 替换为你的图像文件路径</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 边界框坐标</span></span><br><span class="line">bbox = [<span class="number">174</span>, <span class="number">101</span>, <span class="number">349</span>, <span class="number">351</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取图像</span></span><br><span class="line">image = cv2.imread(image_path)</span><br><span class="line"><span class="keyword">if</span> image <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Error: 无法加载图像。请检查你的路径。&quot;</span>)</span><br><span class="line">    exit()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画框</span></span><br><span class="line"><span class="comment"># bbox 的格式是 [xmin, ymin, xmax, ymax]</span></span><br><span class="line">cv2.rectangle(image, (bbox[<span class="number">0</span>], bbox[<span class="number">1</span>]), (bbox[<span class="number">2</span>], bbox[<span class="number">3</span>]), (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图像</span></span><br><span class="line">cv2.imshow(<span class="string">&quot;Image with Bounding Box&quot;</span>, image)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)  <span class="comment"># 等待按键</span></span><br><span class="line">cv2.destroyAllWindows()  <span class="comment"># 关闭所有打开的窗口</span></span><br></pre></td></tr></table></figure><h3 id="字符串转为整数"><a href="#字符串转为整数" class="headerlink" title="字符串转为整数"></a>字符串转为整数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">obj_struct[<span class="string">&#x27;bbox&#x27;</span>] = [<span class="built_in">int</span>(<span class="built_in">float</span>(bbox.find(<span class="string">&#x27;xmin&#x27;</span>).text)),</span><br><span class="line">                      <span class="built_in">int</span>(<span class="built_in">float</span>(bbox.find(<span class="string">&#x27;ymin&#x27;</span>).text)),</span><br><span class="line">                      <span class="built_in">int</span>(<span class="built_in">float</span>(bbox.find(<span class="string">&#x27;xmax&#x27;</span>).text)),</span><br><span class="line">                      <span class="built_in">int</span>(<span class="built_in">float</span>(bbox.find(<span class="string">&#x27;ymax&#x27;</span>).text))]</span><br><span class="line"><span class="comment"># 直接int(&#x27;107.2&#x27;)会报错，所以必须先把 浮点数的字符串 转为 浮点数 后转为 整数</span></span><br></pre></td></tr></table></figure><h2 id="new-resnet-py"><a href="#new-resnet-py" class="headerlink" title="new_resnet.py"></a>new_resnet.py</h2>]]></content>
      
      
      <categories>
          
          <category> Model </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>requirement</title>
      <link href="/requirement/"/>
      <url>/requirement/</url>
      
        <content type="html"><![CDATA[<h2 id="环境打包指南"><a href="#环境打包指南" class="headerlink" title="环境打包指南"></a>环境打包指南</h2><h3 id="1-生成requirements-txt"><a href="#1-生成requirements-txt" class="headerlink" title="1 生成requirements.txt"></a>1 生成requirements.txt</h3><h4 id="1-1-生成环境包"><a href="#1-1-生成环境包" class="headerlink" title="1.1 生成环境包"></a>1.1 生成环境包</h4><p>一个包含你项目当前环境中所有已安装包的列表</p><p>最好切换到项目的根目录，这样会在当前位置生成txt文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip freeze &gt; requirements.txt</span><br></pre></td></tr></table></figure><h4 id="1-2-生成项目包"><a href="#1-2-生成项目包" class="headerlink" title="1.2 生成项目包"></a>1.2 生成项目包</h4><p>一个包含你项目代码中所有已安装包的列表</p><p>最好切换到项目的根目录，这样会在当前位置生成txt文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pipreqs ./ --encoding=utf-8</span><br></pre></td></tr></table></figure><h3 id="2-删除txt外的多余包"><a href="#2-删除txt外的多余包" class="headerlink" title="2 删除txt外的多余包"></a>2 删除txt外的多余包</h3><p>会卸载当前环境下所有不在 <code>requirements.txt</code> 文件中的包。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip-sync requirements.txt</span><br></pre></td></tr></table></figure><h3 id="3-查看当前环境下的安装包"><a href="#3-查看当前环境下的安装包" class="headerlink" title="3 查看当前环境下的安装包"></a>3 查看当前环境下的安装包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip list</span><br></pre></td></tr></table></figure><h3 id="4-下载txt内的要求包"><a href="#4-下载txt内的要求包" class="headerlink" title="4 下载txt内的要求包"></a>4 下载txt内的要求包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><h3 id="5-查看依赖包"><a href="#5-查看依赖包" class="headerlink" title="5 查看依赖包"></a>5 查看依赖包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip show `xxx`</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>datasets</title>
      <link href="/datasets/"/>
      <url>/datasets/</url>
      
        <content type="html"><![CDATA[<h2 id="PASCAL-VOC2012"><a href="#PASCAL-VOC2012" class="headerlink" title="PASCAL VOC2012"></a>PASCAL VOC2012</h2><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>PASCALVOC挑战赛 (The PASCAL Visual object Classes ) 是一个世界级的<strong>计算机视觉挑战赛</strong>，PASCAL全称: Pattern Analysis, StaticalModeling and Computational Learning，是一个由欧盟资助的网络组织PASCALVOC挑战赛主要包括以下几类: <strong>图像分类</strong>(object classification)，<strong>目标检测</strong>(Object Detection)，<strong>目标分割</strong>(Object Segmentation)，<strong>动作识别</strong>(Action Classification)等</p><p>注意：2012年的test数据集不公开。故可以使用VOC2012的trainval进行训练，VOC2007的test进行测试。</p><h3 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h3><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">VOC2012</span><br><span class="line">├── Annotations             所有的图像标注信息（XML文件```</span><br><span class="line">├── ImageSets</span><br><span class="line">│   ├── Action              人的行为动作图像信息</span><br><span class="line">│   ├── Layout              人的各个部位图像信息</span><br><span class="line">│   ├── Main                目标检测分类图像信息```</span><br><span class="line">│   │   ├── train.txt       训练集 5717```</span><br><span class="line">│   │   ├── val.txt         验证集 5823```</span><br><span class="line">│   │   └── trainval.txt    训练集+验证集 11540```</span><br><span class="line">│   └── Segmentation        目标分割图像信息</span><br><span class="line">├── JPEGImages              所有图像文件```</span><br><span class="line">├── SegmentationClass       图像分割png图（基于分类）</span><br><span class="line">└── SegmentationObject      图像分割png图（基于目标）</span><br></pre></td></tr></table></figure><h3 id="XML格式"><a href="#XML格式" class="headerlink" title="XML格式"></a>XML格式</h3><figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&lt;annotation&gt;                                 标注</span><br><span class="line">  &lt;folder&gt;JPEGImages&lt;/folder&gt;                   文件夹名</span><br><span class="line">  &lt;filename&gt;2007_000027.jpg&lt;/filename&gt;          文件名</span><br><span class="line">  &lt;source&gt;                                      来源</span><br><span class="line">    &lt;database&gt;The VOC2007 Database&lt;/database&gt;       数据库</span><br><span class="line">    &lt;annotation&gt;PASCAL VOC2007&lt;/annotation&gt;         标注</span><br><span class="line">    &lt;image&gt;flickr&lt;/image&gt;                           图像来源</span><br><span class="line">  &lt;/source&gt;                                     来源结束</span><br><span class="line">  &lt;size&gt;                                        大小</span><br><span class="line">    &lt;width&gt;486&lt;/width&gt;                              宽486</span><br><span class="line">    &lt;height&gt;500&lt;/height&gt;                            高500</span><br><span class="line">    &lt;depth&gt;3&lt;/depth&gt;                                通道3</span><br><span class="line">  &lt;/size&gt;                                       大小结束</span><br><span class="line">  &lt;segmented&gt;0&lt;/segmented&gt;                      分割</span><br><span class="line">  &lt;object&gt;                                      物体</span><br><span class="line">    &lt;name&gt;person&lt;/name&gt;                              名称</span><br><span class="line">    &lt;pose&gt;Unspecified&lt;/pose&gt;                         姿势确定</span><br><span class="line">    &lt;truncated&gt;0&lt;/truncated&gt;                        是否截断</span><br><span class="line">    &lt;difficult&gt;0&lt;/difficult&gt;                        训练难易</span><br><span class="line">    &lt;bndbox&gt;                                        边界框</span><br><span class="line">      &lt;xmin&gt;174&lt;/xmin&gt;                                  左上角坐标x</span><br><span class="line">      &lt;ymin&gt;101&lt;/ymin&gt;                                  左上角坐标y</span><br><span class="line">      &lt;xmax&gt;349&lt;/xmax&gt;                                  右下角坐标x</span><br><span class="line">      &lt;ymax&gt;351&lt;/ymax&gt;                                  右下角坐标y</span><br><span class="line">    &lt;/bndbox&gt;                                           边界框结束</span><br><span class="line">    &lt;part&gt;                                          部位</span><br><span class="line">      &lt;name&gt;head&lt;/name&gt;                                 部位名称</span><br><span class="line">      &lt;bndbox&gt;                                          边界框</span><br><span class="line">        &lt;xmin&gt;169&lt;/xmin&gt;                                    左上角坐标x</span><br><span class="line">        &lt;ymin&gt;104&lt;/ymin&gt;                                    左上角坐标y</span><br><span class="line">        &lt;xmax&gt;209&lt;/xmax&gt;                                    右下角坐标x</span><br><span class="line">        &lt;ymax&gt;146&lt;/ymax&gt;                                    右下角坐标y</span><br><span class="line">      &lt;/bndbox&gt;                                         边界框结束</span><br><span class="line">    &lt;/part&gt;                                         部位结束</span><br><span class="line">  &lt;/object&gt;                                     物体结束</span><br><span class="line">&lt;/annotation&gt;                                标注结束</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="自建数据集"><a href="#自建数据集" class="headerlink" title="自建数据集"></a>自建数据集</h3><p>将图像和XML标注文件分别存于两个文件夹，并应用算法从中生成训练集和测试集。</p>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>cv2</title>
      <link href="/cv2/"/>
      <url>/cv2/</url>
      
        <content type="html"><![CDATA[<h3 id="OpenCV-基本操作笔记"><a href="#OpenCV-基本操作笔记" class="headerlink" title="OpenCV 基本操作笔记"></a>OpenCV 基本操作笔记</h3><p><strong>1. 导入 OpenCV 库</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br></pre></td></tr></table></figure></p><p><strong>2. 读取图像</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image = cv.imread(<span class="string">&#x27;path_to_image.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure></p><p>读取结果为‘BGR’且‘HWC’</p><p><strong>3. 显示图像</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cv.imshow(<span class="string">&#x27;Window Name&#x27;</span>, image)</span><br><span class="line">cv.waitKey(<span class="number">0</span>)  <span class="comment"># 等待按键事件</span></span><br><span class="line">cv.destroyAllWindows()</span><br></pre></td></tr></table></figure></p><p><strong>4. 转换为灰度图像</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)</span><br></pre></td></tr></table></figure></p><p><strong>5. 保存图像</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv.imwrite(<span class="string">&#x27;path_to_save.jpg&#x27;</span>, image)</span><br></pre></td></tr></table></figure></p><p><strong>6. 调整图像大小</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">resized_image = cv.resize(image, (width, height))</span><br></pre></td></tr></table></figure></p><p><strong>7. 图像裁剪</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cropped_image = image[y_start:y_end, x_start:x_end]</span><br></pre></td></tr></table></figure></p><p><strong>8. 图像旋转</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取图像尺寸</span></span><br><span class="line">(h, w) = image.shape[:<span class="number">2</span>]</span><br><span class="line"><span class="comment"># 计算旋转中心</span></span><br><span class="line">center = (w // <span class="number">2</span>, h // <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 旋转矩阵</span></span><br><span class="line">M = cv.getRotationMatrix2D(center, angle, <span class="number">1.0</span>)  <span class="comment"># angle 是旋转角度</span></span><br><span class="line">rotated_image = cv.warpAffine(image, M, (w, h))</span><br></pre></td></tr></table></figure></p><p><strong>9. 图像翻转</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flipped_image = cv.flip(image, flipCode)  <span class="comment"># flipCode: 0 垂直翻转，1 水平翻转</span></span><br></pre></td></tr></table></figure></p><p><strong>10. 边缘检测</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">edges = cv.Canny(image, threshold1, threshold2)</span><br></pre></td></tr></table></figure></p><p><strong>11. 图像模糊（滤波）</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">blurred_image = cv.GaussianBlur(image, (kernel_width, kernel_height), sigmaX)</span><br></pre></td></tr></table></figure></p><p><strong>12. 图像二值化</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">_, binary_image = cv.threshold(gray_image, thresh, maxval, cv.THRESH_BINARY)</span><br></pre></td></tr></table></figure></p><p><strong>13. 图像腐蚀与膨胀</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 腐蚀</span></span><br><span class="line">eroded = cv.erode(image, kernel, iterations=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 膨胀</span></span><br><span class="line">dilated = cv.dilate(image, kernel, iterations=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><p><strong>14. 图像轮廓检测</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">contours, _ = cv.findContours(binary_image, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)</span><br><span class="line"><span class="comment"># 绘制轮廓</span></span><br><span class="line">cv.drawContours(image, contours, -<span class="number">1</span>, (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), <span class="number">3</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pandas</title>
      <link href="/PANDAS/"/>
      <url>/PANDAS/</url>
      
        <content type="html"><![CDATA[<p>当然可以。以下是 Pandas 库中 <code>pd.read_csv</code>、<code>DataFrame.head</code> 和 <code>DataFrame.iloc</code> 方法的基本用法总结：</p><h3 id="1-pd-read-csv"><a href="#1-pd-read-csv" class="headerlink" title="1. pd.read_csv"></a>1. <code>pd.read_csv</code></h3><p>用于从 CSV 文件中读取数据并创建 DataFrame。</p><p><strong>基本用法</strong>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(filepath_or_buffer, sep=<span class="string">&#x27;,&#x27;</span>, header=<span class="number">0</span>, names=<span class="literal">None</span>, index_col=<span class="literal">None</span>, usecols=<span class="literal">None</span>, ...)</span><br></pre></td></tr></table></figure></p><ul><li><code>filepath_or_buffer</code>: CSV 文件的路径或类文件对象。</li><li><code>sep</code> 或 <code>delimiter</code>: 用于分割数据的字符，默认为逗号 <code>,</code>。</li><li><code>header</code>: 指定作为列名的行号，默认第一行为列名，如果没有列名则设置为 <code>None</code>。</li><li><code>names</code>: 显式指定列名的列表。</li><li><code>index_col</code>: 用作行索引的列编号或列名。</li><li><code>usecols</code>: 读取指定的列，接受列号或列名的列表。</li><li>其他参数可用于进一步定制读取过程，如数据类型转换、日期解析等。</li></ul><h3 id="2-DataFrame-head"><a href="#2-DataFrame-head" class="headerlink" title="2. DataFrame.head"></a>2. <code>DataFrame.head</code></h3><p>用于查看 DataFrame 的前几行。</p><p><strong>基本用法</strong>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.head(n=<span class="number">5</span>)</span><br></pre></td></tr></table></figure></p><ul><li><code>n</code>: 指定要显示的行数，默认为 5。</li></ul><h3 id="3-DataFrame-iloc"><a href="#3-DataFrame-iloc" class="headerlink" title="3. DataFrame.iloc"></a>3. <code>DataFrame.iloc</code></h3><p>用于基于索引位置选择数据。</p><p><strong>基本用法</strong>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.iloc[row_index, column_index]</span><br></pre></td></tr></table></figure></p><ul><li><code>row_index</code>: 行的位置索引（整数或整数列表）。</li><li><code>column_index</code>: 列的位置索引（整数或整数列表）。</li></ul><p><code>iloc</code> 用于按位置选择数据，不论数据的索引是怎样的。它是基于整数的位置选择方法，所以接受的是整数或整数的切片对象。</p><p><strong>示例</strong>:</p><ul><li>选择第一行：<code>df.iloc[0]</code></li><li>选择前三行和前两列：<code>df.iloc[0:3, 0:2]</code></li></ul><p>这些方法是 Pandas 库中非常重要的数据操作基础，广泛应用于数据处理和分析中。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>plt</title>
      <link href="/PLT/"/>
      <url>/PLT/</url>
      
        <content type="html"><![CDATA[<h3 id="Matplotlib-基础"><a href="#Matplotlib-基础" class="headerlink" title="Matplotlib 基础"></a>Matplotlib 基础</h3><p><strong>1. 导入库</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure></p><p><strong>2. 基本绘图</strong><br>绘制简单的线图。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">y = [<span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">25</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><strong>3. 添加标题和轴标签</strong><br>给图形添加标题和 X 轴、Y 轴标签。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, y)</span><br><span class="line">plt.title(<span class="string">&quot;Sample Plot&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;X Axis&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Y Axis&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><strong>4. 多条线绘制和图例</strong><br>在同一幅图上绘制多条线，并添加图例。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y2 = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(x, y, label=<span class="string">&quot;Line 1&quot;</span>)</span><br><span class="line">plt.plot(x, y2, label=<span class="string">&quot;Line 2&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><strong>5. 散点图</strong><br>绘制散点图。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><strong>6. 条形图</strong><br>绘制条形图。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.bar(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><strong>7. 直方图</strong><br>绘制直方图。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">data = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, <span class="number">1000</span>)</span><br><span class="line">plt.hist(data, bins=<span class="number">30</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><strong>8. 子图</strong><br>创建包含多个子图的图形。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 1行2列</span></span><br><span class="line">ax[<span class="number">0</span>].plot(x, y)</span><br><span class="line">ax[<span class="number">1</span>].scatter(x, y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><strong>9. 自定义样式</strong><br>自定义线条颜色、类型和标记。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, y, color=<span class="string">&#x27;red&#x27;</span>, linestyle=<span class="string">&#x27;--&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p><strong>10. 保存图形</strong><br>将图形保存为文件。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(x, y)</span><br><span class="line">plt.savefig(<span class="string">&quot;plot.png&quot;</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Image-basic</title>
      <link href="/Image-basic/"/>
      <url>/Image-basic/</url>
      
        <content type="html"><![CDATA[<h2 id="JPG-转-Tensor"><a href="#JPG-转-Tensor" class="headerlink" title="JPG 转 Tensor"></a>JPG 转 Tensor</h2><p>这里是详细说明 JPG 图像通过 OpenCV 的 <code>imread</code> 读取，然后转换为 PyTorch 张量的整个流程，包括数据格式和通道的变化：</p><h3 id="1-JPG-图像通过-OpenCV-imread-读取："><a href="#1-JPG-图像通过-OpenCV-imread-读取：" class="headerlink" title="1. JPG 图像通过 OpenCV imread 读取："></a>1. <strong>JPG 图像通过 OpenCV <code>imread</code> 读取</strong>：</h3><ul><li>JPG 图像是一种常用的压缩图像格式。</li><li>当使用 OpenCV 的 <code>imread</code> 函数读取 JPG 图像时，图像被加载为一个 NumPy 数组。</li><li>此数组的形状是 <code>(Height, Width, Channels)</code>，其中 Height 和 Width 分别是图像的高度和宽度（以像素为单位），Channels 是颜色通道的数量。</li><li>对于彩色图像，OpenCV 默认以 BGR（蓝、绿、红）格式读取，所以 Channels = 3。</li><li>数组中的每个元素是一个 0 到 255 之间的整数，代表相应像素的颜色强度。</li></ul><h3 id="2-转换为-PyTorch-张量："><a href="#2-转换为-PyTorch-张量：" class="headerlink" title="2. 转换为 PyTorch 张量："></a>2. <strong>转换为 PyTorch 张量</strong>：</h3><ul><li>PyTorch 通常处理的是张量（Tensor）格式的数据。</li><li>当将 OpenCV 读取的图像转换为 PyTorch 张量时，数据格式会从 NumPy 数组变为 PyTorch 张量。</li><li>这个转换过程通常包括改变通道的顺序和数据类型。</li><li>PyTorch 中的张量通常使用 <code>(Channels, Height, Width)</code> 的顺序，这与 OpenCV 的 <code>(Height, Width, Channels)</code> 不同。</li><li>因此，通常会对图像数据进行转置操作，比如 <code>np.transpose(image, (2, 0, 1))</code>，以将其从 <code>(H, W, C)</code> 转换为 <code>(C, H, W)</code>。</li><li>此外，如果原始图像是以 BGR 格式读取的，可能还需要将其转换为 RGB 格式，因为 PyTorch 通常使用 RGB 格式。</li></ul><h3 id="3-通道的变化："><a href="#3-通道的变化：" class="headerlink" title="3. 通道的变化："></a>3. <strong>通道的变化</strong>：</h3><ul><li>在 OpenCV 中，彩色图像默认以 BGR 格式读取，且尺寸顺序为‘HWC’</li><li>在 PyTorch 中，彩色图像通常表示为 RGB 格式，且尺寸顺序为‘CHW’</li><li>因此，在转换过程中需要将 BGR 格式转换为 RGB 格式，这通常通过重新排列通道来实现。</li></ul><h3 id="4-整个流程的示例代码如下："><a href="#4-整个流程的示例代码如下：" class="headerlink" title="4. 整个流程的示例代码如下："></a>4. <strong>整个流程的示例代码如下</strong>：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用OpenCV读取图像</span></span><br><span class="line">image = cv2.imread(<span class="string">&#x27;path_to_image.jpg&#x27;</span>)  <span class="comment"># 图像为 (H, W, C) 和 BGR 格式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将BGR转换为RGB</span></span><br><span class="line">image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将图像从 (H, W, C) 转换为 (C, H, W)</span></span><br><span class="line">image = np.transpose(image, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将NumPy数组转换为PyTorch张量</span></span><br><span class="line">tensor = torch.from_numpy(image)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor 现在是 (C, H, W) 格式的张量</span></span><br></pre></td></tr></table></figure><p>这样，JPG 图像就被转换成了 PyTorch 可以直接处理的张量格式。在这个张量中，图像数据的每个元素都代表对应像素点在特定通道上的颜色强度。</p><h2 id="灰度图像和黑白图像"><a href="#灰度图像和黑白图像" class="headerlink" title="灰度图像和黑白图像"></a>灰度图像和黑白图像</h2><p>灰度图像和黑白图像是两种常见的图像类型，它们在图像处理和计算机视觉中有着广泛的应用。下面是对它们的介绍以及它们的数据特征：</p><h3 id="1-灰度图像："><a href="#1-灰度图像：" class="headerlink" title="1. 灰度图像："></a>1. <strong>灰度图像</strong>：</h3><ul><li>灰度图像是指只有灰度级别的图像，没有彩色信息。在灰度图像中，每个像素只有一个亮度值，通常表示为 0（黑色）到 255（白色）之间的整数。</li><li>在灰度图像中，像素值较低（接近0）的区域看起来更黑，像素值较高（接近255）的区域看起来更亮。</li><li>灰度图像的数据结构较为简单，每个像素只有一个通道（通常是单通道），这使得处理速度更快，占用的存储空间也更小。</li><li>灰度图像常用于图像处理中的各种算法，如边缘检测、图像分割等，因为去除了色彩信息，可以更专注于图像的结构和形状。</li></ul><h3 id="2-黑白图像（二值图像）："><a href="#2-黑白图像（二值图像）：" class="headerlink" title="2. 黑白图像（二值图像）："></a>2. <strong>黑白图像（二值图像）</strong>：</h3><ul><li>黑白图像，也称为二值图像，是一种特殊的灰度图像，每个像素只有两种可能的值：0（黑色）或 255（白色）。</li><li>在二值图像中，图像通常是高对比度的，只显示黑色和白色，没有灰度中间值。</li><li>二值图像常用于阈值处理、文档扫描、某些类型的图像分析（如形态学操作）等应用。</li><li>生成二值图像的一种常见方法是应用阈值处理，将灰度图像中的每个像素转换为黑色或白色，这个过程称为二值化。</li></ul><p>在数据表示上，灰度图像通常是一个二维数组（Height x Width），其中每个元素代表一个像素的亮度值。而二值图像同样是二维数组，但每个元素只有两种可能的值（0或255）。在图像处理库中，如OpenCV或Pillow，通常提供了将彩色图像转换为灰度图像或二值图像的功能。</p><h2 id="读取图像文件"><a href="#读取图像文件" class="headerlink" title="读取图像文件"></a>读取图像文件</h2><h3 id="1-Scikit-image-io-imread"><a href="#1-Scikit-image-io-imread" class="headerlink" title="1. Scikit-image (io.imread)"></a>1. Scikit-image (<code>io.imread</code>)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line">image = io.imread(image_path)</span><br></pre></td></tr></table></figure><ul><li><strong>特点</strong>：默认以 RGB 格式读取图像。适用于科学计算和图像分析。</li><li><strong>颜色格式</strong>：RGB。</li><li><strong>适用场景</strong>：图像处理、计算机视觉研究和分析。</li></ul><h3 id="2-OpenCV-cv2-imread"><a href="#2-OpenCV-cv2-imread" class="headerlink" title="2. OpenCV (cv2.imread)"></a>2. OpenCV (<code>cv2.imread</code>)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">image = cv2.imread(image_path)</span><br></pre></td></tr></table></figure><ul><li><strong>特点</strong>：默认以 BGR 格式读取图像。提供了丰富的图像处理和计算机视觉功能。</li><li><strong>颜色格式</strong>：BGR。</li><li><strong>适用场景</strong>：实时图像处理、计算机视觉和机器学习应用。</li></ul><h3 id="3-PIL-Pillow-Image-open"><a href="#3-PIL-Pillow-Image-open" class="headerlink" title="3. PIL / Pillow (Image.open)"></a>3. PIL / Pillow (<code>Image.open</code>)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image = Image.<span class="built_in">open</span>(image_path)</span><br></pre></td></tr></table></figure><ul><li><strong>特点</strong>：以 PIL 图像对象形式读取图像，需要转换为 numpy 数组才能用于像素级操作。</li><li><strong>颜色格式</strong>：RGB（或原图像的格式）。</li><li><strong>适用场景</strong>：基本图像处理，与其他Python库集成。</li></ul><h3 id="4-Matplotlib-plt-imread"><a href="#4-Matplotlib-plt-imread" class="headerlink" title="4. Matplotlib (plt.imread)"></a>4. Matplotlib (<code>plt.imread</code>)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">image = plt.imread(image_path)</span><br></pre></td></tr></table></figure><ul><li><strong>特点</strong>：以 numpy 数组形式读取图像，通常用于图像展示和绘图。</li><li><strong>颜色格式</strong>：RGB。</li><li><strong>适用场景</strong>：图像展示和绘制图表。</li></ul><h3 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h3><ul><li><strong>颜色格式</strong>：OpenCV 默认以 BGR 格式读取，而其他库通常以 RGB 格式读取。</li><li><strong>数据类型</strong>：OpenCV 和 Matplotlib 返回 numpy 数组，PIL 返回一个图像对象，scikit-image 也返回 numpy 数组。</li><li><strong>用途</strong>：OpenCV 更适合复杂的图像处理和计算机视觉任务。PIL 简单易用，适合基本图像处理。Matplotlib 主要用于绘图和图像展示，scikit-image 适合科学研究中的图像分析。</li></ul><h2 id="show-用于显示图像"><a href="#show-用于显示图像" class="headerlink" title="show 用于显示图像"></a><code>show</code> 用于显示图像</h2><h3 id="1-OpenCV-cv-imshow"><a href="#1-OpenCV-cv-imshow" class="headerlink" title="1. OpenCV (cv.imshow)"></a>1. OpenCV (<code>cv.imshow</code>)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">image = cv2.imread(image_path)</span><br><span class="line">cv2.imshow(<span class="string">&#x27;Window Name&#x27;</span>, image)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure><ul><li><strong>特点</strong>：直接使用 OpenCV 窗口显示图像。</li><li><strong>用途</strong>：实时图像处理和计算机视觉应用。</li><li><strong>注意</strong>：需要使用 <code>cv2.waitKey()</code> 来等待键盘事件，<code>cv2.destroyAllWindows()</code> 关闭所有 OpenCV 创建的窗口。</li></ul><h3 id="2-PIL-Pillow-Image-show"><a href="#2-PIL-Pillow-Image-show" class="headerlink" title="2. PIL / Pillow (Image.show)"></a>2. PIL / Pillow (<code>Image.show</code>)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image = Image.<span class="built_in">open</span>(image_path)</span><br><span class="line">image.show()</span><br></pre></td></tr></table></figure><ul><li><strong>特点</strong>：在默认图片查看器中打开图像。</li><li><strong>用途</strong>：基本图像处理和快速预览。</li><li><strong>注意</strong>：这种方式会临时保存图像到磁盘，然后使用系统默认的图像查看器打开。</li></ul><h3 id="3-Matplotlib-plt-imshow-和-plt-show"><a href="#3-Matplotlib-plt-imshow-和-plt-show" class="headerlink" title="3. Matplotlib (plt.imshow 和 plt.show)"></a>3. Matplotlib (<code>plt.imshow</code> 和 <code>plt.show</code>)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">image = plt.imread(image_path)</span><br><span class="line">plt.imshow(image)</span><br><span class="line">plt.axis(<span class="string">&#x27;off&#x27;</span>)  <span class="comment"># 可选，关闭坐标轴</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><ul><li><strong>特点</strong>：使用 Matplotlib 绘图窗口显示图像。</li><li><strong>用途</strong>：数据分析和绘图，结合图表和图像展示。</li><li><strong>注意</strong>：可以与 Matplotlib 的其他绘图功能结合，如添加标题、坐标轴标签等。</li></ul><h3 id="4-Scikit-image-io-imshow"><a href="#4-Scikit-image-io-imshow" class="headerlink" title="4. Scikit-image (io.imshow)"></a>4. Scikit-image (<code>io.imshow</code>)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line">image = io.imread(image_path)</span><br><span class="line">io.imshow(image)</span><br><span class="line">io.show()</span><br></pre></td></tr></table></figure><ul><li><strong>特点</strong>：使用 Matplotlib 显示图像（如果安装了 Matplotlib）。</li><li><strong>用途</strong>：科学计算和图像分析。</li><li><strong>注意</strong>：实际显示依赖于安装的后端，通常是 Matplotlib。</li></ul><h3 id="比较-1"><a href="#比较-1" class="headerlink" title="比较"></a>比较</h3><ul><li><strong>OpenCV</strong> 的 <code>imshow</code> 方法适合于实时图像处理和计算机视觉应用，允许快速更新和操作窗口。</li><li><strong>PIL/Pillow</strong> 的 <code>show</code> 方法适合于快速预览图像，但依赖于外部程序。</li><li><strong>Matplotlib</strong> 的 <code>imshow</code> 结合 <code>show</code> 方法适用于数据分析和图像展示，允许高度定制化的图表和图像展示。</li><li><strong>Scikit-image</strong> 的 <code>imshow</code> 方法提供了一个与 Matplotlib 集成的简单接口，适用于科学研究中的图像展示。</li></ul>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PIL-pillow</title>
      <link href="/PIL-pillow/"/>
      <url>/PIL-pillow/</url>
      
        <content type="html"><![CDATA[<p>PIL（Python Imaging Library）是一个强大的图像处理库，用于打开、操作和保存许多不同格式的图像文件。Pillow 是 PIL 的一个活跃的分支版本，因此它通常被称为 “Pillow”。以下是一些使用 Pillow (PIL) 的基本操作：</p><ol><li><p><strong>打开和显示图像</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image = Image.<span class="built_in">open</span>(<span class="string">&#x27;path_to_image.jpg&#x27;</span>)  <span class="comment"># 打开图像文件</span></span><br><span class="line">image.show()  <span class="comment"># 显示图像</span></span><br></pre></td></tr></table></figure></li><li><p><strong>图像转换</strong>：</p><ul><li>转换图像格式（例如，将 RGB 转换为灰度）：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gray_image = image.convert(<span class="string">&#x27;L&#x27;</span>)  <span class="comment"># 转换为灰度图像</span></span><br></pre></td></tr></table></figure></li><li>改变图像大小：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">resized_image = image.resize((width, height))  <span class="comment"># 更改图像大小</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>裁剪图像</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cropped_image = image.crop((left, top, right, bottom))  <span class="comment"># 裁剪图像</span></span><br></pre></td></tr></table></figure></li><li><p><strong>保存图像</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image.save(<span class="string">&#x27;path_to_save_image.jpg&#x27;</span>)  <span class="comment"># 保存图像到文件</span></span><br></pre></td></tr></table></figure></li><li><p><strong>图像旋转和翻转</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rotated_image = image.rotate(<span class="number">90</span>)  <span class="comment"># 旋转图像90度</span></span><br><span class="line">flipped_image = image.transpose(Image.FLIP_LEFT_RIGHT)  <span class="comment"># 水平翻转图像</span></span><br></pre></td></tr></table></figure></li><li><p><strong>绘图和添加文本</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> ImageDraw, ImageFont</span><br><span class="line">draw = ImageDraw.Draw(image)</span><br><span class="line">font = ImageFont.load_default()  <span class="comment"># 加载默认字体</span></span><br><span class="line">draw.text((x, y), <span class="string">&quot;Hello&quot;</span>, (r, g, b), font=font)  <span class="comment"># 在图像上添加文本</span></span><br></pre></td></tr></table></figure></li></ol><p>Pillow 提供了丰富的功能来处理和分析图像数据，它是许多图像处理和计算机视觉项目的基础。由于其简单易用的接口，它在 Python 图像处理领域中非常受欢迎。</p>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch-17</title>
      <link href="/Pytorch-17/"/>
      <url>/Pytorch-17/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>   目前应已对基本的分类网络架构非常了解：AlexNet，VGG16、NiN、 GoogLeNet以及ResNet。现在开始进入我们计算机视觉应用，我们要学习：<strong>将学过的架构应用于计算机视觉问题</strong>、<strong>将解决过程打包成项目</strong></p><p>   要考虑的东西：数据、算力 、训练过程、训练技巧（之前Pytorch-11的demo还是太浅显了）</p><p>   经过这一堂课，将能够<strong>自由调用任意数据</strong>、<strong>任意卷积架构</strong>，<strong>实现较为恰当的训练</strong>、<strong>最终落地成自己的深度视觉案例</strong>。</p><p>   最基本地来说，如果只考虑对图像的内容进行判断情况，我们至少也有<strong>识别（recognition）</strong>、<strong>检测（detection）</strong>、<strong>分割 （segmentation）</strong>三种最基本的任务。</p><div class="table-container"><table><thead><tr><th>任务类型</th><th>描述</th><th>数据特点</th><th>应用场景</th></tr></thead><tbody><tr><td>图像识别</td><td>对图像中的单一对象进行属性判断</td><td>数据集规整简单，物体轮廓完整，拍摄清晰</td><td>例如机场人脸识别</td></tr><tr><td>检测任务</td><td>对图像中的多个或单个对象进行定位和属性判断</td><td>图像复杂，使用边界框定位对象</td><td>如道路车辆识别、景区人群监控</td></tr><tr><td>分割任务</td><td>对图像中的多个或单个对象每个像素进行分类，确定精确边界</td><td>高细节，多类别标签，像素级判断</td><td>美颜相机、换脸特效等</td></tr></tbody></table></div><p>   因此，在图像数据集的读取过程中，你可能发现一个图像数据集中会带有很多个<strong>指向不同任务的标签</strong>、 甚至很多个<strong>指向不同任务的训练集</strong>。</p><div class="table-container"><table><thead><tr><th>数据集</th><th>描述</th><th>参数</th><th>标签类型</th><th>应用</th></tr></thead><tbody><tr><td>CelebA</td><td>人脸数据集，用于人脸相关的计算机视觉任务</td><td><code>target_type</code></td><td><code>attr</code>（属性）、<code>identity</code>（个体）、<code>bbox</code>（边界框）、<code>landmarks</code>（特征）</td><td>人脸识别、检测任务</td></tr><tr><td>Cityscapes</td><td>城市街道场景数据集，用于图像分割任务</td><td><code>mode</code>、<code>target_type</code></td><td><code>fine</code>/<code>coarse</code>（模式）、<code>instance</code>（实例分割）、<code>semantic</code>（语义分割）、<code>polygon</code>（多边形分割）、<code>color</code>（颜色分割）</td><td>图像分割任务</td></tr></tbody></table></div><p>   我们不能使用同样的代码加载不同的数据 集。</p><p>   几乎所有数据集都被要求只能使用于学术场景，许多数据集需 要注册、申请才能够使用，许多数据集甚至完全不开源。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将.jpg转为tensor的函数</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 函数：读取JPG图片文件，使用OpenCV，然后将其转换为PyTorch张量</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">image_to_tensor</span>(<span class="params">image_path</span>):</span><br><span class="line">    <span class="comment"># 使用OpenCV读取图片</span></span><br><span class="line">    image = cv2.imread(image_path)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将图片从BGR转换为RGB</span></span><br><span class="line">    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将图片数据转换为浮点数，并缩放到[0, 1]区间</span></span><br><span class="line">    image = image.astype(np.float32) / <span class="number">255.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 调整数据的布局从(H, W, C)变为(C, H, W)</span></span><br><span class="line">    image = np.transpose(image, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将numpy数组转换为torch张量</span></span><br><span class="line">    tensor = torch.from_numpy(image)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果需要添加批次维度，可以取消以下代码的注释</span></span><br><span class="line">    <span class="comment"># tensor = tensor.unsqueeze(0)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line">read_image_to_tensor(<span class="string">r&#x27;D:\Desktop\QNJS\Dataset\[][][]zzzzzzip\datasets4\picturestotensor\Train\female\000059.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将tensor转为图片的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tensor_to_image</span>(<span class="params">tensor</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Convert a torch tensor into a numpy.ndarray for visualization.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 检查张量是否有批次维度</span></span><br><span class="line">    <span class="keyword">if</span> tensor.ndim == <span class="number">4</span> <span class="keyword">and</span> tensor.shape[<span class="number">0</span>] == <span class="number">1</span>:  <span class="comment"># 形状为[1, C, H, W]</span></span><br><span class="line">        tensor = tensor.squeeze(<span class="number">0</span>)  <span class="comment"># 去除批次维度</span></span><br><span class="line">    <span class="comment"># 将张量的值范围从[0, 1]转换为[0, 255]</span></span><br><span class="line">    tensor = tensor.mul(<span class="number">255</span>).byte()</span><br><span class="line">    <span class="comment"># 转置张量的维度为[H, W, C]</span></span><br><span class="line">    tensor = tensor.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).numpy()</span><br><span class="line">    <span class="comment"># 使用matplotlib展示图像</span></span><br><span class="line">    plt.imshow(tensor)</span><br><span class="line">    <span class="comment"># 关闭坐标轴</span></span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    <span class="comment"># 显示图像</span></span><br><span class="line">    plt.show()</span><br><span class="line">tensor_to_image(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机五张将tensor转为图片的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotsample</span>(<span class="params">data</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">5</span>,figsize=(<span class="number">10</span>,<span class="number">10</span>)) <span class="comment">#建立子图</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        num = random.randint(<span class="number">0</span>,<span class="built_in">len</span>(data)-<span class="number">1</span>) <span class="comment">#首先选取随机数，随机选取五次</span></span><br><span class="line">        <span class="comment">#抽取数据中对应的图像对象，make_grid函数可将任意格式的图像的通道数升为3，而不改变图像原始的数据</span></span><br><span class="line">        <span class="comment">#而展示图像用的imshow函数最常见的输入格式也是3通道</span></span><br><span class="line">        npimg = torchvision.utils.make_grid(data[num][<span class="number">0</span>]).numpy()</span><br><span class="line">        nplabel = data[num][<span class="number">1</span>] <span class="comment">#提取标签</span></span><br><span class="line">        <span class="comment">#将图像由(3, weight, height)转化为(weight, height, 3)，并放入imshow函数中读取</span></span><br><span class="line">        axs[i].imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">        axs[i].set_title(nplabel) <span class="comment">#给每个子图加上标签</span></span><br><span class="line">        axs[i].axis(<span class="string">&quot;off&quot;</span>) <span class="comment">#消除每个子图的坐标轴</span></span><br><span class="line">        </span><br><span class="line">plotsample(data)</span><br></pre></td></tr></table></figure><h2 id="一、数据"><a href="#一、数据" class="headerlink" title="一、数据"></a>一、数据</h2><h3 id="1-认识经典数据"><a href="#1-认识经典数据" class="headerlink" title="1 认识经典数据"></a>1 认识经典数据</h3><h4 id="1-1入门数据：MNIST、其他数字与字母识别"><a href="#1-1入门数据：MNIST、其他数字与字母识别" class="headerlink" title="1.1入门数据：MNIST、其他数字与字母识别"></a>1.1入门数据：MNIST、其他数字与字母识别</h4><p>最适合用于教学和实验、几乎对所有的电脑都无负担的MNIST一族。</p><p>许多研究论文采用MNIST或Fashion-MNIST数据集作为基准测试，因为它们是计算机视觉领域最初广泛使用的数据集之一。如果你开发了一种新的网络架构，并认为其达到了最先进（State of the Art, SOTA）水平，但在MNIST或Fashion-MNIST上的性能未能达到95%的准确率，那么这个架构的学习能力可能还不足。因此，MNIST常被用来作为性能评估的基准线。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line">mnist = torchvision.datasets.FashionMNIST(root=<span class="string">&#x27;.\\&#x27;</span></span><br><span class="line">                                         ,train=<span class="literal">False</span></span><br><span class="line">                                         ,download=<span class="literal">True</span></span><br><span class="line">                                         ,transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">fmnist = torchvision.datasets.FashionMNIST(root =<span class="string">&#x27;I：\F盘 + 代码\F盘datasets&#x27;</span></span><br><span class="line">                                         ,train =<span class="literal">True</span> <span class="comment">#根据类的不同，参数可能发生变化</span></span><br><span class="line">                                         ,download =<span class="literal">False</span> <span class="comment">#未下载则设置为True</span></span><br><span class="line">                                         ,transform =transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">svhn = torchvision.datasets.SVHN(root =<span class="string">&#x27;I：\F盘 + 代码\F盘datasets\SVHN&#x27;</span></span><br><span class="line">                                 ,split =<span class="string">&quot;train&quot;</span></span><br><span class="line">                                 ,download = <span class="literal">False</span></span><br><span class="line">                                 ,transform = transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">omnist = torchvision.datasets.Omniglot(root =<span class="string">&#x27;I：\F盘 + 代码\F盘datasets&#x27;</span></span><br><span class="line">                                       ,background = <span class="literal">True</span></span><br><span class="line">                                       ,download = <span class="literal">False</span></span><br><span class="line">                                       ,transform = transforms.ToTensor())</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th>数据集名称</th><th>数据描述</th></tr></thead><tbody><tr><td>FashionMNIST</td><td>衣物用品数据集</td></tr><tr><td>MNIST</td><td>手写数字数据集（白底黑字）</td></tr><tr><td>KuzushijiMNIST</td><td>日语手写字符识别数据集，包含48个平假名字符和一个平假名的模糊版本</td></tr><tr><td>QMNIST</td><td>与MNIST高度相似的手写数字数据集</td></tr><tr><td>EMNIST</td><td>与MNIST高度相似，在MNIST的基础上拓展的手写数字数据集</td></tr><tr><td>Omniglot</td><td>全语种手写字母数据集，包含来自50个不同字母的1623个不同的手写字符， 专用于“一次性学习”（字符）</td></tr><tr><td>USPS</td><td>另一个体系的手写数字数据集，常用来与MNIST对比（黑底白字）</td></tr><tr><td>SVHN</td><td>实际街景数字数据集（Street View House Number），是数字识别和位置测量中重要的一个数据集。有源自街景的彩色街道门牌号，但在PyTorch中内置的是32x32的彩色数据集。注意，使用本数据集需要SciPy库的支持。</td></tr></tbody></table></div><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E5%85%A5%E9%97%A8%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E6%95%B0%E6%8D%AE.png" alt=""></p><ul><li>导入数据</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line">mnist = torchvision.datasets.FashionMNIST(root=<span class="string">&#x27;.\\&#x27;</span></span><br><span class="line">                                         ,train=<span class="literal">False</span></span><br><span class="line">                                         ,download=<span class="literal">False</span></span><br><span class="line">                                         ,transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">fminst = torchvision.datasets.FashionMNIST(root=<span class="string">r&#x27;D:\Desktop\QNJS\Dataset\FashionMNIST&#x27;</span></span><br><span class="line">                                           ,train=<span class="literal">False</span></span><br><span class="line">                                           ,download=<span class="literal">False</span></span><br><span class="line">                                           ,transform=transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">svhn = torchvision.datasets.SVHN(root =<span class="string">&#x27;I：\F盘 + 代码\F盘datasets\SVHN&#x27;</span></span><br><span class="line">                                 ,split =<span class="string">&quot;train&quot;</span></span><br><span class="line">                                 ,download = <span class="literal">False</span></span><br><span class="line">                                 ,transform = transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">omnist = torchvision.datasets.Omniglot(root =<span class="string">&#x27;I：\F盘 + 代码\F盘datasets&#x27;</span></span><br><span class="line">                                       ,background = <span class="literal">True</span></span><br><span class="line">                                       ,download = <span class="literal">False</span></span><br><span class="line">                                       ,transform = transforms.ToTensor())</span><br><span class="line"><span class="built_in">print</span>(fminst)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">只能看到样本量</span></span><br><span class="line"><span class="string">Dataset FashionMNIST</span></span><br><span class="line"><span class="string">    Number of datapoints: 10000</span></span><br><span class="line"><span class="string">    Root location: D:\Desktop\QNJS\Dataset\FashionMNIST</span></span><br><span class="line"><span class="string">    Split: Test</span></span><br><span class="line"><span class="string">    StandardTransform</span></span><br><span class="line"><span class="string">Transform: ToTensor()</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><ul><li>查看更多特征<ul><li>使用.data的方式查看特征，.targets的方式查看标签，不是所有的数据集都支持这两种方法！<ul><li>保险方法是：索引的方式调用单个样本</li></ul></li><li>查看样本量：len(fminst)、fminst</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 样本尺寸</span></span><br><span class="line"><span class="built_in">print</span>(fminst[<span class="number">0</span>][<span class="number">0</span>].shape)</span><br><span class="line"><span class="comment"># 样本量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(fminst))</span><br><span class="line"><span class="built_in">print</span>(fminst)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 报错概率最低的查看方法</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> fminst:</span><br><span class="line">    <span class="built_in">print</span>(x.shape,y)</span><br><span class="line">    <span class="keyword">break</span>  <span class="comment"># 打印其中一个</span></span><br></pre></td></tr></table></figure><ul><li>可视化</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#可视化</span></span><br><span class="line"><span class="comment">#实际上，在读图时如果不加ToTensor的预处理，很可能直接读出PIL文件</span></span><br><span class="line"><span class="comment">#PIL可以直接可视化</span></span><br><span class="line">fmnist = torchvision.datasets.FashionMNIST(root =<span class="string">&#x27;I：\F盘 + 代码\F盘datasets&#x27;</span></span><br><span class="line">                                         ,train =<span class="literal">True</span> <span class="comment">#根据类的不同，参数可能发生变化</span></span><br><span class="line">                                         ,download =<span class="literal">False</span> <span class="comment">#未下载则设置为True</span></span><br><span class="line">                                        <span class="comment"># ,transform = transforms.ToTensor()</span></span><br><span class="line">                                         )</span><br><span class="line">fminst[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># (&lt;PIL.Image.Image image mode=L size=28x28 at 0x268283D32E0&gt;, 9)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotsample</span>(<span class="params">data</span>):</span><br><span class="line">    fig, axs = plt.subplots(<span class="number">1</span>,<span class="number">5</span>,figsize=(<span class="number">10</span>,<span class="number">10</span>)) <span class="comment">#建立子图</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        num = random.randint(<span class="number">0</span>,<span class="built_in">len</span>(data)-<span class="number">1</span>) <span class="comment">#首先选取随机数，随机选取五次</span></span><br><span class="line">        <span class="comment">#抽取数据中对应的图像对象，make_grid函数可将任意格式的图像的通道数升为3，而不改变图像原始的数据</span></span><br><span class="line">        <span class="comment">#而展示图像用的imshow函数最常见的输入格式也是3通道</span></span><br><span class="line">        npimg = torchvision.utils.make_grid(data[num][<span class="number">0</span>]).numpy()</span><br><span class="line">        nplabel = data[num][<span class="number">1</span>] <span class="comment">#提取标签</span></span><br><span class="line">        <span class="comment">#将图像由(3, weight, height)转化为(weight, height, 3)，并放入imshow函数中读取</span></span><br><span class="line">        axs[i].imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">        axs[i].set_title(nplabel) <span class="comment">#给每个子图加上标签</span></span><br><span class="line">        axs[i].axis(<span class="string">&quot;off&quot;</span>) <span class="comment">#消除每个子图的坐标轴</span></span><br><span class="line"></span><br><span class="line">plotsample(fminst)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 随机打印五张图片</span></span><br></pre></td></tr></table></figure><h4 id="1-2-竞赛数据集"><a href="#1-2-竞赛数据集" class="headerlink" title="1.2 竞赛数据集"></a>1.2 竞赛数据集</h4><div class="table-container"><table><thead><tr><th>数据集名称</th><th>数据描述</th></tr></thead><tbody><tr><td>ImageNet2012</td><td>ImageNet大规模视觉挑战赛（ILSVRC）在2012年所使用的比赛数据。1000分类 通用数据集，涵盖动植物、人类、生活用品、食物、景色、交通工具等类别。任 何互联网大厂在深度学习模型上线之前必用的测试数据集。2012年版本数据集由 于版权原因已在全网下架，现只分享给拥有官方学术头衔的机构或个人（这意味 着，你在申请数据时必须使用xxx@xxx.edu的电子邮件地址，任何免费的、非学 术机构性质的电子邮件地址都不能获得数据申请）。也因此，PyTorch中的 torchvision.datasets.ImageNet类已失效。</td></tr><tr><td>ImageNet2019</td><td>ILSVRC在2017年之后就取消了识别任务，并将比赛转到Kaggle上举办，现在 ImageNet2019版本可在Kaggle上免费下载，主要用于检测任务。</td></tr><tr><td>PASCAL VOCSegmentation<br>PASCAL VOCDetection</td><td>模式分析，统计建模和计算学习大赛（Pattern Analysis, Statistical Modelling and Computation Learning，PSACAL），视觉对象分类（Visual Object Classes）数据集。与ImageNet类型相似，覆盖动植物、人类、生活用品、食 物、交通、等类别，但数据量相对较小。在PyTorch中被分为 VOCSegementation与VOCDetection两个类，分别支持分割和检测任务，虽然带 有类别标签但基本不支持识别任务。同时，PyTorch支持从2007到2012年的5个 版本的下载，通常我们都使用最新版本。</td></tr><tr><td>CocoCaptions<br>CocoDetection</td><td>微软Microsoft Common Objects in Context数据集，是大规模场景理解挑战赛 （Large-scale Scene Understanding challenge，LSUN）中的核心数据，主要 覆盖复杂的日常场景，是继ImageNet之后，最受关注的物体检测、语义分割、图 像理解（Captions）方面的数据集，也是唯一关注图像理解的大规模挑战赛。其中，用于图像理解的部分是2015年之前的数据集，用于检测和分割的则是2017及之后的版本。需要安装COCO API才可以调用。PyTorch没有提供用于分割的 API，但是我们依然可以自己下载数据集用于分割。</td></tr><tr><td>LSUN</td><td>城市景观、城市建筑、人文风光数据集，包含10种场景、20种对象的大型数据 集。其中，场景图像被用于LSUN大规模场景理解挑战赛。20分类包含包含交通 工具、动物、人类等图像，可用于识别任务。在我们导入数据时，我们会选择 LSUN中的某一个场景或一个对象进行训练，因此个人基于LSUN进行的识别任务 是2分类的。</td></tr></tbody></table></div><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E7%AB%9E%E8%B5%9B%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9B%B8%E5%85%B3%E4%BF%A1%E6%81%AF.png" alt=""></p><blockquote><p>LSUN是lmdb格式的数据，必要时需要pip install lmdb</p></blockquote><p>   下面以LSUN为例子进行说明</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data_val = torchvision.datasets.LSUN(root=<span class="string">r&#x27;D:\Desktop\QNJS\Dataset\lsun-master\lsun-master\data&#x27;</span>,</span><br><span class="line">                                     classes=[<span class="string">&#x27;classroom_val&#x27;</span>,<span class="string">&#x27;church_outdoor_val&#x27;</span>],</span><br><span class="line">                                     transform=transforms.ToTensor(),</span><br><span class="line">                                     )</span><br><span class="line">image_tensor = data_val[<span class="number">0</span>][<span class="number">0</span>] <span class="comment"># 数据有600个，0-299为classroom，300-600为church_outdoor</span></span><br><span class="line"><span class="comment"># 第一个类别标签为0，第二个类别标签为1</span></span><br></pre></td></tr></table></figure><h4 id="1-3-景物、人脸、通用、其他"><a href="#1-3-景物、人脸、通用、其他" class="headerlink" title="1.3 景物、人脸、通用、其他"></a>1.3 景物、人脸、通用、其他</h4><div class="table-container"><table><thead><tr><th>数据集名称</th><th>简要描述</th></tr></thead><tbody><tr><td>CelebA</td><td>全球名人(单人)人脸数据集，可能是全球最大的人脸数据集之一，图片质量很高， 拥有丰富的标签。虽然PyTorch官方说明中允许下载，但实际上下载链接已失效， 需自行下载。且由于CelebA下载后的文件是7z压缩格式，类datasets.CelebA无法读取，因此下载后的图像无法使用CelebA类来进行导入。JPG可以抽样。</td></tr><tr><td>CIFAR10</td><td>10分类通用数据集，涵盖10类动物与交通工具，可能是除了MNIST系列之外，最常用于教学/基准线的图像识别数据集</td></tr><tr><td>CIFAR100</td><td>100分类通用数据集，涵盖动植物、人类、食物、交通工具。CIFAR100是在 CIFAR10的基础上拓展出来的，每个图像样本都含有20个粗粒度标签和100个细粒 度标签，通常我们使用细粒度标签进行分类。</td></tr><tr><td>STL10</td><td>从ImageNet数据集中抽样的、专用于无监督/半监督深度学习的数据集。</td></tr><tr><td>SBD</td><td>2011版本VOC的衍生数据集，全称为语义边界数据集（Semantic Boundaries Dataset and Benchmark），专门用于语义轮廓识别。</td></tr><tr><td>Cityscapes  Cityscapes extra</td><td>城市街景数据集，包含50个城市、不同季节、不同时段的街景图片和视频。可用于 密集语义分割和车辆、人群的实例分割。拓展数据集可用于检测任务、以及雨幕、 大雾的语义分割以及全景分割。需注册才能够进行下载，仅分享给拥有官方学术头衔的机构。</td></tr><tr><td>Places365</td><td>目前为止最大规模的景观数据集，由MIT支持，涵盖城市景观、自然风光、室内室 外各种场景。</td></tr></tbody></table></div><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E9%80%9A%E7%94%A8%E6%95%B0%E6%8D%AE%E9%9B%86%E5%86%85%E5%AE%B9%E7%9B%B8%E5%85%B3.png" alt=""></p><ul><li><p>能把CelebA运行起来，以及可以在Palace365做一些预测即可。</p></li><li><p>下面以CIFAR10/100为例，进行说明。</p></li><li><p>注意目前只有MINST和CIFAR可以.data/.targets/.classes。</p></li><li><p>SBU和SBD数据也可以使用torchvision.datasets进行读取</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#import torch, torchvision</span></span><br><span class="line"><span class="comment">#import torchvision.transforms as transforms</span></span><br><span class="line"><span class="comment">#import matplotlib.pyplot as plt</span></span><br><span class="line"><span class="comment">#import numpy as np</span></span><br><span class="line"></span><br><span class="line">data = torchvision.datasets.CIFAR10(root = <span class="string">r&quot;D:\Desktop\QNJS\Dataset\cifar&quot;</span></span><br><span class="line">                                   ,train=<span class="literal">True</span></span><br><span class="line">                                   ,download=<span class="literal">False</span></span><br><span class="line">                                   ,transform = transforms.ToTensor()</span><br><span class="line">                                   )</span><br><span class="line"></span><br><span class="line">data[<span class="number">0</span>][<span class="number">0</span>].shape  <span class="comment"># torch.Size([3, 32, 32])</span></span><br><span class="line">data.classes  <span class="comment"># classes&#x27;s list</span></span><br><span class="line">np.unique(data.targets)  <span class="comment"># array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br></pre></td></tr></table></figure><h4 id="1-4-总结"><a href="#1-4-总结" class="headerlink" title="1.4 总结"></a>1.4 总结</h4><p>   以上数据均可通过torchvision.datasets进行导入，相对友好的教学数据集是LSUN和CIFAR。</p><p>   对于自己的数据集，我们可以通过下面的形式进行导入。</p><h3 id="2-使用自己的数据、图片创造数据集"><a href="#2-使用自己的数据、图片创造数据集" class="headerlink" title="2 使用自己的数据、图片创造数据集"></a>2 使用自己的数据、图片创造数据集</h3><h4 id="2-1-从图像png-jpg到四维tensor"><a href="#2-1-从图像png-jpg到四维tensor" class="headerlink" title="2.1 从图像png/jpg到四维tensor"></a>2.1 从图像png/jpg到四维tensor</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = torchvision.datasets.ImageFolder(root=<span class="string">&quot;XXXX&quot;</span>                     </span><br><span class="line">                            ,transform=torchvision.transforms.ToTensor()</span><br><span class="line">                                       )</span><br></pre></td></tr></table></figure><p>   可以接受 .jpg、.jpeg、.png、.ppm、.bmp、.pgm、.tif、.tiff、.webp这9种不同的图片格式作为输入，并且还能够通过文件夹的分类自动识别标签。</p><p>   在Opencv库里有imread函数，可以将图片转为像素矩阵，后我们可将其转为张量，故图片转为张量的形式并不难。</p><p>   重点是怎么把标签读取进来，并将图片和标签打包成一个元组。</p><p>   在如下的文件夹结构中，这样的层次关系如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">F:/JPG-Datasets</span><br><span class="line">│</span><br><span class="line">├── train</span><br><span class="line">│   ├── Female</span><br><span class="line">│   │   ├── image1.jpg</span><br><span class="line">│   │   ├── image2.jpg</span><br><span class="line">│   │   └── ...</span><br><span class="line">│   └── Male</span><br><span class="line">│       ├── image1.jpg</span><br><span class="line">│       ├── image2.jpg</span><br><span class="line">│       └── ...</span><br><span class="line">│</span><br><span class="line">└── <span class="built_in">test</span></span><br><span class="line">    ├── Female</span><br><span class="line">    │   ├── image1.jpg</span><br><span class="line">    │   ├── image2.jpg</span><br><span class="line">    │   └── ...</span><br><span class="line">    └── Male</span><br><span class="line">        ├── image1.jpg</span><br><span class="line">        ├── image2.jpg</span><br><span class="line">        └── ...</span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">train_dataset = torchvision.datasets.ImageFolder(root=r<span class="string">&quot;D:\Desktop\QNJS\Dataset\[][][]zzzzzzip\datasets4\picturestotensor\Train&quot;</span>,</span><br><span class="line">                                        transform=torchvision.transforms.ToTensor()</span><br><span class="line">                                       )</span><br><span class="line"><span class="built_in">print</span>(train_dataset)</span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">Dataset ImageFolder</span></span><br><span class="line"><span class="string">    Number of datapoints: 60</span></span><br><span class="line"><span class="string">    Root location: D:\Desktop\QNJS\Dataset\[][][]zzzzzzip\datasets4\picturestotensor\Train</span></span><br><span class="line"><span class="string">    StandardTransform</span></span><br><span class="line"><span class="string">Transform: ToTensor()</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基本操作</span></span><br><span class="line">train_dataset.classes</span><br><span class="line">train_dataset.data</span><br><span class="line">train_dataset.targets</span><br><span class="line">np.unique(train_dataset.targets)</span><br><span class="line"></span><br><span class="line">train_dataset.imgs <span class="comment">#查看具体的图像地址,一般不会用到</span></span><br><span class="line"><span class="string">&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">[(&#x27;F:\\datasets4\\picturestotensor\\Train\\female\\000001.jpg&#x27;, 0),</span></span><br><span class="line"><span class="string"> (&#x27;F:\\datasets4\\picturestotensor\\Train\\female\\000002.jpg&#x27;, 0),</span></span><br><span class="line"><span class="string"> ...</span></span><br><span class="line"><span class="string"> (&#x27;F:\\datasets4\\picturestotensor\\Train\\male\\Pet-Peacock.png&#x27;, 1)]</span></span><br><span class="line"><span class="string">&quot;</span><span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>​    当图像标签类别较少时，可以轻松按类别整理图像。但标签多或数据量大时，手动分类变得费时。幸运的是，许多大型图像数据集，如LSUN，已预先按标签分类。</p><p>​    使用ImageFolder读取后的数据是无法轻易更改的，比如标签。大多会选择更加灵活的方式：<strong>自己写一个读取数据用的类</strong>。</p><hr><p>​    CLASS <code>torch.utils.data.Dataset</code>的存在地位相当于是<code>nn.Module</code>，在PyTorch中，许多torchvision.datasets中读数据的类，都继承自 Dataset。</p><p>​    如果一个读取数据的类继承自Dataset，那它读取出的数据一定是可以通过<strong>索引</strong>的方式进行调用和查看的。</p><p>​    Dataset中规定，如果一个子类要继承Dataset，则必须在子类中定义 <code>__getitem__()</code>方法。这个方法中的代码必须满足三个功能：1）读取单个图片并转化为张量。 2）读取该图片对应的标签。 3）将该图片的张量与对应标签打包成一个样本并输出。</p><p>​    定义好后。Dataset类中包含自动循环 <code>__getitem__()</code> 并拼接其输出结果的功能。该子类的输出就一定是打包好的整个数据集。</p><blockquote><p>下面以celebA数据集的子集为例，完成读取类别识别的任务</p><ul><li>celebAsubset<ul><li>Anno<ul><li>identity_CelebA_1000.txt</li><li>list_attr_celeba_1000</li></ul></li><li>Eval<ul><li>list_eval_partition.txt</li></ul></li><li>Img<ul><li>img_align_celeba_png.7z</li><li>Img_celeba.7z</li></ul></li><li>README</li></ul></li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">imgpath = <span class="string">r&quot;D:\Desktop\QNJS\Dataset\[][][]zzzzzzip\datasets4\picturestotensor\celebAsubset\Img\Img_celeba.7z\img_celeba&quot;</span></span><br><span class="line">csvpath = <span class="string">r&quot;D:\Desktop\QNJS\Dataset\[][][]zzzzzzip\datasets4\picturestotensor\celebAsubset\Anno\identity_CelebA_1000.txt&quot;</span></span><br><span class="line"></span><br><span class="line">identity = pd.read_csv(csvpath</span><br><span class="line">                       ,sep = <span class="string">&quot; &quot;</span></span><br><span class="line">                       ,header = <span class="literal">None</span>)   <span class="comment"># 不要把txt的第一行当做列名</span></span><br><span class="line"></span><br><span class="line">idx = <span class="number">0</span></span><br><span class="line">imgdir = os.path.join(imgpath,identity.iloc[idx,<span class="number">0</span>])  <span class="comment"># 图像目录</span></span><br><span class="line">image = io.imread(imgdir) <span class="comment"># 提取出的，索引为idx的图像的像素值矩阵</span></span><br><span class="line">label = identity.iloc[idx,<span class="number">1</span>]</span><br><span class="line">sample = (torch.tensor(image),<span class="built_in">int</span>(label))</span><br><span class="line"></span><br><span class="line"><span class="comment">##----基于此完成了：txt与jpg文件下，打包读取一张图片形成元组----##</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    自定义数据集，用于读取celebA数据集中的个体识别（identity recognition）数据的标签和图像</span></span><br><span class="line"><span class="string">    图像格式为jpg</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,csv_file, root_dir, transform = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        参数说明：</span></span><br><span class="line"><span class="string">            csv_file (字符串): 标签csv/txt的具体地址</span></span><br><span class="line"><span class="string">            root_dir (string): 所有图片所在的根目录</span></span><br><span class="line"><span class="string">            transform (callable, optional): 选填，需要对样本进行的预处理</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()  <span class="comment"># 选填</span></span><br><span class="line">        self.identity = pd.read_csv(csv_file,sep=<span class="string">&quot; &quot;</span>,header=<span class="literal">None</span>)</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.transform = transform</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment">#展示数据中总共有多少个样本</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.identity)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__info__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;CustomData&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\t Number of samples: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(self.identity)))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\t Number of classes: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(np.unique(self.identity.iloc[:,<span class="number">1</span>]))))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\t root_dir: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.root_dir))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,idx</span>):</span><br><span class="line">        <span class="comment">#保证idx不是一个tensor,官方Pytorch推荐</span></span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(idx):</span><br><span class="line">            idx = idx.tolist()</span><br><span class="line">        </span><br><span class="line">        imgdir = os.path.join(self.root_dir,self.identity.iloc[idx,<span class="number">0</span>]) <span class="comment">#图像目录</span></span><br><span class="line">        image = io.imread(imgdir) <span class="comment">#提取出的，索引为idx的图像的像素值矩阵</span></span><br><span class="line">        label = self.identity.iloc[idx,<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.transform != <span class="literal">None</span>:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line">            </span><br><span class="line">        sample = (image,label)</span><br><span class="line">        <span class="keyword">return</span> sample</span><br><span class="line">    </span><br><span class="line">data = CustomDataset(csvpath,imgpath)</span><br><span class="line">data[<span class="number">20</span>]</span><br><span class="line">data.__len__()</span><br><span class="line">data.__info__()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> data:</span><br><span class="line">    <span class="built_in">print</span>(x.shape)</span><br><span class="line">    <span class="built_in">print</span>(y)</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><blockquote><p>下面读取属性识别的标签</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">imgpath = <span class="string">r&quot;D:\Desktop\QNJS\Dataset\[][][]zzzzzzip\datasets4\picturestotensor\celebAsubset\Img\Img_celeba.7z\img_celeba&quot;</span></span><br><span class="line">csvpath = <span class="string">r&quot;D:\Desktop\QNJS\Dataset\[][][]zzzzzzip\datasets4\picturestotensor\celebAsubset\Anno\list_attr_celeba_1000.txt&quot;</span></span><br><span class="line"></span><br><span class="line">attr_ = pd.read_csv(csvpath, header=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">attr_ = pd.DataFrame(attr_.iloc[<span class="number">1</span>:, <span class="number">0</span>].<span class="built_in">str</span>.split().tolist(),</span><br><span class="line">                     columns=attr_.iloc[<span class="number">0</span>, <span class="number">0</span>].split())  <span class="comment"># 规整</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 开始代码------#</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDataset_attr</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    自定义数据集，用于读取celebA数据集中的属性识别（attribute recognition）数据的标签和图像</span></span><br><span class="line"><span class="string">    图像格式为jpg</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, csv_file, root_dir, labelname, transform=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        参数说明：</span></span><br><span class="line"><span class="string">            csv_file (字符串): 标签csv/txt的具体地址</span></span><br><span class="line"><span class="string">            root_dir (string): 所有图片所在的根目录</span></span><br><span class="line"><span class="string">            transform (callable, optional): 选填，需要对样本进行的预处理</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.attr_ = pd.read_csv(csvpath, header=<span class="literal">None</span>)</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.labelname = labelname</span><br><span class="line">        self.transform = transform</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 展示数据中总共有多少个样本</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.attr_)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__info__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;CustomData&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\t Number of samples: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(self.attr_) - <span class="number">1</span>))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\t Number of classes: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="built_in">len</span>(pd.read_csv(csvpath, header=<span class="literal">None</span>).iloc[<span class="number">0</span>,<span class="number">0</span>].split())))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\t root_dir: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.root_dir))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):  <span class="comment"># 索引要求只有一个参数</span></span><br><span class="line">        <span class="comment"># 保证idx不是一个tensor</span></span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(idx):</span><br><span class="line">            idx = idx.tolist()</span><br><span class="line"></span><br><span class="line">        self.attr_ = pd.DataFrame(self.attr_.iloc[<span class="number">1</span>:, <span class="number">0</span>].<span class="built_in">str</span>.split().tolist(),</span><br><span class="line">                                  columns=self.attr_.iloc[<span class="number">0</span>, <span class="number">0</span>].split())</span><br><span class="line"></span><br><span class="line">        imgdic = os.path.join(self.root_dir, self.attr_.iloc[idx, <span class="number">0</span>])  <span class="comment"># 图像目录</span></span><br><span class="line">        image = io.imread(imgdic)  <span class="comment"># 提取出的，索引为idx的图像的像素值矩阵</span></span><br><span class="line">        label = <span class="built_in">int</span>(self.attr_.loc[idx, self.labelname])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform != <span class="literal">None</span>:</span><br><span class="line">            image = self.transform(image)</span><br><span class="line"></span><br><span class="line">        sample = (image, label)</span><br><span class="line">        <span class="keyword">return</span> sample</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">labelname = <span class="string">&quot;Attractive&quot;</span></span><br><span class="line">data = CustomDataset_attr(csvpath, imgpath, labelname)</span><br><span class="line"></span><br><span class="line">data.__info__()</span><br></pre></td></tr></table></figure><h4 id="2-2-从二维表（csv-txt）到四维tensor"><a href="#2-2-从二维表（csv-txt）到四维tensor" class="headerlink" title="2.2 从二维表（csv/txt）到四维tensor"></a>2.2 从二维表（csv/txt）到四维tensor</h4><p>略</p><h4 id="2-3-从mat-pt-lmdb到四维tensor"><a href="#2-3-从mat-pt-lmdb到四维tensor" class="headerlink" title="2.3 从mat/pt/lmdb到四维tensor"></a>2.3 从mat/pt/lmdb到四维tensor</h4><p>略</p><h3 id="3-图片数据的基本预处理与数据增强"><a href="#3-图片数据的基本预处理与数据增强" class="headerlink" title="3 图片数据的基本预处理与数据增强"></a>3 图片数据的基本预处理与数据增强</h3><h4 id="3-1-数据预处理"><a href="#3-1-数据预处理" class="headerlink" title="3.1 数据预处理"></a>3.1 数据预处理</h4><p>​    当我们将所有有效数据导入后，我们至少需要确保：</p><ul><li>1）全部样本的尺寸是一致的（同时，全部样本的通道数是一致的）</li><li>2）图像最终以Tensor形式被输入卷积网络</li><li>3）图像被恰当地归一化</li></ul><div class="table-container"><table><thead><tr><th>类</th><th>说明</th></tr></thead><tbody><tr><td>Compose</td><td>transforms专用的，类似于nn.Sequential的打包功能，可以将数个transforms下 的类打包，形成类似于管道的结构来统一执行。</td></tr><tr><td>Resize</td><td>尺寸调整。需要输入最终希望得到的图像尺寸。注意区别于使用裁剪缩小尺寸或使 用填充放大尺寸。</td></tr><tr><td>CenterCrop</td><td>中心裁剪。需要输入最终希望得到的图像尺寸。</td></tr><tr><td>Normalize</td><td>归一化 (Tensor Only)。对每张图像的每个通道进行归一化，每个通道上的每个像 素会减去该通道像素值的均值，并除以该通道像素值的方差。</td></tr><tr><td>ToTensor</td><td>(PIL Only)将任意图像转变为Tensor</td></tr></tbody></table></div><blockquote><p> 前两项是为了CNN能够运行，第三项是为了训练变得更快速</p></blockquote><p>​    图像预处理时，通常将图像调整到28x28或224x224。若尺寸接近，用裁剪；尺寸差距大，先Resize再裁剪至目标尺寸（行业惯例）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要列表，专业用法</span></span><br><span class="line">transform1 = transforms.Compose([transforms.Resize(<span class="number">256</span>)</span><br><span class="line">                              ,transforms.CenterCrop(<span class="number">224</span>)])</span><br><span class="line"><span class="comment"># 等价于，不需要列表</span></span><br><span class="line">transform2 = nn.Sequential(transforms.Resize(<span class="number">256</span>)</span><br><span class="line">                        ,transforms.CenterCrop(<span class="number">224</span>))</span><br></pre></td></tr></table></figure><p>​    在归一化图像前，先将图像转为Tensor，然后按通道进行标准化。因此类 transforms.Normalize() 往往是在[0,1]区间上执行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([transforms.ToTensor()                             ,transforms.Normalize(<span class="number">0.5</span>,<span class="number">0.5</span>)])</span><br><span class="line"></span><br><span class="line"><span class="comment">#1) 常见且通用的做法，该写法只适用于三通道图像</span></span><br><span class="line">transforms.Normalize(mean=[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], <span class="comment">#代表三个通道上需要减去的值分别是0.5</span></span><br><span class="line">                     std=[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>]) <span class="comment">#代表三个通道上需要除以的值分别是0.5</span></span><br><span class="line"><span class="comment">#在保证数据范围在[0,1]的前提下，使用这个值可以令数据范围拓展到[-1,1] </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#也可写作：</span></span><br><span class="line">transforms.Normalize(<span class="number">0.5</span>,<span class="number">0.5</span>)</span><br><span class="line"><span class="comment">#这种写法中，Normalize类会根据通道数进行相应的计算，任意通道数的图像都可以使用,故推荐写这种写法</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#注意区分，这种写法只能用于单通道（灰度）图像</span></span><br><span class="line">transforms.Normalize([<span class="number">0.5</span>],[<span class="number">0.5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#2) ImageNet数据集上的均值和方差，可被用于任意实物照片分类</span></span><br><span class="line">transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                     std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#3) MNIST数据集上的均值和方差，可被用于MNIST系列（数字字母之类）</span></span><br><span class="line">transforms.Normalize((<span class="number">0.1307</span>), (<span class="number">0.3081</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#你也可以根据自己的数据集和自己希望实现的数值范围，来计算放入Normalize的值</span></span><br><span class="line"><span class="comment">#跑一两个epoch，选择起点最高的组作为归一化的参数</span></span><br></pre></td></tr></table></figure><p>​    <strong>对图像而言，必须完成的预处理就只有尺寸调整和归一化而已。</strong></p><h4 id="3-2-数据增强"><a href="#3-2-数据增强" class="headerlink" title="3.2 数据增强"></a>3.2 数据增强</h4><p>​    数据增强通过修改和重组现有数据来增加数据量，可以缓解数据不足问题，提高模型的鲁棒性和泛化能力，减少过拟合。</p><p>​    PyTorch的<code>torchvision.transforms</code>提供了多种数据增强方法，主要分为<strong>尺寸变化</strong>、<strong>像素值变化</strong>、<strong>视角变化</strong>和<strong>其他变化</strong>。</p><ul><li>尺寸变化</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-%E5%B0%BA%E5%AF%B8%E5%8F%98%E5%8C%96.png" alt=""></p><ul><li>像素变化</li></ul><p>​    能调整亮度、对比度、饱和度、色相等色彩属性。比较少用。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA-%E5%83%8F%E7%B4%A0%E5%8F%98%E5%8C%96.png" alt=""></p><ul><li>变形/视角变化。都比较常用。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-%E5%8F%98%E5%BD%A2%E8%A7%86%E8%A7%92%E5%8F%98%E5%8C%96.png" alt=""></p><ul><li>其他（转换格式、数据处理流程打包）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA-%E5%85%B6%E4%BB%96.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">256</span>),                    <span class="comment"># 将图像大小调整为 256x256</span></span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>),                <span class="comment"># 从图像中心裁剪出 224x224 的区域 ，也可以换为随机裁剪</span></span><br><span class="line">    transforms.ColorJitter(brightness=<span class="number">0.5</span>,     <span class="comment"># 随机调整亮度</span></span><br><span class="line">                           contrast=<span class="number">0.5</span>,       <span class="comment"># 随机调整对比度</span></span><br><span class="line">                           saturation=<span class="number">0.5</span>,     <span class="comment"># 随机调整饱和度</span></span><br><span class="line">                           hue=<span class="number">0.05</span>),          <span class="comment"># 随机调整色调</span></span><br><span class="line">    transforms.RandomHorizontalFlip(),         <span class="comment"># 以 50% 的概率水平翻转图像</span></span><br><span class="line">    transforms.RandomVerticalFlip(),           <span class="comment"># 以 50% 的概率垂直翻转图像</span></span><br><span class="line">    transforms.RandomRotation(<span class="number">30</span>),             <span class="comment"># 随机旋转图像±30度</span></span><br><span class="line">    transforms.RandomGrayscale(p=<span class="number">0.1</span>),         <span class="comment"># 以 10% 的概率将图像转换为灰度图</span></span><br><span class="line">    transforms.ToTensor(),                     <span class="comment"># 将 PIL 图像或 NumPy ndarray 转换为张量</span></span><br><span class="line">    transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],  <span class="comment"># 标准化图像</span></span><br><span class="line">                         std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment">#对于景色数据，水平翻转和随机裁剪都可能会比较有利</span></span><br><span class="line"><span class="comment">#因为建筑可能位于图像的任何地方，而根据尝试，水平翻转后的图像也能够被一眼看出是什么景色</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>​    这些方法<strong>在每次迭代训练时会随机应用变换，从而每次向模型呈现略有不同的数据版本</strong>。这种随机性实际上扩展了数据集的多样性，提升了模型的泛化能力，这就是深度学习框架中数据增强的巧妙设计。———“数据增强”</p><p>​    在PyTorch中，自定义数据集类（继承自<code>Dataset</code>）通常在<code>__getitem__()</code>方法中实现<code>transform</code>操作，而不是在<code>__init__()</code>中，这意味着原始数据被存储在内存中且不会立即进行变换。当通过<code>__getitem__()</code>调用数据时，从原始数据中复制出的样本会被<code>transform</code>处理，而内存中的原始数据保持不变。这种设计既节约内存（只存储一份原始数据），又提高效率（仅对需要的样本进行处理）。数据增强实际上是在从DataLoader中提取数据进行训练时才执行的。</p><p>​    在没有数据增强(<code>transform</code>)的情况下，每个<code>epoch</code>中数据批次不同，但整体数据一致，只是样本训练顺序变化，不增加新样本。引入<code>transform</code>后，即使原始数据相同，每次调用时随机变换（如裁剪、旋转、翻转）产生“新样本”，使每个<code>epoch</code>的数据独特，实质上增加了数据量，达到数据增强效果。因此，使用<code>transform</code>时，模型实际上不会接触到原始未处理的数据。</p><p>​    数据增强虽提高模型的鲁棒性和泛化能力，但也带来一些缺点：首先，由于数据中的随机性，模型收敛速度变慢，迭代周期增长；其次，数据增强的随机性无法通过随机数种子完全控制，导致模型结果可能有一定不稳定性。因此，使用数据增强的模型通常只能给出一个结果范围，而论文中报告的往往是这个范围的上限。</p><h2 id="二-训练与算法"><a href="#二-训练与算法" class="headerlink" title="二 训练与算法"></a>二 训练与算法</h2><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E5%AE%8C%E6%95%B4%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B.png" alt=""></p><h3 id="1-更强大的优化算法"><a href="#1-更强大的优化算法" class="headerlink" title="1 更强大的优化算法"></a>1 更强大的优化算法</h3><h3 id="1-1-AdaGrad"><a href="#1-1-AdaGrad" class="headerlink" title="1.1 AdaGrad"></a>1.1 AdaGrad</h3><p>​    之前的随机梯度下降或小批量随机梯度下降 中，我们所使用的权重迭代公式如下所示：</p><script type="math/tex; mode=display">w_{(t+1)} = w_{(t)} - \eta \frac{\partial L(w)}{\partial w}</script><p>​    <strong>Glorot条件</strong>中所声明的：只有所有权重上的梯度值都比较接近时，模型才能具有较好的学习能力，整体 损失才能够被优化到较好的局部最小值上。</p><p>​    使用每个维度上权重梯度的大小来<strong>自适应</strong>调整学习率，从而避免一个学习率难以适应所有维度的问题。</p><p>​    定义了gt用于表达<strong>第t次迭代时</strong>所有权重需要使用的梯度：</p><script type="math/tex; mode=display">g_t \equiv \frac{\partial L}{\partial w} = \frac{1}{n} \sum_{i=1}^{n} \nabla_w L(x^{(i)}, y^{(i)}, w)</script><p>​    我们使用梯度值的平方和作为学习率的分母来控制步长的大小（这是一种非常激进的策略，梯度的平方和可能是一个非常大的数）</p><script type="math/tex; mode=display">G_t = \sum_{\tau=1}^{t} g_{\tau}^2== \sum_{\tau=1}^{t} g_{\tau}g_{\tau}^T</script><script type="math/tex; mode=display">G_0 = g_0^2\\G_1 = G_0 + g_1^2\\G_2 = G_1 + g_2^2\\\ldots\\G_t = G_{t-1} + g_t^2</script><p>​    权重的具体迭代公式为：</p><script type="math/tex; mode=display">w_t = w_{t-1} - \frac{\eta}{\sqrt{G_t + \epsilon}} * g_t</script><script type="math/tex; mode=display">= w_{t-1} - \frac{\eta}{\sqrt{G_t + \epsilon}} * \frac{\partial L}{\partial w}</script><p>​    说明如下，其中t为迭代次数，m个w，n个样本</p><script type="math/tex; mode=display">\mathbf{g}_t = \begin{bmatrix}g_t(1) \\g_t(2) \\\vdots \\g_t(m)\end{bmatrix}=\begin{bmatrix}\frac{1}{n} \sum_{i=1}^{n} \nabla_w L(x^{(i)}, y^{(i)}, w_t(1)) \\\frac{1}{n} \sum_{i=1}^{n} \nabla_w L(x^{(i)}, y^{(i)}, w_t(2)) \\\vdots \\\frac{1}{n} \sum_{i=1}^{n} \nabla_w L(x^{(i)}, y^{(i)}, w_t(m))\end{bmatrix}</script><p>​    最终的迭代公式为：</p><script type="math/tex; mode=display">\begin{bmatrix}\theta^{(1)}_{t+1} \\\theta^{(2)}_{t+1} \\\vdots \\\theta^{(m)}_{t+1}\end{bmatrix}=\begin{bmatrix}\theta^{(1)}_t \\\theta^{(2)}_t \\\vdots \\\theta^{(m)}_t\end{bmatrix}-\eta\left(\begin{bmatrix}\epsilon & 0 & \cdots & 0 \\0 & \epsilon & \cdots & 0 \\\vdots & \vdots & \ddots & \vdots \\0 & 0 & \cdots & \epsilon\end{bmatrix}+\begin{bmatrix}G^{(1,1)}_t & 0 & \cdots & 0 \\0 & G^{(2,2)}_t & \cdots & 0 \\\vdots & \vdots & \ddots & \vdots \\0 & 0 & \cdots & G^{(m,m)}_t\end{bmatrix}\right)^{-\frac{1}{2}}\begin{bmatrix}g^{(1)}_t \\g^{(2)}_t \\\vdots \\g^{(m)}_t\end{bmatrix}.</script><p>​    我们在一 开始为AdaGrad设置较大的学习率为0.01，并在一开始进行较快的迭代。</p><p>​    通过学习率来修正梯度平稳性使得AdaGrad表现出非常适合于稀疏矩阵的特性。因为在AdaGrad的基础理论中，经过适当训练，AdaGrad会对高频特征设置较低的学习率，对低频特征设置较高的学习率。</p><p>​    此AdaGrad迭 代后期的学习率非常小，而非常容易出现梯度消失现象，故会出现损失值上无法下降的情况。</p><p>​    <code>torch.optim.Adagrad (params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0,eps=1e-10)</code></p><p>​    除非我们的数据是稀疏矩阵，否则我们较少会用到AadGrad，但是AdaGrad的思想却是我们常用的优化算 法的基础。</p><h3 id="1-2-RMSprop"><a href="#1-2-RMSprop" class="headerlink" title="1.2 RMSprop"></a>1.2 RMSprop</h3><p>​    为了解决 AdaGrad所面临的、随着迭代学习率会变得太小的问题，RMSprop算法由此提出。</p><script type="math/tex; mode=display">g_t = \frac{\partial L}{\partial w}\\G_t = \alpha G_{t-1} + (1 - \alpha)g_t^2\\\Delta_t = B\Delta_{t-1} - \frac{\eta}{\sqrt{G_t + \epsilon}} * g_t\\w_t = w_{t-1} + \Delta_t</script><p>​    <code>torch.optim.RMSprop (params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)</code></p><p>​    RMSprop存在可能让学习率增大的情况，违反了优化算法中的“正定性”（positive definiteness）</p><h3 id="1-3-Adam"><a href="#1-3-Adam" class="headerlink" title="1.3 Adam"></a>1.3 Adam</h3><p>​    Adam算法俗称是Adagrad和Momentum的结合，此结合就是RMSprop，故Adam实际上也是RMSprop的优化</p><script type="math/tex; mode=display">g_t = \frac{\partial L}{\partial w}\\\hat{V}_t = \frac{V_t}{1 - \beta_1} = \frac{\beta_1 V_{t-1} + (1 - \beta_1)g_t}{1 - \beta_1} = \frac{\beta_1}{1 - \beta_1} V_{t-1} + g_t\\\hat{G}_t = \frac{G_t}{1 - \beta_2} = \frac{\beta_2 G_{t-1} + (1 - \beta_2)g_t^2}{1 - \beta_2} = \frac{\beta_2}{1 - \beta_2} G_{t-1} + g_t^2\\w_t = w_{t-1} - \frac{\eta}{\sqrt{\hat{G}_t + \epsilon}} * \hat{V}_t</script><script type="math/tex; mode=display"></script><p>​    <code>CLASS torch.optim.Adam (params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)</code></p><p>​    Adam也会出现违反优化算法中的“正定性”（positive definiteness）的情况，故Adam的迭代流程修改为如下流程,称为AMSGrad：</p><script type="math/tex; mode=display">\hat{G}_t = \max(\hat{G}_{t-1}, \hat{G}_t)</script><h3 id="1-4-权重衰减"><a href="#1-4-权重衰减" class="headerlink" title="1.4 权重衰减"></a>1.4 权重衰减</h3><p>​    参数：weight_decay：</p><script type="math/tex; mode=display">w_t = (1 - \lambda)w_{t-1} - \eta * g_t</script><p>​    但需要注意的是，这个性质对于任何除了普通梯度下降之外的算法都不适用，比如，不适用于 AdaGrad，不适用于RMSprop，也不适用于Adam以及Adam的变体</p><script type="math/tex; mode=display">g_t = \frac{\partial L}{\partial w} + \lambda w\\w_t = w_{t-1} - \eta \left( \frac{\hat{V}_t}{\sqrt{\hat{G}_t + \epsilon}} + \lambda w_{t-1} \right)</script><p>​    一般不进行权重衰减。</p><h3 id="2-从经典架构出发"><a href="#2-从经典架构出发" class="headerlink" title="2 从经典架构出发"></a>2 从经典架构出发</h3><h3 id="2-1-调用经典架构"><a href="#2-1-调用经典架构" class="headerlink" title="2.1 调用经典架构"></a>2.1 调用经典架构</h3><p>​    对每种类型的架构，models中都包含了至少一个实现架构本身的父类 （呈现为“驼峰式”命名）以及一个包含预训练功能的子类（全部小写）<br>​    在实际使用模型时，我们几乎总是直接调用小写的类来进行使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models <span class="keyword">as</span> m</span><br><span class="line"></span><br><span class="line">resnet18_ = m.resnet18()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(resnet18_)</span><br><span class="line"><span class="built_in">print</span>(resnet18_.conv1)</span><br><span class="line"><span class="built_in">print</span>(resnet18_.layer1[<span class="number">0</span>].conv1)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models <span class="keyword">as</span> m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vgg16_ = m.vgg16()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vgg16_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;--------------&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(vgg16_.features[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">in_channels = <span class="number">1</span></span><br><span class="line">out_num = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">vgg16_.features[<span class="number">0</span>] = nn.Conv2d(in_channels, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">vgg16_.classifier[<span class="number">6</span>] = nn.Linear(in_features=<span class="number">4096</span>, out_features=out_num, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;--------------&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(vgg16_)</span><br></pre></td></tr></table></figure><p>​    大部分时候完全套用经典架构都不能满足我们建模的需求，因此我们需要基于经典架构构建我们自己的架构。</p><h3 id="2-2-基于经典架构自建架构"><a href="#2-2-基于经典架构自建架构" class="headerlink" title="2.2 基于经典架构自建架构"></a>2.2 基于经典架构自建架构</h3><p>​    我们可能有许多不同的建立架构的思路。最常见的方式是按照VGG的方式对网络进行加深，另一种则是使用经典网络中的块（例如残差网络中经典的残差单元、GoogLeNet中的inception等结构）来加深网络。</p><p>​    经验证明，在inception和残差单元之前增加一些普通卷积层会有好处。</p><p>​    缺点是网络的架构和具体的层无法在代码中清晰地 显示出来，同时层与层内部的层次结构也不一致。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models <span class="keyword">as</span> m</span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"></span><br><span class="line"><span class="comment"># 先查看网络结构，挑选我们要使用的部分</span></span><br><span class="line">vgg16_bn_ = m.vgg16_bn()  <span class="comment"># 初始化的参数</span></span><br><span class="line"></span><br><span class="line">vgg16_bn_.features[<span class="number">7</span>:<span class="number">14</span>]</span><br><span class="line">resnet18_ = m.resnet18()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet1</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 第一个卷积层自己写，以保证输入数据在尺寸、通道数上都正确</span></span><br><span class="line">        self.conv1 =nn.Sequential(nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">              , vgg16_bn_.features[<span class="number">1</span>:<span class="number">3</span>]) <span class="comment"># B R</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 后续的架构直接从经典架构中选</span></span><br><span class="line">        <span class="comment"># 对尺寸很小的数据集而言，我们的深度本来就不深，因此可以试着在特征图数量上有所增加</span></span><br><span class="line">        self.block2 = vgg16_bn_.features[<span class="number">7</span>:<span class="number">14</span>]  <span class="comment"># 卷 卷 最大池化</span></span><br><span class="line">        self.block3 = resnet18_.layer3  <span class="comment"># 残差块</span></span><br><span class="line"></span><br><span class="line">        self.avgpool = resnet18_.avgpool</span><br><span class="line"></span><br><span class="line">        self.fc = nn.Linear(in_features=<span class="number">256</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.block3(self.block2(x))</span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">256</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">data = torch.ones(<span class="number">10</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">net = MyNet1()</span><br><span class="line"><span class="built_in">print</span>(net(data).size())</span><br><span class="line"></span><br><span class="line">summary(net,input_size=(<span class="number">10</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicConv2d</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_, out_=<span class="number">10</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv = nn.Sequential(nn.Conv2d(in_, out_, **kwargs)</span><br><span class="line">                                  , nn.BatchNorm2d(out_)</span><br><span class="line">                                  , nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                  )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels=<span class="number">1</span>, out_features=<span class="number">10</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.block1 = nn.Sequential(BasicConv2d(in_=in_channels, out_=<span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">                                    , BasicConv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">                                    , nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">                                    , nn.Dropout2d(<span class="number">0.25</span>))</span><br><span class="line">        self.block2 = nn.Sequential(BasicConv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">                                    , BasicConv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">                                    , BasicConv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">                                    , nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">                                    , nn.Dropout2d(<span class="number">0.25</span>))</span><br><span class="line"></span><br><span class="line">        self.classifier_ = nn.Sequential(</span><br><span class="line">              nn.Linear(<span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">256</span>)</span><br><span class="line">            , nn.BatchNorm1d(<span class="number">256</span>)  <span class="comment"># 此时数据已是二维，因此需要BatchNorm1d</span></span><br><span class="line">            , nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">            , nn.Linear(<span class="number">256</span>, out_features)</span><br><span class="line">            , nn.LogSoftmax(<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.block2(self.block1(x))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">64</span> * <span class="number">7</span> * <span class="number">7</span>)</span><br><span class="line">        output = self.classifier_(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">data = torch.ones(<span class="number">10</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">net2 = MyNet2()</span><br><span class="line"><span class="built_in">print</span>(net2(data).size())</span><br><span class="line"><span class="comment"># 查看自己构建的网络架构和参数量</span></span><br><span class="line">summary(net2, input_size=(<span class="number">10</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))</span><br></pre></td></tr></table></figure><h3 id="2-3-模型的预训练-迁移学习"><a href="#2-3-模型的预训练-迁移学习" class="headerlink" title="2.3 模型的预训练/迁移学习"></a>2.3 模型的预训练/迁移学习</h3><p>预训练在实际使用中能够提供帮助的原因主要是：</p><ol><li><p><strong>层级化的信息提取</strong>：在一个神经网络中，不同层级的层提取的信息层次不同。越靠近输入层的部分通常负责提取更加基础和广泛的特征（例如，形状、边缘等），而越接近输出层的部分则提取与特定任务更紧密相关的高级特征。</p></li><li><p><strong>共享常识性知识</strong>：预训练模型的浅层部分可以被视为一种常识性知识的提取器，这些知识在不同任务间是通用的。通过迁移这部分层及其权重到新任务，可以帮助新任务快速学习和适应，尤其是在新任务的数据相对有限的情况下。</p></li><li><p><strong>加速学习和减少训练成本</strong>：预训练模型的这些层及其权重作为一种良好的初始化，可以使得新任务的模型更快地收敛到一个较好的性能，从而减少训练时间和成本。</p></li></ol><p>预训练技术虽然强大，但其应用存在一定的门槛和限制：</p><ol><li><p><strong>相似任务的可用性</strong>：有效利用预训练模型的前提是，必须有一个与当前任务高度相似的任务A，并且该任务A已有一个训练良好的模型。</p></li><li><p><strong>模型和权重的可访问性</strong>：即便找到了相似的任务，获取其训练好的模型架构和权重也是一个挑战，尤其是对于个人研究者或小型团队来说。</p></li><li><p><strong>公开可用的预训练模型限制</strong>：尽管存在一些公开的预训练模型（如基于ImageNet的模型），但它们可能不适用于所有类型的任务，特别是在一些专业领域（如医疗、金融）。</p></li><li><p><strong>输入数据的一致性</strong>：要成功应用预训练模型，任务A和任务B的输入数据格式和尺寸必须一致，否则可能需要进行额外的数据预处理或模型调整。</p></li></ol><p>您的分析非常透彻地阐述了预训练和迁移学习的实际应用策略。我将为您总结并提供一个例子来进一步阐释。</p><p>在实际应用中，预训练和迁移学习的区别和操作方法包括：</p><ol><li><p><strong>预训练（Pretraining）</strong>：将一个在任务A上训练好的模型的架构和权重迁移到任务B上，并对整个模型进行再训练。这种方法是期望原始任务的模型为新任务提供一个较好的初始化权重。</p></li><li><p><strong>迁移学习（Transfer Learning）</strong>：将任务A的模型迁移到任务B，但只对模型的一部分（通常是新增的层）进行训练，而保持原始模型的部分层（通常是接近输入的层）的权重固定。这种方法利用原始任务的模型作为一个固定的特征提取器。</p></li><li><p><strong>混合方法</strong>：在实际操作中，往往会采用一种混合策略。一开始可能会锁定迁移过来的层，只训练新增的层。随着训练的进行，可以逐渐解锁一部分层，使其参与到后续的训练中，这样可以平衡保留已学习知识和适应新任务的需要。</p></li></ol><p>预训练模型在编程实践中的应用主要涉及以下几个关键步骤：</p><ol><li><p><strong>选择模型</strong>：首先，选择一个适合任务的模型架构，如残差网络（ResNet）。</p></li><li><p><strong>使用预训练权重</strong>：在实例化模型时，可以选择是否使用预训练权重。这通常通过设置一个参数（如 <code>pretrained=True</code>）来实现。如果设置为 <code>True</code>，则模型将加载预训练的权重；如果为 <code>False</code> 或未设置，则模型将使用随机初始化的权重。</p></li><li><p><strong>下载权重</strong>：当选择使用预训练权重时，权重文件通常会从互联网上自动下载。注意，在下载过程中需要保持网络连接，并确保没有网络代理（VPN）干扰。</p></li><li><p><strong>模型训练</strong>：在下载权重之后，可以根据具体任务对模型进行进一步的训练。根据需要，可以选择训练整个模型或仅训练部分层。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models <span class="keyword">as</span> m</span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"></span><br><span class="line">resnet18_ = m.resnet18()</span><br><span class="line">rs18pt = m.resnet18(pretrained=<span class="literal">True</span>)  <span class="comment"># resnet18_pretrained</span></span><br><span class="line">fcin = rs18pt.fc.in_features</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> rs18pt.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet_pretrained</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 迁移层</span></span><br><span class="line">        self.pretrained = nn.Sequential(rs18pt.conv1,</span><br><span class="line">                                        rs18pt.bn1,</span><br><span class="line">                                        rs18pt.relu,</span><br><span class="line">                                        rs18pt.maxpool,</span><br><span class="line">                                        rs18pt.layer1,</span><br><span class="line">                                        rs18pt.layer2</span><br><span class="line">                                        )</span><br><span class="line">        <span class="comment"># 允许训练的层</span></span><br><span class="line">        self.train_ = nn.Sequential(resnet18_.layer3</span><br><span class="line">                                    , resnet18_.layer4</span><br><span class="line">                                    , resnet18_.avgpool)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 输出的线性层自己写，以确保输出的类别数量正确</span></span><br><span class="line">        self.fc = nn.Linear(in_features=fcin, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.pretrained(x)</span><br><span class="line">        x = self.train_(x)</span><br><span class="line">        x = x.view(x.shape[<span class="number">0</span>], <span class="number">512</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = MyNet_pretrained()</span><br><span class="line">summary(net, input_size=(<span class="number">10</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>), depth=<span class="number">3</span>, device=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解锁</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.pretrained[<span class="number">5</span>][<span class="number">1</span>].parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>​    可以迁移其他的模型和权重：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#本段代码仅做事例，不可运行</span></span><br><span class="line"><span class="comment">#=======【从url获取模型权重】========</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载状态字典</span></span><br><span class="line">state_dict = torch.hub.load_state_dict_from_url(url)</span><br><span class="line">model.load_state_dict()</span><br><span class="line"><span class="comment">#然后就可以用我们对resnet18使用的一系列手段进行迁移学习了</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#=======【从保存好的权重文件中获取权重】=======</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设 model 是您的模型实例</span></span><br><span class="line"><span class="comment"># 保存当前模型的状态字典到硬盘，以便之后能够重新加载</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;current_model_weights.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果需要，也可以保留在内存中的一份深拷贝</span></span><br><span class="line">original_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...执行一些操作，可能会改变模型的权重...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设您加载了新的权重，进行了一些操作，并想要评估它们</span></span><br><span class="line">new_model_wts = torch.load(<span class="string">&#x27;model_weights.pth&#x27;</span>)</span><br><span class="line">model.load_state_dict(new_model_wts)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ...评估 new_model_wts...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果新权重表现不佳，您想要恢复到原始权重</span></span><br><span class="line">model.load_state_dict(original_model_wts)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#=======【从保存好的模型中获取权重】======</span></span><br><span class="line"><span class="comment"># 保存完整模型</span></span><br><span class="line">torch.save(model, <span class="string">&#x27;model.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载完整模型</span></span><br><span class="line">model = torch.load(<span class="string">&#x27;model.pth&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="2-4-模型选择"><a href="#2-4-模型选择" class="headerlink" title="2.4 模型选择"></a>2.4 模型选择</h3><p>首先，我们需要对<strong>各个模型</strong>有较为清晰的认知：    </p><ul><li><strong>ResNet和GoogLeNet</strong>：这两个架构在参数效率上表现良好，适合较大的数据集。它们有许多改进的可能性，但在小数据集上的改进或基于这些架构构建新架构可能较为复杂。</li><li><strong>VGG</strong>：具有简单的架构思想，易于构建和修改。移除线性层可以显著减少参数量，使其成为一个不错的选择。</li><li><strong>AlexNet</strong>：架构最为简单（卷积x2 + 卷积x3），适合在小数据集上进行修改和扩展，可以结合其他网络的思想进行改进。</li></ul><p>第二，我们需要关注<strong>数据</strong>的复杂程度：</p><ul><li><p><strong>图像尺寸和信息量</strong>：大尺寸图像通常含有更多信息，特别是真实拍摄的照片相比人造图像或清洗过的图像更为复杂。</p></li><li><p><strong>标签类别数量</strong>：在分类任务中，标签类别越多意味着信息更加复杂；在回归任务中，数据波动大和分布不明确也代表更高的复杂性。</p></li><li><p><strong>网络架构选择</strong>：对于复杂样本（如大尺寸真实照片或类别多的数据集），需要更深更宽的网络架构，如50层以上的残差网络。例如，ImageNet和由真实照片生成的CIFAR-10数据集需要深层网络。CIFAR-10甚至需要110层残差网络来达到较高的准确率。</p></li><li><p><strong>适应不同数据集的架构深度</strong>：对于相对简单的数据集（如Fashion-MNIST），一般使用20层以下的浅层网络；对于表格数据，甚至可以使用约10层的网络。</p></li></ul><p>第三，我们需要关注数据的规模和我们拥有的<strong>算力</strong>：</p><ul><li><p><strong>数据规模和算力</strong>：数据规模可以从特征量和样本量判断，而算力限制决定了可用网络的深度。选择高参数利用率的架构，如残差网络或简化的VGG，是关键。</p></li><li><p><strong>数据量对训练策略的影响</strong>：大数据量时可以通过减小批量大小慢慢训练；数据不足时可能需要更浅的网络，并增加训练次数或使用数据增强来防止过拟合。</p></li><li><p><strong>资源限制下的策略</strong>：在计算资源受限的情况下，如果数据难度高且数据量小，神经网络可能不是最佳选择。</p></li><li><p><strong>有足够算力时的建议</strong>：如果算力充足，建议在多种架构上进行至少10个epochs的尝试，选择表现最好的模型进行进一步优化。</p></li><li><p><strong>算力有限时的策略</strong>：在算力有限的情况下，首选尝试ResNet18，根据其在10个epochs内的表现决定是否加深网络。如果ResNet18表现不佳或计算资源不足，可以尝试修改AlexNet架构。</p></li><li><p><strong>避免在无GPU环境下使用原版VGG16</strong>，因为其计算资源需求较高。</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9.png" alt=""></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE%E6%A8%A1%E5%9E%8B.png" alt=""></p><div class="table-container"><table><thead><tr><th></th><th>偏差大</th><th>偏差小</th></tr></thead><tbody><tr><td>方差大</td><td>模型不适合这个数据<br/>换模型</td><td>过拟合<br>模型很复杂<br/>对某些数据集预测很准确 对某些数据集预测很糟糕</td></tr><tr><td>方差小</td><td>欠拟合<br/>模型相对简单<br/>预测很稳定<br/>但对所有的数据预测都不太准确</td><td>泛化误差小，我们的目标</td></tr></tbody></table></div><h2 id="三-【案例】SVHN街道实景门牌识别"><a href="#三-【案例】SVHN街道实景门牌识别" class="headerlink" title="三 【案例】SVHN街道实景门牌识别"></a>三 【案例】SVHN街道实景门牌识别</h2><p>​    SVHN全称Street View House Number数据集（尺寸为32x32，通道为3，样本量也在10万左右），它是深度学习诞生初期被创造出来的众多数字识别数据集中的一个，也是<strong>唯一一个</strong>基于实拍图片制作而成的数字识别数据集。整个数据集支持<strong>识别</strong>、<strong>检测</strong>、<strong>无监督</strong>三种任务，SVHN数据集也因此具有<strong>三种不同的benchmark</strong>。</p><p>​    我们在SVHN文件夹中会看到两个mat格式的文件。</p><h3 id="1-设置库，导入环境"><a href="#1-设置库，导入环境" class="headerlink" title="1 设置库，导入环境"></a>1 设置库，导入环境</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#配置相关环境</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>]=<span class="string">&#x27;True&#x27;</span> <span class="comment">#用于避免jupyter环境突然关闭</span></span><br><span class="line">torch.backends.cudnn.benchmark=<span class="literal">True</span> <span class="comment">#用于加速GPU运算的代码</span></span><br><span class="line"><span class="comment">#导入pytorch一个完整流程所需的可能全部的包</span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> T</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models <span class="keyword">as</span> m</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment">#导入作为辅助工具的各类包</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment">#可视化</span></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time <span class="comment">#计算时间、记录时间</span></span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> random <span class="comment">#控制随机性</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> gc <span class="comment">#garbage collector 垃圾回收</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#设置全局的随机数种子，这些随机数种子只能提供有限的控制</span></span><br><span class="line"><span class="comment">#并不能完全令模型稳定下来</span></span><br><span class="line">torch.manual_seed(<span class="number">1412</span>) <span class="comment">#torch</span></span><br><span class="line">random.seed(<span class="number">1412</span>) <span class="comment">#random</span></span><br><span class="line">np.random.seed(<span class="number">1412</span>) <span class="comment">#numpy.random</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>导入数据——定义架构——超参数损失函数——训练函数——训练测试——可视化</p><h3 id="2-数据导入、数据探索、数据增强"><a href="#2-数据导入、数据探索、数据增强" class="headerlink" title="2 数据导入、数据探索、数据增强"></a>2 数据导入、数据探索、数据增强</h3><p>​    通常在第一次导入图像的时候，我们不会使用数据增强的任何手段，而是直接ToTensor()导入进行<strong>查看</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">train = torchvision.datasets.SVHN(root =<span class="string">&#x27;D:\Desktop\QNJS\Dataset\SVHN&#x27;</span></span><br><span class="line">                                 ,split =<span class="string">&quot;train&quot;</span> </span><br><span class="line">                                 ,download = <span class="literal">False</span></span><br><span class="line">                               <span class="comment"># ,transform = T.ToTensor()</span></span><br><span class="line">                                 )</span><br><span class="line">test = torchvision.datasets.SVHN(root =<span class="string">&#x27;D:\Desktop\QNJS\Dataset\SVHN&#x27;</span></span><br><span class="line">                                 ,split =<span class="string">&quot;test&quot;</span></span><br><span class="line">                                 ,download = <span class="literal">False</span></span><br><span class="line">                                 ,transform = T.ToTensor())</span><br><span class="line"><span class="built_in">print</span>(np.unique(train.labels))</span><br><span class="line"><span class="comment">####以上为查看数据部分</span></span><br><span class="line"></span><br><span class="line">trainT = T.Compose([T.RandomCrop(<span class="number">28</span>)</span><br><span class="line">                   ,T.RandomRotation(degrees = [-<span class="number">30</span>,<span class="number">30</span>])</span><br><span class="line">                   ,T.ToTensor()</span><br><span class="line">                   ,T.Normalize(mean = [<span class="number">0.485</span>,<span class="number">0.456</span>,<span class="number">0.406</span>]</span><br><span class="line">                                ,std = [<span class="number">0.229</span>,<span class="number">0.224</span>,<span class="number">0.225</span>])])</span><br><span class="line"></span><br><span class="line">testT = T.Compose([T.CenterCrop(<span class="number">28</span>)</span><br><span class="line">                  ,T.ToTensor()</span><br><span class="line">                  ,T.Normalize(mean = [<span class="number">0.485</span>,<span class="number">0.456</span>,<span class="number">0.406</span>]</span><br><span class="line">                                ,std = [<span class="number">0.229</span>,<span class="number">0.224</span>,<span class="number">0.225</span>])])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train = torchvision.datasets.SVHN(root =<span class="string">&#x27;D:\Desktop\QNJS\Dataset\SVHN&#x27;</span></span><br><span class="line">                                 ,split =<span class="string">&quot;train&quot;</span></span><br><span class="line">                                 ,download = <span class="literal">False</span></span><br><span class="line">                                 ,transform = trainT</span><br><span class="line">                                 )</span><br><span class="line">test = torchvision.datasets.SVHN(root =<span class="string">&#x27;D:\Desktop\QNJS\Dataset\SVHN&#x27;</span></span><br><span class="line">                                 ,split =<span class="string">&quot;test&quot;</span></span><br><span class="line">                                 ,download = <span class="literal">False</span></span><br><span class="line">                                 ,transform = testT</span><br><span class="line">                                )</span><br></pre></td></tr></table></figure><h3 id="3-基于经典架构构筑自己的网络"><a href="#3-基于经典架构构筑自己的网络" class="headerlink" title="3 基于经典架构构筑自己的网络"></a>3 基于经典架构构筑自己的网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#基于小型数据集，首先考虑使用各个经典架构中比较浅、但学习能力又比较强的架构</span></span><br><span class="line"><span class="comment">#比如ResNet18、VGG16、Inception也可以考虑</span></span><br><span class="line">torch.manual_seed(<span class="number">1412</span>)</span><br><span class="line">resnet18_ = m.resnet18()</span><br><span class="line">vgg16_ = m.vgg16() <span class="comment">#VGG本来参数量就很大，因此我个人较少使用vgg16_bn</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyResNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.block1 = nn.Sequential(nn.Conv2d(<span class="number">3</span>,<span class="number">64</span>,kernel_size=<span class="number">3</span></span><br><span class="line">                                              ,stride=<span class="number">1</span>,padding=<span class="number">1</span>,bias=<span class="literal">False</span>)</span><br><span class="line">                                   ,resnet18_.bn1</span><br><span class="line">                                   ,resnet18_.relu) <span class="comment">#删除池化层</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#后续的架构直接从经典架构中选</span></span><br><span class="line">        <span class="comment">#对尺寸很小的数据集而言，我们的深度本来就不深，因此可以试着在特征图数量上有所增加（增加宽度）</span></span><br><span class="line">        self.block2 = resnet18_.layer2 <span class="comment">#2个残差单元</span></span><br><span class="line">        self.block3 = resnet18_.layer3 <span class="comment">#2个残差单元</span></span><br><span class="line">        <span class="comment">#自适应平均池化+线性层，此处都与残差网络一致</span></span><br><span class="line">        self.avgpool = resnet18_.avgpool</span><br><span class="line">        <span class="comment">#输出的线性层自己写，以确保输出的类别数量正确</span></span><br><span class="line">        self.fc = nn.Linear(in_features=<span class="number">256</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.block1(x)</span><br><span class="line">        x = self.block2(x)</span><br><span class="line">        x = self.block3(x)</span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = x.view(x.shape[<span class="number">0</span>],<span class="number">256</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyVgg</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#在9层之后增加一个单独的卷积层，再加入池化层，构成(卷积x2+池化) + (卷积x3+池化)的类似AlexNet的结构</span></span><br><span class="line">        self.features = nn.Sequential(*vgg16_.features[<span class="number">0</span>:<span class="number">9</span>] <span class="comment">#星号用于解码</span></span><br><span class="line">                                     ,nn.Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">                                     ,nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                     ,nn.MaxPool2d(<span class="number">2</span>,<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>))</span><br><span class="line">        <span class="comment">#进入线性层时输入通道数发生变化，因此线性层需要重写</span></span><br><span class="line">        <span class="comment">#输出层也需要重写</span></span><br><span class="line">        self.avgpool = vgg16_.avgpool</span><br><span class="line">        self.fc = nn.Sequential(nn.Linear(<span class="number">7</span>*<span class="number">7</span>*<span class="number">128</span>, out_features=<span class="number">4096</span>,bias=<span class="literal">True</span>)</span><br><span class="line">                                ,*vgg16_.classifier[<span class="number">1</span>:<span class="number">6</span>]</span><br><span class="line">                                ,nn.Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">10</span>,bias=<span class="literal">True</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = x.view(x.shape[<span class="number">0</span>],<span class="number">7</span>*<span class="number">7</span>*<span class="number">128</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 卷积层里面的参数大小是(in,out,kh,kw)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line">summary(MyResNet(),(<span class="number">10</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>),depth=<span class="number">1</span>,device=<span class="string">&quot;cpu&quot;</span>) </span><br><span class="line">summary(MyVgg(),(<span class="number">10</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>),depth=<span class="number">1</span>,device=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment">#残差网络的参数量少很多，但是总计算量是VGG的两倍还多</span></span><br><span class="line"><span class="comment">#同时，VGG模型占用的内存更大，所以VGG需要更大的显存，但在GPU上VGG理论上应该更快</span></span><br><span class="line"><span class="comment">#在这个过程中，我们是从已经实例化的类中直接复制层来使用</span></span><br><span class="line"><span class="comment">#因此我们复用经典架构的部分，参数已经被实例化好了</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#因此实例化具体的MyResNet()时没有参数生成</span></span><br><span class="line">[*MyResNet().block2[<span class="number">0</span>].parameters()][<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>] <span class="comment">#复制的部分参数是一致的</span></span><br><span class="line">[*resnet18_.layer2[<span class="number">0</span>].conv1.parameters()][<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"><span class="comment">#没有复用经典架构的部分，则在我们实例化网络的时候才有参数</span></span><br><span class="line">[*resnet18_.fc.parameters()] <span class="comment">#自己设立的部分参数是不同的</span></span><br><span class="line">[*MyResNet().fc.parameters()]</span><br></pre></td></tr></table></figure><h3 id="4-一套完整的训练函数"><a href="#4-一套完整的训练函数" class="headerlink" title="4 一套完整的训练函数"></a>4 一套完整的训练函数</h3><h4 id="4-1-迭代与预测"><a href="#4-1-迭代与预测" class="headerlink" title="4.1 迭代与预测"></a>4.1 迭代与预测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">IterOnce</span>(<span class="params">net,criterion,opt,x,y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对模型进行一次迭代的函数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    net: 实例化后的架构</span></span><br><span class="line"><span class="string">    criterion: 损失函数</span></span><br><span class="line"><span class="string">    opt: 优化算法</span></span><br><span class="line"><span class="string">    x: 这一个batch中所有的样本</span></span><br><span class="line"><span class="string">    y: 这一个batch中所有样本的真实标签</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    sigma = net.forward(x)</span><br><span class="line">    loss = criterion(sigma,y)</span><br><span class="line">    loss.backward()</span><br><span class="line">    opt.step()</span><br><span class="line">    opt.zero_grad(set_to_none=<span class="literal">True</span>) <span class="comment">#比起设置梯度为0，让梯度为None会更节约内存</span></span><br><span class="line">    yhat = torch.<span class="built_in">max</span>(sigma,<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">    correct = torch.<span class="built_in">sum</span>(yhat == y)</span><br><span class="line">    <span class="keyword">return</span> correct,loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">TestOnce</span>(<span class="params">net,criterion,x,y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    对一组数据进行测试并输出测试结果的函数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    net: 经过训练后的架构</span></span><br><span class="line"><span class="string">    criterion：损失函数</span></span><br><span class="line"><span class="string">    x：要测试的数据的所有样本</span></span><br><span class="line"><span class="string">    y：要测试的数据的真实标签</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#对测试，一定要阻止计算图追踪</span></span><br><span class="line">    <span class="comment">#这样可以节省很多内存，加速运算</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): </span><br><span class="line">        sigma = net.forward(x)</span><br><span class="line">        loss = criterion(sigma,y)</span><br><span class="line">        yhat = torch.<span class="built_in">max</span>(sigma,<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">        correct = torch.<span class="built_in">sum</span>(yhat == y)</span><br><span class="line">    <span class="keyword">return</span> correct,loss</span><br></pre></td></tr></table></figure><h4 id="4-2-提前停止"><a href="#4-2-提前停止" class="headerlink" title="4.2 提前停止"></a>4.2 提前停止</h4><p>例如，当连续n次迭代中，损失函数的减小值都低于阈值tol，或者测试集的分数提升值都低于阈值tol的时候，我们就可以令迭代停止了。</p><p>我们需要让本轮迭代的损失与 历史迭代最小损失比较，如果历史最小损失 - 本轮迭代的损失 &gt; tol，我们才认可损失函数减小了。</p><p><strong>白话：让测试集的loss是下降的，不是平稳或上升的。</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EarlyStopping</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, patience=<span class="number">5</span>, tol=<span class="number">0.0005</span></span>):</span><br><span class="line">        self.patience = patience</span><br><span class="line">        self.tol = tol</span><br><span class="line">        self.counter = <span class="number">0</span></span><br><span class="line">        self.lowest_loss = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)  <span class="comment"># 使用无穷大作为初始值</span></span><br><span class="line">        self.early_stop = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, val_loss</span>):</span><br><span class="line">        loss_improvement = self.lowest_loss - val_loss</span><br><span class="line">        <span class="keyword">if</span> loss_improvement &gt; self.tol:</span><br><span class="line">            self.lowest_loss = val_loss</span><br><span class="line">            self.counter = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.counter += <span class="number">1</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;\t NOTICE: Early stopping counter &#123;&#125; of &#123;&#125;&quot;</span>.<span class="built_in">format</span>(self.counter, self.patience))</span><br><span class="line">            <span class="keyword">if</span> self.counter &gt;= self.patience:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;\t NOTICE: Early Stopping Actived&#x27;</span>)</span><br><span class="line">                self.early_stop = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.early_stop</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="4-3-训练、测试、监控、保存权重、绘图"><a href="#4-3-训练、测试、监控、保存权重、绘图" class="headerlink" title="4.3 训练、测试、监控、保存权重、绘图"></a>4.3 训练、测试、监控、保存权重、绘图</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit_test</span>(<span class="params">net, batchdata, testdata, criterion, opt, epochs, tol, modelname, PATH</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">run_epoch</span>(<span class="params">data_loader, training=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;运行一个训练或测试的 epoch，并返回平均损失和准确率。&quot;&quot;&quot;</span></span><br><span class="line">        total_loss, total_correct = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> data_loader:</span><br><span class="line">            x, y = x.to(device), y.to(device)</span><br><span class="line">            <span class="keyword">if</span> training:</span><br><span class="line">                opt.zero_grad()</span><br><span class="line">                net.train()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                net.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">            outputs = net(x)</span><br><span class="line">            loss = criterion(outputs, y)</span><br><span class="line">            <span class="keyword">if</span> training:</span><br><span class="line">                loss.backward()</span><br><span class="line">                opt.step()</span><br><span class="line"></span><br><span class="line">            total_loss += loss.item()</span><br><span class="line">            total_correct += (outputs.argmax(<span class="number">1</span>) == y).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">        avg_loss = total_loss / <span class="built_in">len</span>(data_loader.dataset)</span><br><span class="line">        accuracy = total_correct / <span class="built_in">len</span>(data_loader.dataset)</span><br><span class="line">        <span class="keyword">return</span> avg_loss, accuracy</span><br><span class="line"></span><br><span class="line">    train_loss_history, test_loss_history = [], []</span><br><span class="line">    best_accuracy = <span class="number">0.0</span></span><br><span class="line">    early_stopper = EarlyStopping(tol=tol)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        train_loss, train_accuracy = run_epoch(batchdata, training=<span class="literal">True</span>)</span><br><span class="line">        test_loss, test_accuracy = run_epoch(testdata, training=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span> - Train Loss: <span class="subst">&#123;train_loss:<span class="number">.4</span>f&#125;</span>, &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;Test Loss: <span class="subst">&#123;test_loss:<span class="number">.4</span>f&#125;</span>, Train Acc: <span class="subst">&#123;train_accuracy:<span class="number">.2</span>f&#125;</span>%, &#x27;</span></span><br><span class="line">              <span class="string">f&#x27;Test Acc: <span class="subst">&#123;test_accuracy:<span class="number">.2</span>f&#125;</span>%&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        train_loss_history.append(train_loss)</span><br><span class="line">        test_loss_history.append(test_loss)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> test_accuracy &gt; best_accuracy:</span><br><span class="line">            best_accuracy = test_accuracy</span><br><span class="line">            torch.save(net.state_dict(), os.path.join(PATH, <span class="string">f&#x27;<span class="subst">&#123;modelname&#125;</span>.pt&#x27;</span>))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;\tWeights Saved&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> early_stopper(test_loss):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;\tEarly Stopping Triggered&#x27;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_loss_history, test_loss_history</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line"><span class="comment"># net = YourModel()  # 你的模型实例</span></span><br><span class="line"><span class="comment"># criterion = nn.CrossEntropyLoss()</span></span><br><span class="line"><span class="comment"># opt = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)</span></span><br><span class="line"><span class="comment"># train_loss, test_loss = fit_test(net, train_loader, test_loader, criterion, opt, 10, 0.0001, &quot;model_name&quot;, &quot;./models&quot;)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">full_procedure</span>(<span class="params">net, epochs, bs, modelname, PATH, lr=<span class="number">0.001</span></span>):</span><br><span class="line">    torch.manual_seed(<span class="number">1412</span>)</span><br><span class="line">    batchdata = DataLoader(train, batch_size=bs, shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">    testdata = DataLoader(test, batch_size=bs, shuffle=<span class="literal">False</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">    criterion = nn.CrossEntropyLoss(reduction=<span class="string">&quot;sum&quot;</span>)</span><br><span class="line">    opt = optim.RMSprop(net.parameters(), lr=lr)</span><br><span class="line">    <span class="keyword">return</span> fit_test(net, batchdata, testdata, criterion, opt, epochs, <span class="number">10</span>**(-<span class="number">5</span>), modelname, PATH)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">plotloss</span>(<span class="params">trainloss, testloss</span>):</span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line">    plt.plot(trainloss, color=<span class="string">&quot;red&quot;</span>, linestyle=<span class="string">&quot;-&quot;</span>, linewidth=<span class="number">2</span>, label=<span class="string">&quot;Train Loss&quot;</span>)</span><br><span class="line">    plt.plot(testloss, color=<span class="string">&quot;orange&quot;</span>, linestyle=<span class="string">&quot;--&quot;</span>, linewidth=<span class="number">2</span>, label=<span class="string">&quot;Test Loss&quot;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;Training and Testing Loss Over Epochs&quot;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Epochs&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.grid(<span class="literal">True</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">这些 PyTorch 函数是用来管理和获取关于 GPU 显存的信息。以下是每个函数的解释和可能的输出示例：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">1. **torch.cuda.is_available()**</span></span><br><span class="line"><span class="string">   - **功能**：检查当前环境是否有可用的 GPU。</span></span><br><span class="line"><span class="string">   - **输出示例**：`True` 或 `False`。</span></span><br><span class="line"><span class="string">   - **解释**：如果输出 `True`，表示有至少一个可用的 GPU；`False` 表示没有可用的 GPU。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">2. **torch.cuda.device_count()**</span></span><br><span class="line"><span class="string">   - **功能**：返回可用的 GPU 数量。</span></span><br><span class="line"><span class="string">   - **输出示例**：`1` 或更高的整数。</span></span><br><span class="line"><span class="string">   - **解释**：输出表示系统中可用的 GPU 数量。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">3. **torch.cuda.get_device_name(device)**</span></span><br><span class="line"><span class="string">   - **功能**：获取指定 GPU 的名称。</span></span><br><span class="line"><span class="string">   - **输出示例**：`&#x27;NVIDIA GeForce RTX 3080&#x27;`。</span></span><br><span class="line"><span class="string">   - **解释**：输出指定 GPU 设备的名称。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">4. **torch.cuda.memory_allocated()**</span></span><br><span class="line"><span class="string">   - **功能**：返回当前已分配的 GPU 内存量（以字节为单位）。</span></span><br><span class="line"><span class="string">   - **输出示例**：`1048576`（1MB）。</span></span><br><span class="line"><span class="string">   - **解释**：表示当前已分配给张量和缓存的 GPU 内存量。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">5. **torch.cuda.max_memory_allocated()**</span></span><br><span class="line"><span class="string">   - **功能**：返回自程序开始以来分配的最大 GPU 内存量（以字节为单位）。</span></span><br><span class="line"><span class="string">   - **输出示例**：`2097152`（2MB）。</span></span><br><span class="line"><span class="string">   - **解释**：表示程序运行过程中分配的最大 GPU 内存量。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">6. **torch.cuda.memory_reserved()**</span></span><br><span class="line"><span class="string">   - **功能**：返回缓存内存分配器正在使用的 GPU 内存量（以字节为单位）。</span></span><br><span class="line"><span class="string">   - **输出示例**：`4194304`（4MB）。</span></span><br><span class="line"><span class="string">   - **解释**：表示当前由 PyTorch 的缓存分配器保留的 GPU 内存量。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">7. **torch.cuda.get_device_capability(device)**</span></span><br><span class="line"><span class="string">   - **功能**：获取指定 GPU 的计算能力。</span></span><br><span class="line"><span class="string">   - **输出示例**：`(7, 5)`。</span></span><br><span class="line"><span class="string">   - **解释**：表示 GPU 的计算能力版本，例如 7.5。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">8. **torch.cuda.empty_cache()**</span></span><br><span class="line"><span class="string">   - **功能**：清空 PyTorch 的缓存分配器中未使用的缓存内存，但不会清空由张量占用的内存。</span></span><br><span class="line"><span class="string">   - **输出**：无输出，这是一个执行操作。</span></span><br><span class="line"><span class="string">   - **解释**：此函数用于清理 PyTorch 分配器持有但未使用的内存，有助于减少 GPU 内存碎片化。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">以上函数主要用于监控和管理 GPU 资源，特别在进行大规模的深度学习训练时非常有用，以确保有效利用 GPU 资源，避免内存溢出等问题。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">import torch</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">def print_gpu_info():</span></span><br><span class="line"><span class="string">    if torch.cuda.is_available():</span></span><br><span class="line"><span class="string">        device_count = torch.cuda.device_count()</span></span><br><span class="line"><span class="string">        print(f&quot;Number of GPUs Available: &#123;device_count&#125;&quot;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        for i in range(device_count):</span></span><br><span class="line"><span class="string">            print(f&quot;\nGPU &#123;i&#125;: &#123;torch.cuda.get_device_name(i)&#125;&quot;)</span></span><br><span class="line"><span class="string">            print(f&quot;  - Compute Capability: &#123;torch.cuda.get_device_capability(i)&#125;&quot;)</span></span><br><span class="line"><span class="string">            print(f&quot;  - Memory Allocated: &#123;torch.cuda.memory_allocated(i)&#125; bytes&quot;)</span></span><br><span class="line"><span class="string">            print(f&quot;  - Max Memory Allocated: &#123;torch.cuda.max_memory_allocated(i)&#125; bytes&quot;)</span></span><br><span class="line"><span class="string">            print(f&quot;  - Memory Reserved: &#123;torch.cuda.memory_reserved(i)&#125; bytes&quot;)</span></span><br><span class="line"><span class="string">            torch.cuda.empty_cache()  # 清空当前 GPU 的未使用的缓存内存</span></span><br><span class="line"><span class="string">            print(&quot;  - Cache Memory Cleared&quot;)</span></span><br><span class="line"><span class="string">    else:</span></span><br><span class="line"><span class="string">        print(&quot;No GPU available, using CPU.&quot;)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">print_gpu_info()</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h3 id="5-模型选择"><a href="#5-模型选择" class="headerlink" title="5 模型选择"></a>5 模型选择</h3>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>pip&amp;conda</title>
      <link href="/pip-conda/"/>
      <url>/pip-conda/</url>
      
        <content type="html"><![CDATA[<h3 id="查找site-packages文件夹"><a href="#查找site-packages文件夹" class="headerlink" title="查找site-packages文件夹"></a>查找site-packages文件夹</h3><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip -V</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/pip-V.png" alt=""></p><h3 id="创建新环境"><a href="#创建新环境" class="headerlink" title="创建新环境"></a>创建新环境</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n myenv python=3.8</span><br></pre></td></tr></table></figure><h3 id="查看已建环境"><a href="#查看已建环境" class="headerlink" title="查看已建环境"></a>查看已建环境</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda <span class="built_in">env</span> list</span><br></pre></td></tr></table></figure><h3 id="查看已安装包"><a href="#查看已安装包" class="headerlink" title="查看已安装包"></a>查看已安装包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>yolov3-paper</title>
      <link href="/yolov3-paper/"/>
      <url>/yolov3-paper/</url>
      
        <content type="html"><![CDATA[<h1 id="Yolov3-An-incremental-improvement"><a href="#Yolov3-An-incremental-improvement" class="headerlink" title="Yolov3:An incremental improvement"></a>Yolov3:An incremental improvement</h1><h2 id="Abstract—摘要"><a href="#Abstract—摘要" class="headerlink" title="Abstract—摘要"></a><strong>Abstract—摘要</strong></h2><p>我们提出了对YOLO的一些更新! 我们做了一些小的设计改动，使其变得更好。我们还训练了这个新的网络，这个网络非常棒。它比上次大了一点，但更准确。不过它仍然很快，不用担心。在320×320的情况下，YOLOv3在28.2mAP的情况下运行22毫秒，与SSD一样准确，但速度快三倍。当我们看一下旧的0.5 IOU mAP检测指标时，YOLOv3是相当好的。在Titan X上，它在51毫秒内实现了57.9个AP50，而RetinaNet在198毫秒内实现了57.5个AP50，性能相似但快3.8倍。像往常一样，所有的代码都在网上，<a href="https://pjreddie.com/yolo/。">https://pjreddie.com/yolo/。</a></p><blockquote><p><strong>对比YOLOv2：</strong> 做了一些小改进，然后训练了一个更深的模型，准确度有所提升，并且依旧很快。</p><p><strong>对比SSD：</strong> 在320×320的情况下，YOLOv3在28.2mAP的情况下运行22毫秒，与SSD一样准确，但速度快三倍。</p><p><strong>对比RetinaNet：</strong> 在Titan X上，它在51毫秒内实现了57.9个AP50，而RetinaNet在198毫秒内实现了57.5个AP50，性能相似但快3.8倍。</p><p>这是可爱的作者直接拿了RetinaNet的图，然后加上了自己的YOLO曲线，可以看到YOLO的线都已经弄到坐标轴外面去了，这是故意的，用这种方法凸显自己模型更快（还有个原因是因为懒）。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-abstract.png" alt=""></p></blockquote><h2 id="一、Introduction—（随性的）引言"><a href="#一、Introduction—（随性的）引言" class="headerlink" title="一、Introduction—（随性的）引言"></a><strong>一、Introduction—（随性的）引言</strong></h2><p>有时，你一整年全在敷衍了事而不自知。比如今年我就没做太多研究，在推特上挥霍光阴，置 GANs 于不顾。我有一点去年留下的动力[12] [1]；我设法对YOLO做了一些改进。但是，说实话，没有什么超级有趣的东西，只是做了一些小改动，让它变得更好。我还对其他人的研究提供了一些帮助。</p><p>实际上，这也是我们今天来到这里的原因。我们有一个可以上镜的最后期限[4]，我们需要引用我对YOLO的一些随机更新，但我们没有来源。所以准备好接受技术报告吧!</p><p>技术报告的好处是，它们不需要介绍，你们都知道我们为什么在这里。因此，这个介绍的结尾将为本文的其余部分做一个标志。首先，我们将告诉你YOLOv3是什么情况。然后我们会告诉你我们是如何做的。我们也会告诉你一些我们尝试过但没有成功的事情。最后，我们将思考这一切意味着什么。</p><blockquote><p>“有时，你一整年全在敷衍了事而不自知。比如今年我就没做太多研究，在推特上挥霍光阴，置 GANs 于不顾。”——最开始读时我一度以为这是网友的恶搞翻译，直到后来自己看了原文。。。不得不服，果真是大佬才敢那么任性吧！</p><p>很别具一格，没啥实际内容，大家直接往后翻吧hh。就是说这篇论文实际上算是一篇学术报告，是为了给他们即将出现的新论文做引用的。</p></blockquote><h2 id="二、The-Deal—改进的细节"><a href="#二、The-Deal—改进的细节" class="headerlink" title="二、The Deal—改进的细节"></a><strong>二、The Deal—改进的细节</strong></h2><h3 id="2-1-Bounding-Box-Prediction—边界框预测"><a href="#2-1-Bounding-Box-Prediction—边界框预测" class="headerlink" title="2.1 Bounding Box Prediction—边界框预测"></a>2.1 Bounding Box Prediction—边界框预测</h3><p>按照YOLO9000，我们的系统使用维度集群作为锚定框来预测边界框[15]。该网络为每个边界框预测4个坐标，tx, ty, tw, th。如果单元格从图像的左上角偏移（cx，cy），并且边界框先验具有宽度和高度pw，ph，那么预测对应于：</p><script type="math/tex; mode=display">b_x = \sigma(t_x) + c_x</script><script type="math/tex; mode=display">b_y = \sigma(t_y) + c_y</script><script type="math/tex; mode=display">b_w = p_w e^{t_w}</script><script type="math/tex; mode=display">b_h = p_h e^{t_h}</script><script type="math/tex; mode=display">Pr(\text{object}) \ast IOU(b, \text{object}) = \sigma(t_o)</script><p>在训练期间，我们使用平方误差损失之和。如果某个真实框的坐标是ˆt，我们的梯度就是真实框坐标值（从真实框中计算）减去我们的预测坐标值：ˆt<em> - t</em>。这个真实框坐标值可以通过倒置上述方程轻松计算出来。 </p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-deal.png" alt=""></p><p> YOLOv3使用逻辑回归法为每个边界框预测一个目标分数。如果边界框先验比其他边界框先验更多地与目标真实框重叠，则该分数应为1。如果先验边界框不是最好的，但与真实框目标的重叠程度超过了某个阈值，我们就忽略这个预测，遵循[17]。我们使用0.5的阈值。与[17]不同的是，我们的系统只为每个真实框对象分配一个先验边界框。如果没有为一个真实框对象分配一个先验边界框，它不会对坐标或类别的预测产生任何损失，只有目标置信度。</p><blockquote><h4 id="与YOLOv2相同之处"><a href="#与YOLOv2相同之处" class="headerlink" title="与YOLOv2相同之处"></a>与YOLOv2相同之处</h4><p>使用dimension clusters来找到先验anchor boxes，然后通过anchor boxes来预测边界框。</p><h4 id="YOLOv3的改进"><a href="#YOLOv3的改进" class="headerlink" title="YOLOv3的改进"></a>YOLOv3的改进</h4><p>在YOLOv3 中，利用逻辑回归来预测每个边界框的客观性分数( object score )，也就是YOLOv1 论文中说的confidence :</p><ul><li><strong>正样本：</strong> 如果当前预测的包围框比之前其他的任何包围框更好的与ground truth对象重合，那它的置信度就是 1。</li><li><strong>忽略样本：</strong> 如果当前预测的包围框不是最好的，但它和 ground truth对象重合了一定的阈值（这里是0.5）以上，神经网络会忽略这个预测。</li><li><strong>负样本:</strong> 若bounding box 没有与任一ground truth对象对应，那它的置信度就是 0</li></ul><blockquote><p><strong>Q1：为什么YOLOv3要将正样本confidence score设置为1?</strong></p><p>置信度意味着该预测框是或者不是一个真实物体，是一个二分类，所以标签是1、0更加合理。并且在学习小物体时，有很大程度会影响IOU。如果像YOLOv1使用bounding box与ground truth对象的IOU作为confidence，那么confidence score始终很小，无法有效学习，导致检测的Recall不高。</p><p><strong>Q2：为什么存在忽略样本?</strong></p><p>由于YOLOV3采用了多尺度的特征图进行检测，而不同尺度的特征图之间会有重合检测的部分。例如检测一个物体时，在训练时它被分配到的检测框是第一个特征图的第三个bounding box，IOU为0.98，此时恰好第二个特征图的第一个bounding box与该ground truth对象的IOU为0.95，也检测到了该ground truth对象，如果此时给其confidence score强行打0，网络学习的效果会不理想。</p></blockquote><p>与Faster-RCNN 不同，YOLOv3 仅对每一个真实物件分配一个anchor box，若没有分配到anchor box 的真实物件，便不会有坐标误差，仅会具有object score 误差。</p></blockquote><h3 id="2-2-Class-Prediction—类预测（单标签分类改进为多标签分类）"><a href="#2-2-Class-Prediction—类预测（单标签分类改进为多标签分类）" class="headerlink" title="2.2 Class Prediction—类预测（单标签分类改进为多标签分类）"></a>2.2 Class Prediction—类预测（单标签分类改进为多标签分类）</h3><p>每个框都使用多标签分类法预测边界框可能包含的类别。我们不使用softmax，因为我们发现它对于良好的性能是不必要的，相反，我们只是使用独立的逻辑分类器。在训练过程中，我们使用二元交叉熵损失来进行分类预测。</p><p>当我们转向更复杂的领域，如开放图像数据集[7]时，这种提法会有所帮助。在这个数据集中，有许多重叠的标签（即女人和人）。</p><p>使用softmax的假设是，每个框都有一个确切的类别，但情况往往并非如此。多标签方法可以更好地模拟数据。</p><blockquote><h4 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h4><p>每个框使用多标签分类预测边界框可能包含的类</p><h4 id="YOLOv3使用的方法"><a href="#YOLOv3使用的方法" class="headerlink" title="YOLOv3使用的方法"></a>YOLOv3使用的方法</h4><p>（1）YOLOv3 使用的是<strong>logistic 分类器</strong>，而不是之前使用的softmax。</p><p>（2）在YOLOv3 的训练中，便使用了<strong>Binary Cross Entropy ( BCE, 二元交叉熵)</strong> 来进行类别预测。</p><blockquote><p><strong>Q：softmax被替代的原因？</strong></p><p>（1）softmax只适用于单目标多分类(甚至类别是互斥的假设)，但目标检测任务中可能一个物体有多个标签。(属于多个类并且类别之间有相互关系)，比如Person和Women。</p><p>（2）logistic激活函数来完成，这样就能预测每一个类别是or不是。</p></blockquote></blockquote><h3 id="2-3-Predictions-Across-Scales—跨尺度预测"><a href="#2-3-Predictions-Across-Scales—跨尺度预测" class="headerlink" title="2.3 Predictions Across Scales—跨尺度预测"></a>2.3 Predictions Across Scales—跨尺度预测</h3><p>YOLOv3在三个不同的尺度上对框进行预测。我们的系统使用类似于特征金字塔网络[8]的概念从这些尺度上提取特征。从我们的基础特征提取器中，我们添加了几个卷积层。最后一个卷积层预测一个3维张量，编码边界框、对象性和类别预测。在我们与COCO[10]的实验中，我们在每个尺度上预测3个框，所以张量是N×N×[3∗(4+1+80)]，用于4个边界盒的偏移，1个对象性预测，和80个类别预测。</p><p>接下来，我们从前两层中提取特征图，并对其进行2倍的上采样。我们还从网络中的早期特征图中取出一个特征图，用连接法将其与我们的上采样特征合并。这种方法使我们能够从上采样的特征中获得更有意义的语义信息，并从早期的特征图中获得更精细的信息。然后，我们再增加几个卷积层来处理这个合并的特征图，最终预测出一个类似的张量，尽管现在的张量是原来的两倍。</p><p>我们再进行一次同样的设计，以预测最终规模的框。因此，我们对第三个尺度的预测得益于所有先前的计算以及网络早期的细化特征。</p><p>我们仍然使用k-means聚类法来确定我们的边界框预设。我们只是任意地选择了9个聚类和3个尺度，然后在各个尺度上均匀地划分聚类。在COCO数据集上，这9个聚类是。(10 × 13), (16 × 30), (33 × 23), (30 × 61), (62 × 45), (59 × 119), (116 × 90), (156 × 198), (373 × 326)。</p><blockquote><h4 id="YOLOv3的改进-1"><a href="#YOLOv3的改进-1" class="headerlink" title="YOLOv3的改进"></a>YOLOv3的改进</h4><p><strong>灵感来源：</strong> YOLOv3借鉴了FPN的方法，采用多尺度的特征图对不同大小的物体进行检测，以提升小物体的预测能力。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-%E8%B7%A8%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B.png" alt=""></p><p><strong>（1）YOLOv3采用了3个不同尺度的特征图（三个不同卷积层提取的特征）</strong></p><p>YOLOv3通过下采样32倍、16倍和8倍得到3个不同尺度的特征图。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-%E8%B7%A8%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B1.png" alt=""></p><p>例如输入416X416的图像，则会得到<strong>13X13</strong> (416/32)，<strong>26X26</strong>(416/16) 以及<strong>52X52</strong>(416/8)这3个尺度的特征图。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-%E8%B7%A8%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B2.png" alt=""></p><p><strong>（2）YOLOv3每个尺度的特征图上使用3个anchor box。</strong></p><p>使用dimension clusters得到9个聚类中心（anchor boxes），并将这些anchor boxes划分到3个尺度特征图上，尺度更大的特征图使用更小的先验框。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-%E8%B7%A8%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B3.png" alt=""></p><p><strong>（3）YOLOv3对每个尺度下的特征图都进行边界框的预测。</strong></p><p>每种尺度的特征图上可以得到<strong>N × N × [3 ∗ (4 + 1 + 80)]</strong> 的结果（分别是N x N个 gird cell ，3种尺度的anchor boxes，4个边界框偏移值、1个目标预测置信度以及80种类别的预测概率。）</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-%E8%B7%A8%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B4.png" alt=""></p><p>该方法允许从上采样的特征中获取更有意义的语义信息，从早期的特征图中获取更细粒度的信息。</p><h4 id="不同尺度下的预测方法"><a href="#不同尺度下的预测方法" class="headerlink" title="不同尺度下的预测方法"></a>不同尺度下的预测方法</h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-%E8%B7%A8%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B5.png" alt=""></p><p><strong>（1）第一种尺度：</strong></p><p><strong>特征图：</strong> 对原图下采样32x得到(13 x 13)特征图</p><p><strong>预测：</strong> 在上述特征图后添加几个卷积层，最后输出一个 N × N × [3 ∗ (4 + 1 + 80)] 的张量表示预测。——图中第3个红色部分</p><p><strong>最终输出：</strong> [13, 13, 255]</p><p><strong>（2）第二种尺度：</strong></p><p><strong>特征图：</strong> 来源于两种计算</p><p>   1.对原图下采样16x得到 (26 x 26)特征图</p><p>   2.对第一种尺度得到的(13 x 13)特征图进行上采样，得到(26 x 26)特征图。</p><p>两种计算得到的（26 x 26）特征图通过连接合并在一起。</p><p><strong>预测：</strong> 在合并后的特征图后添加几个卷积层，最后输出一个 N × N × [3 ∗ (4 + 1 + 80)] 的张量表示预测。这个张量的大小是尺度一输出张量大小的两倍。——图中第2个红色部分</p><p><strong>最终输出：</strong> [26, 26, 255]</p><p><strong>（3）第三种尺度：</strong></p><p><strong>特征图：</strong> 来源于两种计算</p><p> 1.对原图下采样8x得到 (52 x 52)特征图</p><p> 2.对第二种尺度得到的(26 x 26)特征图进行上采样，得到(52 x 52)特征图。</p><p>两种方式得到的（52 x 52）的特征图通过连接合并在一起。</p><p><strong>预测：</strong> 在合并后的特征图后添加几个卷积层，最后输出一个 N × N × [3 ∗ (4 + 1 + 80)] 的张量表示预测。这个张量的大小是尺度二输出的两倍——图中第1个红色部分。</p><p>对第三尺度的预测受益于所有的先验计算以及网络早期的细粒度特性。</p><p><strong>最终输出：</strong> [52, 52, 255]</p><blockquote><p><strong>Q：合并（加入残差啊思想）的目的：</strong></p><p>在每一种维度输出之前还有一个分支就是和下一路进行concat拼接(上一层进行上采样后拼接)。这样加入残差思想，保留各种维度特征(底层像素+高层语义)。三个尺度就可以预测各种不同大小的物体了。</p></blockquote><h4 id="结构模型示意图"><a href="#结构模型示意图" class="headerlink" title="结构模型示意图"></a>结构模型示意图</h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-%E8%B7%A8%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B6.png" alt=""></p><p>YOLOv3总共输出3个特征图，第一个特征图下采样32倍，第二个特征图下采样16倍，第三个下采样8倍。输入图像经过Darknet-53（无全连接层），再经过YOLOblock生成的特征图被当作两用，第一用为经过3×3卷积层、1×1卷积之后生成特征图一，第二用为经过1×1卷积层加上采样层，与Darnet-53网络的中间层输出结果进行拼接，产生特征图二。同样的循环之后产生特征图三。</p></blockquote><h3 id="2-4-Feature-Extractor—特征提取"><a href="#2-4-Feature-Extractor—特征提取" class="headerlink" title="2.4 Feature Extractor—特征提取"></a>2.4 Feature Extractor—特征提取</h3><p>我们使用一个新的网络来进行特征提取。</p><p>我们的新网络是YOLOv2中使用的网络、Darknet-19和新式的Darknet网络之间的一种混合方法。我们的网络使用连续的3×3和1×1卷积层，但现在也有一些快捷连接，而且明显更大。它有53个卷积层，所以我们把它叫做… 等待它… Darknet-53!</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-net.png" alt=""></p><p>这个新网络比Darknet19强大得多，但仍然比ResNet-101或ResNet-152更有效率。下面是一些ImageNet的结果:</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-result.png" alt=""></p><blockquote><p>作者是将darknet-19里加入了ResNet残差连接，改进之后的模型叫Darknet-53</p><h4 id="Darknet-53主要做了如下改进："><a href="#Darknet-53主要做了如下改进：" class="headerlink" title="Darknet-53主要做了如下改进："></a><strong>Darknet-53主要做了如下改进：</strong></h4><p>（1）<strong>没有采用最大池化层</strong>，转而采用步长为2的卷积层进行下采样。</p><p>（2）为了防止过拟合，在每个卷积层之后加入了<strong>一个BN层和一个Leaky ReLU</strong>。</p><p>（3）引入了<strong>残差网络</strong>的思想，目的是为了让网络可以提取到更深层的特征，同时避免出现梯度消失或爆炸。</p><p>（4）<strong>将网络的中间层和后面某一层的上采样进行张量拼接</strong>，达到多尺度特征融合的目的。</p><p>Darknet-53的性能与最先进的分类器相当，但浮点运算更少，速度更快。Darknet-53还实现了每秒最高的浮点运算。这意味着网络结构更好地利用了GPU，使其评估更高效，从而更快。</p></blockquote><h3 id="2-5-Training—训练"><a href="#2-5-Training—训练" class="headerlink" title="2.5 Training—训练"></a>2.5 Training—训练</h3><p>我们仍然在完整的图像上进行训练，没有硬性的负面挖掘或任何这些东西。我们使用多尺度训练，大量的数据增强，批量归一化，所有标准的东西。我们使用Darknet神经网络框架进行训练和测试[14]。</p><blockquote><p>（1）训练完整的图像，没有硬负面挖掘。</p><p>（2）使用多尺度的训练，大量的数据扩充，批量标准化。</p><p>（3）使用Darknet神经网络框架来训练和测试。</p></blockquote><h2 id="三、How-We-Do—我们怎样做"><a href="#三、How-We-Do—我们怎样做" class="headerlink" title="三、How We Do—我们怎样做"></a>三、How We Do—我们怎样做</h2><p>YOLOv3是相当不错的，见表3。在COCOs怪异的平均AP指标方面，它与SSD的变体相当，但速度快了3倍。虽然在这个指标上，YOLOv3和RetinaNet等模型一样。</p><p>然而，相比于IOU=0.5（或图表中的AP50）的 “老 “检测指标mAP时，YOLOv3显得非常强大。它几乎与RetinaNet持平，远高于SSD的变体。这表明YOLOv3是一个非常强大的检测器，擅长为目标生成合理的框。然而，随着IOU阈值的增加，性能明显下降，表明YOLOv3努力使框与目标完全对齐。</p><p>在过去，YOLO在处理小物体时很吃力。然而，现在我们看到这一趋势发生了逆转。通过新的多尺度预测，我们看到YOLOv3具有相对较高的APS性能。然而，它在中等和较大尺寸物体上的性能相对较差。需要进行更多的调查来了解这个问题的真相。</p><p>当我们在AP50指标上绘制准确度与速度的关系时（见图5），我们看到YOLOv3比其他检测系统有明显的优势。也就是说，它更快、更好。<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-howdo.png" alt=""></p><blockquote><p><strong>APs：</strong> 小目标（area(框大小）&lt;32×32）的AP</p><p><strong>APm：</strong> 中目标（32×32&lt;area&lt;96×96）的AP</p><p><strong>APl：</strong> 大目标（96×96&lt;area）的AP</p><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><ul><li>YOLOv3 擅于预测出「合适」，但无法预测出非常精准的边界框。</li><li>YOLOv3 小目标预测能力提升，但中大目标的预测反而相对较差。</li><li>若将速度考量进来，YOLOv3 整体来说表现非常出色。</li></ul><h4 id="YOLOv3在小目标-密集目标的改进"><a href="#YOLOv3在小目标-密集目标的改进" class="headerlink" title="YOLOv3在小目标\密集目标的改进"></a>YOLOv3在小目标\密集目标的改进</h4><p>1.<strong>grid cell个数增加</strong>，YOLOv1（7×7），YOLOv2（13×13），YOLOv3（13×13+26×26+52×52）</p><p>2.<strong>YOLOv2和YOLOv3可以输入任意大小的图片</strong>，输入图片越大，产生的grid cell越多，产生的预测框也就越多</p><p>3.<strong>专门小目标预先设置了一些固定长宽比的anchor</strong>，直接生成小目标的预测框是比较难的，但是在小预测框基础上再生成小目标的预测框是比较容易的</p><p>4.<strong>多尺度预测（借鉴了FPN）</strong>，既发挥了深层网络的特化语义特征，又整合了浅层网络的细腻度的像素结构信息</p><p>5.对于小目标而言，边缘轮廓是非常重要的，即浅层网络的边缘信息。<strong>在损失函数中有着惩罚小框项</strong></p><p>6.网络结构：<strong>网络加了跨层连接和残差连接（shortcut connection）</strong>，这样可以整合各个层的特征，这样使得网络本身的特征提取能力提升了</p></blockquote><h2 id="四、Things-We-Tried-That-Didn’t-Work—那些我们尝试了但没有奏效的方法"><a href="#四、Things-We-Tried-That-Didn’t-Work—那些我们尝试了但没有奏效的方法" class="headerlink" title="四、Things We Tried That Didn’t Work—那些我们尝试了但没有奏效的方法"></a><strong>四、Things We Tried That Didn’t Work—那些我们尝试了但没有奏效的方法</strong></h2><p>我们在做YOLOv3的时候尝试了很多东西。很多东西都没有成功。以下是我们能记住的东西。</p><p> <strong>锚框的X、Y偏移量预测</strong>。我们尝试使用正常的锚定框预测机制，即用线性激活的方式将x、y偏移量预测为框宽或框高的倍数。我们发现这种提法降低了模型的稳定性，而且效果不是很好。</p><p><strong>线性x，y预测，而不是逻辑预测</strong>。我们尝试用线性激活来直接预测x，y偏移量，而不是用逻辑激活。这导致了mAP下降了几个点。</p><p> <strong>Focal loss</strong>。我们尝试使用焦点损失。它使我们的mAP下降了大约2点。YOLOv3可能已经对焦点损失试图解决的问题很稳健，因为它有独立的对象性预测和条件类预测。因此，对于大多数例子来说，类别预测没有损失？还是什么？我们并不完全确定。</p><p> <strong>双重IOU阈值和真实分配</strong>。Faster R-CNN在训练过程中使用两个IOU阈值。如果一个预测与真实框重叠0.7，它就是一个正面的例子，重叠[0.3 - 0.7]，它就会被忽略，对于所有地面真相对象来说，小于0.3就是一个负面的例子。我们尝试过类似的策略，但没有得到好的结果。</p><p>我们相当喜欢我们目前的表述，它似乎至少处于一个局部最优状态。这些技术中的一些可能最终会产生好的结果，也许它们只是需要一些调整来稳定训练。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov3-didntwork.png" alt=""></p><blockquote><p>这部分不用深入研究，因为这是作者也没有捣鼓清楚的内容~</p><p>（1）<strong>预测相对于初始anchor宽高倍数的偏移量</strong> ，使用线性激活预测x, y偏移为的anchor box的宽度或高度的倍数。这种方法降低了模型的稳定性，效果不是很好</p><p>（2）<strong>使用线性激活来直接预测x, y偏移量</strong>，而不是逻辑逻辑激活。这导致了一些mAP的下降。</p><p>（3）<strong>Focal loss</strong> （用于图像领域解决数据不平衡造成的模型性能问题，也就是正负样本不均衡，正样本少的问题)。mAPx下降2个点。YOLOv3可能已经对focal loss试图解决的问题很健壮，因为它有独立的目标预测和条件类预测。</p><p>（4）<strong>Faster RCNN在训练中使用两个IOU阈值</strong>。如果预测框与真值 IOU大于0.7的，则边界框作为正样本。如果IOU在0.3-0.7之间它被忽略，小于0.3阈值时，它是一个负样本。我们尝试了类似的策略，但没有得到好的结果。</p></blockquote><h2 id="五、-What-This-All-Means—这一切意味着什么？"><a href="#五、-What-This-All-Means—这一切意味着什么？" class="headerlink" title="五、 What This All Means—这一切意味着什么？"></a><strong>五、 What This All Means—这一切意味着什么？</strong></h2><p>YOLOv3是一个好的检测器。它的速度很快，很准确。它在0.5和0.95 IOU之间的COCO平均AP指标上不那么好。但它在旧的检测指标0.5 IOU上是非常好的。</p><p>我们到底为什么要转换指标？最初的COCO论文中只有这样一句话：”关于评估指标的全面讨论。“一旦评估服务器完成，将增加对评估指标的全面讨论”。Russakovsky等人报告说，人类很难区分0.3和0.5的IOU! “训练人类目测一个IOU为0.3的边界框并将其与一个IOU为0.5的边界框区分开来是非常困难的。” [18] 如果人类很难区分，那么这又有多大关系呢？但也许一个更好的问题是：“既然我们有了这些探测器，我们要用它们做什么？” 很多做这项研究的人都在谷歌和Facebook。</p><p>我想至少我们知道这项技术是在良好的手中，绝对不会被用来收集你的个人信息并出售给…，等等，你是说这正是它将被用来做什么？哦。</p><p>好吧，其他大量资助视觉研究的人是军方，他们从来没有做过任何可怕的事情，比如用新技术杀死很多人，哦，等等…1，我很希望大多数使用计算机视觉的人只是在用它做快乐的好事，比如计算国家公园里斑马的数量[13]，或者跟踪他们的猫在家里徘徊[19]。但是，计算机视觉已经被用于可疑的用途，作为研究人员，我们有责任至少考虑我们的工作可能造成的伤害，并想办法减轻它。我们欠世界这么多。</p><p>最后，请不要@我。(因为我终于退出了Twitter）。</p><blockquote><p>作者希望计算机视觉可以用在好的、对的事情上面。Love&amp;Peace~</p></blockquote><div class="table-container"><table><thead><tr><th></th><th>YOLOv1</th><th>YOLOv2</th><th>YOLOv3</th></tr></thead><tbody><tr><td>输入图像尺寸</td><td>输入的是448×448的三通道图像</td><td>输入的是416×416的三通道图像</td><td>输入的是416×416的三通道图像</td></tr><tr><td>grid cell</td><td>每一张图像划分为7×7=49个grid cell</td><td>每一张图像划分为13×13=169个grid cell</td><td>yolov3会产生三个尺度：13×13、26×26、52×52，也对应着grid cell个数。</td></tr><tr><td>bbox/anchor</td><td>每个grid cell 生成2个 bbox（没有anchor），与真实框IOU最大的那个框负责拟合真实框</td><td>每个grid cell 生成5个anchor框，通过IOU计算选一个anchor产生预测框去拟合真实框</td><td>每个grid cell生成3个anchor框，通过与gt的IOU计算选一个anchor产生预测框去拟合真实框</td></tr><tr><td>输出张量</td><td>输出7 <em> 7 </em> 30维的张量(30的含义:两组bbox的xywh和置信度+20个类别)</td><td>输出13 <em> 13 </em> 125维张量(125的含义：五组anchor的xywh和置信度+20个类别)</td><td>输出三个不同尺寸的张量，但最后都是255，比如S <em> S </em> 255，（255含义:三组anchor里xywh+置信度+分类数(COCO数据集80个分类)，所以就是3 * (80+5)。）</td></tr><tr><td>预测框数量</td><td>7 <em> 7 </em>2 = 98个预测框</td><td>13 <em> 13 </em> 5 = 845个预测框</td><td>( 52 <em> 52 + 26 </em> 26 +13 <em> 13) </em> 3 = 10647个预测框</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>yolov2-paper</title>
      <link href="/yolov2-paper/"/>
      <url>/yolov2-paper/</url>
      
        <content type="html"><![CDATA[<h1 id="YOLO9000-Better-Faster-Stronger"><a href="#YOLO9000-Better-Faster-Stronger" class="headerlink" title="YOLO9000: Better, Faster, Stronger"></a>YOLO9000: Better, Faster, Stronger</h1><h2 id="Abstract—摘要"><a href="#Abstract—摘要" class="headerlink" title="Abstract—摘要"></a>Abstract—摘要</h2><p>我们介绍了YOLO9000，一个最先进的实时目标检测系统，可以检测超过9000个目标类别。首先，我们提出了对YOLO检测方法的各种改进，这些改进既是新的，也是来自先前的工作。改进后的模型YOLOv2在标准检测任务上是最先进的，如PASCAL VOC和COCO。使用一种新的、多尺度的训练方法，同一个YOLOv2模型可以在不同的规模下运行，在速度和准确性之间提供了一个简单的权衡。在67FPS时，YOLOv2在VOC 2007上得到76.8mAP。在40 FPS时，YOLOv2得到78.6 mAP，超过了最先进的方法，如带有ResNet和SSD的Faster R-CNN，同时运行速度仍然很高。最后，我们提出了一种联合训练目标检测和分类的方法。使用这种方法，我们在COCO检测数据集和ImageNet分类数据集上同时训练YOLO9000。我们的联合训练使YOLO9000能够预测没有标记检测数据的目标类别的检测情况。我们在ImageNet检测任务上验证了我们的方法。尽管只有200个类中的44个有检测数据，YOLO9000在ImageNet检测验证集上得到了19.7的mAP。在COCO上没有的的156个类中，YOLO9000得到了16.0 mAP。但YOLO能检测的不仅仅是200个类；它能预测9000多个不同目标类别的检测。而且它仍然是实时运行的。</p><blockquote><h4 id="YOLOv1的不足"><a href="#YOLOv1的不足" class="headerlink" title="YOLOv1的不足"></a>YOLOv1的不足</h4><p>1）定位不准确</p><p>2）和基于region proposal的方法相比召回率较低。</p><h4 id="本文的改进"><a href="#本文的改进" class="headerlink" title="本文的改进"></a>本文的改进</h4><ul><li><strong>YOLO9000：</strong> 先进，实时的目标检测方法，可检测9000多类物体</li><li><strong>多尺度训练方法（ multi-scale training）：</strong> 相同的YOLOv2模型可以在不同的大小下运行，在速度和精度之间提供了一个简单的折中</li><li><strong>mAP表现更好：</strong> 67FPS，在VOC 2007上76.8 mAP，在40 FPS，78.6mAP；而且速度更快。</li><li><strong>提出了一种联合训练目标检测和分类的方法：</strong> 使用该方法在COCO目标检测数据集和Imagenet图像分类数据集上，训练出了YOLO9000</li><li><strong>可以检测出更多的类别：</strong> 即使这些类别没有在目标检测的数据集中出现</li></ul></blockquote><h2 id="一、-Introduction—引言"><a href="#一、-Introduction—引言" class="headerlink" title="一、 Introduction—引言"></a><strong>一、 Introduction—引言</strong></h2><p>通用的目标检测应该是快速、准确的，并且能够识别各种各样的目标。自从引入神经网络以来，检测框架已经变得越来越快和准确。然而，大多数检测方法仍然被限制在一小部分目标上。</p><p>与分类和标记等其他任务的数据集相比，当前的目标检测数据集是有限的。最常见的检测数据集包含几千到几十万张图像，有几十到几百个标签[3] [10] [2]。分类数据集有数以百万计的图像，有数万或数十万个类别[20] [2]。</p><p>我们希望检测能够达到目标分类的水平。然而，为检测而给图像贴标签比为分类或标记贴标签要昂贵得多（标签通常是给用户免费提供的）。因此，我们不太可能在不久的将来看到与分类数据集相同规模的检测数据集。</p><p>我们提出了一种新的方法来利用我们已经拥有的大量分类数据，并利用它来扩大当前检测系统的范围。我们的方法使用目标分类的分层观点，使我们能够将不同的数据集结合在一起。</p><p>我们还提出了一种联合训练算法，使我们能够在检测和分类数据上训练目标检测器。我们的方法利用标记的检测图像来学习精确定位目标，同时使用分类图像来增加其词汇量和鲁棒性。</p><p>使用这种方法，我们训练了YOLO9000，一个实时的目标检测器，可以检测超过9000个不同的物体类别。首先，我们在基础YOLO检测系统的基础上进行改进，以产生YOLOv2，一个最先进的实时检测器。然后，我们使用我们的数据集组合方法和联合训练算法，在ImageNet的9000多个类别以及COCO的检测数据上训练一个模型。</p><p>我们所有的代码和预训练的模型都可以在线获得：<a href="http://pjreddie.com/yolo9000/。">http://pjreddie.com/yolo9000/。</a></p><blockquote><h4 id="目标检测现状的不足"><a href="#目标检测现状的不足" class="headerlink" title="目标检测现状的不足"></a>目标检测现状的不足</h4><ul><li>当前的目标检测数据集是有限的</li><li>目标检测能检测的对象种类非常有限，可检测的物体少</li></ul><h4 id="本文工作"><a href="#本文工作" class="headerlink" title="本文工作"></a>本文工作</h4><ul><li>1）<strong>使用联合数据集：</strong> 利用已有的分类数据集，来拓展目标检测的范围。利用对象分类的分层视图，使得可以将不同数据集组合到一起</li><li>2）<strong>提出联合训练算法：</strong> 可以在检测数据集和分类数据集上训练目标分类器，用标记的目标检测数据集优化定位精度，利用分类图像来增加其词汇量鲁棒性</li><li>3）<strong>改进YOLOv1提出YOLOv2：</strong> 一种最先进的实时检测器</li><li>4）<strong>提出YOLO9000：</strong> 先将YOLO优化成YOLOv2，再用联合方法训练出YOLO9000</li></ul></blockquote><h2 id="二、-Better—更好"><a href="#二、-Better—更好" class="headerlink" title="二、 Better—更好"></a><strong>二、 Better—更好</strong></h2><h3 id="2-1-Batch-Normalization—批量归一化"><a href="#2-1-Batch-Normalization—批量归一化" class="headerlink" title="2.1 Batch Normalization—批量归一化"></a>2.1 Batch Normalization—批量归一化</h3><p>相比于最先进的检测系统，YOLO存在着各种缺陷。与<a href="https://so.csdn.net/so/search?q=Faster&amp;spm=1001.2101.3001.7020">Faster</a> R-CNN相比，对YOLO的错误分析表明，YOLO出现了大量的定位错误。此外，与基于区域建议的方法相比，YOLO的召回率相对较低。因此，我们主要关注的是在保持分类精度的同时，提高召回率和定位准确度。</p><p>计算机视觉通常趋向于更大、更深的网络[6] [18] [17]。更好的性能往往取决于训练更大的网络或将多个模型集合在一起。然而，在YOLOv2中，我们希望有一个更准确的检测器，但仍然是快速的。我们没有扩大我们的网络，而是简化了网络，然后让表征更容易学习。我们将过去工作中的各种想法与我们自己的新概念结合起来，以提高YOLO的性能。在表2中可以看到结果的总结。</p><p><strong>批量归一化</strong> 批量归一化导致收敛性的显著改善，同时消除了对其他形式的规范化的需求[7]。通过在YOLO的所有卷积层上添加批量归一化，我们在mAP上得到了超过2%的改善。批量规范化也有助于规范化模型。有了批归一化，我们可以在不过拟合的情况下去除模型中的dropout。</p><blockquote><h4 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h4><p>CNN在训练过程中网络每层输入的分布一直在改变, 会使训练过程难度加大，对网络的每一层的输入(每个卷积层后)都做了归一化，<strong>这样网络就不需要每层都去学数据的分布，收敛会更快</strong>。</p><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p>在YOLO模型的所有卷积层上添加<strong>Batch Normalization</strong>。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/BatchNormalization.png" alt=""></p><h4 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h4><p>mAP获得了2%的提升。Batch Normalization 也有助于规范化模型，可以在舍弃dropout优化后依然不会过拟合。</p></blockquote><h3 id="2-2-High-Resolution-Classifier—高分辨率分类器"><a href="#2-2-High-Resolution-Classifier—高分辨率分类器" class="headerlink" title="2.2 High Resolution Classifier—高分辨率分类器"></a>2.2 High Resolution Classifier—高分辨率分类器</h3><p>高分辨率分类器所有最先进的检测方法都使用在ImageNet上预训练的分类器[16]。从AlexNet开始，大多数分类器在小于256×256的输入图像上运行[8]。最初的YOLO在224×224的情况下训练分类器网络，并将分辨率提高到448以进行检测训练。这意味着网络在切换到检测学习时还必须调整到新的输入分辨率。</p><p>对于YOLOv2，我们首先在ImageNet上以448×448的完整分辨率对分类网络进行微调，并进行10个epoch。这让网络有时间调整其滤波器，以便在更高的分辨率输入下更好地工作。然后，我们再对检测网络的结果微调。这个高分辨率的分类网络使我们的mAP增加了近4%。</p><blockquote><h4 id="分类器介绍"><a href="#分类器介绍" class="headerlink" title="分类器介绍"></a>分类器介绍</h4><p>检测方法都使用在ImageNet上预先训练的分类器作为预训练模型。从AlexNet开始，大多数分类器的输入都小于256×256。</p><h4 id="v1中的使用"><a href="#v1中的使用" class="headerlink" title="v1中的使用"></a>v1中的使用</h4><p>v1中预训练使用的是分类数据集，大小是224×224 ，然后迁移学习，微调时使用YOLO模型做目标检测的时候才将输入变成448 × 448。这样改变尺寸，网络就要多重新学习一部分，会带来性能损失。</p><h4 id="v2中的改进"><a href="#v2中的改进" class="headerlink" title="v2中的改进"></a>v2中的改进</h4><p>v2直接在预训练中输入的就是<strong>448×448</strong>的尺寸，微调的时候也是<strong>448 × 448</strong>。</p><h4 id="效果-1"><a href="#效果-1" class="headerlink" title="效果"></a>效果</h4><p>使mAP增加了近4%</p></blockquote><h3 id="2-3-Convolutional-With-Anchor-Boxes—带有Anchor-Boxes的卷积"><a href="#2-3-Convolutional-With-Anchor-Boxes—带有Anchor-Boxes的卷积" class="headerlink" title="2.3 Convolutional With Anchor Boxes—带有Anchor Boxes的卷积"></a>2.3 Convolutional With Anchor Boxes—带有Anchor Boxes的卷积</h3><p><strong>带有锚框的卷积</strong> YOLO直接使用卷积特征提取器顶部的全连接层来预测边界框的坐标。Faster R-CNN不直接预测坐标，而是使用手工挑选的先验因素来预测边界框[15]。Faster R-CNN中的区域生成网络（RPN）只使用卷积层来预测锚框的偏移量和置信度。由于预测层是卷积，RPN预测了特征图中每个位置的偏移量。预测偏移量而不是坐标可以简化问题，使网络更容易学习。</p><p>我们从YOLO中移除全连接层，并使用锚框来预测边界框。首先，我们消除了一个池化层，使网络卷积层的输出具有更高的分辨率。我们还缩小了网络，使其在分辨率为416×416的输入图像上运行，而不是448×448。我们这样做是因为我们希望在我们的特征图中有奇数个位置，以便只有一个中心单元。目标，尤其是大型目标，往往会占据图像的中心位置，所以在中心位置有一个单一的位置来预测这些目标是很好的，而不是在中心附近的四个位置。YOLO的卷积层对图像进行了32倍的降样，所以通过使用416的输入图像，我们得到了一个13×13的输出特征图。</p><p>引入锚框后，我们将类别预测机制与空间位置分开处理，单独预测每个锚框的类和目标。和原来的YOLO一样，目标预测仍然预测先验框和真实框的IOU，而类别预测则预测在有目标存在下，该类别的条件概率。</p><p>使用锚框，我们得到的准确率会有小幅下降。YOLO每张图片只预测了98个框，但使用锚框后，我们的模型预测了超过一千个框。在没有锚框的情况下，我们的中间模型mAP为69.5，召回率为81%。有了锚框，我们的模型mAP为69.2，召回率为88%。即使mAP下降了，平均召回率的增加意味着我们的模型有更大的改进空间。</p><blockquote><h4 id="什么是Anchor？"><a href="#什么是Anchor？" class="headerlink" title="什么是Anchor？"></a>什么是Anchor？</h4><p><strong>定义：</strong> Anchor（先验框） 就是一组预设的边框，在训练时，以真实的边框位置相对于预设边框的偏移来构建训练样本。 这就相当于，预设边框先大致在可能的位置“框”出来目标，然后再在这些预设边框的基础上进行调整。<strong>简言之就是在图像上预设好的不同大小，不同长宽比的参照框。</strong></p><p><strong>Anchor Box：</strong>一个Anchor Box可以由边框的纵横比和边框的面积（尺度)来定义，相当于一系列预设边框的生成规则，根据Anchor Box，可以在图像的任意位置，生成一系列的边框。由于Anchor Box 通常是以CNN提取到的Feature Map 的点为中心位置，生成边框，所以一个Anchor Box不需要指定中心位置。</p><h4 id="Anchor-Box的构成"><a href="#Anchor-Box的构成" class="headerlink" title="Anchor Box的构成"></a><strong>Anchor Box的构成</strong></h4><ul><li>使用CNN提取的Feature Map的点，来定位目标的位置。</li><li>使用Anchor Box的Scale来表示目标的大小。</li><li>使用Anchor Box的Aspect Ratio来表示目标的形状。</li></ul><h4 id="之前研究"><a href="#之前研究" class="headerlink" title="之前研究"></a>之前研究</h4><p><strong>YOLOv1:</strong> 使用全连接层来直接预测边界框（x,y,w,h,c）其中边界框的坐标是相对于cell的，宽与高是相对于整张图片。由于各个图片中存在不同尺度和长宽比的物体，YOLOv1在训练过程中学习适应不同物体的形状是比较困难的，这也导致YOLOv1在精确定位方面表现较差。</p><p><strong>Faster R-CNN：</strong> 不是直接预测目标边界框，而是使用手工挑选的先验Anchor Boxes。利用RPN预测的边界框是相对于Anchor Boxes的坐标和高宽的偏移offset。RPN在特征图的每个位置预测Anchor Box偏移量而不是坐标，简化了问题，使网络更容易学习。</p><p><strong>YOLOv2的改进</strong><br>（1）删掉全连接层和最后一个pooling层，使得最后的卷积层可以有更高分辨率的特征</p><p>（2）缩小网络操作的输入图像为416×416</p><blockquote><p><strong>Q：为什么是416×416，而不是448×448？</strong></p><p>YOLOv2模型下采样的总步长为32,对于416×416大小的图片，最终得到的特征图大小为13×13（416/32=13），特征图中有奇数个位置，所以只有一个中心单元格。物体往往占据图像的中心，所以最好在中心有一个单独的位置来预测这些物体，而不是在附近的四个位置。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-%E5%A5%87%E5%81%B6%E6%95%B0.png" alt=""></p></blockquote><p>（3）使用Anchor Boxes</p><h4 id="效果-2"><a href="#效果-2" class="headerlink" title="效果"></a>效果</h4><p>使用Anchor，模型的mAP值从69.5降到了69.2，下降了一丢丢，而召回率却从81%提高到了88%。</p><blockquote><p>Q：精度（precision）和召回率（recall）：</p><p><strong>precision：</strong> 预测框中包含目标的比例。</p><p><strong>recall：</strong> 真正目标被检测出来的比例。</p><p>简言之，recall表示得找全，precision表示得找准。\</p></blockquote><h4 id="v2和v1对比"><a href="#v2和v1对比" class="headerlink" title="v2和v1对比"></a>v2和v1对比</h4><div class="table-container"><table><thead><tr><th></th><th><strong>YOLOv1</strong></th><th><strong>YOLOv2</strong></th></tr></thead><tbody><tr><td><strong>初始设置</strong></td><td>初始生成两个boxes，加大了学习复杂度。</td><td>Anchor初始是固定的，但在训练过程中会进行微调。使用Anchor boxes之后，每个位置的各个Anchor box都单独预测一组分类概率值。</td></tr><tr><td><strong>输出公式</strong></td><td>(框数 * 信息数)+分类数</td><td>框数 *(信息数+分类数)</td></tr><tr><td><strong>公式含义</strong></td><td>在YOLOv1中，类别概率是由grid cell来预测的，每个cell都预测2个boxes，每个boxes包含5个值，每个grid cell 携带的是30个信息。但是每个cell只预测一组分类概率值，供2个boxes共享。</td><td>在YOLOv2中，类别概率是属于box的，每个box对应一个类别概率，而不是由cell决定，因此这边每个box对应25个预测值。每个grid cell携带的是 25 × 5 =125个信息，25是 xywh+置信度+分类数，5就是5个Anchor。</td></tr><tr><td><strong>输出框</strong></td><td>7 × 7 × 2 = 98个框</td><td>13 × 13 × 5 = 845个框</td></tr><tr><td><strong>输出值</strong></td><td>7 ×7 × 30</td><td>13 × 13 × 5 × 25</td></tr></tbody></table></div><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1vsv2.png" alt=""></p></blockquote><h3 id="2-4-Dimension-Clusters—维度聚类（K-means聚类确定Anchor初始值）"><a href="#2-4-Dimension-Clusters—维度聚类（K-means聚类确定Anchor初始值）" class="headerlink" title="2.4 Dimension Clusters—维度聚类（K-means聚类确定Anchor初始值）"></a>2.4 Dimension Clusters—维度聚类（K-means聚类确定Anchor初始值）</h3><p><strong>维度集群</strong> 在与YOLO一起使用锚框时，我们遇到了两个问题。第一个问题是，框的尺寸是手工挑选的。网络可以学习适当地调整框，但是如果我们为网络挑选更好的先验锚框来开始，我们可以使网络更容易学习预测好的检测结果。</p><p>我们在训练集的边界框上运行k-means聚类，以自动找到好的先验参数，而不是手工选择先验参数。如果我们使用标准的k-means和欧氏距离，大的框比小的框产生更多的误差。然而，我们真正想要的是能获得好的IOU分数的先验锚框，这与框的大小无关。因此，对于距离度量，我们使用：d( box , centroid )=1−IOU( box , centroid )。</p><p>我们对不同的k值运行k-means，并绘制出最接近中心点的平均IOU，见图2。我们选择k = 5作为模型复杂性和高召回率之间的良好权衡。聚类中心点与手工挑选的锚框有明显不同。短而宽的框较少，高而薄的框较多。</p><p>我们在表1中比较了我们的聚类策略和手工挑选的锚框的平均IOU与最接近的先验。在只有5个先验的情况下，中心点的表现与9个锚框相似，平均IOU分别为61.0，60.9。如果我们使用9个中心点，我们会看到一个高得多的平均IOU。这表明，使用k-means来生成我们的边界框，使模型开始有一个更好的表示，并使任务更容易学习。</p><blockquote><h4 id="使用Anchor的问题一"><a href="#使用Anchor的问题一" class="headerlink" title="使用Anchor的问题一"></a>使用Anchor的问题一</h4><p>Anchor Boxes的尺寸是手工指定了长宽比和尺寸，相当于一个超参数，这违背了YOLO对于目标检测模型的初衷，<strong>因为如果指定了Anchor的大小就没办法适应各种各样的物体了</strong>。</p><h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><p>在训练集的边界框上运行K-means聚类训练bounding boxes，可以自动找到更好的boxes宽高维度。由上面分析已知，设置先验Anchor Boxes的主要目的是为了使得预测框与真值的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离指标</p><blockquote><p>K-means算法步骤：</p><p>1.选择初始化的K个样本作为初始聚类中心</p><p>2.针对数据集中每个样本，计算它到K个聚类中心的距离，并将其分到距离最小的聚类中心所对应的类中</p><p>3.针对每个类别，重新计算它的聚类中心</p><p>4.重复上面的步骤2、3，直到达到某个终止条件（迭代次数、最小误差变化）</p></blockquote><p><strong>公式：</strong> d(box, centroid) = 1 − IOU(box, centroid) （box:其他框， centroid：聚类中心框）</p><p>如下图，选取不同的k值（聚类的个数）运行K-means算法，并画出平均IOU和K值的曲线图。当k = 5时，可以很好的权衡模型复杂性和高召回率。与手工挑选的相比，K-means算法挑选的检测框形状多为瘦高型。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/K-means.png" alt=""></p><blockquote><p><strong>Q：为什么不尽量选择大的k值？</strong></p><p>因为K越大就生成越多的Anchor，越多的框自然准确率就能上去了，但同时也成倍的增加了模型的复杂度。R-CNN就是因为提取2K个候选框拉跨的。</p></blockquote></blockquote><h3 id="2-5-Direct-location-prediction—直接的位置预测"><a href="#2-5-Direct-location-prediction—直接的位置预测" class="headerlink" title="2.5 Direct location prediction—直接的位置预测"></a>2.5 Direct location prediction—直接的位置预测</h3><p><strong>直接的位置预测</strong>。 当YOLO使用锚框时，我们遇到了第二个问题：模型的不稳定性，特别是在早期迭代中。大部分的不稳定性来自于对框的（x，y）位置的预测。在区域生成网络中，网络预测值tx和ty，（x，y）中心坐标的计算方法是：</p><script type="math/tex; mode=display">x = (t_x \ast w_a) - x_a\\y = (t_y \ast h_a) - y_a</script><p>例如，tx=1的预测会将框向右移动，移动的宽度为锚框的宽度，tx=-1的预测会将框向左移动相同的长度。</p><p>这个公式是不受限制的，所以任何锚框都可以在图像中的任何一点结束，而不管这个框是在哪个位置预测的。在随机初始化的情况下，模型需要很长时间才能稳定地预测出合理的偏移量。</p><p>我们不预测偏移量，而是遵循YOLO的方法，预测相对于网格单元位置的坐标。这使得真实值的界限在0到1之间。我们使用逻辑激活来约束网络的预测，使其落在0~1这个范围内。</p><p>网络在输出特征图中的每个单元预测了5个边界框。该网络为每个边界框预测了5个坐标，即tx、ty、tw、th和to。如果单元格与图像左上角的偏移量为（cx，cy），且先验框的宽度和高度为pw，ph，则预测值对应于：<br>由于我们限制了位置预测，参数化更容易学习，使网络更稳定。使用维度聚类以及直接预测边界框中心位置，比起使用锚框的版本，YOLO提高了近5%。 </p><blockquote><h4 id="使用Anchor的问题二"><a href="#使用Anchor的问题二" class="headerlink" title="使用Anchor的问题二"></a>使用Anchor的问题二</h4><p>模型不稳定，特别是在早期迭代期间。大多数不稳定性来自于对边框(x, y)位置的预测。</p><h4 id="RPN网络的位置预测"><a href="#RPN网络的位置预测" class="headerlink" title="RPN网络的位置预测"></a>RPN网络的位置预测</h4><p><strong>方法：</strong> 预测相对于Anchor Box的坐标的偏移，和相对于Anchor Box高宽的偏移。</p><p><strong>计算公式：</strong> 预测框中心坐标= 输出的偏移量×Anchor宽高+Anchor中心坐标</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-rpn.png" alt=""></p><p><strong>不足：</strong> 这个公式是不受约束的，因此任何锚框可以出现在图像中的任何位置。在随机初始化的情况下，模型需要很长时间才能稳定到预测合理的偏移量。</p><h4 id="YOLOv2的改进"><a href="#YOLOv2的改进" class="headerlink" title="YOLOv2的改进"></a><strong>YOLOv2的改进</strong></h4><p><strong>方法：</strong> <strong>预测边界框中心点相对于对应cell左上角位置的相对偏移值。</strong>将网格归一化为1×1，坐标控制在每个网格内，同时配合sigmod函数将预测值转换到0~1之间的办法，做到每一个Anchor只负责检测周围正负一个单位以内的目标box。</p><p><strong>计算公式：</strong> 一个网格相对于图片左上角的偏移量是cx，cy。先验框的宽度和高度分别是pw和ph，则预测的边界框相对于特征图的中心坐标(bx，by)和宽高bw、bh</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-impro.png" alt=""></p><h4 id="效果-3"><a href="#效果-3" class="headerlink" title="效果"></a>效果</h4><p>使模型更容易稳定训练，mAP值提升了约5%。</p></blockquote><h3 id="2-6-Fine-Grained-Features—细粒度的特征"><a href="#2-6-Fine-Grained-Features—细粒度的特征" class="headerlink" title="2.6 Fine-Grained Features—细粒度的特征"></a>2.6 Fine-Grained Features—细粒度的特征</h3><p>细粒度的特征这个修改后的YOLO在13×13的特征图上预测探测结果。虽然这对大型物体来说是足够的，但它可能会受益于更细粒度的特征来定位较小的物体。Faster R-CNN和SSD都在网络中的各种特征图上运行他们的网络，以获得多个分辨率。我们采取了一种不同的方法，只需要增加一个直通层，从早期的层中提取26×26分辨率的特征。</p><p>直通层通过将相邻的特征堆叠到不同的通道而不是空间位置上，将高分辨率的特征与低分辨率的特征串联起来，类似于ResNet中的恒等映射。这种细粒度的特征。这就把26×26×512的特征图变成了13×13×2048的特征图，它可以与原始特征连接起来。我们的检测器在这个扩展的特征图之上运行，这样它就可以访问细粒度的特征。这使性能有了1%的适度提高。</p><blockquote><h4 id="为什么使用细粒特征？"><a href="#为什么使用细粒特征？" class="headerlink" title="为什么使用细粒特征？"></a>为什么使用细粒特征？</h4><p>这个修改后的YOLO在13 × 13特征图上进行检测。虽然这对于大型对象来说已经足够了，但是对于较小的对象来说，更细粒度的特性可能会使得检测效果更好。</p><h4 id="使用细粒度特征"><a href="#使用细粒度特征" class="headerlink" title="使用细粒度特征"></a>使用细粒度特征</h4><p><strong>Faster R-CNN和SSD：</strong> 使用了多尺度的特征图来分别检测不同大小的物体，前面更精细的特征图可以用来预测小物体。</p><p><strong>YOLOv2：</strong> 不同的方法，为网络简单地添加一个直通层（ passthrough layer），获取前层26×26分辨率特征。</p><h4 id="直通层（-passthrough-layer）"><a href="#直通层（-passthrough-layer）" class="headerlink" title="直通层（ passthrough layer）"></a>直通层（ passthrough layer）</h4><ul><li>将相邻的特征叠加到不同的通道来，将高分辨率的特征与低分辨率的特征连接起来</li><li>将前层26×26×512的特征图转换为13×13×2048的特征图，并与原最后层特征图进行拼接。</li></ul><p><strong>具体计算过程：</strong> YOLO v2提取Darknet-19最后一个maxpooling层的输入，得到26×26×512的特征图。经过1×1×64的卷积以降低特征图的维度，得到26×26×64的特征图，然后经过pass through层的处理变成13x13x256的特征图（抽取原特征图每个2x2的局部区域组成新的channel，即原特征图大小降低4倍，channel增加4倍），再与13×13×1024大小的特征图连接，变成13×13×1280的特征图，最后在这些特征图上做预测。<br><strong>具体操作：</strong></p><p><img src="C:/Users/17145/AppData/Roaming/Typora/typora-user-images/image-20240202225417118.png" alt="image-20240202225417118"></p><p>一个feature map，也就是在最后的池化之前，分成两路：一路是做拆分，分成四块，四块拼成一个长条，另一个是做正常的池化卷积操作，最后两个长条叠加输出。</p><blockquote><p><strong>Q：如何拆分成四块的？</strong></p><p>并不是简单的“两刀切4块”，而是在每个2×2的小区域上都选择左上角块，具体看下图。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-%E6%8B%86%E5%88%86%E5%9B%9B%E4%B8%AA%E5%9D%97.png" alt=""></p></blockquote><p><strong>注意：</strong> 这里的叠加不是ResNet里的add，而是拼接，是DenseNet里的concat。</p><h4 id="效果-4"><a href="#效果-4" class="headerlink" title="效果"></a>效果</h4><p>提升了1%的mAP</p></blockquote><h3 id="2-7-Multi-Scale-Training—多尺度的训练"><a href="#2-7-Multi-Scale-Training—多尺度的训练" class="headerlink" title="2.7 Multi-Scale Training—多尺度的训练"></a>2.7 Multi-Scale Training—多尺度的训练</h3><p><strong>多尺度的训练</strong>原始的YOLO使用448×448的输入分辨率。通过添加锚框，我们将分辨率改为416×416。然而，由于我们的模型只使用卷积层和池化层，因此可以实时调整大小。我们希望YOLOv2能够鲁棒地运行在不同尺寸的图像上，所以我们将多尺度训练应用到模型中。</p><p>我们不需要修改输入图像的大小，而是每隔几个迭代就改变网络。每10个批次，我们的网络就会随机选择一个新的图像尺寸。由于我们的模型缩减了32倍，我们从以下32的倍数中抽取：{320, 352, …, 608}。因此，最小的选项是320 × 320，最大的是608 × 608。我们将调整网络的尺寸，然后继续训练。</p><p>这种制度迫使网络学会在各种输入维度上进行良好的预测。这意味着同一个网络可以预测不同分辨率下的检测结果。网络在较小的尺寸下运行得更快，因此YOLOv2在速度和准确性之间提供了一个简单的权衡。</p><p>在低分辨率下，YOLOv2作为一个廉价、相当准确的检测器运行。在288×288时，它以超过90 FPS的速度运行，其mAP几乎与Faster R-CNN一样好。这使它成为较小的GPU、高帧率视频或多个视频流的理想选择。</p><p>在高分辨率下，YOLOv2是一个最先进的检测器，在VOC 2007上的mAP为78.6，而运行速度仍高于实时速度。</p><blockquote><h4 id="YOLOv1"><a href="#YOLOv1" class="headerlink" title="YOLOv1"></a>YOLOv1</h4><p><strong>方法：</strong> 使用448×448的固定分辨率输入。</p><h4 id="YOLOv2的改进-1"><a href="#YOLOv2的改进-1" class="headerlink" title="YOLOv2的改进"></a>YOLOv2的改进</h4><p><strong>原理：</strong> YOLOv2模型只使用了卷积和池化层，所以可以动态调整输入大小。每隔几次迭代就改变网络，而不是固定输入图像的大小。</p><p><strong>做法：</strong> 网络每10批训练后随机选择一个新的图像尺寸大小。由于模型下采样了32倍，从以下32的倍数{320,352，…，608}作为图像维度的选择。将网络输入调整到那个维度，并继续训练。</p><p><strong>作用：</strong> 这种机制使得网络可以更好地预测不同尺寸的图片，意味着同一个网络可以进行不同分辨率的检测任务，在输入size较大时，训练速度较慢，在输入size较小时，训练速度较快，而multi-scale training又可以提高准确率，因此算是准确率和速度都取得一个不错的平衡。</p><p><strong>YOLOv2和其他网络成绩对比：</strong><br>在小尺寸图片检测中，YOLOv2成绩很好，输入为228 × 228的时候，帧率达到90FPS，mAP几乎和Faster R-CNN的水准相同。使得其在低性能GPU、高帧率视频、多路视频场景中更加适用。在大尺寸图片检测中，YOLOv2达到了先进水平，VOC2007 上mAP为78.6%，仍然高于平均水准。<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-%E5%A4%9A%E5%B0%BA%E5%BA%A6%E8%AE%AD%E7%BB%83.png" alt=""></p></blockquote><h3 id="2-8-Further-Experiments—进一步的实验"><a href="#2-8-Further-Experiments—进一步的实验" class="headerlink" title="2.8 Further Experiments—进一步的实验"></a>2.8 Further Experiments—进一步的实验</h3><p><strong>进一步的实验</strong>我们训练YOLOv2对VOC 2012进行检测。表4显示了YOLOv2与其他最先进的检测系统的性能比较。YOLOv2实现了73.4 mAP，同时运行速度远远超过比较的方法。我们还对COCO进行了训练，并在表5中与其他方法进行了比较。在VOC指标（IOU = 0.5）上，YOLOv2得到44.0 mAP，与SSD和Faster R-CNN相当。</p><blockquote><p>作者在VOC2012上对YOLOv2进行训练，下图是和其他方法的对比。YOLOv2精度达到了73.4%，并且速度更快。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-%E8%BF%9B%E4%B8%80%E6%AD%A5%E8%AE%AD%E7%BB%83.png" alt=""></p><p>同时YOLOV2也在COCO上做了测试（IOU=0.5），也和Faster R-CNN、SSD作了成绩对比。总的来说，比上不足，比下有余。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-coco.png" alt=""></p></blockquote><h2 id="三、Faster—更快"><a href="#三、Faster—更快" class="headerlink" title="三、Faster—更快"></a>三、Faster—更快</h2><p>我们希望检测是准确的，但我们也希望它是快速的。大多数检测的应用，如机器人或自动驾驶汽车，都依赖于低延迟的预测。为了最大限度地提高性能，我们在设计YOLOv2时从头到尾都是快速的。</p><p>大多数检测框架依靠VGG-16作为基础特征提取器[17]。VGG-16是一个强大的、准确的分类网络，但它是不必要的复杂。VGG-16的卷积层需要306.9亿次浮点运算来处理一张224×224分辨率的图像。</p><p>YOLO框架使用一个基于Googlenet架构的定制网络[19]。这个网络比VGG-16更快，一个前向通道只用了85.2亿次运算。然而，它的准确性比VGG16略差。对于224×224的单张图像，前5名的准确率，YOLO在ImageNet上的自定义模型精度为88.0%，而VGG-16为90.0%。</p><blockquote><p>这一段开头先批评一波VGG，说VGG慢的不行，所以YOLOv1用的GoogLeNet，也就是Inceptionv1。速度很快，但是对比VGG精度稍微有所下降</p><p>通常目标检测框架： 大多数检测框架依赖于VGG-16作为基本的特征提取器。VGG-16是一个强大、精确的分类网络，但是它计算复杂。</p><p>YOLO框架： 使用基于GoogLeNet架构的自定义网络。虽说整体mAP 表现较VGG-16 差一些，但是却换来更快速、更少的预测运算。</p><p>YOLOv2 框架： 使用的是一个全新的架构: Darknet-19</p></blockquote><h3 id="3-1-Darknet-19"><a href="#3-1-Darknet-19" class="headerlink" title="3.1 Darknet-19"></a>3.1 Darknet-19</h3><p><strong>Darknet-19</strong>我们提出一个新的分类模型，作为YOLOv2的基础。我们的模型建立在先前的网络设计工作以及该领域的常识之上。与VGG模型类似，我们主要使用3×3的过滤器，并在每个池化步骤后将通道的数量增加一倍[17]。按照网络中的网络（NIN）的工作，我们使用全局平均池来进行预测，以及使用1×1滤波器来压缩3×3卷积之间的特征表示[9]。我们使用批量归一化来稳定训练，加速收敛，并使模型正规化[7]。</p><p>我们的最终模型，称为Darknet-19，有19个卷积层和5个maxpooling层。完整的描述见表6。Darknet-19只需要55.8亿次操作来处理一幅图像，却在ImageNet上达到了72.9%的最高准确率和91.2%的top-5准确率。</p><blockquote><h4 id="Darknet-19介绍"><a href="#Darknet-19介绍" class="headerlink" title="Darknet-19介绍"></a><strong>Darknet-19介绍</strong></h4><p>一个新的分类模型作为YOLOv2的基础框架。与VGG模型类似，主要使用3×3的卷积，并在每个池化步骤后加倍通道数。使用全局平均池进行预测，并使用1×1卷积压缩特征图通道数以降低模型计算量和参数，每个卷积层后使用BN层以加快模型收敛同时防止过拟合。最后用average pooling层代替全连接层进行预测。</p><p><strong>Darknet-19细节：</strong> 有19个卷积层和5个maxpooling层。（v1的GooLeNet是4个卷积层和2个全连接层）</p><p><strong>结构如下：</strong>（这是分类的模型，不是目标检测的模型）<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-dark19.png" alt=""></p><p>采用 YOLOv2，模型的mAP值没有显著提升，但计算量减少了。</p><blockquote><p><strong>Q：为什么去掉全连接层了呢？</strong></p><p>因为全连接层容易过拟合，训练慢。（参数太多）如下图，YOLOv1中通过全连接层将7×7×1024的特征图变换为7×7×30的特征图。但是这种变换完全可以通过一个3×3的卷积核做到，从而节省参数。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-dark.png" alt=""></p></blockquote></blockquote><h3 id="3-2-Training-for-classification—分类的训练"><a href="#3-2-Training-for-classification—分类的训练" class="headerlink" title="3.2 Training for classification—分类的训练"></a>3.2 Training for classification—分类的训练</h3><p><strong>分类的训练</strong> 我们使用随机梯度下降法在标准的ImageNet 1000类分类数据集上训练网络160次，使用Darknet神经网络框架[13]，起始学习率为0.1，多项式速率衰减为4次方，权重衰减为0.0005，动量为0.9。在训练过程中，我们使用标准的数据增强技巧，包括随机作物、旋转、色调、饱和度和曝光度的转变。</p><p>如上所述，在对224×224的图像进行初始训练后，我们在更大的尺寸（448）上对我们的网络进行微调。在这种微调中，我们用上述参数进行训练，但只用了10个epoch，并以10-3的学习率开始。在这个更高的分辨率下，我们的网络达到了76.5%的最高准确率和93.3%的Top-5准确率。</p><blockquote><p><strong>参数设置</strong><br><strong>（1）训练数据集：</strong> 标准ImageNet 1000类分类数据集</p><p><strong>（2）训练参数：</strong> 对网络进行160个epochs的训练，使用初始学习率为0.1随机梯度下降法、4的多项式率衰减法、0.0005的权值衰减法和0.9的动量衰减法</p><p><strong>（3）模型：</strong> 使用的是Darknet神经网络框架。</p><p><strong>（4）数据增强：</strong> 在训练中使用标准的数据增强技巧，包括随机的裁剪、旋转、色相、饱和度和曝光变化。</p><p>如上所述，在最初的224×224图像训练之后，然后放到448 × 448上微调，但只训练约10个周期。在这个高分辨率下，网络达到很高精度。微调时，10epoch，初始lr0.001。</p><p><strong>结果：</strong> 高分辨率下训练的分类网络在top-1准确率76.5%，top-5准确率93.3%。</p></blockquote><h3 id="3-3-Training-for-detection—检测的训练"><a href="#3-3-Training-for-detection—检测的训练" class="headerlink" title="3.3 Training for detection—检测的训练"></a>3.3 Training for detection—检测的训练</h3><p><strong>检测的训练</strong> 我们对这个网络进行了修改，去掉了最后一个卷积层，而是增加了三个3×3的卷积层，每个卷积层有1024个过滤器，然后是最后一个1×1的卷积层，输出的数量是我们检测所需的。对于VOC，我们预测5个框的5个坐标，每个框有20个类别，所以有125个过滤器。我们还从最后的3×3×512层向第二个卷积层添加了一个直通层，以便我们的模型可以使用细粒度的特征。</p><p>我们用10-3的起始学习率训练网络160个epoch，在60和90个epoch时除以10。我们使用0.0005的权重衰减和0.9的动量。我们使用与YOLO和SSD类似的数据增强，包括随机裁剪、颜色转换等。我们在COCO和VOC上使用同样的训练策略。</p><blockquote><h4 id="网络微调"><a href="#网络微调" class="headerlink" title="网络微调"></a><strong>网络微调</strong></h4><ul><li>移除最后一个卷积层、global avgpooling层和softmax</li><li>增加3个3x3x1024的卷积层</li><li>增加passthrough层</li><li>增加一个1×1个卷积层作为网络输出层。输出的channel数为num<em> anchors×(5+num</em> calsses)（num_anchors在文中为5，num _classes=20是类别个数，5是坐标值和置信度）</li></ul><h4 id="细节"><a href="#细节" class="headerlink" title="细节"></a>细节</h4><p><strong>（1）网络最后一层即1X1卷积层卷积核个数同网络输出维度相同：</strong> 对于VOC，预测5个边界框，每个边界框有5个坐标，每个边界框有20个类，所以最后一个1×1卷积层有125个卷积核。</p><p><strong>（2）passthrough层：</strong> 倒数第二个3X3卷积到最后一个3X3卷积层增加passthrough层。模型可以使用细粒度的特征。</p><p><strong>（3）训练参数：</strong> 10−3的起始学习率对网络进行160个周期的训练，并在60和90个周期时将其除以10。使用重量衰减为0.0005，动量为0.9。</p><h4 id="YOLOv2的训练"><a href="#YOLOv2的训练" class="headerlink" title="YOLOv2的训练"></a>YOLOv2的训练</h4><p>（1）在ImageNet训练Draknet-19，模型输入为224×224，共160个epochs</p><p>（2）将网络的输入调整为448×448,继续在ImageNet数据集上finetune分类模型，训练10 个epochs。参数除了epoch和learning rate改变外，其他都没变，这里learning rate改为0.001。</p><p>（3）修改Darknet-16分类模型为检测模型（看上面的网络微调部分），并在监测数据集上继续finetune模型<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-train.png" alt=""></p></blockquote><h2 id="四、Stronger—更强（YOLO9000部分）"><a href="#四、Stronger—更强（YOLO9000部分）" class="headerlink" title="四、Stronger—更强（YOLO9000部分）"></a>四、Stronger—更强（YOLO9000部分）</h2><p>我们提出了一种对分类和检测数据进行联合训练的机制。我们的方法使用标记为检测的图像来学习特定的检测信息，如边界框坐标预测和目标类，以及如何对普通目标进行分类。它使用只有类别标签的图像来扩大它可以检测的类别的数量。</p><p>在训练过程中，我们混合了来自检测和分类数据集的图像。当我们的网络看到被标记为检测的图像时，我们可以根据完整的YOLOv2损失函数进行反向传播。当它看到一个分类图像时，我们只从架构的分类特定部分反向传播损失。</p><p>这种方法带来了一些挑战。检测数据集只有常见的物体和一般的标签，如 “狗 “或 “船”。分类数据集有更广泛和更深入的标签范围。ImageNet有一百多个狗的品种，包括 “诺福克梗”、”约克夏梗 “和 “贝灵顿梗”。如果我们想在这两个数据集上进行训练，我们需要一个连贯的方法来合并这些标签。</p><p>大多数分类方法在所有可能的类别中使用softmax层来计算最终的概率分布。使用softmax时，假定这些类别是相互排斥的。这给合并数据集带来了问题，例如，你不会想用这个模型来合并ImageNet和COCO，因为 “诺福克梗 “和 “狗 “这两个类别并不相互排斥。</p><p>我们可以使用一个多标签模型来结合数据集，而这个模型并不假定相互排斥。这种方法忽略了我们所知道的关于数据的所有结构，例如，所有的COCO类都是互斥的。</p><blockquote><h3 id="YOLOv2和YOLO9000的关系"><a href="#YOLOv2和YOLO9000的关系" class="headerlink" title="YOLOv2和YOLO9000的关系"></a>YOLOv2和YOLO9000的关系</h3><p>YOLOv2和YOLO9000算法在2017年CVPR上被提出，重点解决YOLOv1召回率和定位精度方面的误差。</p><p><strong>YOLOv2：</strong> 是在YOLOv1的基础上改进得到，改进之处主要有：Batch Normalization (批量归一化)、High Resolution Classfier(高分辨率的分类器)、Convolutional With Anchor Boxes (带锚框的卷积)、Dimension Clusters (维度聚类)、Direct location prediction (直接位置预测)、Fine-Grained Feature (细粒度特性)、Multi-Scale Training (多尺度训练)，它的特点是“更好，更快，更强”。</p><p><strong>YOLO9000：</strong> 的主要检测网络也是YOLO v2，同时使用WordTree来混合来自不同的资源的训练数据，并使用联合优化技术同时在ImageNet和COCO数据集上进行训练，目的是利用数量较大的分类数据集来帮助训练检测模型，因此，YOLO9000的网络结构允许实时地检测超过9000种物体分类，进一步缩小了检测数据集与分类数据集之间的大小代沟。</p><h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p>联合coco目标检测数据集和imagenet分类数据集。</p><ul><li>输入的若为目标检测标签的，则在模型中反向传播目标检测的损失函数。</li><li>输入的若为分类标签的，则反向传播分类的损失函数</li></ul><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ul><li>coco的数据集标签分类的比较粗，比如狗，猫，而imagenet分类则比较细化，比如二哈狗，金毛狗。</li><li>这时候如果用softmax进行最后的分类，则会产生问题，因为softmax输出最大概率的那个分类，各种分类之间彼此互斥，若狗，二哈狗，金毛狗在一起的话就会出问题。</li><li>所以要联合训练，必须让标签有一定程度上的一致性。</li></ul></blockquote><h3 id="4-1-Hierarchical-classification—分层分类"><a href="#4-1-Hierarchical-classification—分层分类" class="headerlink" title="4.1 Hierarchical classification—分层分类"></a>4.1 Hierarchical classification—分层分类</h3><p><strong>分层分类</strong> ImageNet的标签是从WordNet中提取的，WordNet是一个语言数据库，用于构造概念和它们之间的关系[12]。在WordNet中，”Norfolk terrier “和 “Yorkshire terrier “都是 “terrier “的外来语，而 “terrier “是 “猎狗 “的一种，是 “狗 “的一种，是 “犬类 “的一种等等。大多数分类方法都假定标签有一个平面结构，然而对于结合数据集来说，结构正是我们所需要的。</p><p>WordNet的结构是一个有向图，而不是一棵树，因为语言是复杂的。例如，”狗 “既是 “犬类 “的一种类型，也是 “家畜 “的一种类型，它们都是WordNet中的主题词。我们没有使用完整的图结构，而是通过从ImageNet中的概念建立一棵分层的树来简化这个问题。</p><p>为了建立这棵树，我们检查了ImageNet中的视觉名词，并查看了它们通过WordNet图到根节点的路径，在这个例子中是 “物理对象”。许多同义词在图中只有一条路径，因此我们首先将所有这些路径添加到我们的树上。然后，我们反复检查我们剩下的概念，并添加路径，使树的增长尽可能少。因此，如果一个概念有两条通往根的路径，其中一条路径会给我们的树增加三条边，而另一条只增加一条边，我们就选择较短的路径。</p><p>最后的结果是WordTree，一个视觉概念的分层模型。为了用WordTree进行分类，我们在每个节点上预测条件概率，即在给定的同义词中，每个同义词的概率。如果我们想计算一个特定节点的绝对概率，我们只需沿着树的路径到根节点，然后乘以条件概率。</p><p>为了分类的目的，我们假设该图像包含一个物体。Pr(物理对象) = 1。</p><p>为了验证这种方法，我们在使用1000类ImageNet建立的WordTree上训练Darknet-19模型。为了建立WordTree1k，我们加入了所有的中间节点，将标签空间从1000扩大到1369。在训练过程中，我们在树上传播基础事实标签，这样，如果一张图片被标记为 “诺福克梗”，它也会被标记为 “狗 “和 “哺乳动物”，等等。为了计算条件概率，我们的模型预测了一个由1369个值组成的向量，我们计算了所有作为同一概念的假名的系统集的softmax，见图5。<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E5%88%86%E5%B1%82%E5%88%86%E7%B1%BB.png" alt=""></p><p>使用与之前相同的训练参数，我们的分层式Darknet-19达到了71.9%的top-1准确率和90.4%的top-5准确率。尽管增加了369个额外的概念，并让我们的网络预测树状结构，但我们的准确率只下降了一点。以这种方式进行分类也有一些好处。在新的或未知的对象类别上，性能会优雅地下降。例如，如果网络看到一张狗的照片，但不确定它是什么类型的狗，它仍然会以高置信度预测 “狗”，但在假名中分布的置信度会降低。</p><p>这种表述也适用于检测。现在，我们不是假设每张图片都有一个物体，而是使用YOLOv2的物体性预测器来给我们提供Pr（物理物体）的值。检测器会预测出一个边界框和概率树。我们向下遍历这棵树，在每一个分叉处采取最高的置信度路径，直到我们达到某个阈值，我们就可以预测那个物体类别。</p><blockquote><ul><li>ImageNet的标签是从WordNet中提取的，WordNet是一个语言数据库，用于构造概念和它们之间的关系。</li><li>WordNet的结构是一个有向图，而不是一棵树，因为语言是复杂的。</li><li><p>作者们并不采用整个WordNet 的图结构，而是从中抽取其视觉名词重新制作一个树状结构。</p></li><li><p>在WordTree结构上进行操作，需要预测的是每一个节点相对于父节点的条件概率，要计算某个几点的绝对概率 或者说联合概率，就直接从他乘到根节点。</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-%E6%9C%89%E5%90%91%E5%9B%BE.png" alt=""></p></blockquote><h3 id="4-2-Dataset-combination-with-WordTree—用WordTree组合数据集"><a href="#4-2-Dataset-combination-with-WordTree—用WordTree组合数据集" class="headerlink" title="4.2 Dataset combination with WordTree—用WordTree组合数据集"></a>4.2 Dataset combination with WordTree—用WordTree组合数据集</h3><p><strong>用WordTree组合数据集。</strong>我们可以使用WordTree以合理的方式将多个数据集组合在一起。我们只需将数据集中的类别映射到树上的同位素。图6显示了一个使用WordTree来结合ImageNet和COCO的标签的例子。WordNet是非常多样化的，所以我们可以将这种技术用于大多数数据集。</p><blockquote><p>原始正常的数据集中数据结构是WordNet(有向图)。作者改造成了WordTree(树)。</p><p><strong>WordTree的生成方式如下：</strong></p><ul><li>遍历Imagenet的label，然后在WordNet中寻找该label到根节点(指向一个物理对象)的路径；</li><li>如果路径只有一条，那么就将该路径直接加入到分层树结构中；</li><li>否则，从剩余的路径中选择一条最短路径，加入到分层树。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov2-wordtree.png" alt=""></p><p>混合后的数据集形成一个有9418类的WordTree。生成的WordTree模型如下图所示。另外考虑到COCO数据集相对于ImageNet数据集数据量太少了，为了平衡两个数据集，作者进一步对COCO数据集过采样，使COCO数据集与ImageNet数据集的数据量比例接近1：4。</p></blockquote><h3 id="4-3-Joint-classification-and-detection—联合分类和检测"><a href="#4-3-Joint-classification-and-detection—联合分类和检测" class="headerlink" title="4.3 Joint classification and detection—联合分类和检测"></a>4.3 Joint classification and detection—联合分类和检测</h3><p><strong>联合分类和检测</strong> 现在我们可以使用WordTree结合数据集，我们可以训练分类和检测的联合模型。我们想训练一个极大规模的检测器，所以我们使用COCO检测数据集和ImageNet完整版本中的前9000个类来创建我们的联合数据集。我们还需要评估我们的方法，所以我们加入了ImageNet检测挑战中尚未包括的任何类别。这个数据集的相应WordTree有9418个类。ImageNet是一个更大的数据集，所以我们通过对COCO的过度采样来平衡数据集，使ImageNet只比它大4:1。</p><p>使用这个数据集，我们训练YOLO9000。我们使用基本的YOLOv2架构，但只有3个先验因素，而不是5个，以限制输出大小。当我们的网络看到一个检测图像时，我们像平常一样反向传播损失。对于分类损失，我们只在标签的相应级别或以上反向传播损失。例如，如果标签是 “狗”，我们不给树上更远的预测分配任何错误，”德国牧羊犬 “与 “金毛猎犬”，因为我们没有这些信息。</p><p>当它看到一个分类图像时，我们只反向传播分类损失。要做到这一点，我们只需找到预测该类的最高概率的边界框，并计算其预测树上的损失。我们还假设预测框与地面真实标签至少有0.3 IOU的重叠，我们根据这一假设反向传播对象性损失。</p><p>通过这种联合训练，YOLO9000学会了使用COCO中的检测数据来寻找图像中的物体，并学会了使用ImageNet中的数据对这些物体进行分类。</p><p>我们在ImageNet检测任务上评估了YOLO9000。ImageNet的检测任务与COCO共享44个对象类别，这意味着YOLO9000只看到了大多数测试图像的分类数据，而不是检测数据。YOLO9000总体上得到了19.7的mAP，在它从未见过任何标记的检测数据的156个不相干的对象类别上得到了16.0的mAP。这个mAP比DPM取得的结果要高，但是YOLO9000是在不同的数据集上训练的，只有部分监督[4]。它还同时检测了9000个其他物体类别，而且都是实时的。</p><p>当我们分析YOLO9000在ImageNet上的表现时，我们看到它能很好地学习新的动物物种，但在学习服装和设备等类别时却很困难。新的动物更容易学习，因为对象性预测可以很好地从COCO中的动物中概括出来。相反，COCO没有任何类型的衣服的边界框标签，只有人的标签，所以YOLO9000在为 “太阳镜 “或 “游泳裤 “等类别建模时很吃力。</p><blockquote><h3 id="YOLO9000是怎样进行联合训练的？"><a href="#YOLO9000是怎样进行联合训练的？" class="headerlink" title="YOLO9000是怎样进行联合训练的？"></a>YOLO9000是怎样进行联合训练的？</h3><p>YOLO9000采用 YOLO v2的结构，<strong>Anchorbox由原来的5调整到3</strong>，对每个Anchorbox预测其对应的边界框的位置信息x , y , w , h和置信度以及所包含的物体分别属于9418类的概率，所以每个Anchorbox需要预测4+1+9418=9423个值。每个网格需要预测3×9423=28269个值。在训练的过程中，当网络遇到来自检测数据集的图片时，用完整的 YOLO v2 loss进行反向传播计算，当网络遇到来自分类数据集的图片时，只用分类部分的loss进行反向传播。</p><h3 id="YOLO-9000是怎么预测的？"><a href="#YOLO-9000是怎么预测的？" class="headerlink" title="YOLO 9000是怎么预测的？"></a>YOLO 9000是怎么预测的？</h3><p>WordTree中每个节点的子节点都属于同一个子类，分层次的对每个子类中的节点进行一次softmax处理，以得到同义词集合中的每个词的下义词的概率。当需要预测属于某个类别的概率时，需要预测该类别节点的条件概率。即在WordTree上找到该类别名词到根节点的路径，计算路径上每个节点的概率之积。<strong>预测时， YOLO v2得到置信度，同时会给出边界框位置以及一个树状概率图，沿着根节点向下，沿着置信度最高的分支向下，直到达到某个阈值，最后到达的节点类别即为预测物体的类别。</strong></p></blockquote><h3 id="五、Conclusion—结论"><a href="#五、Conclusion—结论" class="headerlink" title="五、Conclusion—结论"></a><strong>五、Conclusion—结论</strong></h3><p>我们介绍了YOLOv2和YOLO9000，实时检测系统。YOLOv2是最先进的，在各种检测数据集上比其他检测系统快。此外，它可以在各种图像尺寸下运行，在速度和准确性之间提供平稳的权衡。</p><p>YOLO9000是一个实时框架，通过联合优化检测和分类来检测9000多个物体类别。我们使用WordTree来结合各种来源的数据和我们的联合优化技术，在ImageNet和COCO上同时训练。YOLO9000是朝着缩小检测和分类之间的数据集大小差距迈出的有力一步。</p><p>我们的许多技术可以在目标检测之外进行推广。我们对ImageNet的WordTree表示为图像分类提供了一个更丰富、更详细的输出空间。使用分层分类的数据集组合在分类和分割领域将是有用的。像多尺度训练这样的训练技术可以在各种视觉任务中提供好处。</p><p>对于未来的工作，我们希望将类似的技术用于弱监督的图像分割。我们还计划在训练过程中使用更强大的匹配策略为分类数据分配弱标签来提高我们的检测结果。计算机视觉有着得天独厚的大量标记数据。我们将继续寻找方法，将不同来源和结构的数据结合起来，为视觉世界建立更强大的模型。</p><blockquote><p><strong>YOLOv2</strong> 是最先进的，在各种检测数据集上比其他检测系统更快。此外，它可以在各种图像大小下运行，以在速度和精度之间提供平滑的折中。</p><p><strong>对比yolov1所作出的改进：</strong></p><ul><li>加了BN（卷积后，激活函数前）;</li><li>加了高分辨率分类器;加了anchor(聚类得到个数,1个gird cell 生成5个anchor);限制预测框；</li><li>加入细粒度特征(类似于concat的残差)加入对尺度训练改进骨干网络(GoogleNet 变darknet-19)通过WordTree将不同数据集结合联合训练。</li><li>用一种新颖的方法扩充了数据集。</li></ul><p><strong>YOLO9000</strong> 是一个实时框架，通过联合优化检测和分类，可检测9000多个对象类别。我们使用WordTree合并来自不同来源的数据，并使用我们的联合优化技术在ImageNet和CoCo上同时进行训练。</p><p><strong>WordTree</strong> 的概念可以让分类标注提供更大的运用空间，并且可以利用来进行弱监督学习，也可以利用这样的概念结合各种不同任务的资料集，对于分类有很大的助益。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>yolov1-paper</title>
      <link href="/yolov1-paper/"/>
      <url>/yolov1-paper/</url>
      
        <content type="html"><![CDATA[<h1 id="You-Only-Look-Once-Unified-Real-Time-Object-Detection"><a href="#You-Only-Look-Once-Unified-Real-Time-Object-Detection" class="headerlink" title="You Only Look Once: Unified, Real-Time Object Detection"></a>You Only Look Once: Unified, Real-Time Object Detection</h1><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="1-Yolo模型演进"><a href="#1-Yolo模型演进" class="headerlink" title="1. Yolo模型演进"></a>1. Yolo模型演进</h3><div class="table-container"><table><thead><tr><th>版本</th><th>发布日期</th><th>备注</th></tr></thead><tbody><tr><td>YOLOv1</td><td>CVPR 2016</td><td>初始版本</td></tr><tr><td>YOLOv2/YOLO9000</td><td>CVPR 2017</td><td>改进版本，引入YOLO9000</td></tr><tr><td>YOLOv3</td><td>2018</td><td>进一步改进</td></tr><tr><td>YOLOv4</td><td>2020.4.24</td><td>性能优化</td></tr><tr><td>YOLOv5</td><td>2020.5.2</td><td>Ultralytics公司发布</td></tr><tr><td>v5.0</td><td>2021.4.12</td><td>YOLOv5的一个版本</td></tr></tbody></table></div><h3 id="2-计算机视觉的任务"><a href="#2-计算机视觉的任务" class="headerlink" title="2. 计算机视觉的任务"></a>2. 计算机视觉的任务</h3><div class="table-container"><table><thead><tr><th>任务类型</th><th>输入</th><th>输出</th><th>描述</th></tr></thead><tbody><tr><td>图像分类</td><td>图像</td><td>类别</td><td>输入图像，预测图像的类别</td></tr><tr><td><strong>目标检测</strong></td><td>图像</td><td>类别+框</td><td>输入图像，预测图像中对象的类别及其位置（边界框）,目标定位/目标识别</td></tr><tr><td>语义分割</td><td>图像</td><td>像素分类</td><td>输入图像，对每个像素进行分类，所有实例被视为一个整体（如所有人）</td></tr><tr><td><strong>实例分割</strong></td><td>图像</td><td>像素分类+实例区分</td><td>输入图像，对每个像素进行分类，并区分同一类别的不同实例（如不同的人）</td></tr></tbody></table></div><ul><li>只有<strong>目标检测</strong>和<strong>实例分割</strong>实现了<code>单实例</code>的识别</li></ul><h2 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h2><h3 id="Abstract—摘要"><a href="#Abstract—摘要" class="headerlink" title="Abstract—摘要"></a>Abstract—摘要</h3><p>我们提出的YOLO是一种新的目标检测方法。以前的目标检测方法通过重新利用分类器来执行检测。与先前的方案不同，我们将目标检测看作回归问题从空间上<strong>定位边界框（bounding box）并预测该框的类别概率</strong>。我们使用<strong>单个神经网络</strong>，在一次评估中直接从完整图像上预测边界框和类别概率。由于整个检测流程仅用一个网络，所以可以直接对检测性能进行<strong>端到端</strong>的优化。</p><p>我们的统一架构速度极快。我们的基本YOLO模型以<strong>45 fps</strong>（帧/秒）的速度实时处理图像。该网络的一个较小版本——Fast YOLO，以<strong>155 fps</strong>这样惊人的速度运行，同时仍然达到其他实时检测器的两倍。与最先进的（state-of-the-art，SOTA）检测系统相比，YOLO虽然产生了较多的定位误差，但它几乎不会发生把背景预测为目标这样的假阳性（False Positive）的错误。最后，YOLO能<strong>学习到泛化性很强的目标表征</strong>。当从自然图像学到的模型用于其它领域如艺术画作时，它的表现都优于包括DPM和R-CNN在内的其它检测方法。</p><blockquote><h4 id="之前的方法（RCNN系列）"><a href="#之前的方法（RCNN系列）" class="headerlink" title="之前的方法（RCNN系列）"></a>之前的方法（RCNN系列）</h4><p>（1）通过region proposal产生大量的可能包含待检测物体的<strong>potential bounding box</strong></p><p>（2）再用分类器去判断每个<strong>bounding box</strong>里是否包含有物体，以及物体所属类别的<strong>probability或者 confidence</strong></p><p>（3）最后回归预测</p><h4 id="YOLO的简介："><a href="#YOLO的简介：" class="headerlink" title="YOLO的简介："></a>YOLO的简介：</h4><p>本文将检测变为一个regression problem（回归问题），YOLO 从输入的图像，仅仅经过一个神经网络，直接得到一些bounding box以及每个bounding box所属类别的概率。</p><p>因为整个的检测过程仅仅有一个网络，所以它可以直接进行end-to-end的优化。</p></blockquote><h3 id="一、Introduction—前言"><a href="#一、Introduction—前言" class="headerlink" title="一、Introduction—前言"></a>一、<strong>Introduction—前言</strong></h3><p>人们只需瞄一眼图像，立即知道图像中的物体是什么，它们在哪里以及它们如何相互作用。人类的视觉系统是快速和准确的，使得我们在无意中就能够执行复杂的任务，如驾驶。快速且准确的目标检测算法可以让计算机在没有专门传感器的情况下驾驶汽车，使辅助设备能够向人类用户传达实时的场景信息，并解锁通用、响应性的机器人系统的潜能。</p><p>目前的检测系统通过<strong>重用分类器</strong>来执行检测。为了检测目标，这些系统为该目标提供一个分类器，在测试图像的不同的位置和不同的尺度上对其进行评估。像deformable parts models（<strong>DPM</strong>，可变形部分模型）这样的系统使用<strong>滑动窗口方法</strong>，其分类器在整个图像上<strong>均匀间隔</strong>的位置上运行[10]。</p><p><strong>我们将目标检测看作是一个单一的回归问题，直接从图像像素得到边界框坐标和类别概率。</strong>使用我们的系统——You Only Look Once（YOLO），便能得到图像上的物体是什么和物体的具体位置。</p><p>YOLO非常简单（见图1），它仅用单个卷积网络就能同时预测多个边界框和它们的类别概率。YOLO<strong>在整个图像上训练，并能直接优化检测性能</strong>。与传统的目标检测方法相比，这种统一的模型下面所列的<strong>一些优点</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Yolov1-Figure1.png" alt="image-20240105230745471"></p><blockquote><h4 id="之前的研究："><a href="#之前的研究：" class="headerlink" title="之前的研究："></a>之前的研究：</h4><p><strong>DPM：</strong> 系统为检测对象使用分类器，并在测试图像的不同位置和尺度对其进行评估</p><p><strong>R-CNN：</strong>SS方法提取候选框＋CNN＋分类+回归。</p><h4 id="YOLO处理步骤："><a href="#YOLO处理步骤：" class="headerlink" title="YOLO处理步骤："></a>YOLO处理步骤：</h4><p>(1)将输入图像的大小调整为448×448，分割得到7*7网格；</p><p>(2)通过CNN提取特征和预测；</p><p>(3)利用非极大值抑制（NMS）进行筛选</p></blockquote><p><strong>第一，YOLO速度非常快</strong>。由于我们将检测视为<strong>回归问题</strong>，所以我们不需要复杂的流程。测试时，我们在一张新图像上简单的运行我们的神经网络来预测检测结果。在Titan X GPU上不做批处理的情况下，YOLO的基础版本以每秒45帧的速度运行，而快速版本运行速度超过150fps。这意味着我们可以在不到25毫秒的延迟内实时处理流媒体视频。此外，YOLO实现了其它实时系统两倍以上的平均精度。关于我们的系统在网络摄像头上实时运行的演示，请参阅我们的项目网页：YOLO: Real-Time Object Detection。</p><p><strong>第二，YOLO是在整个图像上进行推断的</strong>。与基于滑动窗口和候选框的技术不同，YOLO在训练期间和测试时都会顾及到整个图像，所以它隐式地包含了关于类的上下文信息以及它们的外观。Fast R-CNN是一种很好的检测方法[14]，但由于它看不到更大的上下文，会将背景块误检为目标。与Fast R-CNN相比，YOLO的背景误检数量少了一半。</p><p><strong>第三，YOLO能学习到目标的泛化表征</strong>（generalizable representations of objects）。把在自然图像上进行训练的模型，用在艺术图像进行测试时，YOLO大幅优于DPM和R-CNN等顶级的检测方法。由于YOLO具有高度泛化能力，因此在应用于新领域或碰到意外的输入时不太可能出故障。</p><p><strong>YOLO在精度上仍然落后于目前最先进的检测系统</strong>。虽然它可以快速识别图像中的目标，但它在定位某些物体尤其是小的物体上精度不高。</p><p>我们在实验中会进一步探讨精度／时间的权衡。我们所有的训练和测试代码都是开源的，而且各种预训练模型也都可以下载。</p><blockquote><h4 id="YOLO的定义："><a href="#YOLO的定义：" class="headerlink" title="YOLO的定义："></a>YOLO的定义：</h4><p>YOLO将目标检测重新定义为<strong>单个回归问题</strong>，<strong>从图像像素直接到边界框坐标和类概率</strong>。YOLO可以在一个图像来预测：哪些对象是存在的？它们在哪里？</p><p>如 Figure 1：将图像输入单独的一个 CNN 网络，就会预测出 bounding box，以及这些 bounding box 所属类别的概率。</p><p>YOLO 用<strong>一整幅图像</strong>来训练，同时可以直接优化性能检测。</p><p>性能检测对比：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolo%E6%A3%80%E6%B5%8B%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94.png" alt="yolo检测性能一笔"></p><h4 id="YOLO的优点："><a href="#YOLO的优点：" class="headerlink" title="YOLO的优点："></a>YOLO的优点：</h4><p>1）<strong>YOLO的速度非常快。</strong>能够达到实时的要求。在 Titan X 的 GPU 上 能够达到 45 帧每秒。</p><p>2）<strong>YOLO在做预测时使用的是全局图像。</strong>与FastR-CNN相比，YOLO产生的背景错误数量不到一半。</p><p>3）<strong>YOLO 学到物体更泛化的特征表示。</strong>因此当应用于新域或意外输入时，不太可能崩溃。</p></blockquote><h3 id="二、Uniﬁed-Detection—统一检测"><a href="#二、Uniﬁed-Detection—统一检测" class="headerlink" title="二、Uniﬁed Detection—统一检测"></a><strong>二、Uniﬁed Detection—统一检测</strong></h3><p><strong>我们将目标检测的独立部分(the separate components )整合到单个神经网络中。</strong>我们的网络使用整个图像的特征来预测每个边界框。它还可以同时预测一张图像中的所有类别的所有边界框。这意味着我们的网络对整张图像和图像中的所有目标进行全局推理(reason globally)。YOLO设计可实现端到端训练和实时的速度，同时保持较高的平均精度。</p><p><strong>我们的系统将输入图像分成 S×S 的网格</strong>。如果目标的中心落入某个网格单元(grid cell)中，那么该网格单元就负责检测该目标。</p><p><strong>每个网格单元都会预测 B个 边界框和这些框的置信度分数（confidence scores）</strong>。这些置信度分数反映了该模型对那个框内是否包含目标的置信度，以及它对自己的预测的准确度的估量。在形式上，我们将<strong>置信度</strong>定义为 <code>confidence = Pr(Object) * IOUpred truth</code> 。如果该单元格中不存在目标（即<code>Pr(Object)=0</code>），则置信度分数应为 0。否则（即<code>Pr(Object)=1</code>），我们希望置信度分数等于预测框（predict box）与真实标签框（ground truth）之间联合部分的交集（IOU）。</p><p>每个网格单元还预测了C类的条件概率，<code>Pr(Classi|Object)</code>。这些概率是以包含目标的网格单元为条件的。我们只预测每个网格单元的一组类别概率，而不考虑框B的数量。</p><p>在测试时，我们将条件类概率和单个框的置信度预测相乘：</p><script type="math/tex; mode=display">\Pr(\text{Class}_i|\text{Object}) \cdot \Pr(\text{Object}) \cdot \text{IOU}^{\text{truth}}_{\text{pred}} = \Pr(\text{Class}_i) \cdot \text{IOU}^{\text{truth}}_{\text{pred}}</script><p>这给我们提供了每个框的特定类别的置信度分数。这些分数既是对该类出现在框里的概率的编码，也是对预测的框与目标的匹配程度的编码。</p><blockquote><h4 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h4><p>YOLO将目标检测问题作为<strong>回归问题</strong>。会将输入图像分成S×S的网格，如果一个物体的中心点落入到一个cell中，那么该cell就要负责预测该物体，一个格子只能预测一个物体，会生成两个预测框。</p><h4 id="对于每个grid-cell："><a href="#对于每个grid-cell：" class="headerlink" title="对于每个grid cell："></a>对于每个grid cell：</h4><p>1）预测B个边界框，每个框都有一个置信度分数（confidence score）这些框大小尺寸等等都随便，只有一个要求，就是<strong>生成框的中心点必须在grid cell里</strong>。</p><p>2）每个边界框包含5个元素：<strong>(x,y,w,h)</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolo-xywhc.png" alt="yolo-xywhc"></p><ul><li><strong>x，y：</strong>是指bounding box的预测框的中心坐标相较于该bounding box归属的grid cell左上角的偏移量，在0-1之间。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolo-xy.png" alt=""></p><p>在上图中，<strong>绿色虚线框</strong>代表grid cell，<strong>绿点</strong>表示该grid cell的左上角坐标，为（0，0）；<strong>红色和蓝色框</strong>代表该grid cell包含的两个bounding box，<strong>红点和蓝点</strong>表示这两个bounding box的中心坐标。有一点很重要，bounding box的中心坐标一定在该grid cell内部，因此，红点和蓝点的坐标可以归一化在0-1之间。在上图中，红点的坐标为（0.5，0.5），即x=y=0.5，蓝点的坐标为（0.9，0.9），即x=y=0.9。</p><ul><li><strong>w，h：</strong> 是指该bounding box的宽和高，但也归一化到了0-1之间，表示相较于原始图像的宽和高（即448个像素）。比如该bounding box预测的框宽是44.8个像素，高也是44.8个像素，则w=0.1，h=0.1。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolo-wh.png" alt=""></p><p><strong>红框</strong>的x=0.8，y=0.5，w=0.1，h=0.2。</p><p>3）不管框 B 的数量是多少，<strong>每个Grid Cell只负责预测一个目标</strong>。</p><p>4）预测 C 个条件概率类别（物体属于每一种类别的可能性）</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-figure2.png" alt=""></p><blockquote><p><strong>综上，S×S 个网格，每个网格要预测 B个bounding box （中间上图），还要预测 C 个类（中间下图）。</strong>将两图合并，网络输出就是一个 S × S × (5×B+C)。（S x S个网格，每个网格都有B个预测框，每个框又有5个参数，再加上每个网格都有C个预测类）</p><h4 id="预测特征组成"><a href="#预测特征组成" class="headerlink" title="预测特征组成"></a>预测特征组成</h4><p>最终的预测特征由边框的位置、边框的置信度得分以及类别概率组成，这三者的含义如下：</p><ul><li><strong>边框位置x,y,w,h：</strong> 对每一个边框需要预测其中心坐标及宽、高这4个量， 两个边框共计8个预测值边界框宽度w和高度h用图像宽度和高度归一化。因此 x,y,w,h 都在0和1之间。</li><li><strong>置信度得分(box confidence score) c ：</strong> 框包含一个目标的可能性以及边界框的准确程度。类似于Faster RCNN 中是前景还是背景。由于有两个边框，因此会存在两个置信度预测值。</li><li><strong>类别概率：</strong> 由于PASCAL VOC数据集一共有20个物体类别，因此这里预测的是边框属于哪一个类别。</li></ul><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><ul><li>一个cell预测的两个边界框共用一个类别预测， 在<strong>训练</strong>时会选取与标签IoU更大的一个边框负责回归该真实物体框，在<strong>测试</strong>时会选取置信度更高的一个边框，另一个会被舍弃，因此7×7=49个gird cell最多只能预测49个物体。</li><li>因为每一个 grid cell只能有一个分类，也就是他只能预测一个物体，这也是导致YOLO对小目标物体性能比较差的原因。<strong>如果所给图片极其密集，导致 grid cell里可能有多个物体，但是YOLO模型只能预测出来一个，那这样就会忽略在本grid cell内的其他物体。</strong></li></ul></blockquote><h4 id="2-1-Network-Design—网络设计"><a href="#2-1-Network-Design—网络设计" class="headerlink" title="2.1 Network Design—网络设计"></a>2.1 Network Design—网络设计</h4><p>我们将此模型作为卷积神经网络来实现，并在Pascal VOC检测数据集[9]上进行评估。网络的初始卷积层从图像中提取特征，而全连接层负责预测输出概率和坐标。</p><p>我们的网络架构受图像分类模型<strong>GoogLeNet</strong>的启发[34]。我们的网络有24个卷积层，后面是2个全连接层。我们只使用1×1降维层，后面是3×3卷积层，这与Lin等人[22]类似，而不是GoogLeNet使用的Inception模块。</p><p>我们还训练了快速版本的YOLO，旨在推动快速目标检测的界限。快速YOLO使用具有较少卷积层（9层而不是24层）的神经网络，在这些层中使用较少的卷积核。除了网络规模之外，基本版YOLO和快速YOLO的所有训练和测试参数都是相同的。</p><p>我们网络的最终输出是7×7×30的预测张量。</p><blockquote><h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p>YOLO网络结构借鉴了 GoogLeNet。输入图像的尺寸为448×448，经过24个卷积层，2个全连接的层（FC），最后在reshape操作，输出的特征图大小为7×7×30。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-net.png" alt=""></p><h4 id="网络详解"><a href="#网络详解" class="headerlink" title="网络详解"></a>网络详解</h4><p>1）YOLO主要是建立一个CNN网络生成预测<strong>7×7×1024</strong> 的张量 。</p><p>2）然后使用两个全连接层执行线性回归，以进行<strong>7×7×2</strong> 边界框预测。将具有高置信度得分（大于0.25）的结果作为最终预测。</p><p>3）在3×3的卷积后通常会接一个通道数更低<strong>1×1</strong>的卷积，这种方式既降低了计算量，同时也提升了模型的非线性能力。</p><p>4）除了最后一层使用了线性激活函数外，其余层的激活函数为 <strong>Leaky ReLU</strong> 。</p><p>5）在训练中使用了 <strong>Dropout</strong> 与数据增强的方法来防止过拟合。</p><p>6）对于最后一个卷积层，它输出一个形状为 <strong>(7, 7, 1024)</strong> 的张量。 然后张量展开。使用2个全连接层作为一种线性回归的形式，它输出1470个参数，然后reshape为 <strong>(7, 7, 30)</strong> 。</p><p>张量剖面图</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E5%BC%A0%E9%87%8F%E5%89%96%E9%9D%A2%E5%9B%BE.png" alt=""></p><ul><li><strong>7×7：</strong> 一共划分成7×7的网格。</li><li><strong>30：</strong> 30包含了两个预测框的参数和Pascal VOC的类别参数：每个预测框有5个参数：x,y,w,h,confidence。另外，Pascal VOC里面还有20个类别；所以最后的30实际上是由<code>5x2+20</code>组成的，也就是说这一个30维的向量就是一个gird cell的信息。</li><li><strong>7×7×30：</strong> 总共是7 × 7个gird cell一共就是7 × 7 ×（2 × 5+ 20）= 7 × 7 × 30 tensor = 1470 </li></ul></blockquote><h4 id="2-2-Training—训练"><a href="#2-2-Training—训练" class="headerlink" title="2.2 Training—训练"></a>2.2 Training—训练</h4><p>我们在ImageNet的1000类竞赛数据集[30]上<strong>预训练我们的卷积层</strong>。对于预训练，我们使用图3中的前20个卷积层，接着是平均池化层和全连接层。我们对这个网络进行了大约一周的训练，并且在ImageNet 2012验证集上获得了单一裁剪图像88%的top-5准确率，与Caffe模型池中的GoogLeNet模型相当。我们使用<strong>Darknet</strong>框架进行所有的训练和推断[26]。</p><p>然后我们转换模型来执行检测训练。Ren等人表明，<strong>预训练网络中增加卷积层和连接层可以提高性能</strong>[29]。按照他们的方法，我们<strong>添加了四个卷积层和两个全连接层，这些层的权重都用随机值初始化</strong>。检测通常需要细粒度的视觉信息，因此我们将网络的输入分辨率从224×224改为448×448。</p><p><strong>模型的最后一层预测类概率和边界框坐标</strong>。我们通过图像宽度和高度来规范边界框的宽度和高度，使它们落在0和1之间。我们将边界框x和y坐标参数化为特定网格单元位置的<strong>偏移量</strong>，所以它们的值被限定在在0和1之间。</p><p><strong>模型的最后一层</strong>使用线性激活函数，而<strong>所有其它的层</strong>使用下面的Leaky-ReLU：</p><script type="math/tex; mode=display">\phi(x) = \begin{cases} x, & \text{if } x > 0 \\0.1x, & \text{otherwise}\end{cases}</script><p>我们对模型输出的<strong>平方和误差</strong>(sum-squared error)进行优化。我们选择使用平方和误差，是因为它易于优化，但是它并不完全符合最大化平均精度（average precision）的目标。它给分类误差与定位误差的权重是一样的，这点可能并不理想。<strong>另外</strong>，每个图像都有<strong>很多网格单元并没有包含任何目标</strong>，这将这些单元格的“置信度”分数推向零，通常<strong>压制了包含目标的单元格的梯度</strong>。这可能导致模型不稳定，从而导致训练在早期就发散(diverge)。</p><p>为了弥补平方和误差的缺陷，我们增加了边界框坐标预测的损失，并减少了不包含目标的框的置信度预测的损失。我们使用两个参数来实现这一点。</p><script type="math/tex; mode=display">\lambda_{\text{coord}} = 5 \quad \text{and} \quad \lambda_{\text{noobj}} = 0.5</script><p>平方和误差对大框和小框的误差权衡是一样的，而我们的错误指标(error metric)应该要体现出，<strong>大框的小偏差的重要性不如小框的小偏差的重要性。</strong>为了部分解决这个问题，我们直接预测边界框宽度和高度的<strong>平方根</strong>，而不是宽度和高度。</p><p><strong>YOLO为每个网格单元预测多个边界框。</strong>在训练时，每个目标我们只需要一个边界框预测器来负责。若某预测器的预测值与目标的实际值的IOU值最高，则这个预测器被指定为“负责”预测该目标。这导致边界框预测器的专业化。每个预测器可以更好地预测特定大小，方向角，或目标的类别，从而改善整体召回率(recall)。</p><p><strong>在训练期间，我们优化以下多部分损失函数：</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-loss.png" alt=""></p><p>注意，如果目标存在于该网格单元中（前面讨论的条件类别概率），则损失函数仅惩罚(penalizes)分类错误。如果预测器“负责”实际边界框（即该网格单元中具有最高IOU的预测器），则它也仅惩罚边界框坐标错误。</p><p>我们用Pascal VOC 2007和2012的训练集和验证数据集进行了大约 135个epoch 的网络训练。因为我们仅在Pascal VOC 2012上进行测试，所以我们的训练集里包含了Pascal VOC 2007的测试数据。在整个训练过程中，我们使用：batch size=64，momentum=0.9，decay=0.0005。</p><p>我们的学习率（learning rate）计划如下：在第一个epoch中，我们将学习率从0.001慢慢地提高到 0.01。如果从大的学习率开始训练，我们的模型通常会由于不稳定的梯度而发散(diverge)。我们继续以0.01进行75个周期的训练，然后以0.001进行30个周期的训练，最后以0.0001进行30个周期的训练。</p><p><strong>为避免过拟合，我们使用了Dropout和大量的数据增强</strong>。 在第一个连接层之后的dropout层的丢弃率设置为0.5，以防止层之间的相互适应[18]。 对于数据增强(data augmentation)，我们引入高达20％的原始图像大小的随机缩放和平移(random scaling and translations )。我们还在 HSV 色彩空间中以高达 1.5 的因子随机调整图像的曝光度和饱和度。</p><blockquote><h4 id="预训练分类网络"><a href="#预训练分类网络" class="headerlink" title="预训练分类网络"></a>预训练分类网络</h4><p>在 ImageNet 1000数据集上预训练一个分类网络，<strong>这个网络使用Figure3中的前20个卷积层，然后是一个平均池化层和一个全连接层。</strong>（此时网络输入是224×224）。</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-ImageNet%E9%A2%84%E8%AE%AD%E7%BB%83%E7%BD%91%E7%BB%9C.png" alt=""></p><h4 id="训练检测网络"><a href="#训练检测网络" class="headerlink" title="训练检测网络"></a>训练检测网络</h4><p>经过上一步的预训练，就已经把主干网络的前20个卷积层给训练好了，前20层的参数已经学到了图片的特征。接下来的步骤本质就是迁移学习，<strong>在训练好的前20层卷积层后加上4层卷积层和2层全连接层，然后在目标检测的任务上进行迁移学习。</strong></p><p>在整个网络（24+2）的训练过程中，除最后一层采用ReLU函数外，其他层均采用leaky ReLU激活函数。leaky ReLU相对于ReLU函数可以解决在输入为负值时的零梯度问题。YOLOv1中采用的leaky ReLU函数的表达式为：</p><script type="math/tex; mode=display">\phi(x) = \begin{cases} x, & \text{if } x > 0 \\0.1x, & \text{otherwise}\end{cases}</script><h4 id="NMS非极大值抑制"><a href="#NMS非极大值抑制" class="headerlink" title="NMS非极大值抑制"></a>NMS非极大值抑制</h4><p><strong>概念：</strong>NMS算法主要解决的是一个目标被多次检测的问题，意义主要在于在一个区域里交叠的很多框选一个最优的。</p><p>1）对于上述的98列数据，先看某一个类别，也就是只看98列的这一行所有数据，<strong>先拿出最大值概率的那个框，剩下的每一个都与它做比较</strong>，如果两者的IoU大于某个阈值，则认为这俩框重复识别了同一个物体，就将其中低概率的重置成0。</p><p>2）最大的那个框和其他的框比完之后，<strong>再从剩下的框找最大的，继续和其他的比</strong>，依次类推对所有类别进行操作。 注意，这里不能直接选择最大的，因为<strong>有可能图中有多个该类别的物体</strong>，所以IoU如果小于某个阈值，则会被保留。</p><p>3）最后得到一个稀疏矩阵，因为里面有很多地方都被重置成0，拿出来不是0的地方拿出来概率和类别，就得到最后的目标检测结果了。</p><p><strong>注意：</strong> <strong>NMS只发生在预测阶段，训练阶段是不能用NMS的</strong>，因为在训练阶段不管这个框是否用于预测物体的，他都和损失函数相关，不能随便重置成0。</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p><strong>损失函数包括：</strong></p><ul><li>localization loss -&gt; 坐标损失</li><li>confidence loss -&gt; 置信度损失</li><li>classification loss -&gt; 分类损失</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-loss%E8%A7%A3%E9%87%8A.png" alt=""></p><h5 id="损失函数详解："><a href="#损失函数详解：" class="headerlink" title="损失函数详解："></a>损失函数详解：</h5><p><strong>（1）坐标损失</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E5%9D%90%E6%A0%87%E6%8D%9F%E5%A4%B1.png" alt=""></p><ul><li>第一行： 负责检测物体的框中心点（x, y）定位误差。</li><li>第二行： 负责检测物体的框的高宽（w,h)定位误差，这个根号的作用就是为了修正对大小框一视同仁的缺点，削弱大框的误差。</li></ul><blockquote><h5 id="加根号的作用："><a href="#加根号的作用：" class="headerlink" title="加根号的作用："></a>加根号的作用：</h5><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E5%A4%A7%E6%A1%86%2B%E5%B0%8F%E6%A1%86.png" alt=""></p><p>在上图中，大框和小框的bounding box和ground truth都是差了一点。但对于实际预测来讲，大框（大目标）差的这一点也许没啥事儿，而小框（小目标）差的这一点可能就会导致bounding box的方框和目标差了很远。而如果还是使用第一项那样直接算平方和误差，就相当于把大框和小框一视同仁了，这样显然不合理。而如果使用开根号处理，就会一定程度上改善这一问题 。即要想办法<strong>夸大</strong>小框的偏差<br><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E5%A4%B8%E5%A4%A7%E5%B0%8F%E6%A1%86.png" alt=""></p><p>这样一来，同样是差一点，小框产生的误差会更大，即对小框惩罚的更严重。</p></blockquote><p><strong>（2）置信度损失</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E7%BD%AE%E4%BF%A1%E5%BA%A6%E6%8D%9F%E5%A4%B1.png" alt=""></p><ul><li>第一行： 负责检测物体的那个框的置信度误差。</li><li>第二行： 不负责检测物体的那个框的置信度误差。</li></ul><p><strong>（3）分类损失</strong></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E5%88%86%E7%B1%BB%E6%8D%9F%E5%A4%B1.png" alt=""></p><p>负责检测物体的grid cell分类的误差。</p><h4 id="特殊符号的含义："><a href="#特殊符号的含义：" class="headerlink" title="特殊符号的含义："></a>特殊符号的含义：</h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E7%89%B9%E6%AE%8A%E7%AC%A6%E5%8F%B7%E5%90%AB%E4%B9%89.png" alt=""></p></blockquote><h4 id="2-3-Inference—推论"><a href="#2-3-Inference—推论" class="headerlink" title="2.3 Inference—推论"></a>2.3 Inference—推论</h4><p>就像在训练中一样，预测测试图像的检测只需要一次网络评估。在Pascal VOC上，每张图像上网络预测 98 个边界框和每个框的类别概率。YOLO在测试时非常快，因为它只需要一次网络评估(network evaluation)，这与基于分类器的方法不同。</p><p><strong>网格设计强化了边界框预测中的空间多样性</strong>。通常一个目标落在哪一个网格单元中是很明显的，而网络只能为每个目标预测一个边界框。然而，一些大的目标或接近多个网格单元的边界的目标能被多个网格单元定位。<strong>非极大值抑制(Non-maximal suppression,NMS)可以用来修正这些多重检测</strong>。非最大抑制对于YOLO的性能的影响不像对于R-CNN或DPM那样重要，但也能增加2−3%的mAP。</p><blockquote><p>1）预测测试图像的检测只需要一个网络评估。</p><p>2）测试时间快</p><p>3）当图像中的物体较大，或者处于 grid cells 边界的物体，可能在多个 cells 中被定位出来。</p><p>4）利用NMS去除重复检测的物体，使mAP提高，但和RCNN等相比不算大。</p></blockquote><h4 id="2-4-Limitations-of-YOLO—YOLO的局限性"><a href="#2-4-Limitations-of-YOLO—YOLO的局限性" class="headerlink" title="2.4 Limitations of YOLO—YOLO的局限性"></a>2.4 Limitations of YOLO—YOLO的局限性</h4><p>由于每个格网单元只能预测两个框，并且只能有一个类，因此YOLO对边界框预测<strong>施加了很强的空间约束</strong>。这个空间约束限制了我们的模型可以预测的邻近目标的数量。我们的模型难以预测群组中出现的小物体（比如鸟群）。</p><p>由于我们的模型学习是从数据中预测边界框，因此它<strong>很难泛化</strong>到新的、不常见的长宽比或配置的目标。我们的模型也使用<strong>相对较粗糙的特征</strong>来预测边界框，因为输入图像在我们的架构中<strong>历经了多个下采样层</strong>(downsampling layers)。</p><p>最后，我们的训练基于一个逼近检测性能的损失函数，这个损失函数<strong>无差别地</strong>处理小边界框与大边界框的误差。大边界框的小误差通常是无关要紧的，但小边界框的小误差对IOU的影响要大得多。我们的<strong>主要错误来自于不正确的定位</strong>。</p><blockquote><p>1）对于图片中一些群体性小目标检测效果比较差。因为yolov1网络到后面感受野较大，小目标的特征无法再后面7×7的grid中体现，<strong>针对这一点，yolov2已作了一定的修改，加入前层（感受野较小）的特征进行融合。</strong></p><p>2）原始图片只划分为7x7的网格，当两个物体靠的很近时（挨在一起且中点都落在同一个格子上的情况），效果比较差。因为yolov1的模型决定了一个grid只能预测出一个物体，所以就会丢失目标，针对这一点，<strong>yolov2引入了anchor的概念，一个grid有多少个anchor理论上就可以预测多少个目标</strong>。<br>3）每个网格只对应两个bounding box，当物体的长宽比不常见(也就是训练数据集覆盖不到时)，效果较差。</p><p>4）最终每个网格只对应一个类别，容易出现漏检(物体没有被识别到)。</p></blockquote><h3 id="三、Comparison-to-Other-Detection-Systems—与其他目标检测算法的比较"><a href="#三、Comparison-to-Other-Detection-Systems—与其他目标检测算法的比较" class="headerlink" title="三、Comparison to Other Detection Systems—与其他目标检测算法的比较"></a>三、Comparison to Other Detection Systems—与其他目标检测算法的比较</h3><p>目标检测是计算机视觉中的核心问题。检测流程通常是<strong>首先</strong>从输入图像上提取一组鲁棒特征（Haar [25]，SIFT [23]，HOG [4]，卷积特征[6]）。<strong>然后</strong>，分类器[36,21,13,10]或定位器[1,32]被用来识别特征空间中的目标。这些分类器或定位器或在整个图像上或在图像中的一些子区域上以滑动窗口的方式运行[35,15,39]。我们将YOLO检测系统与几种顶级检测框架进行比较，突出了关键的相似性和差异性。</p><p><strong>Deformable parts models</strong>。可变形部分模型（DPM）使用<strong>滑动窗口</strong>方法进行目标检测[10]。DPM使用不相交的流程来提取静态特征，对区域进行分类，预测高评分区域的边界框等。我们的系统用单个卷积神经网络替换所有这些不同的部分。<strong>网络同时进行特征提取，边界框预测，非极大值抑制和上下文推理</strong>。网络的特征feature是在<strong>在线</strong>(in-line)训练出来的而不是静态，因此可以根据特定的检测任务进行优化。我们的统一架构比DPM更快，更准确。</p><p><strong>R-CNN</strong>。R-CNN及其变体(variants)使用<strong>区域候选</strong>而不是滑动窗口来查找图像中的目标。选择性搜索[35]生成潜在的边界框(Selective Search generates potential bounding boxes)，卷积网络提取特征，SVM对框进行评分，线性模型调整边界框，非最大抑制消除重复检测(eliminates duplicate detections)。 这个复杂流水线的每个阶段都必须独立地进行精确调整(precisely tuned independently)，所得到的系统非常缓慢，在测试时间每个图像需要超过40秒[14]</p><p>YOLO与R-CNN有一些相似之处。每个网格单元提出潜在的边界框并使用卷积特征对这些框进行评分。然而，我们的系统对网格单元的候选框<strong>施加空间限制</strong>，这有助于缓解对同一目标的多次检测的问题。 我们的系统还<strong>生成了更少的边界框</strong>，每张图像只有98个，而选择性搜索则有约2000个。最后，我们的系统将这些单独的组件(individual components)组合成一个单一的、共同优化的模型。</p><p><strong>其它快速检测器</strong>。 Fast R-CNN 和 Faster R-CNN 通过共享计算和使用神经网络替代选择性搜索[14]，[28]来提出候选区域来加速 R-CNN 框架。虽然它们提供了比 R-CNN 更快的速度和更高的准确度，但仍然不能达到实时性能。</p><p>许多研究工作集中在加快DPM流程上[31] [38] [5]。它们加速HOG计算，使用级联(cascades)，并将计算推动到（多个）GPU上。但是，实际上只有30Hz的DPM [31]可以实时运行。</p><p>YOLO并没有试图优化大型检测流程的单个组件，相反，而是完全抛弃(throws out…entirely)了大型检测流程，并通过设计来提高速度。</p><p>像人脸或行人等单个类别的检测器可以高度优化，因为他们只需处理较少的多样性[37]。YOLO是一种通用的检测器，它可以同时(simultaneously)检测多个目标。</p><p><strong>Deep MultiBox</strong>。与R-CNN不同，Szegedy等人 训练一个卷积神经网络来预测感兴趣的区域(regions of interest,ROI)[8]，而不是使用选择性搜索。 MultiBox还可以通过用单个类别预测替换置信度预测来执行单个目标检测。 但是，MultiBox无法执行一般的目标检测，并且<strong>仍然只是较大检测流水线中的一部分</strong>，需要进一步的图像补丁分类。 YOLO和MultiBox都使用卷积网络来预测图像中的边界框，但<strong>YOLO是一个完整的检测系统</strong>。</p><p><strong>OverFeat</strong>。Sermanet等人训练了一个卷积神经网络来执行定位，并使该定位器进行检测[32]。OverFeat高效地执行滑动窗口检测，但它仍然是一个<strong>不相交的系统(</strong>disjoint system)。OverFeat优化了定位功能，而不是检测性能。像DPM一样，定位器在进行预测时只能看到局部信息。OverFeat无法推断全局上下文，因此需要大量的后处理来产生连贯的检测。</p><p><strong>MultiGrasp</strong>。我们的系统在设计上类似于Redmon等[27]的抓取检测。 <strong>我们的网格边界框预测方法基于MultiGrasp系统进行回归分析</strong>。 然而，抓取检测比物体检测要简单得多。 MultiGrasp只需要为包含一个目标的图像预测一个可抓取区域。 它不必估计目标的大小，位置或边界或预测它的类别，只需找到适合抓取的区域就可以了。 而<strong>YOLO则是预测图像中多个类的多个目标的边界框和类概率</strong>。</p><blockquote><h4 id="DPM"><a href="#DPM" class="headerlink" title="DPM"></a>DPM</h4><p>用传统的HOG特征方法，也用的是传统的支持向量机SVM分类器，然后人工造一个模板，再用滑动窗口方法不断的暴力搜索整个待识别图，去套那个模板。这个方法比较大的问题就是在于设计模板，计算量巨大，而且是个静态的，没办法匹配很多变化的东西，鲁棒性差。</p><h4 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h4><ul><li>第一阶段：每个图片使用选择性搜索SS方法提取2000个候选框。</li><li>第二阶段：将每个候选框送入CNN网络进行分类(使用的SVM)。</li></ul><p>YOLO对比他们俩都很强，YOLO和R-CNN也有相似的地方，比如也是提取候选框，YOLO的候选框就是上面说过的那98个 bounding boxes，也是用到了NMS非极大值抑制，也用到了CNN提取特征。</p><h4 id="Other-Fast-Detectors"><a href="#Other-Fast-Detectors" class="headerlink" title="Other Fast Detectors"></a>Other Fast Detectors</h4><p>Fast和Faster R-CNN ：这俩模型都是基于R-CNN的改版，速度和精度都提升了很多，但是也没办法做到实时监测，也就是说FPS到不了30，作者在这里并没有谈准确度的问题，实际上YOLO的准确度在这里是不占优势的，甚至于比他们低。</p><h4 id="Deep-MultiBox"><a href="#Deep-MultiBox" class="headerlink" title="Deep MultiBox"></a>Deep MultiBox</h4><p>训练卷积神经网络来预测感兴趣区域，而不是使用选择性搜索。多盒也可以用单个类预测替换置信预测来执行单个目标检测。YOLO和MultiBox都使用卷积网络来预测图像中的边界框，但YOLO是一个完整的检测系统。</p><h4 id="OverFeat"><a href="#OverFeat" class="headerlink" title="OverFeat"></a>OverFeat</h4><p>OverFeat有效地执行滑动窗口检测，优化了定位，而不是检测性能。与DPM一样，定位器在进行预测时只看到本地信息。OverFeat不能推理全局环境。</p><h4 id="MultiGrasp"><a href="#MultiGrasp" class="headerlink" title="MultiGrasp"></a>MultiGrasp</h4><p>YOLO在设计上与Redmon等人的抓取检测工作相似。边界盒预测的网格方法是基于多重抓取系统的回归到抓取。</p><p>总之，作者就是给前人的工作都数落一遍，凸显自己模型的厉害（学到了！）</p></blockquote><h3 id="四、Experiments—实验"><a href="#四、Experiments—实验" class="headerlink" title="四、Experiments—实验"></a><strong>四、Experiments—实验</strong></h3><h4 id="4-1-Comparison-to-Other-RealTime-Systems—与其他实时系统的比较"><a href="#4-1-Comparison-to-Other-RealTime-Systems—与其他实时系统的比较" class="headerlink" title="4.1 Comparison to Other RealTime Systems—与其他实时系统的比较"></a>4.1 Comparison to Other RealTime Systems—与其他实时系统的比较</h4><p>目标检测方面的许多研究工作都集中在使标准的检测流程更快[5]，[38]，[31]，[14]，[17]，[28]。然而，只有Sadeghi等人实际上产生了一个实时运行的检测系统（每秒30帧或更好）[31]。我们将YOLO与DPM的GPU实现进行了比较，其在30Hz或100Hz下运行。虽然其它的算法没有达到实时性的标准，我们也比较了它们的<strong>mAP和速度</strong>的关系，从而探讨目标检测系统中<strong>精度和性能之间的权衡</strong>。</p><p><strong>Fast YOLO</strong>是PASCAL上最快的目标检测方法；据我们所知，它是现有的最快的目标检测器。具有52.7%的mAP，实时检测的精度是以前的方法的两倍以上。<strong>普通版YOLO</strong>将mAP推到63.4%的同时保持了实时性能。</p><p>我们还使用VGG-16训练YOLO。 这个模型比<strong>普通版YOLO</strong>更精确，但也更慢。 它的作用是与依赖于VGG-16的其他检测系统进行比较，但由于它比实时更慢，所以本文的其他部分将重点放在我们更快的模型上。</p><p>最快的DPM可以在不牺牲太多mAP的情况下有效加速DPM，但仍然会将实时性能降低2倍[38]。与神经网络方法相比，DPM的检测精度相对较低，这也是限制它的原因。</p><p>减去R的R-CNN用静态侯选边界框取代选择性搜索[20]。虽然速度比R-CNN更快，但它仍然无法实时，并且由于该方法无法找到好的边界框，准确性受到了严重影响。</p><p>Fast R-CNN加快了R-CNN的分类阶段，但它仍然依赖于选择性搜索，每个图像需要大约2秒才能生成边界候选框。因此，它虽然具有较高的mAP，但的速度是0.5 fps，仍然远未达到实时。</p><p>最近的Faster R-CNN用神经网络替代了选择性搜索来候选边界框，类似于Szegedy等人[8]的方法。在我们的测试中，他们最准确的模型达到了 7fps，而较小的、不太准确的模型以18 fps运行。 Faster R-CNN的VGG-16版本比YOLO高出10mAP，但比YOLO慢了6倍。 Zeiler-Fergus 版本的Faster R-CNN只比YOLO慢2.5倍，但也不如YOLO准确。</p><blockquote><p>Table 1 在Pascal VOC 2007 上与其他检测方法的对比</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E5%9C%A8Pascal%20VOC%202007%20%E4%B8%8A%E4%B8%8E%E5%85%B6%E4%BB%96%E6%A3%80%E6%B5%8B%E6%96%B9%E6%B3%95%E7%9A%84%E5%AF%B9%E6%AF%94.png" alt=""></p><p><strong>结论：</strong>实时目标检测（FPS&gt;30），YOLO最准，Fast YOLO最快。 </p></blockquote><h4 id="4-2-VOC-2007-Error-Analysis—VOC-2007误差分析"><a href="#4-2-VOC-2007-Error-Analysis—VOC-2007误差分析" class="headerlink" title="4.2 VOC 2007 Error Analysis—VOC 2007误差分析"></a>4.2 VOC 2007 Error Analysis—VOC 2007误差分析</h4><p>为了进一步研究YOLO和最先进的检测器之间的差异，我们详细分析了VOC 2007的分类(breakdown)结果。我们将YOLO与Fast R-CNN进行比较，因为Fast R-CNN是PASCAL上性能最高的检测器之一并且它的检测代码是可公开得到的。</p><p>我们使用Hoiem等人的方法和工具[19]，对于测试的每个类别，我们查看该类别的前N个预测。每个预测都或是正确的，或是根据错误的类型进行分类：</p><ul><li>Correct: correct class and IOU&gt;0.5</li><li>Localization: correct class, 0.1&lt;IOU&lt;0.5</li><li>Similar: class is similar, IOU&gt;0.1</li><li>Other: class is wrong, IOU&gt;0.1</li><li>Background: IOU&lt;0.1 for any object（所有目标的IOU都&lt;0.1）</li></ul><p>YOLO难以正确地定位目标，因此定位错误比YOLO的所有其他错误总和都要多。Fast R-CNN定位错误更少，但把背景误认成目标的错误比较多。它的最高检测结果中有13.6％是不包含任何目标的误报(false positive，背景)。 Fast R-CNN把背景误认成目标的概率比YOLO高出3倍。 </p><blockquote><p>本文使用HoeMm等人的方法和工具。对于测试时间的每个类别，查看该类别的N个预测。每个预测要么是正确的，要么是基于错误类型进行分类的：</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E6%B5%8B%E8%AF%95%E7%B1%BB%E5%88%AB.png" alt=""></p><p><strong>参数含义:</strong></p><ul><li><strong>Correct：</strong>正确分类，且预测框与ground truth的IOU大于0.5，既预测对了类别，预测框的位置和大小也很合适。 </li><li><strong>Localization：</strong>正确分类，但预测框与ground truth的IOU大于0.1小于0.5，即虽然预测对了类别，但预测框的位置不是那么的严丝合缝，不过也可以接受。</li><li><strong>Similar：</strong> 预测了相近的类别，且预测框与ground truth的IOU大于0.1。即预测的类别虽不正确但相近，预测框的位置还可以接受。</li><li><strong>Other：</strong>预测类别错误，预测框与ground truth的IOU大于0.1。即预测的类别不正确，但预测框还勉强把目标给框住了。</li><li><strong>Background：</strong>预测框与ground truth的IOU小于0.1，即该预测框的位置为背景，没有目标。</li></ul><p>Figure 4 显示了所有20个类中每种错误类型的平均细分情况</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E9%94%99%E8%AF%AF%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%B9%B3%E5%9D%87%E7%BB%86%E5%88%86.png" alt=""></p><p><strong>结论：</strong>YOLO定位错误率高于Fast R-CNN；Fast R-CNN背景预测错误率高于YOLO </p></blockquote><h4 id="4-3-Combining-Fast-R-CNN-and-YOLO—Fast-R-CNN与YOLO的结合"><a href="#4-3-Combining-Fast-R-CNN-and-YOLO—Fast-R-CNN与YOLO的结合" class="headerlink" title="4.3 Combining Fast R-CNN and YOLO—Fast R-CNN与YOLO的结合"></a>4.3 Combining Fast R-CNN and YOLO—Fast R-CNN与YOLO的结合</h4><p><strong>YOLO误认背景为目标的情况比Fast R-CNN少得多</strong>。 通过使用YOLO消除Fast R-CNN的背景检测，我们获得了显著的性能提升。 对于R-CNN预测的每个边界框，我们检查YOLO是否预测了一个相似的框。 如果确实如此，那么我们会根据YOLO预测的概率和两个框之间的重叠情况提高预测值。</p><p>最好的Fast R-CNN模型在VOC 2007测试集中达到了 71.8％ 的mAP。 当与YOLO合并时，其mAP增加了 3.2％ 至 75.0％。 我们还尝试将顶级Fast R-CNN模型与其他几个版本的Fast R-CNN结合起来。 这写的结合的平均增长率在 0.3％ 至 0.6％ 之间。</p><p>结合YOLO后获得的性能提高不仅仅是模型集成的副产品，因为结合不同版本的Fast R-CNN几乎没有什么益处。 相反，正是因为YOLO在测试时出现了各种各样的错误，所以它在提高Fast R-CNN的性能方面非常有效。</p><p>不幸的是，这种组合不会从YOLO的速度中受益，因为我们分别运行每个模型，然后合并结果。 但是，由于YOLO速度如此之快，与Fast R-CNN相比，它不会增加任何显著的计算时间。</p><p>Table2 模型组合在VOC 2007上的实验结果对比</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E6%A8%A1%E5%9E%8B%E7%BB%84%E5%90%88%E5%9C%A8VOC%202007%E4%B8%8A%E7%9A%84%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E5%AF%B9%E6%AF%94.png" alt=""></p><p><strong>结论：</strong>因为YOLO在测试时犯了各种错误，所以它在提高快速R-CNN的性能方面非常有效。但是这种组合并不受益于YOLO的速度，由于YOLO很快，和Fast R-CNN相比，它不增加任何有意义的计算时间。</p><h4 id="4-4-VOC-2012-Results—VOC-2012结果"><a href="#4-4-VOC-2012-Results—VOC-2012结果" class="headerlink" title="4.4 VOC 2012 Results—VOC 2012结果"></a>4.4 VOC 2012 Results—VOC 2012结果</h4><p>在VOC 2012测试集中，YOLO的mAp得分是57.9％。这比现有最先进的技术水平低，更接近使用VGG-16的原始的R-CNN，见表3。与其最接近的竞争对手相比，我们的系统很难处理小物体上(struggles with small objects)。在瓶子、羊、电视/监视器等类别上，YOLO得分比R-CNN和Feature Edit低8-10％。然而，在其他类别，如猫和火车YOLO取得了更好的表现。</p><p>我们的Fast R-CNN + YOLO模型组合是性能最高的检测方法之一。 Fast R-CNN与YOLO的组合提高了2.3％，在公共排行榜上提升了5个位置。</p><blockquote><p>Table 3 在VOC2012上mAP排序</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-VOC2012%E4%B8%8AmAP%E6%8E%92%E5%BA%8F.png" alt=""></p><p><strong>结论：</strong>Fast R-CNN从与YOLO的组合中得到2.3%的改进，在公共排行榜上提升了5个百分点。</p></blockquote><h4 id="4-5-Generalizability-Person-Detection-in-Artwork—泛化性：图像中的人物检测"><a href="#4-5-Generalizability-Person-Detection-in-Artwork—泛化性：图像中的人物检测" class="headerlink" title="4.5 Generalizability: Person Detection in Artwork—泛化性：图像中的人物检测"></a>4.5 Generalizability: Person Detection in Artwork—泛化性：图像中的人物检测</h4><p>用于目标检测的学术数据集的训练和测试数据是<strong>服从同一分布</strong>的。但在现实世界的应用中，很难预测所有可能的用例，他的测试数据可能与系统已经看到的不同[3]。我们将YOLO与其他检测系统在毕加索(Picasso)数据集[12]和人物艺术(People-Art)数据集[3]上进行了比较，这两个数据集用于测试艺术品上的人物检测。</p><p>作为参考(for reference)，我们提供了VOC 2007的人形检测的AP，其中所有模型仅在VOC 2007数据上训练。在Picasso数据集上测试的模型在是在VOC 2012上训练，而People-Art数据集上的模型则在VOC 2010上训练。</p><p>R-CNN在VOC 2007上有很高的AP值。然而，当应用于艺术图像时，R-CNN显着下降。R-CNN使用选择性搜索来调整自然图像的候选边界框。R-CNN在分类器阶段只能看到小区域，而且需要有很好的候选框。</p><p>DPM在应用于艺术图像时可以很好地保持其AP。之前的研究认为DPM表现良好，因为它具有强大的物体形状和布局空间模型。虽然DPM不会像R-CNN那样退化，但它的AP本来就很低。</p><p>YOLO在VOC 2007上表现出色，其应用于艺术图像时其AP降低程度低于其他方法。与DPM一样，YOLO模拟目标的大小和形状，以及目标之间的关系和目标通常出现的位置之间的关系。艺术图像和自然图像在像素级别上有很大不同，但它们在物体的大小和形状方面相似，因此YOLO仍然可以预测好的边界框和检测结果。</p><blockquote><p>Figure 5 通用性（Picasso 数据集和 People-Art数据集）</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E9%80%9A%E7%94%A8%E6%80%A7%EF%BC%88Picasso%20%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%20People-Art%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%89.png" alt=""></p><p><strong>结论：</strong>YOLO都具有很好的检测结果</p></blockquote><h3 id="五、Real-Time-Detection-In-The-Wild—自然环境下的实时检测"><a href="#五、Real-Time-Detection-In-The-Wild—自然环境下的实时检测" class="headerlink" title="五、Real-Time Detection In The Wild—自然环境下的实时检测"></a><strong>五、Real-Time Detection In The Wild—自然环境下的实时检测</strong></h3><p>YOLO是一款快速，精确的物体检测器，非常适合计算机视觉应用。 我们将YOLO连接到网络摄像头，并验证它是否保持实时性能，包括从摄像头获取图像并显示检测结果的时间。</p><p>由此产生的系统是互动的和参与的。 虽然YOLO单独处理图像，但当连接到网络摄像头时，它的功能类似于跟踪系统，可在目标移动并在外观上发生变化时检测目标。 系统演示和源代码可在我们的项目网站上找到：<a href="http://pjreddie.com/yolo/">YOLO: Real-Time Object Detection</a>。</p><blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/yolov1-%E6%95%88%E6%9E%9C.png" alt=""></p><p><strong>结论：</strong>将YOLO连接到一个网络摄像头上，并验证它是否保持了实时性能，包括从摄像头中获取图像和显示检测结果的时间。结果证明效果很好，如上图所示，除了第二行第二个将人误判为飞机以外，别的没问题。</p></blockquote><h3 id="六、Conclusion—结论"><a href="#六、Conclusion—结论" class="headerlink" title="六、Conclusion—结论"></a><strong>六、Conclusion—结论</strong></h3><p>我们介绍YOLO——一种用于物体检测的统一模型。 我们的模型构造简单，可以直接在完整图像上训练。 与基于分类器的方法不同，YOLO是通过与检测性能直接对应的损失函数进行训练的，并且整个模型是一起训练的。</p><p>快速YOLO是文献中最快的通用目标检测器，YOLO推动实时对象检测的最新技术。 YOLO还能很好地推广到新领域，使其成为快速，鲁棒性强的应用的理想选择。</p><blockquote><h4 id="到底什么是YOLO？"><a href="#到底什么是YOLO？" class="headerlink" title="到底什么是YOLO？"></a>到底什么是YOLO？</h4><ul><li>YOLO眼里目标检测是一个回归问题</li><li><p>一次性喂入图片，然后给出bbox和分类概率</p></li><li><p>简单来说，只看一次就知道图中物体的类别和位置</p></li></ul><h4 id="YOLO过程总结："><a href="#YOLO过程总结：" class="headerlink" title="YOLO过程总结："></a>YOLO过程总结：</h4><p><strong>训练阶段:</strong></p><p>首先将一张图像分成 S × S个 gird cell，然后将它一股脑送入CNN，生成S × S × (B × 5 + C）个结果，最后根据结果求Loss并反向传播梯度下降。</p><p><strong>预测、验证阶段：</strong></p><p>首先将一张图像分成 S × S网格(gird cell)，然后将它一股脑送入CNN，生成S × S × (B × 5 + C）个结果，最后用NMS选出合适的预选框。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(16)-深度视觉入门</title>
      <link href="/Pytorch-16/"/>
      <url>/Pytorch-16/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch_receptive_field <span class="keyword">import</span> receptive_field</span><br></pre></td></tr></table></figure><h2 id="一、卷积相关知识"><a href="#一、卷积相关知识" class="headerlink" title="一、卷积相关知识"></a>一、卷积相关知识</h2><h3 id="1-卷积"><a href="#1-卷积" class="headerlink" title="1. 卷积"></a>1. 卷积</h3><ul><li><code>二维矩阵的卷积</code>表示其中一矩阵旋转<code>180°</code>后，与另一个矩阵<code>求点积(依次相乘求和)</code>的结果<ul><li><code>卷</code>就是旋转，<code>积</code>就是点积.</li></ul></li><li>如今不进行旋转这一步,故本质是<code>互相关</code></li><li><strong>相关概念：</strong><ul><li>卷积核</li><li>感受野</li><li>特征图</li></ul></li></ul><h3 id="2-卷积遇见深度学习"><a href="#2-卷积遇见深度学习" class="headerlink" title="2. 卷积遇见深度学习"></a>2. 卷积遇见深度学习</h3><ul><li><code>检测边缘</code>、<code>锐化</code>、<code>模糊</code>、<code>图像降噪</code>等卷积相关的操作，是在从图像中<strong>提取部分信息</strong>，也被叫做<code>“特征提取”技术</code>。</li><li>通过学习寻找卷积核<ul><li><code>感受野</code>与<code>卷积核</code>点积，其操作与<code>DNN中的权重与特征相乘</code>十分相似</li><li><code>卷积层</code>被建立后，<code>卷积核的值</code>会被随机生成，之后就可以被<code>自动学习</code>出来了</li><li>以此来实现<code>自动找出最佳卷积</code>,提取出<code>对分类最有利的特征</code></li></ul></li><li><strong>参数共享</strong>：卷积带来参数量骤减<ul><li>预测效果好，计算量小</li></ul></li><li><strong>稀疏交互</strong>：获取更深入的特征<ul><li><code>DNN</code>中，上层的任意神经元都必须和<code>下层的每个神经元相连</code></li><li><code>CNN</code>中，下层的一个神经元只和<code>上层中被扫描的那些神经元</code>有关</li><li>让其拥有提取<code>更深特征</code>的能力</li></ul></li><li>绝对不是我们认为的<code>先提取细节</code>，<code>再拼接局部</code>，<code>最后组成图像</code></li></ul><h3 id="3-PyTorch构筑CNN"><a href="#3-PyTorch构筑CNN" class="headerlink" title="3. PyTorch构筑CNN"></a><strong>3. </strong>PyTorch<strong>构筑</strong>CNN</h3><div class="table-container"><table><thead><tr><th style="text-align:center">卷积层类型</th><th style="text-align:center">应用数据类型</th><th style="text-align:left">说明</th></tr></thead><tbody><tr><td style="text-align:center">Conv1d</td><td style="text-align:center">时序数据</td></tr><tr><td style="text-align:center">Conv2d</td><td style="text-align:center">图像数据</td></tr><tr><td style="text-align:center">Conv3d</td><td style="text-align:center">视频数据</td></tr></tbody></table></div><h3 id="4-普通卷积的参数"><a href="#4-普通卷积的参数" class="headerlink" title="4. 普通卷积的参数"></a>4. 普通卷积的参数</h3><ul><li><p>CLASS torch.nn.Conv2d (<code>in_channels</code>, <code>out_channels</code>, <code>kernel_size</code>, <code>stride</code>=1, <code>padding</code>=0, <strong>dilation=1</strong>,<strong>groups=1</strong>, bias=True, padding_mode=’zeros’)</p><ul><li>卷积核尺寸<code>kernel_size</code><ul><li>必填, 整数或数组</li><li>最好是<code>正方形(奇数)</code><ul><li>3x3/5x5/7x7</li><li>奇数保证图像的信息是<code>不断的向中心来压缩</code>；若<code>偶数</code>相当于把像素<code>往一个角去压缩</code>,导致<code>失真</code></li></ul></li></ul></li><li>输入与输出:<code>in_channels</code>，<code>out_channels</code><ul><li><strong>in_channels=3/三通道</strong>:不同数值的<code>3个卷积核</code>与<code>3通道</code>卷积后三个新通道相加得<code>1个feature_map</code><ul><li>无论多少通道进来，最终都是<code>1个feature_map</code></li></ul></li><li><strong>out_channels/扫描次数</strong>:得到<code>x个feature_map</code><ul><li>当前层的out_channels就是下一层的in_channels</li></ul></li><li><strong>in_channels*out_channels=卷积核数量</strong></li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">data = torch.ones(size=(<span class="number">10</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)) <span class="comment">#10张尺寸为28*28的、拥有3个通道的图像</span></span><br><span class="line"></span><br><span class="line">conv1 = nn.Conv2d(in_channels = <span class="number">3</span>  <span class="comment"># 全部通道的扫描值被合并</span></span><br><span class="line">                ,out_channels = <span class="number">6</span>  <span class="comment"># 6个卷积核形成6个feature map</span></span><br><span class="line">                 ,kernel_size = <span class="number">3</span>) <span class="comment"># 3x3的卷积核</span></span><br><span class="line">conv2 = nn.Conv2d(in_channels = <span class="number">6</span>  <span class="comment"># 对下一层网络来说，输入的是上层生成的6个feature map</span></span><br><span class="line">                         ,out_channels = <span class="number">4</span> <span class="comment">#全部特征图的扫描值被合并，4个卷积核形成4个新的feature map</span></span><br><span class="line">                 ,kernel_size = <span class="number">3</span>)</span><br><span class="line"><span class="comment">#通常在网络中，我们不会把参数都写出来，只会写成：</span></span><br><span class="line"><span class="comment">#conv1 = nn.Conv2d(3,6,3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看一下通过卷积后的数据结构</span></span><br><span class="line"><span class="built_in">print</span>(conv1(data).shape)  <span class="comment"># torch.Size([10, 6, 26, 26])</span></span><br><span class="line"><span class="built_in">print</span>(conv2(conv1(data)).shape) <span class="comment"># torch.Size([10, 4, 24, 24])</span></span><br></pre></td></tr></table></figure><ul><li><p>偏差bias</p><ul><li>整数或数组(1)</li><li><strong>布尔值</strong>，<code>&quot;True&quot;</code>则代表在卷积层中<code>使用偏置</code>，反之则不使用偏置</li></ul></li><li><p>步长: <code>stride</code></p><ul><li>加速对特征图的扫描，并加速缩小特征图，令计算更快</li></ul></li><li><p>填充:<code>padding</code></p><ul><li><p>整数或数组(0)</p></li><li><p>扫描不均衡(有多有少扫)</p></li><li><p>扫描不完全(边缘被舍弃)</p></li></ul></li><li><p>填充模式:<code>padding_mode</code></p><ul><li><p><code>zero_padding</code>:零填充</p></li><li><p><code>circular</code>:环形填充</p></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 感受特征图尺寸的变化</span></span><br><span class="line">data = torch.ones(size=(<span class="number">10</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>)) <span class="comment">#10张尺寸为28*28的、拥有3个通道的图像</span></span><br><span class="line">conv1 = nn.Conv2d(<span class="number">3</span>,<span class="number">6</span>,<span class="number">3</span>) </span><br><span class="line">conv2 = nn.Conv2d(<span class="number">6</span>,<span class="number">4</span>,<span class="number">3</span>) </span><br><span class="line">conv3 = nn.Conv2d(<span class="number">4</span>,<span class="number">16</span>,<span class="number">5</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>) </span><br><span class="line">conv4 = nn.Conv2d(<span class="number">16</span>,<span class="number">3</span>,<span class="number">5</span>,stride=<span class="number">3</span>,padding=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#conv1，输入结构28*28</span></span><br><span class="line"><span class="comment">#(28 + 0 - 3)/1 + 1 = 26</span></span><br><span class="line"><span class="comment">#验证一下</span></span><br><span class="line"><span class="built_in">print</span>(conv1(data).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#conv2，输入结构26*26</span></span><br><span class="line"><span class="comment">#(26 + 0 - 3)/1 + 1 = 24</span></span><br><span class="line"><span class="comment">#验证</span></span><br><span class="line"><span class="built_in">print</span>(conv2(conv1(data)).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#conv3，输入结构24*24</span></span><br><span class="line"><span class="comment">#(24 + 2 - 5)/2 + 1 = 11，扫描不完全的部分会被舍弃</span></span><br><span class="line"><span class="built_in">print</span>(conv3(conv2(conv1(data))).shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#conv4，输入结构11*11</span></span><br><span class="line"><span class="comment">#(11 + 4 - 5)/3 + 1 = 4.33，扫描不完全的部分会被舍弃</span></span><br><span class="line"><span class="built_in">print</span>(conv4(conv3(conv2(conv1(data)))).shape)</span><br></pre></td></tr></table></figure><h3 id="5-池化层"><a href="#5-池化层" class="headerlink" title="5. 池化层"></a>5. 池化层</h3><ul><li><strong>最大池化</strong>(Max Pooling)/nn.MaxPool</li><li><strong>平局池化</strong>(Avg Pooling)/nn.AvgPool</li><li>特点：<ul><li>提供<code>非线性变化</code></li><li>有一定的<code>平移不变性</code></li><li>池化层<code>不会增加参数量</code></li><li><code>统一规律</code>对所有的feature_map<code>一次降维</code></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">data = torch.ones(size=(<span class="number">10</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line"></span><br><span class="line">conv1 = nn.Conv2d(<span class="number">3</span>,<span class="number">6</span>,<span class="number">3</span>) <span class="comment">#(28 + 0 - 3)/1 + 1 = 26</span></span><br><span class="line">conv3 = nn.Conv2d(<span class="number">6</span>,<span class="number">16</span>,<span class="number">5</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>) <span class="comment"># (26 + 2 - 5)/2 +1 = 12</span></span><br><span class="line">pool1 = nn.MaxPool2d(<span class="number">2</span>) <span class="comment">#唯一需要输入的参数，kernel_size=2，则默认使用(2,2)结构的核</span></span><br><span class="line"><span class="comment"># (12 + 0 - 2)/2 + 1 =6</span></span><br><span class="line"><span class="comment">#验证一下</span></span><br><span class="line">pool1(conv3(conv1(data))).shape  <span class="comment"># torch.Size([10, 16, 6, 6])</span></span><br></pre></td></tr></table></figure><h3 id="6-Droupout2d与BatchNorm2d"><a href="#6-Droupout2d与BatchNorm2d" class="headerlink" title="6. Droupout2d与BatchNorm2d"></a>6. Droupout2d与BatchNorm2d</h3><ul><li><p>控制过拟合，提升模型的泛化能力</p></li><li><p>CLASS torch.nn.BatchNorm2d (num_features, eps=1e-05, momentum=0.1, affine=True,track_running_stats=True)</p><ul><li><p>要填写的参数<code>几乎只有num_features</code></p></li><li><p><code>输入神经元</code>的个数/<code>上一层特征图</code>的数量</p></li><li><p><strong><em>**</em></strong>不改变feature_map的数量<strong>*</strong>***</p></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = torch.ones(size=(<span class="number">10</span>,<span class="number">3</span>,<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line">conv1 = nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,padding=<span class="number">2</span>)  <span class="comment"># 28</span></span><br><span class="line">bn1 = nn.BatchNorm2d(<span class="number">32</span>) </span><br><span class="line">bn1(conv1(data)).shape <span class="comment">#不会改变feature map的形状 torch.Size([10, 32, 28, 28])</span></span><br></pre></td></tr></table></figure><ul><li><strong>CLASS torch.nn.Dropout2d (p=0.5, inplace=False)</strong><ul><li>Dropout层本身<code>不带有任何需要学习的参数</code>，因此<code>不会影响参数量。</code></li><li>留下的神经元数值会由此除以<code>(1-p)</code></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data = torch.ones(size=(<span class="number">10</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line">conv1 = nn.Conv2d(<span class="number">1</span>,<span class="number">32</span>,<span class="number">5</span>,padding=<span class="number">2</span>)</span><br><span class="line">dp1 = nn.Dropout2d(<span class="number">0.5</span>)</span><br><span class="line">dp1(conv1(data)).shape <span class="comment">#不会改变feature map的形状  torch.Size([10, 32, 28, 28])</span></span><br></pre></td></tr></table></figure><h2 id="二、经典网络"><a href="#二、经典网络" class="headerlink" title="二、经典网络"></a>二、经典网络</h2><h3 id="1-LeNet-5—1998"><a href="#1-LeNet-5—1998" class="headerlink" title="1. LeNet-5—1998"></a>1. LeNet-5—1998</h3><p>LeNet-5是LeNet系列的最终稳定版，它被美国银行用于手写数字识别，该网络有以下特点：</p><ul><li>所有卷积核均为5x5，步长为1</li><li>所有池化方式为平均池化</li><li>所有激活函数采用Sigmoid</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/lenet5&#39;snet.png" alt=""></p><h4 id="1-1-net-py"><a href="#1-1-net-py" class="headerlink" title="1.1 net.py"></a>1.1 net.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个网络模型类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyLeNet5</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 初始化网络</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyLeNet5, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>, padding=<span class="number">2</span>)  <span class="comment"># 6,28,28</span></span><br><span class="line">        self.pool1 = nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)  <span class="comment"># 6,14,14</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)  <span class="comment"># 16,10,10</span></span><br><span class="line">        self.pool2 = nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)  <span class="comment"># 16,5,5</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)  <span class="comment"># 假设有10个类别</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.sigmoid(self.conv1(x))</span><br><span class="line">        x = self.pool1(x)</span><br><span class="line">        x = F.sigmoid(self.conv2(x))</span><br><span class="line">        x = self.pool2(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x = torch.rand((<span class="number">1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>))  <span class="comment"># 或者您可以调整为32x32的输入，如果MNIST数据集已经是28x28，则可以保持这个尺寸</span></span><br><span class="line">    model = MyLeNet5()</span><br><span class="line">    y = model(x)</span><br><span class="line">    <span class="built_in">print</span>(y)  <span class="comment"># 这将打印出未经softmax处理的logits</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="1-2-train-py"><a href="#1-2-train-py" class="headerlink" title="1.2 train.py"></a>1.2 train.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> main <span class="keyword">import</span> MyLeNet5</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换为Tensor格式</span></span><br><span class="line">data_transforms = transforms.Compose([transforms.ToTensor()])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载训练数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>,train=<span class="literal">True</span>,transform=data_transforms,download=<span class="literal">True</span>)</span><br><span class="line">train_dataloader = torch.utils.data.DataLoader(dataset =train_dataset, batch_size = <span class="number">16</span>, shuffle = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载测试数据集</span></span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>,train=<span class="literal">False</span>,transform=data_transforms,download=<span class="literal">True</span>)</span><br><span class="line">test_dataloader = torch.utils.data.DataLoader(dataset =test_dataset, batch_size = <span class="number">16</span>, shuffle = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># GPU训练</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用网络模型</span></span><br><span class="line">model = MyLeNet5().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义交叉熵损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optm = torch.optim.SGD(model.parameters(), lr=<span class="number">1e-2</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率每隔10轮，变为原来的0.1</span></span><br><span class="line">lrs = lr_scheduler.StepLR(optm,step_size=<span class="number">10</span>,gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">dataloader,model,loss_fn,optm</span>):</span><br><span class="line">    loss,accuracy,n = <span class="number">0.0</span>, <span class="number">0.0</span> ,<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batchm,(X,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        X,y = X.to(device),y.to(device)</span><br><span class="line">        output = model(X)</span><br><span class="line">        cur_loss = loss_fn(output,y)</span><br><span class="line">        _,pred  = torch.<span class="built_in">max</span>(output,axis = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        cur_accuracy= torch.<span class="built_in">sum</span>(y==pred)/output.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        optm.zero_grad()</span><br><span class="line">        cur_loss.backward()</span><br><span class="line">        optm.step()</span><br><span class="line"></span><br><span class="line">        loss += cur_loss.item()</span><br><span class="line">        accuracy += cur_accuracy</span><br><span class="line">        n = n+<span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;train_loss&quot;</span> + <span class="built_in">str</span>(loss/n))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;train_acc&quot;</span> + <span class="built_in">str</span>(accuracy / n))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">val</span>(<span class="params">dataloader,model,loss_fn</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    loss,accuracy,n = <span class="number">0.0</span>, <span class="number">0.0</span> ,<span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> batchm, (X, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">            X, y = X.to(device), y.to(device)</span><br><span class="line">            output = model(X)</span><br><span class="line">            cur_loss = loss_fn(output, y)</span><br><span class="line">            _, pred = torch.<span class="built_in">max</span>(output, axis=<span class="number">1</span>)</span><br><span class="line">            cur_accuracy = torch.<span class="built_in">sum</span>(pred == y).item() / y.size(<span class="number">0</span>)</span><br><span class="line">            loss += cur_loss.item()</span><br><span class="line">            accuracy += cur_accuracy</span><br><span class="line">            n = n + <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;val_loss&quot;</span> + <span class="built_in">str</span>(loss / n))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;val_acc&quot;</span> + <span class="built_in">str</span>(accuracy / n))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> accuracy/n</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line">epoch = <span class="number">50</span></span><br><span class="line">min_acc = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;epoch<span class="subst">&#123;t+<span class="number">1</span>&#125;</span>\n-----------&quot;</span>)</span><br><span class="line">    train(train_dataloader,model,loss_fn,optm)</span><br><span class="line">    a = val(test_dataloader,model,loss_fn)</span><br><span class="line">    <span class="comment"># 保存最好的模型权重</span></span><br><span class="line">    <span class="keyword">if</span> a &gt; min_acc:</span><br><span class="line">        folder = <span class="string">&#x27;save_model&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(folder):</span><br><span class="line">            os.mkdir(folder)</span><br><span class="line">        min_acc = a</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;save best model&#x27;</span>)</span><br><span class="line">        torch.save(model.state_dict(),<span class="string">&#x27;save_model/best_model.pth&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Done&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="1-3-test-py"><a href="#1-3-test-py" class="headerlink" title="1.3 test.py"></a>1.3 test.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> main <span class="keyword">import</span> MyLeNet5</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToPILImage</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转化为tensor格式</span></span><br><span class="line">data_transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载训练数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=data_transform, download=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 给训练集创建一个数据加载器, shuffle=True用于打乱数据集，每次都会以不同的顺序返回。</span></span><br><span class="line">train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=<span class="number">16</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 加载训练数据集</span></span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=data_transform, download=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 给训练集创建一个数据加载器, shuffle=True用于打乱数据集，每次都会以不同的顺序返回。</span></span><br><span class="line">test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=<span class="number">16</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#  如果显卡可用，则用显卡进行训练</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用net里面定义的模型，如果GPU可用则将模型转到GPU</span></span><br><span class="line">model = MyLeNet5().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载 train.py 里训练好的模型</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">r&quot;D:\Desktop\QNJS\Model\LeNet-5\save_model\best_model.pth&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取预测结果</span></span><br><span class="line">classes = [</span><br><span class="line">    <span class="string">&quot;0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;1&quot;</span>,</span><br><span class="line">    <span class="string">&quot;2&quot;</span>,</span><br><span class="line">    <span class="string">&quot;3&quot;</span>,</span><br><span class="line">    <span class="string">&quot;4&quot;</span>,</span><br><span class="line">    <span class="string">&quot;5&quot;</span>,</span><br><span class="line">    <span class="string">&quot;6&quot;</span>,</span><br><span class="line">    <span class="string">&quot;7&quot;</span>,</span><br><span class="line">    <span class="string">&quot;8&quot;</span>,</span><br><span class="line">    <span class="string">&quot;9&quot;</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把tensor转成Image， 方便可视化</span></span><br><span class="line">show = ToPILImage()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入验证阶段</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># 对test_dataset里10000张手写数字图片进行推理</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    x, y = test_dataset[i][<span class="number">0</span>], test_dataset[i][<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># tensor格式数据可视化</span></span><br><span class="line">    show(x).show()</span><br><span class="line">    <span class="comment"># 扩展张量维度为4维</span></span><br><span class="line">    x = Variable(torch.unsqueeze(x, dim=<span class="number">0</span>).<span class="built_in">float</span>(), requires_grad=<span class="literal">False</span>).to(device)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        pred = model(x)</span><br><span class="line">        <span class="comment"># 得到预测类别中最高的那一类，再把最高的这一类对应classes中的哪一类标签</span></span><br><span class="line">        predicted, actual = classes[torch.argmax(pred[<span class="number">0</span>])], classes[y]</span><br><span class="line">        <span class="comment"># 最终输出的预测值与真实值</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;predicted: &quot;<span class="subst">&#123;predicted&#125;</span>&quot;, actual:&quot;<span class="subst">&#123;actual&#125;</span>&quot;&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="2-AlexNet—2012"><a href="#2-AlexNet—2012" class="headerlink" title="2. AlexNet—2012"></a>2. AlexNet—2012</h3><ul><li>AlexNet证明了卷积神经网络特征提取远胜于人工特征，将ILSVRC错误率从25.8%降至15.3%。</li><li>输入→(卷积+池化)→(卷积+池化)→(卷积x3+池化)→(线性x3)→输出</li><li>使用小卷积核、多通道和深层网络，引导后续CNN发展。</li><li>采用ReLU激活函数，解决Sigmoid和Tanh的问题。<ul><li><strong>梯度消失问题</strong>：<ul><li>对于 Sigmoid 和 Tanh 函数，当输入值过大或过小时，梯度趋近于零。</li><li>在深层网络中导致权重更新缓慢，学习过程变得低效。</li></ul></li><li><strong>非零中心化问题</strong>：<ul><li>Sigmoid 函数输出严格为正，非零中心化，导致优化过程复杂化。</li><li>Tanh 虽零中心化，但同样受梯度消失影响。</li></ul></li><li><strong>ReLU 激活函数的优势：</strong><ul><li>ReLU 在正数部分梯度恒为 1，避免梯度消失，适用于深层网络。</li><li>ReLU 计算简单，运算效率高于 Sigmoid 和 Tanh。</li><li>ReLU 存在“死亡ReLU”问题（输入负时输出为零），但总体性能优于 Sigmoid 和 Tanh。</li></ul></li></ul></li><li>引入Dropout控制过拟合；图像增强技术扩大数据集，进一步减轻过拟合。</li><li>GPU加速训练，实现有效训练。</li><li>小步幅池化产生重叠区域，有助于缓解过拟合。</li></ul><h4 id="2-1-AlexNet-py"><a href="#2-1-AlexNet-py" class="headerlink" title="2.1 AlexNet.py"></a>2.1 AlexNet.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyAlexNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyAlexNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>)  <span class="comment"># [None, 3, 224, 224] --&gt; [None, 96, 55, 55]</span></span><br><span class="line">        self.pool1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>)  <span class="comment"># [None, 96, 55, 55] --&gt; [None, 96, 27, 27]</span></span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">96</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)  <span class="comment"># [None, 96, 27, 27] --&gt; [None, 256, 27, 27]</span></span><br><span class="line">        self.pool2 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)  <span class="comment"># [None, 256, 27, 27] --&gt; [None, 256, 13, 13]</span></span><br><span class="line">        self.conv3 = nn.Conv2d(in_channels=<span class="number">256</span>, out_channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)  <span class="comment"># [None, 256, 13, 13] --&gt; [None, 384, 13, 13]</span></span><br><span class="line">        self.conv4 = nn.Conv2d(in_channels=<span class="number">384</span>, out_channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)  <span class="comment"># [None, 384, 13, 13] --&gt; [None, 384, 13, 13]</span></span><br><span class="line">        self.conv5 = nn.Conv2d(in_channels=<span class="number">384</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)  <span class="comment"># [None, 384, 13, 13] --&gt; [None, 256, 13, 13]</span></span><br><span class="line">        self.pool3 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)  <span class="comment"># [None, 256, 13, 13] --&gt; [None, 256, 6, 6]</span></span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">256</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">2048</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">2048</span>, <span class="number">2048</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">2048</span>, <span class="number">1000</span>)</span><br><span class="line">        self.fc4 = nn.Linear(<span class="number">1000</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = self.pool1(x)</span><br><span class="line"></span><br><span class="line">        x = F.relu(self.conv2(x))</span><br><span class="line">        x = self.pool2(x)</span><br><span class="line"></span><br><span class="line">        x = F.relu(self.conv3(x))</span><br><span class="line">        x = F.relu(self.conv4(x))</span><br><span class="line">        x = F.relu(self.conv5(x))</span><br><span class="line">        x = self.pool3(x)</span><br><span class="line"></span><br><span class="line">        x = x.view(-<span class="number">1</span>,<span class="number">256</span>*<span class="number">6</span>*<span class="number">6</span>)</span><br><span class="line">        x = F.relu(F.dropout(self.fc1(x), <span class="number">0.5</span>))</span><br><span class="line">        x = F.relu(F.dropout(self.fc2(x), <span class="number">0.5</span>))</span><br><span class="line">        x = F.relu(F.dropout(self.fc3(x), <span class="number">0.5</span>))</span><br><span class="line">        x = self.fc4(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x = torch.rand([<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>])</span><br><span class="line">    model = MyAlexNet()</span><br><span class="line">    summary(model, input_size=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br></pre></td></tr></table></figure><h4 id="2-2-split-data-py"><a href="#2-2-split-data-py" class="headerlink" title="2.2 split_data.py"></a>2.2 split_data.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os  <span class="comment"># 导入os模块，用于处理文件和目录</span></span><br><span class="line"><span class="keyword">from</span> shutil <span class="keyword">import</span> copy  <span class="comment"># 从shutil模块导入copy函数，用于复制文件</span></span><br><span class="line"><span class="keyword">import</span> random  <span class="comment"># 导入random模块，用于生成随机数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个函数，用于创建不存在的目录</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mkfile</span>(<span class="params">file</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(file):</span><br><span class="line">        os.makedirs(file)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取data文件夹下所有子文件夹的名称（即分类的类名）</span></span><br><span class="line">file_path = <span class="string">r&#x27;D:\Desktop\QNJS\Model\AlexNet\data_name&#x27;</span></span><br><span class="line">flower_class = [cla <span class="keyword">for</span> cla <span class="keyword">in</span> os.listdir(file_path)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建训练集 train 文件夹，并在其下为每个类别创建子目录</span></span><br><span class="line">mkfile(<span class="string">&#x27;data/train&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> cla <span class="keyword">in</span> flower_class:</span><br><span class="line">    mkfile(<span class="string">&#x27;data/train/&#x27;</span> + cla)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建验证集 val 文件夹，并在其下为每个类别创建子目录</span></span><br><span class="line">mkfile(<span class="string">&#x27;data/val&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> cla <span class="keyword">in</span> flower_class:</span><br><span class="line">    mkfile(<span class="string">&#x27;data/val/&#x27;</span> + cla)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置划分比例，训练集 : 验证集 = 8 : 2</span></span><br><span class="line">split_rate = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历每个类别并按比例分割成训练集和验证集</span></span><br><span class="line"><span class="keyword">for</span> cla <span class="keyword">in</span> flower_class:</span><br><span class="line">    cla_path = file_path + <span class="string">&#x27;/&#x27;</span> + cla + <span class="string">&#x27;/&#x27;</span>  <span class="comment"># 获取某一类别的完整路径</span></span><br><span class="line">    images = os.listdir(cla_path)  <span class="comment"># 列出该路径下的所有图像</span></span><br><span class="line">    num = <span class="built_in">len</span>(images)  <span class="comment"># 计算图像总数</span></span><br><span class="line">    eval_index = random.sample(images, k=<span class="built_in">int</span>(num * split_rate))  <span class="comment"># 随机选择部分图像作为验证集(1/5)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历该类别的所有图像，进行分类处理</span></span><br><span class="line">    <span class="keyword">for</span> index, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">        <span class="comment"># 如果图像在验证集中，则复制到验证集目录</span></span><br><span class="line">        <span class="keyword">if</span> image <span class="keyword">in</span> eval_index:</span><br><span class="line">            image_path = cla_path + image</span><br><span class="line">            new_path = <span class="string">&#x27;data/val/&#x27;</span> + cla</span><br><span class="line">            copy(image_path, new_path)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 否则，复制到训练集目录</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            image_path = cla_path + image</span><br><span class="line">            new_path = <span class="string">&#x27;data/train/&#x27;</span> + cla</span><br><span class="line">            copy(image_path, new_path)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印处理进度</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\r[&#123;&#125;] processing [&#123;&#125;/&#123;&#125;]&quot;</span>.<span class="built_in">format</span>(cla, index + <span class="number">1</span>, num), end=<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 当一个类别处理完毕后换行</span></span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有类别处理完毕，打印完成信息</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;processing done!&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="2-3-train-py"><a href="#2-3-train-py" class="headerlink" title="2.3 train.py"></a>2.3 train.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入所需的库</span></span><br><span class="line"><span class="keyword">import</span> torch  <span class="comment"># PyTorch 深度学习框架</span></span><br><span class="line"><span class="keyword">import</span> os  <span class="comment"># 用于操作文件和目录</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn  <span class="comment"># PyTorch 的神经网络模块</span></span><br><span class="line"><span class="keyword">from</span> AlexNet <span class="keyword">import</span> MyAlexNet  <span class="comment"># 从 AlexNet 文件导入 MyAlexNet 类</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler  <span class="comment"># 用于调整学习率的工具</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms  <span class="comment"># PyTorch 图像转换工具</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader  <span class="comment"># 用于创建数据加载器的工具</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  <span class="comment"># 用于绘图的库</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> ImageFolder</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> UnidentifiedImageError</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 Matplotlib 绘图时的字体和字符处理</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 设置字体为 SimHei 以支持中文显示</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 正常显示负号</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练数据集和验证数据集的根目录路径</span></span><br><span class="line">ROOT_TRAIN = <span class="string">r&#x27;D:\Desktop\QNJS\Model\AlexNet\data\train&#x27;</span>  <span class="comment"># 训练集路径</span></span><br><span class="line">ROOT_TEST = <span class="string">r&#x27;D:\Desktop\QNJS\Model\AlexNet\data\val&#x27;</span>  <span class="comment"># 验证集路径</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义图像归一化转换</span></span><br><span class="line">normalize = transforms.Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练数据的转换流程</span></span><br><span class="line">train_transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),  <span class="comment"># 调整图像大小为 224x224</span></span><br><span class="line">    transforms.RandomVerticalFlip(),  <span class="comment"># 随机垂直翻转图像</span></span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 将图像转换为 Tensor</span></span><br><span class="line">    normalize])  <span class="comment"># 应用归一化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义验证数据的转换流程</span></span><br><span class="line">val_transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),  <span class="comment"># 调整图像大小为 224x224</span></span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 将图像转换为 Tensor</span></span><br><span class="line">    normalize])  <span class="comment"># 应用归一化</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义自定义的图像数据集类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomImageFolder</span>(<span class="title class_ inherited__">ImageFolder</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">super</span>(CustomImageFolder, self).__getitem__(index)</span><br><span class="line">        <span class="keyword">except</span> UnidentifiedImageError:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Unidentified image at index <span class="subst">&#123;index&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> self[<span class="number">0</span>]  <span class="comment"># 返回数据集中的第一个图像作为替代</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用自定义的图像数据集类来加载训练和验证数据集</span></span><br><span class="line">train_dataset = CustomImageFolder(ROOT_TRAIN, transform=train_transform)</span><br><span class="line">val_dataset = CustomImageFolder(ROOT_TEST, transform=val_transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据加载器</span></span><br><span class="line">train_dataloader = DataLoader(train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">val_dataloader = DataLoader(val_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检测是否有可用的 GPU，否则使用 CPU</span></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化 MyAlexNet 模型并将其移动到设备上（GPU 或 CPU）</span></span><br><span class="line">model = MyAlexNet().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义交叉熵损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器（随机梯度下降）</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义学习率调度器，每10轮降低学习率</span></span><br><span class="line">lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=<span class="number">10</span>, gamma=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">dataloader, model, loss_fn, optimizer</span>):</span><br><span class="line">    loss, current, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch, (x, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        image, y = x.to(device), y.to(device)  <span class="comment"># 将数据移动到相应的设备</span></span><br><span class="line">        output = model(image)  <span class="comment"># 获取模型输出</span></span><br><span class="line">        cur_loss = loss_fn(output, y)  <span class="comment"># 计算损失</span></span><br><span class="line">        _, pred = torch.<span class="built_in">max</span>(output, axis=<span class="number">1</span>)  <span class="comment"># 获取预测结果</span></span><br><span class="line">        cur_acc = torch.<span class="built_in">sum</span>(y == pred) / output.shape[<span class="number">0</span>]  <span class="comment"># 计算准确率</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 清除之前的梯度</span></span><br><span class="line">        cur_loss.backward()  <span class="comment"># 反向传播计算当前的梯度</span></span><br><span class="line">        optimizer.step()  <span class="comment"># 更新模型参数</span></span><br><span class="line">        loss += cur_loss.item()  <span class="comment"># 累加损失</span></span><br><span class="line">        current += cur_acc.item()  <span class="comment"># 累加准确率</span></span><br><span class="line">        n += <span class="number">1</span>  <span class="comment"># 计数增加</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算平均损失和准确率</span></span><br><span class="line">    train_loss = loss / n</span><br><span class="line">    train_acc = current / n</span><br><span class="line">    <span class="comment"># 输出训练的损失和准确率</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;train_loss&#x27;</span> + <span class="built_in">str</span>(train_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;train_acc&#x27;</span> + <span class="built_in">str</span>(train_acc))</span><br><span class="line">    <span class="keyword">return</span> train_loss, train_acc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个验证函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">val</span>(<span class="params">dataloader, model, loss_fn</span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># 设置模型为评估模式</span></span><br><span class="line">    loss, current, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 在不跟踪梯度的情况下执行前向传播</span></span><br><span class="line">        <span class="keyword">for</span> batch, (x, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">            image, y = x.to(device), y.to(device)  <span class="comment"># 将数据移动到相应的设备</span></span><br><span class="line">            output = model(image)  <span class="comment"># 获取模型输出</span></span><br><span class="line">            cur_loss = loss_fn(output, y)  <span class="comment"># 计算损失</span></span><br><span class="line">            _, pred = torch.<span class="built_in">max</span>(output, axis=<span class="number">1</span>)  <span class="comment"># 获取预测结果</span></span><br><span class="line">            cur_acc = torch.<span class="built_in">sum</span>(y == pred) / output.shape[<span class="number">0</span>]  <span class="comment"># 计算准确率</span></span><br><span class="line">            loss += cur_loss.item()  <span class="comment"># 累加损失</span></span><br><span class="line">            current += cur_acc.item()  <span class="comment"># 累加准确率</span></span><br><span class="line">            n += <span class="number">1</span>  <span class="comment"># 计数增加</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算平均损失和准确率</span></span><br><span class="line">    val_loss = loss / n</span><br><span class="line">    val_acc = current / n</span><br><span class="line">    <span class="comment"># 输出验证的损失和准确率</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;val_loss&#x27;</span> + <span class="built_in">str</span>(val_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;val_acc&#x27;</span> + <span class="built_in">str</span>(val_acc))</span><br><span class="line">    <span class="keyword">return</span> val_loss, val_acc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义绘制损失和准确率图的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matplot_loss</span>(<span class="params">train_loss, val_loss</span>):</span><br><span class="line">    plt.plot(train_loss, label=<span class="string">&#x27;train_loss&#x27;</span>)</span><br><span class="line">    plt.plot(val_loss, label=<span class="string">&#x27;val_loss&#x27;</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;训练集和验证集loss值对比图&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">matplot_acc</span>(<span class="params">train_acc, val_acc</span>):</span><br><span class="line">    plt.plot(train_acc, label=<span class="string">&#x27;train_acc&#x27;</span>)</span><br><span class="line">    plt.plot(val_acc, label=<span class="string">&#x27;val_acc&#x27;</span>)</span><br><span class="line">    plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;acc&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;训练集和验证集acc值对比图&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练过程</span></span><br><span class="line">loss_train = []</span><br><span class="line">acc_train = []</span><br><span class="line">loss_val = []</span><br><span class="line">acc_val = []</span><br><span class="line"></span><br><span class="line">epoch = <span class="number">20</span></span><br><span class="line">min_acc = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;epoch<span class="subst">&#123;t + <span class="number">1</span>&#125;</span>\n-----------&quot;</span>)</span><br><span class="line">    train_loss, train_acc = train(train_dataloader, model, loss_fn, optimizer)</span><br><span class="line">    val_loss, val_acc = val(val_dataloader, model, loss_fn)</span><br><span class="line">    lr_scheduler.step()  <span class="comment"># 更新学习率</span></span><br><span class="line"></span><br><span class="line">    loss_train.append(train_loss)</span><br><span class="line">    acc_train.append(train_acc)</span><br><span class="line">    loss_val.append(val_loss)</span><br><span class="line">    acc_val.append(val_acc)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存最好的模型权重</span></span><br><span class="line">    <span class="keyword">if</span> val_acc &gt; min_acc:</span><br><span class="line">        folder = <span class="string">&#x27;save_model&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(folder):</span><br><span class="line">            os.mkdir(<span class="string">&#x27;save_model&#x27;</span>)</span><br><span class="line">        min_acc = val_acc</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;save best model, 第<span class="subst">&#123;t+<span class="number">1</span>&#125;</span>轮&quot;</span>)</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">&#x27;save_model/best_model.pth&#x27;</span>)</span><br><span class="line">    <span class="comment"># 保存最后一轮的权重文件</span></span><br><span class="line">    <span class="keyword">if</span> t == epoch-<span class="number">1</span>:</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">&#x27;save_model/last_model.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制损失和准确率图</span></span><br><span class="line">matplot_loss(loss_train, loss_val)  <span class="comment"># 绘制训练集和验证集的损失对比图</span></span><br><span class="line">matplot_acc(acc_train, acc_val)  <span class="comment"># 绘制训练集和验证集的准确率对比图</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Done!&#x27;</span>)  <span class="comment"># 训练完成后打印完成信息</span></span><br></pre></td></tr></table></figure><h4 id="2-4-test-py"><a href="#2-4-test-py" class="headerlink" title="2.4 test.py"></a>2.4 test.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch  <span class="comment"># 导入 PyTorch 库</span></span><br><span class="line"><span class="keyword">from</span> AlexNet <span class="keyword">import</span> MyAlexNet  <span class="comment"># 从 AlexNet 文件导入 MyAlexNet 类</span></span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable  <span class="comment"># 导入用于创建可微分变量的工具</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms  <span class="comment"># 导入图像数据集和转换工具</span></span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, ToPILImage  <span class="comment"># 导入转换为Tensor和PIL Image的工具</span></span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> ImageFolder  <span class="comment"># 导入处理文件夹数据集的工具</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader  <span class="comment"># 导入创建数据加载器的工具</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练集和验证集的文件路径</span></span><br><span class="line">ROOT_TRAIN = <span class="string">r&#x27;D:\Desktop\QNJS\Model\AlexNet\data\train&#x27;</span></span><br><span class="line">ROOT_TEST = <span class="string">r&#x27;D:\Desktop\QNJS\Model\AlexNet\data\val&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义图像归一化转换</span></span><br><span class="line">normalize = transforms.Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练数据的转换流程</span></span><br><span class="line">train_transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),  <span class="comment"># 调整图像大小为 224x224</span></span><br><span class="line">    transforms.RandomVerticalFlip(),  <span class="comment"># 随机垂直翻转图像</span></span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 将图像转换为 Tensor</span></span><br><span class="line">    normalize])  <span class="comment"># 应用归一化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义验证数据的转换流程</span></span><br><span class="line">val_transform = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">224</span>, <span class="number">224</span>)),  <span class="comment"># 调整图像大小为 224x224</span></span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 将图像转换为 Tensor</span></span><br><span class="line">    normalize])  <span class="comment"># 应用归一化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 ImageFolder 加载训练和验证数据集</span></span><br><span class="line">train_dataset = ImageFolder(ROOT_TRAIN, transform=train_transform)</span><br><span class="line">val_dataset = ImageFolder(ROOT_TEST, transform=val_transform)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据加载器</span></span><br><span class="line">train_dataloader = DataLoader(train_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">val_dataloader = DataLoader(val_dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查是否有可用的 GPU，否则使用 CPU</span></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化 MyAlexNet 模型并将其移动到设备上（GPU 或 CPU）</span></span><br><span class="line">model = MyAlexNet().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载训练好的模型权重</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">r&quot;D:\Desktop\QNJS\Model\AlexNet\save_model\best_model.pth&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义类别</span></span><br><span class="line">classes = [<span class="string">&quot;cat&quot;</span>, <span class="string">&quot;dog&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个将张量转换为 PIL 图像的对象</span></span><br><span class="line">show = ToPILImage()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置模型为评估模式</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 循环10次，获取前10个图像的预测结果</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">13</span>,<span class="number">14</span>):</span><br><span class="line">    x, y = val_dataset[i][<span class="number">0</span>], val_dataset[i][<span class="number">1</span>]  <span class="comment"># 获取图像及其标签</span></span><br><span class="line">    show(x).show()  <span class="comment"># 显示图像</span></span><br><span class="line">    <span class="comment"># 将图像数据转换为模型可以接受的形式</span></span><br><span class="line">    x = Variable(torch.unsqueeze(x, dim=<span class="number">0</span>).<span class="built_in">float</span>(), requires_grad=<span class="literal">True</span>).to(device)</span><br><span class="line">    <span class="comment"># x = torch.tensor(x).to(device)  # 去掉这行</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 在不计算梯度的情况下执行前向传播</span></span><br><span class="line">        pred = model(x)  <span class="comment"># 获取模型预测结果</span></span><br><span class="line">        <span class="comment"># 获取预测和实际类别名称</span></span><br><span class="line">        predicted, actual = classes[torch.argmax(pred[<span class="number">0</span>])], classes[y]</span><br><span class="line">        <span class="comment"># 打印预测和实际类别</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;predicted:&quot;<span class="subst">&#123;predicted&#125;</span>&quot;, Actual:&quot;<span class="subst">&#123;actual&#125;</span>&quot;&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="三、架构对学习能力-鲁棒性的影响"><a href="#三、架构对学习能力-鲁棒性的影响" class="headerlink" title="三、架构对学习能力/鲁棒性的影响"></a>三、架构对学习能力/鲁棒性的影响</h2><ul><li><p>深度学习中的经典架构主要是基本准则，通常无法直接解决具体问题。</p></li><li><p>经典架构可能需大幅修改以适应新数据。</p></li><li>评估模型时考虑效果、效率和可解释性。</li></ul><h3 id="1-深度"><a href="#1-深度" class="headerlink" title="1. 深度"></a>1. 深度</h3><h4 id="1-1-深度与效果"><a href="#1-1-深度与效果" class="headerlink" title="1.1 深度与效果"></a>1.1 <strong>深度与效果</strong></h4><ul><li>深度学习中，更深的网络通常具有更强的学习能力。</li><li>加深模型的层数是提升性能的常用方法。</li></ul><h4 id="1-2-卷积神经网络的深度"><a href="#1-2-卷积神经网络的深度" class="headerlink" title="1.2 卷积神经网络的深度"></a>1.2 卷积神经网络的深度</h4><ul><li>网络深度指带权重层的数量，也是总层数。</li><li>深度越大，参数量和网络规模越大。</li><li>使用“广”来描述特征图数量较多的卷积层。</li></ul><h4 id="1-3-深度的限制因素"><a href="#1-3-深度的限制因素" class="headerlink" title="1.3 深度的限制因素"></a>1.3 深度的限制因素</h4><ul><li>输入图像尺寸限制了网络的最大深度。</li><li>实际可构建的网络深度通常有限。</li></ul><h4 id="1-4-卷积神经网络设计原则"><a href="#1-4-卷积神经网络设计原则" class="headerlink" title="1.4 卷积神经网络设计原则"></a>1.4 卷积神经网络设计原则</h4><ul><li>全连接层(FC)通常较大但层数少，多为1-3层，尺寸不超9x9。</li><li>使用小卷积核(如5x5, 3x3等)以保持信息和减少参数。</li><li>大卷积核可能迅速减少特征图尺寸，限制网络深度。</li><li>池化层的padding应小于池化核尺寸的1/2，否则可能引发错误。</li><li>卷积层的padding通常设小于卷积核尺寸以避免增加无效计算和噪声。</li></ul><h4 id="1-5-特征图尺寸变化策略"><a href="#1-5-特征图尺寸变化策略" class="headerlink" title="1.5  特征图尺寸变化策略"></a>1.5  特征图尺寸变化策略</h4><ul><li><strong>宽高减半或更多</strong>：步长大于2或使用大卷积核时可显著减小特征图。（躺平操作）</li><li><strong>逐层递减架构</strong>：不用池化层，通过卷积层的填充与卷积核尺寸搭配，每次卷积后减小2或4像素。</li><li><strong>重复架构</strong>：通过卷积层保持特征图尺寸不变，将尺寸减小工作交给池化层。</li></ul><h4 id="1-6-架构选择与效率对比"><a href="#1-6-架构选择与效率对比" class="headerlink" title="1.6 架构选择与效率对比"></a>1.6 架构选择与效率对比</h4><ul><li><strong>逐层递减架构</strong>：递减2或4。</li><li><strong>重复架构</strong>：保持特征图尺寸不变，依赖池化层减小尺寸。（3 1/5 2搭配）</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E9%80%92%E5%87%8F%E6%9E%B6%E6%9E%84%E4%B8%8E%E9%87%8D%E5%A4%8D%E6%9E%B6%E6%9E%84.png" alt=""></p><ul><li>“重复”架构优于“递减”架构，因为计算效率高。在Fashion-MNIST上“重复”架构更稳定、表现更好。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/ISLVRC%E7%BD%91%E7%BB%9C%E5%B1%82%E6%95%B0%E5%8E%86%E5%8F%B2.png" alt=""></p><h4 id="1-7-VGGNet与网络深度"><a href="#1-7-VGGNet与网络深度" class="headerlink" title="1.7 VGGNet与网络深度"></a>1.7 VGGNet与网络深度</h4><ul><li>VGGNet强化学习能力，通过多个卷积层增加深度。</li><li>VGGNet优于小型数据集，未获ILSVRC冠军。</li><li>架构分为五块，每次池化划分新块，“n个卷积+1个池化”为一块。</li></ul><h4 id="1-8-深度与效益"><a href="#1-8-深度与效益" class="headerlink" title="1.8 深度与效益"></a>1.8 深度与效益</h4><ul><li>VGG架构多版本：A、B(VGG13)、C、D(VGG16)、E(VGG19)。</li><li>VGG19效果微小提升，深度增加不保证效果提升。</li><li>准确率提升递减，深度和效果关系非线性。</li><li>参数量大，VGG19达1.43亿，对计算资源要求高。</li><li>有效深度提升需降低成本，2017年前达到约220层。</li><li>探索新方法提高深度与性能，而非单纯加深。</li></ul><h4 id="1-9-VGG16的复现—2014"><a href="#1-9-VGG16的复现—2014" class="headerlink" title="1.9 VGG16的复现—2014"></a>1.9 VGG16的复现—2014</h4><ul><li>输入→（卷积x2+池化）x2 →（卷积x3+池化）x3 → FC层x3 →输出</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/VGG16.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VGG16</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># block1</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.pool1 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># block2</span></span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv4 = nn.Conv2d(<span class="number">128</span>, <span class="number">128</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.pool2 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># block3</span></span><br><span class="line">        self.conv5 = nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv6 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv7 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.pool3 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># block4</span></span><br><span class="line">        self.conv8 = nn.Conv2d(<span class="number">256</span>, <span class="number">512</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv9 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv10 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.pool4 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># block5</span></span><br><span class="line">        self.conv11 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv12 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv13 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.pool5 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># FC层</span></span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span>, <span class="number">4096</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>)</span><br><span class="line">        self.linear3 = nn.Linear(<span class="number">4096</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = self.pool1(F.relu(self.conv2(x)))</span><br><span class="line">        x = F.relu(self.conv3(x))</span><br><span class="line">        x = self.pool2(F.relu(self.conv4(x)))</span><br><span class="line">        x = F.relu(self.conv5(x))</span><br><span class="line">        x = F.relu(self.conv6(x))</span><br><span class="line">        x = self.pool3(F.relu(self.conv7(x)))</span><br><span class="line">        x = F.relu(self.conv8(x))</span><br><span class="line">        x = F.relu(self.conv9(x))</span><br><span class="line">        x = self.pool4(F.relu(self.conv10(x)))</span><br><span class="line">        x = F.relu(self.conv11(x))</span><br><span class="line">        x = F.relu(self.conv12(x))</span><br><span class="line">        x = self.pool5(F.relu(self.conv13(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span>)</span><br><span class="line"></span><br><span class="line">        x = F.relu(self.linear1(F.dropout(x, p=<span class="number">0.5</span>)))</span><br><span class="line">        x = F.relu(self.linear2(F.dropout(x, p=<span class="number">0.5</span>)))</span><br><span class="line">        output = F.softmax(self.linear3(x), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">vgg = VGG16()</span><br><span class="line">summary(vgg, input_size=(<span class="number">10</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>), device=<span class="string">&quot;cuda&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="1-10-深度影响效果的三大解释"><a href="#1-10-深度影响效果的三大解释" class="headerlink" title="1.10 深度影响效果的三大解释"></a>1.10 深度影响效果的三大解释</h4><ul><li>深度网络在相同资源下解决复杂问题能力超过浅层网络，能拟合更复杂函数。</li><li>深度网络更可能避免落入大局部最小值，通常表现更佳。</li><li>网络深度增加扩大感受野，通常提升模型效果。</li></ul><h3 id="2-感受野"><a href="#2-感受野" class="headerlink" title="2. 感受野"></a>2. 感受野</h3><h4 id="2-1-认识感受野"><a href="#2-1-认识感受野" class="headerlink" title="2.1 认识感受野"></a>2.1 认识感受野</h4><ul><li>感受野是神经元对应的<code>输入图像</code>区域。</li><li>特征图的每个像素是一个神经元，受限于扫描的<code>原始图像</code>部分。</li></ul><h4 id="2-2-感受野形象俯视"><a href="#2-2-感受野形象俯视" class="headerlink" title="2.2 感受野形象俯视"></a>2.2 感受野形象俯视</h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E6%84%9F%E5%8F%97%E9%87%8E%E5%BD%A2%E8%B1%A1.png" alt=""></p><h4 id="2-3-感受野与架构效率"><a href="#2-3-感受野与架构效率" class="headerlink" title="2.3 感受野与架构效率"></a>2.3 感受野与架构效率</h4><ul><li><strong>感受野扩大</strong>：随网络深度增加，感受野逐渐增大。</li><li><strong>重复架构优势</strong>：通过多次卷积和池化，感受野效率更高。</li><li><strong>池化层作用</strong>：池化层在减小特征图尺寸的同时，加倍扩大感受野。</li><li><strong>重复vs递减架构</strong>：重复架构下感受野增长效率高于递减架构。<ul><li><code>池化层</code>可以让感受野指数级增长</li></ul></li></ul><div class="table-container"><table><thead><tr><th>架构</th><th>进入FC之前，每个神经元上的感受野大小</th><th>ImageNet数据集上TOP5错误率</th></tr></thead><tbody><tr><td>LeNet5</td><td>16</td><td>约25%</td></tr><tr><td>AlexNet</td><td>195</td><td>16.4% (相较LeNet降低约15%以上)</td></tr><tr><td>VGG16</td><td>212</td><td>7.3%</td></tr></tbody></table></div><ul><li>对像素级预测任务，感受野大小至关重要。即保证感受野足够大以提高语义分割和光流估计等任务精确度。</li></ul><h4 id="2-4-感受野的范围和深度"><a href="#2-4-感受野的范围和深度" class="headerlink" title="2.4 感受野的范围和深度"></a>2.4 感受野的范围和深度</h4><ul><li>感受野理论上无上限，可覆盖整个输入图像。</li><li>深度网络中，感受野随深度增加而扩大，边缘神经元感受野可能超出图像。</li><li>在现代深度网络中，<strong>感受野通常远大于输入图像尺寸</strong>。</li></ul><h1 id="ConvNet-Models-and-Their-Receptive-Fields"><a href="#ConvNet-Models-and-Their-Receptive-Fields" class="headerlink" title="ConvNet Models and Their Receptive Fields"></a>ConvNet Models and Their Receptive Fields</h1><div class="table-container"><table><thead><tr><th>ConvNet Model</th><th>Receptive Field (r)</th><th>Effective Stride (S)</th><th>Effective Padding (P)</th><th>Model Year</th></tr></thead><tbody><tr><td>alexnet_v2</td><td>195</td><td>32</td><td>64</td><td>2014</td></tr><tr><td>vgg_16</td><td>212</td><td>32</td><td>90</td><td>2014</td></tr><tr><td>mobilenet_v1</td><td>315</td><td>32</td><td>126</td><td>2017</td></tr><tr><td>mobilenet_v1_075</td><td>315</td><td>32</td><td>126</td><td>2017</td></tr><tr><td>resnet_v1_50</td><td>483</td><td>32</td><td>239</td><td>2015</td></tr><tr><td>inception_v2</td><td>699</td><td>32</td><td>318</td><td>2015</td></tr><tr><td>resnet_v1_101</td><td>1027</td><td>32</td><td>511</td><td>2015</td></tr><tr><td>inception_v3</td><td>1311</td><td>32</td><td>618</td><td>2015</td></tr><tr><td>resnet_v1_152</td><td>1507</td><td>32</td><td>751</td><td>2015</td></tr><tr><td>resnet_v1_200</td><td>1763</td><td>32</td><td>879</td><td>2015</td></tr><tr><td>inception_v4</td><td>2071</td><td>32</td><td>998</td><td>2016</td></tr><tr><td>inception_resnet_v2</td><td>3039</td><td>32</td><td>1482</td><td>2016</td></tr></tbody></table></div><h4 id="2-5-深度卷积网络中的有效感受野"><a href="#2-5-深度卷积网络中的有效感受野" class="headerlink" title="2.5 深度卷积网络中的有效感受野"></a>2.5 深度卷积网络中的有效感受野</h4><ul><li><strong>中心像素点(有效感受野)</strong>：位于图像中心的像素点重叠路径多，对最终特征图影响更大，信息表现更清晰。</li><li><strong>边缘像素点</strong>：像素点位置越靠近图像边缘，对特征图的影响力迅速衰减，显示边缘像素点的影响较小。</li><li>故<strong>集中图像中心信息，用黑边代替边缘，提高模型性能。</strong></li></ul><h4 id="2-6-扩大感受野：膨胀卷积"><a href="#2-6-扩大感受野：膨胀卷积" class="headerlink" title="2.6 扩大感受野：膨胀卷积"></a>2.6 扩大感受野：膨胀卷积</h4><ul><li><p>提升感受野的方法</p><ul><li><strong>加深网络深度</strong>：每增加一个卷积层，感受野线性增加。（参数增加）</li><li><strong>池化层</strong>：利用池化层快速减小特征图尺寸，间接扩大感受野。（特征图尺寸缩减）</li><li><strong>膨胀卷积</strong>：通过膨胀卷积，即空洞卷积，增加感受野覆盖范围。<ul><li>膨胀卷积不改变相邻像素值，与填充不同。</li><li>膨胀率决定计算点周围扩充像素的圈数。</li><li>膨胀卷积在计算点上扩展面积，增加间隙，跳过间隙计算。</li></ul></li></ul></li><li><p>map计算公式</p></li></ul><script type="math/tex; mode=display">H_{out} = \frac{H_{in} + 2P - \text{dilation}[0] \times (K_H - 1) - 1}{S[0]} + 1</script><script type="math/tex; mode=display">W_{out} = \frac{W_{in} + 2P - \text{dilation}[1] \times (K_W - 1) - 1}{S[1]} + 1</script><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/dilation%E5%9B%BE%E4%BE%8B.png" alt=""></p><ul><li>应用<ul><li>膨胀卷积可能导致信息丢失，需评估其在<code>多层网络</code>中的整体效果。</li><li><strong>优势</strong>：膨胀卷积通过扩大感受野，对语义分割等像素级预测任务表现优异。</li><li><strong>局限性</strong>：膨胀卷积可能导致信息损失，尤其是在小物体分割中不太适用，而适合大物体分割。</li></ul></li></ul><h4 id="2-7-感受野的手动计算"><a href="#2-7-感受野的手动计算" class="headerlink" title="2.7 感受野的手动计算"></a>2.7 感受野的手动计算</h4><script type="math/tex; mode=display">r_l = r_{l-1} + (k_l - 1) \cdot \prod_{i=0}^{l-1} s_i</script><ul><li><strong>LeNet</strong></li></ul><div class="table-container"><table><thead><tr><th>Layer</th><th>Parameters &amp; Structure</th><th>Receptive Field Calculation</th><th>Receptive Field Size</th></tr></thead><tbody><tr><td>Original Image</td><td>stride 1</td><td>-</td><td>1</td></tr><tr><td>conv1</td><td>kernel_size 5x5, stride 1</td><td>1+(5-1)*1</td><td>5</td></tr><tr><td>pool1</td><td>kernel_size 2x2, stride 2</td><td>5+(2-1)*(1*1)</td><td>6</td></tr><tr><td>conv2</td><td>kernel_size 5x5, stride 1</td><td>6+(5-1)*(1*1*2)</td><td>14</td></tr><tr><td>pool2</td><td>kernel_size 2x2, stride 2</td><td>14+(2-1)*(1*1*2*1)</td><td>16</td></tr></tbody></table></div><ul><li><strong>AlexNet</strong></li></ul><div class="table-container"><table><thead><tr><th>Layer</th><th>Parameters &amp; Structure</th><th>Receptive Field Calculation</th><th>Receptive Field Size</th></tr></thead><tbody><tr><td>Original Image</td><td>stride 1</td><td></td><td>1</td></tr><tr><td>conv1</td><td>kernel_size 11x11, stride 4</td><td><code>1+(11-1) * 1</code></td><td>11</td></tr><tr><td>pool1</td><td>kernel_size 3x3, stride 2</td><td><code>11+(3-1) * (1 * 4)</code></td><td>19</td></tr><tr><td>conv2</td><td>kernel_size 5x5, stride 1</td><td><code>19+(5-1) * (1 * 4  * 2)</code></td><td>51</td></tr><tr><td>pool2</td><td>kernel_size 3x3, stride 2</td><td><code>51+(3-1) * (1 * 4 * 2 * 1)</code></td><td>67</td></tr><tr><td>conv3</td><td>kernel_size 3x3, stride 1</td><td><code>67+(3-1) * (1 * 4 * 2 * 1 * 2)</code></td><td>99</td></tr><tr><td>conv4</td><td>kernel_size 3x3, stride 1</td><td><code>99+(3-1) * (1 * 4 * 2 * 1 * 2 * 1)</code></td><td>131</td></tr><tr><td>conv5</td><td>kernel_size 3x3, stride 1</td><td><code>131+(3-1) * (1 * 4 * 2 * 1 * 2 * 1 * 1)</code></td><td>163</td></tr><tr><td>pool3</td><td>kernel_size 3x3, stride 2</td><td><code>163+(3-1) * (1 * 4 * 2 * 1 * 2 * 1 * 1 * 1)</code></td><td>195</td></tr></tbody></table></div><h4 id="2-8-感受野的自动计算"><a href="#2-8-感受野的自动计算" class="headerlink" title="2.8 感受野的自动计算"></a>2.8 感受野的自动计算</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch_receptive_field <span class="keyword">import</span> receptive_field</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LeNet5</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 更换为最大池化层，因为torch_receptive_field库只认最大池化层，其他池化层一律报错</span></span><br><span class="line">        self.pool1 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool2 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># FC层是不参与感受野计算的，因此torch_receptive_field不接受FC层的输入</span></span><br><span class="line">        <span class="comment"># 因此在测试架构时，你可以直接不写fc层</span></span><br><span class="line">        <span class="comment"># self.fc1 = nn.Linear(16*5*5,120)</span></span><br><span class="line">        <span class="comment"># self.fc2 = nn.Linear(120,84)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.tanh(self.conv1(x))</span><br><span class="line">        x = self.pool1(x)</span><br><span class="line">        x = F.tanh(self.conv2(x))</span><br><span class="line">        x = self.pool2(x)</span><br><span class="line">        <span class="comment"># 任何关于fc层的计算也需要被注释掉</span></span><br><span class="line">        <span class="comment"># x = x.view(-1,16*5*5)</span></span><br><span class="line">        <span class="comment"># x = F.tanh(self.fc1(x))</span></span><br><span class="line">        <span class="comment"># output = F.softmax(self.fc2(x),dim=1)</span></span><br><span class="line">        <span class="comment"># output = F.softmax(x.view(-1, 16 * 5 * 5), dim=1)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = LeNet5().cuda()</span><br><span class="line">receptive_field_dict = receptive_field(net, (<span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>))  <span class="comment"># 输入数据的结构注意不要写错</span></span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E6%84%9F%E5%8F%97%E9%87%8E%E8%87%AA%E5%8A%A8%E8%AE%A1%E7%AE%97%E7%BB%93%E6%9E%9C.png" alt=""></p><ul><li><code>torch_receptive_field</code> 只支持 <code>Conv2d</code> 和 <code>MaxPool2d</code> 层，不支持全连接层和平均池化层。使用不支持的层会报错。</li></ul><h3 id="3-平移不变性"><a href="#3-平移不变性" class="headerlink" title="3. 平移不变性"></a>3. 平移不变性</h3><h4 id="3-1-不变性定义"><a href="#3-1-不变性定义" class="headerlink" title="3.1 不变性定义"></a>3.1 不变性定义</h4><p>识别对象不受其在图像中变化的姿态影响。</p><ul><li>池化层虽不能确保完全的平移不变性，从感受野的角度，却能有效减少特征图尺寸和模型参数。</li></ul><h4 id="3-2-基础不变性"><a href="#3-2-基础不变性" class="headerlink" title="3.2 基础不变性"></a>3.2 基础不变性</h4><ul><li><strong>平移不变性</strong>：识别图像中任何位置的对象。</li><li><strong>旋转/视野不变性</strong>：识别任何角度或视角下的对象。</li><li><strong>尺寸不变性</strong>：识别不同大小的相同对象。</li><li><strong>明度不变性</strong>：识别在各种光照条件下的对象。</li><li><strong>镜面不变性</strong>：识别镜像反转后的对象。</li><li><strong>颜色不变性</strong>：识别颜色变化后的对象。</li></ul><h4 id="3-3-CNN中的平移不变性"><a href="#3-3-CNN中的平移不变性" class="headerlink" title="3.3 CNN中的平移不变性"></a>3.3 CNN中的平移不变性</h4><ul><li><p><strong>基本概念</strong>：CNN能识别微小平移后的图像对象。 </p></li><li><p><strong>池化层作用</strong>：筛选出关键信息，提供平移不变性。 </p></li><li><strong>深度与不变性</strong>：更深的网络有更强的平移不变性，但对大范围平移适应性较差。</li><li><strong>结构影响</strong>：卷积与池化的组合增强了平移不变性和模型鲁棒性。</li></ul><h4 id="3-4-数据增强"><a href="#3-4-数据增强" class="headerlink" title="3.4 数据增强"></a>3.4 数据增强</h4><ul><li><strong>手段</strong>：旋转、模糊、调高饱和度、放大/缩小、调高亮度、变形、镜面、镜面旋转、去纹理化、去颜色化、边缘增强、显著边缘化（边缘检测）</li><li><strong>数据增强的目的</strong>：通过生成变化图像扩大训练集，提高模型对“旋转”和“镜像”数据的识别能力。<ul><li>平移不变性通常增强模型性能，但在<code>像素级任务</code>中可能造成<code>灾难</code>。</li></ul></li></ul><h2 id="四、架构对参数量-计算量的影响"><a href="#四、架构对参数量-计算量的影响" class="headerlink" title="四、架构对参数量/计算量的影响"></a>四、架构对参数量/计算量的影响</h2><div class="table-container"><table><thead><tr><th>操作</th><th>参数增加方式</th><th>影响因素</th></tr></thead><tbody><tr><td>全连接层</td><td>直接参数</td><td>权重和偏置</td></tr><tr><td>卷积层</td><td>直接和间接参数</td><td>卷积核尺寸、数量（padding和stride不影响参数量)</td></tr><tr><td>池化</td><td>间接参数</td><td>特征图尺寸变化</td></tr><tr><td>激活函数</td><td>无参数</td><td>-</td></tr><tr><td>Dropout</td><td>无参数</td><td>-</td></tr></tbody></table></div><h3 id="4-1-卷积层"><a href="#4-1-卷积层" class="headerlink" title="4.1 卷积层"></a>4.1 卷积层</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">conv1 = nn.Conv2d(<span class="number">3</span>,<span class="number">6</span>,<span class="number">3</span>) <span class="comment">#(3 * 3 * 3)*6 + 6</span></span><br><span class="line">conv2 = nn.Conv2d(<span class="number">6</span>,<span class="number">4</span>,<span class="number">3</span>) <span class="comment">#(3 * 3 * 6)*4 + 4</span></span><br><span class="line"><span class="comment">#检查一下结果</span></span><br><span class="line">conv1.weight.numel()  <span class="comment"># 162 = (3 * 3 * 3)*6</span></span><br><span class="line">conv1.bias.numel()  <span class="comment"># 6</span></span><br><span class="line">conv2.weight.numel()  <span class="comment"># 216 = (3 * 3 * 6)*4</span></span><br><span class="line">conv2.bias.numel()  <span class="comment"># 4</span></span><br><span class="line"></span><br><span class="line">conv3 = nn.Conv2d(<span class="number">4</span>,<span class="number">16</span>,<span class="number">5</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>)  <span class="comment"># (5*5*4)*16 + 16</span></span><br><span class="line">conv4 = nn.Conv2d(<span class="number">16</span>,<span class="number">3</span>,<span class="number">5</span>,stride=<span class="number">3</span>,padding=<span class="number">2</span>)  <span class="comment"># (5*5*16)*3 + 3</span></span><br><span class="line"><span class="built_in">print</span>(conv3.weight.numel())  <span class="comment"># 1600 = (5*5*4)*16</span></span><br><span class="line"><span class="built_in">print</span>(conv3.bias.numel())  <span class="comment"># 16</span></span><br><span class="line"><span class="built_in">print</span>(conv4.weight.numel())  <span class="comment"># 1200 = (5*5*16)*3</span></span><br><span class="line"><span class="built_in">print</span>(conv4.bias.numel())  <span class="comment"># 3</span></span><br></pre></td></tr></table></figure><ul><li>虽然实际使用的卷积核尺寸较小，但高输出数量使得参数量迅速增加。</li><li>特征图随网络加深而减小，数量会增加，尤其是在池化层后加倍。</li><li>可以减少卷积和池化的组合数量（即减少网络深度），或者在网络初始层使用较少的特征图数量，如32。</li></ul><h3 id="4-2-大尺寸卷积核vs小尺寸卷积核"><a href="#4-2-大尺寸卷积核vs小尺寸卷积核" class="headerlink" title="4.2 大尺寸卷积核vs小尺寸卷积核"></a>4.2 大尺寸卷积核vs小尺寸卷积核</h3><ul><li>2个3x3卷积核=1个5x5卷积核（感受野、feature map的角度）</li></ul><div class="table-container"><table><thead><tr><th>Convolution Layer</th><th>Kernel Size</th><th>Calculation for 3x3 Kernel, 2 Layers</th><th>Calculation for 5x5 Kernel, 1 Layer</th></tr></thead><tbody><tr><td>conv1(64,64,3)</td><td>3x3</td><td>3 <em> 3 </em> 64 * 64 + 64 = 36,928</td><td>5 <em> 5 </em> 64 * 64 + 64 = 102,464</td></tr><tr><td>conv2(64,64,3)</td><td>3x3</td><td>3 <em> 3 </em> 64 * 64 + 64 = 36,928</td><td></td></tr><tr><td><strong>Total</strong></td><td></td><td>73,856</td><td>102,464</td></tr></tbody></table></div><ul><li>选择<code>小卷积核</code>的理由：参数变少了、网络变深了，让提取出的特征信息更“抽象”、更“复杂”。</li></ul><h3 id="4-3-1x1卷积核"><a href="#4-3-1x1卷积核" class="headerlink" title="4.3 1x1卷积核"></a>4.3 1x1卷积核</h3><ul><li>1x1卷积核下的<strong>参数量</strong>为：</li></ul><script type="math/tex; mode=display">N_{parameters} = C_{in} \times C_{out} + C_{out}</script><ul><li><strong>优势</strong><ul><li>1x1卷积核的<strong>参数量大幅减少</strong></li><li><strong>不改变特征图尺寸</strong>的前提下进行特征转换</li><li>能够有效地将特征图中的<strong>“位置信息”</strong>传递到下一层</li></ul></li><li><strong>作用</strong>：加深CNN的深度</li></ul><h3 id="NiN网络—2014迟于VGG"><a href="#NiN网络—2014迟于VGG" class="headerlink" title="NiN网络—2014迟于VGG"></a>NiN网络—2014迟于VGG</h3><ul><li><strong>MLP layer</strong> ： 1x1的卷积层<ul><li>用在卷积层之间，调整通道数，减少参数和计算量，有助于构建更深的网络。</li></ul></li><li>3x3conv+2<em>MLP = block </em>3 = 9 layers</li><li><strong>瓶颈设计</strong>：在核尺寸为1x1的2个卷积层之间包装其他卷积层（右侧）<ul><li>只会出现在超过100层的深度网络中</li><li>几乎不会造成信息损失，带来骤减的参数量</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E7%93%B6%E9%A2%88%E8%AE%BE%E8%AE%A1.png" alt=""></p><div class="table-container"><table><thead><tr><th></th><th>1个3x3卷积，输出256（普通卷积）</th><th></th><th>2个1x1卷积层， 1个3x3卷积层（瓶颈设计）</th></tr></thead><tbody><tr><td>conv(256,256,3)</td><td><code>3 * 3 * 256 * 256 + 256 = 590,080</code></td><td>conv(256,32,1)</td><td>256 * 32 + 32 = 8,224</td></tr><tr><td></td><td></td><td>conv(32,32,3)</td><td>3 <em> 3 </em> 32 * 32 + 32 = 9,248</td></tr><tr><td></td><td></td><td>conv(32,256,1)</td><td>32 * 256 + 256 = 8,448</td></tr><tr><td></td><td>590,080 (59w)</td><td></td><td>25,920 (2w)</td></tr></tbody></table></div><h3 id="4-4-减少参数量：分组卷积与深度分离卷积"><a href="#4-4-减少参数量：分组卷积与深度分离卷积" class="headerlink" title="4.4 减少参数量：分组卷积与深度分离卷积"></a>4.4 减少参数量：分组卷积与深度分离卷积</h3><ul><li><strong>分组卷积</strong></li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E5%88%86%E7%BB%84%E5%8D%B7%E7%A7%AF%E5%9B%BE%E4%BE%8B.png" alt=""></p><script type="math/tex; mode=display">\begin{align*}\text{group1} &= \left( K_H \times K_W \times \frac{C_{in}}{g} \right) \times \frac{C_{out}}{g} + \frac{C_{out}}{g} \\\text{group2} &= \left( K_H \times K_W \times \frac{C_{in}}{g} \right) \times \frac{C_{out}}{g} + \frac{C_{out}}{g} \\\text{total} &= \text{group1} + \text{group2} \\&= \left( \left( K_H \times K_W \times \frac{C_{in}}{g} \right) \times \frac{C_{out}}{g} + \frac{C_{out}}{g} \right) \times g \\&= \frac{1}{g} \left( K_H \times K_W \times C_{in} \times C_{out} \right) + C_{out}\end{align*}</script><blockquote><p> <strong>分组的存在不影响偏置，偏置只与输出的特征图数量有关。</strong></p><ul><li>AlexNet包含groups=2的分组卷积</li><li>map_num一般为偶数，故groups也为偶数</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">conv1 = nn.Conv2d(<span class="number">4</span>,<span class="number">8</span>,<span class="number">3</span>) <span class="comment">#(3 * 3 * 4)*8 + 8 = 296</span></span><br><span class="line">conv1_ = nn.Conv2d(<span class="number">4</span>,<span class="number">8</span>,<span class="number">3</span>,groups=<span class="number">2</span>) <span class="comment"># ((3 * 3 * 4)*8)/2 + 8 = 152</span></span><br><span class="line"><span class="comment">#检查一下结果</span></span><br><span class="line">conv1.weight.numel()  <span class="comment"># 288</span></span><br><span class="line">conv1.bias.numel()  <span class="comment"># 8</span></span><br><span class="line">conv1_.weight.numel()  <span class="comment"># 144</span></span><br><span class="line">conv1_.bias.numel()  <span class="comment"># 8</span></span><br><span class="line"><span class="comment">#如果输入了奇数group呢？</span></span><br><span class="line">conv2 = nn.Conv2d(<span class="number">4</span>,<span class="number">8</span>,<span class="number">3</span>,groups=<span class="number">3</span>)  <span class="comment"># 直接报错</span></span><br></pre></td></tr></table></figure><ul><li><strong>深度卷积(groups = Cin)</strong></li></ul><script type="math/tex; mode=display">\text{parameters} = \frac{1}{g} (K_H \times K_W \times C_{\text{in}} \times C_{\text{out}}) + C_{\text{out}}\text{其中 } g = C_{\text{in}}, \text{则有}:\text{parameters} = K_H \times K_W \times C_{\text{out}} + C_{\text{out}}</script><ul><li><strong>深度可分离卷积(groups = Cin 且 1x1)</strong></li></ul><script type="math/tex; mode=display">一个Block的参数如下：Parameter = K_H \times K_W \times C_{\text{depth\_out}} + C_{\text{pair\_in}} \times C_{\text{pair\_out}}</script><blockquote><p>是谷歌的深度学习模型GoogLeNet进化版中非常关键的block</p><ul><li>帮助卷积层减少参数量</li><li>削弱特征图与特征图之间的 联系来控制过拟合</li></ul></blockquote><hr><h3 id="4-5-全连接层"><a href="#4-5-全连接层" class="headerlink" title="4.5 全连接层"></a>4.5 全连接层</h3><h4 id="4-5-1-全连接层的作用"><a href="#4-5-1-全连接层的作用" class="headerlink" title="4.5.1 全连接层的作用"></a>4.5.1 全连接层的作用</h4><ul><li>作为分类器，实现对数据的分类。</li></ul><blockquote><p>本质上来说，卷积层提供了一系列有意义且稳定的特征值，构成了一个 与输入图像相比维数更少的特征空间，而全连接层负责学习这个空间上的（可能是非线性的）函数关 系，并输出预测结果。</p></blockquote><ul><li>作为整合信息的工具，将特征图中的信息进行整合。</li></ul><blockquote><p>若使用“特征图的不同区域”的信息来进行类别划分，可能会造成 “只有某个区域的数据参与了某个标签的预测”的情况。在进行预测之前，将所有可能的信息充分混合、 进行学习，对预测效果有重大的意义。</p></blockquote><ul><li>全连接层的存在让CNN整体变得更容易过拟合。</li></ul><blockquote><p>dropout、batch normalization现在的CNN架构已经不太容易过拟合了。</p></blockquote><ul><li>令人头疼的参数量问题</li></ul><h4 id="4-5-2-全连接层的问题"><a href="#4-5-2-全连接层的问题" class="headerlink" title="4.5.2 全连接层的问题"></a>4.5.2 全连接层的问题</h4><ul><li>在卷积-FC连接中，通常FC的输出神经元个数不会少于输入的通道数。</li><li><p>如何确定最后一个池化或卷积层输出特征图的尺寸？</p><ul><li><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/%E6%9E%B6%E6%9E%84%E5%9B%BE%E6%9C%AA%E7%9F%A5%E4%B8%8B%E7%A1%AE%E5%AE%9A%E8%BE%93%E5%87%BA%E5%B0%BA%E5%AF%B8.png" alt=""></p></li><li><p><strong>方法一</strong>：将Model中所有的<strong>线性层都注释掉</strong>，只留下卷积层，然后将model输入<strong>summary</strong>进行计算。</p><ul><li><p><strong>方法二</strong>：<strong>nn.Sequential</strong></p><ul><li>```python<br>import torch<br>import torch.nn as nn<br>import torch.nn.functional as F<br>from torchinfo import summary </li></ul></li></ul></li></ul></li></ul><pre><code>    data = torch.ones(size=(10,3,229,229))    #不使用类，直接将需要串联的网络、函数等信息写在一个“序列”里    #重现上面的4个卷积层、2个池化层的架构    net = nn.Sequential(nn.Conv2d(3,6,3)                       ,nn.ReLU(inplace=True)                       ,nn.Conv2d(6,4,3)                       ,nn.ReLU(inplace=True)                       ,nn.MaxPool2d(2)                       ,nn.Conv2d(4,16,5,stride=2,padding=1)                       ,nn.ReLU(inplace=True)                       ,nn.Conv2d(16,3,5,stride=3,padding=2)                       ,nn.ReLU(inplace=True)                       ,nn.MaxPool2d(2)                       )    #nn.Sequential组成的序列不是类，因此不需要实例化，可以直接输入数据    print(net(data).shape)  # torch.Size([10, 3, 9, 9])    <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">- ```python</span><br><span class="line">  from torch_receptive_field import receptive_field</span><br><span class="line">  </span><br><span class="line">  #net不是类所以不需要实例化,查看感受野</span><br><span class="line">  rfdict = receptive_field(net.cuda(),(3,229,229))</span><br></pre></td></tr></table></figure>- 实例分析：  - ```python    class VGG16(nn.Module):        def __init__(self):            super().__init__()            self.features_ = nn.Sequential(nn.Conv2d(3,64,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.Conv2d(64,64,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.MaxPool2d(2)                                           ,nn.Conv2d(64,128,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.Conv2d(128,128,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.MaxPool2d(2)                                           ,nn.Conv2d(128,256,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.Conv2d(256,256,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.Conv2d(256,256,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.MaxPool2d(2)                                           ,nn.Conv2d(256,512,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.MaxPool2d(2)                                           ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.Conv2d(512,512,3,padding=1),nn.ReLU(inplace=True)                                           ,nn.MaxPool2d(2)                                          )            self.clf_ = nn.Sequential(nn.Dropout(0.5)                                      ,nn.Linear(512*7*7,4096),nn.ReLU(inplace=True)                                      ,nn.Dropout(0.5)                                      ,nn.Linear(4096,4096),nn.ReLU(inplace=True)                                      ,nn.Linear(4096,1000),nn.Softmax(dim=1)                                     )        def forward(self,x):            x = self.features_(x) #用特征提取的架构提取特征            x = x.view(-1,512*7*7) #调整数据结构，拉平数据            output = self.clf_(x)            return output    <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">      - 常常会使用 `nn.Sequential`来**调试卷积层架构**，并**不断查看感受野**的变化。</span><br><span class="line"></span><br><span class="line">#### 4.5.3 代替全连接层：1x1卷积核与全局平均池化（GAP）</span><br><span class="line"></span><br><span class="line">&gt; 在计算机视觉中，不包含全连接层，只有卷积层和池化层的卷积网络被叫做全卷积网络(FCN)</span><br><span class="line"></span><br><span class="line">- CNN中，只要让特征图的尺寸为1x1，卷积核的尺寸也为1x1，就可以实现和普通全连接层一样的计算。</span><br><span class="line">- 特点：</span><br><span class="line">  - 1）使用1x1卷积层代替全连接层**不能减少参数量**</span><br><span class="line">  - 2）输入层对图像尺寸的**解放了限制**</span><br><span class="line"></span><br><span class="line">- 应用：目标检测中的滑窗识别</span><br><span class="line"></span><br><span class="line">- NiN中，用来替代全连接层的是`GAP层`</span><br><span class="line"></span><br><span class="line">  - ```python</span><br><span class="line">    data = torch.ones(10,7,7)</span><br><span class="line">    gap = nn.AvgPool2d(7)</span><br><span class="line">    gap(data).shape  # (10,1,1)</span><br></pre></td></tr></table></figure></code></pre><ul><li>GAP作为池化层，<strong>没有任何需要学习的参数</strong>，这让GAP的<strong>抗过拟合能力更强。</strong></li></ul><h4 id="4-5-4-NiN—2014（VGG之后）"><a href="#4-5-4-NiN—2014（VGG之后）" class="headerlink" title="4.5.4 NiN—2014（VGG之后）"></a>4.5.4 NiN—2014（VGG之后）</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NiN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.block1 = nn.Sequential(nn.Conv2d(<span class="number">3</span>,<span class="number">192</span>,<span class="number">5</span>,padding=<span class="number">2</span>),nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                    ,nn.Conv2d(<span class="number">192</span>,<span class="number">160</span>,<span class="number">1</span>),nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                    ,nn.Conv2d(<span class="number">160</span>,<span class="number">96</span>,<span class="number">1</span>),nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                    ,nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>)</span><br><span class="line">                                    ,nn.Dropout(<span class="number">0.25</span>))</span><br><span class="line">        self.block2 = nn.Sequential(nn.Conv2d(<span class="number">96</span>,<span class="number">192</span>,<span class="number">5</span>,padding=<span class="number">2</span>),nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                    ,nn.Conv2d(<span class="number">192</span>,<span class="number">192</span>,<span class="number">1</span>),nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                    ,nn.Conv2d(<span class="number">192</span>,<span class="number">192</span>,<span class="number">1</span>),nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                    ,nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>)</span><br><span class="line">                                    ,nn.Dropout(<span class="number">0.25</span>))</span><br><span class="line">        self.block3 = nn.Sequential(nn.Conv2d(<span class="number">192</span>,<span class="number">192</span>,<span class="number">3</span>,padding=<span class="number">1</span>),nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                    ,nn.Conv2d(<span class="number">192</span>,<span class="number">192</span>,<span class="number">1</span>),nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                    ,nn.Conv2d(<span class="number">192</span>,<span class="number">10</span>,<span class="number">1</span>),nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                    ,nn.AvgPool2d(<span class="number">7</span>,stride=<span class="number">1</span>)</span><br><span class="line">                                    ,nn.Softmax(dim=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        output = self.block3(self.block2(self.block1(x)))</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line">    net= NiN()</span><br><span class="line">    net(data).shape <span class="comment">#10个特征图，每个特征图尺寸是1x1</span></span><br></pre></td></tr></table></figure><ul><li>贡献了1x1卷积层的用途</li><li>GoogLeNet以及ResNet都使用了1x1网络</li></ul><h3 id="五、前沿网络-state-of-the-art-models"><a href="#五、前沿网络-state-of-the-art-models" class="headerlink" title="五、前沿网络 state-of-the-art models"></a>五、前沿网络 state-of-the-art models</h3><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/sota-level.png" alt=""></p><div class="table-container"><table><thead><tr><th>网络模型</th><th>核心特点</th><th>关键创新</th><th></th></tr></thead><tbody><tr><td>LeNet</td><td>早期成功的神经网络，由卷积层块和全连接层块组成，使用卷积层学习空间信息，通过池化层降低敏感度</td><td>卷积层后接最大池化层的基本模板</td><td>输入→(卷积+池化)→(卷积+池化)→(线性x2)→输出</td></tr><tr><td>AlexNet</td><td>更大更深的网络，引入ReLU激活函数和dropout方法控制复杂度，以及数据增强技术<em>**</em></td><td>ReLU、dropout、最大池化、数据增强</td><td>输入→(卷积+池化)→(卷积+池化)→(卷积x3+池化)→(线性x3)→输出</td></tr><tr><td>VGG</td><td>使用重复的卷积层后接一个最大池化层的模板，增加了网络深度<em>**</em></td><td>卷积块的标准化设计</td><td>输入→（卷积x2+池化）x2 →（卷积x3+池化）x3 → FC层x3 →输出</td></tr><tr><td>NiN</td><td>引入1x1卷积核（网络中的网络），使用全局平均池化层直接用于分类</td><td>1x1卷积核，全局平均池化层替代全连接层</td><td>输入→（卷积x3+池化）x3→输出</td></tr><tr><td>GoogLeNet</td><td>含有并行连接的Inception块，解决不同大小卷积核组合的问题</td><td>Inception块，减少参数量同时保持性能</td><td></td></tr><tr><td>ResNet</td><td>引入残差学习框架以训练更深的网络，使用跳跃连接（skip connection）</td><td>残差块，跳跃连接，深度可达千层以上</td></tr></tbody></table></div><h4 id="5-1-GoogLeNet（Inception-V1）基础"><a href="#5-1-GoogLeNet（Inception-V1）基础" class="headerlink" title="5.1 GoogLeNet（Inception V1）基础"></a>5.1 GoogLeNet（Inception V1）基础</h4><ul><li><strong>背景</strong>：<ul><li>1）从2014年的竞赛结果来看，Inception V1的效果只比VGG19好一点点（只比VGG降低了0.6%的错误率），但Inception展现出比VGG强大许多的潜力。</li><li>2）自从LeNet5定下了卷积、池化、线性层串联的基本基调，研究者们在相当长 的一段时间内都在这条道路上探索，最终抵达的终点就是VGG。（参数过多、连接稠密、计算量大、容易过拟合）</li></ul></li><li><p><strong>困难</strong>：</p><ul><li>1）在神经网络由稠密变得稀疏（Sparse）的过程中，网络的<strong>学习能力会波动甚至会下降</strong></li><li>2）现代硬件<strong>不擅长处理在随机或非均匀稀疏的数据</strong>上的计算，并且这种不擅长在矩阵计算上表现得尤其明显。</li></ul></li><li><p><strong>权衡</strong>：</p><ul><li><strong>稠密结构</strong>的学习能力更强，但会因为参数量过于巨大而难以训练。</li><li><strong>稀疏结构</strong>的参数量少，但是学习能力会变得不稳定，并且不能很好地利用现有计算资源。</li></ul></li><li><p><strong>思路</strong>：普通卷积、池化层稠密元素组的块无限逼近一个稀疏架构，构造参数量与稀疏网络相似的稠密网络。</p></li></ul><h4 id="5-2-GoogLeNet（Inception-V1）分析"><a href="#5-2-GoogLeNet（Inception-V1）分析" class="headerlink" title="5.2 GoogLeNet（Inception V1）分析"></a>5.2 GoogLeNet（Inception V1）分析</h4><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Inception-V1.png" alt=""></p><div class="table-container"><table><thead><tr><th>branch</th><th>Dispute</th><th>Advantage</th></tr></thead><tbody><tr><td>branch 1</td><td>conv 1x1</td><td>降低通道数</td></tr><tr><td>branch 2</td><td>conv 1x1 +conv 3x3</td><td>1x1卷积核降低通道数以此来降低参数量和计算量,再使用3x3卷积核进行特征提取</td></tr><tr><td>branch 3</td><td>conv 1x1 +conv 5x5</td><td>基本思路与第二条线路一致</td></tr><tr><td>branch 4</td><td>pool 3x3 +conv 1x1</td><td>池化当做一种特征提取的方式，并在池化后使用1x1卷积层来降低通道数。</td></tr></tbody></table></div><blockquote><p><strong>所有的线路都使用 了巧妙的参数组合，让特征图的尺寸保持不变</strong></p><p>稠密链接，没有分组或dropout造成的空隙</p></blockquote><ul><li><p>1）同时使用多种卷积核确保各种类型和层次的信息都被提取出来</p><ul><li><strong>1x1卷积核</strong>可以最大程度提取像素与像素之间的<strong>位置信息</strong></li><li><strong>尺寸较大的卷积核</strong>可以提取相邻像素之间的<strong>联系信息</strong></li><li><strong>最大池化层</strong>可以提取出<strong>局部中最关键的信息提取特征</strong></li><li>外面的池化层是用来减半特征图的</li></ul></li><li><p>2）并联的卷积池化层计算效率更高。</p></li><li>3）大量使用1x1卷积层来整合信息，既实现了“聚类信息”又实现了大规模降低参数量，让特征图数量实现了前所未有的增长。<ul><li>调整特征图数目变少，信息更加密集</li><li>后都跟ReLu，增加非线性变换</li><li>控制整体参数量、解放了特征图的数量</li></ul></li></ul><blockquote><p>VGG16最大k=3，其一参数为230万个，使得最大通道为512；Inception最大k=5，其一参数为45万个，使得最大通道为1024</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Google-Net.png" alt=""></p><ul><li><strong>总结</strong>：<ul><li>Inception内部是稠密部件的并联，而整个GoogLeNet则是数个 Inception块与传统卷积结构的串联。</li><li>inception前有两个blocks：conv+pool</li><li>Inception实际上取代了传统架构中卷积层的地位,因此网络总体有22层，比VGG19多了三层。</li><li>架构的最后，使用实际上一个用来替代全连接层的全局平均池化层</li><li>GoogLeNet还使用了“辅助分类器”，这两个分类器的输入分别是 inception4a和inception4d的输出结果，他们的结构如下：</li><li><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GoogleNet-%E8%BE%85%E5%8A%A9%E5%88%86%E7%B1%BB%E5%99%A8.png" alt=""><ul><li>加重在训练过程中中层inceptions输出结果的权重</li><li>一个GoogLeNet实际上集成了两个浅层网络和一个深层网络的结果来进 行学习和判断，在一个架构中间增加集成的思想</li></ul></li></ul></li></ul><h4 id="5-3-GoogLeNet（Inception-V1）复现"><a href="#5-3-GoogLeNet（Inception-V1）复现" class="headerlink" title="5.3 GoogLeNet（Inception V1）复现"></a>5.3 GoogLeNet（Inception V1）复现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicConv2d</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_channels, out_channels,**kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#  打包</span></span><br><span class="line">        self.conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, bias=<span class="literal">False</span>, **kwargs)</span><br><span class="line">                                 ,nn.BatchNorm2d(out_channels)</span><br><span class="line">                                 ,nn.ReLU(inplace=<span class="literal">True</span>))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span></span><br><span class="line"><span class="params">                 ,in_channels : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch1x1 : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch3x3red : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch3x3 : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch5x5red : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch5x5 : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,pool_proj : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#1x1</span></span><br><span class="line">        self.branch1 = BasicConv2d(in_channels,ch1x1,kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#1x1 + 3x3</span></span><br><span class="line">        self.branch2 = nn.Sequential(BasicConv2d(in_channels, ch3x3red, kernel_size=<span class="number">1</span>)</span><br><span class="line">                                     ,BasicConv2d(ch3x3red, ch3x3, kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>))</span><br><span class="line">        <span class="comment">#1x1 + 5x5</span></span><br><span class="line">        self.branch3 = nn.Sequential(BasicConv2d(in_channels, ch5x5red, kernel_size=<span class="number">1</span>)</span><br><span class="line">                                     ,BasicConv2d(ch5x5red, ch5x5, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>))</span><br><span class="line">        <span class="comment">#pool + 1x1</span></span><br><span class="line">        self.branch4 = nn.Sequential(nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>, padding=<span class="number">1</span>,ceil_mode=<span class="literal">True</span>)</span><br><span class="line">                                    ,BasicConv2d(in_channels,pool_proj,kernel_size=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        branch1 = self.branch1(x) <span class="comment">#28x28,ch1x1</span></span><br><span class="line">        branch2 = self.branch2(x) <span class="comment">#28x28,ch3x3</span></span><br><span class="line">        branch3 = self.branch3(x) <span class="comment">#28x28,ch5x5</span></span><br><span class="line">        branch4 = self.branch4(x) <span class="comment">#28x28,pool_proj</span></span><br><span class="line">        outputs = [branch1, branch2, branch3, branch4]</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>) <span class="comment">#合并</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AuxClf</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_channels : <span class="built_in">int</span>, num_classes : <span class="built_in">int</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.feature_ = nn.Sequential(nn.AvgPool2d(kernel_size=<span class="number">5</span>,stride=<span class="number">3</span>)</span><br><span class="line">                                     ,BasicConv2d(in_channels,<span class="number">128</span>, kernel_size=<span class="number">1</span>))</span><br><span class="line">        self.clf_ = nn.Sequential(nn.Linear(<span class="number">4</span>*<span class="number">4</span>*<span class="number">128</span>, <span class="number">1024</span>)</span><br><span class="line">                                 ,nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                 ,nn.Dropout(<span class="number">0.7</span>)</span><br><span class="line">                                 ,nn.Linear(<span class="number">1024</span>,num_classes))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.feature_(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>,<span class="number">4</span>*<span class="number">4</span>*<span class="number">128</span>)</span><br><span class="line">        x = self.clf_(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GoogLeNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,num_classes: <span class="built_in">int</span> = <span class="number">1000</span>, blocks = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> blocks <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            blocks = [BasicConv2d, Inception, AuxClf]</span><br><span class="line">        conv_block = blocks[<span class="number">0</span>]</span><br><span class="line">        inception_block = blocks[<span class="number">1</span>]</span><br><span class="line">        aux_clf_block = blocks[<span class="number">2</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block1</span></span><br><span class="line">        self.conv1 = conv_block(<span class="number">3</span>,<span class="number">64</span>,kernel_size=<span class="number">7</span>,stride=<span class="number">2</span>,padding = <span class="number">3</span>)</span><br><span class="line">        self.maxpool1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,ceil_mode = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block2</span></span><br><span class="line">        self.conv2 = conv_block(<span class="number">64</span>,<span class="number">64</span>,kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = conv_block(<span class="number">64</span>,<span class="number">192</span>,kernel_size=<span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        self.maxpool2 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,ceil_mode = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block3</span></span><br><span class="line">        self.inception3a = inception_block(<span class="number">192</span>,<span class="number">64</span>,<span class="number">96</span>,<span class="number">128</span>,<span class="number">16</span>,<span class="number">32</span>,<span class="number">32</span>)</span><br><span class="line">        self.inception3b = inception_block(<span class="number">256</span>,<span class="number">128</span>,<span class="number">128</span>,<span class="number">192</span>,<span class="number">32</span>,<span class="number">96</span>,<span class="number">64</span>)</span><br><span class="line">        self.maxpool3 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,ceil_mode = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block4 </span></span><br><span class="line">        self.inception4a = inception_block(<span class="number">480</span>,<span class="number">192</span>,<span class="number">96</span>,<span class="number">208</span>,<span class="number">16</span>,<span class="number">48</span>,<span class="number">64</span>)</span><br><span class="line">        self.inception4b = inception_block(<span class="number">512</span>,<span class="number">160</span>,<span class="number">112</span>,<span class="number">224</span>,<span class="number">24</span>,<span class="number">64</span>,<span class="number">64</span>)</span><br><span class="line">        self.inception4c = inception_block(<span class="number">512</span>,<span class="number">128</span>,<span class="number">128</span>,<span class="number">256</span>,<span class="number">24</span>,<span class="number">64</span>,<span class="number">64</span>)</span><br><span class="line">        self.inception4d = inception_block(<span class="number">512</span>,<span class="number">112</span>,<span class="number">144</span>,<span class="number">288</span>,<span class="number">32</span>,<span class="number">64</span>,<span class="number">64</span>)</span><br><span class="line">        self.inception4e = inception_block(<span class="number">528</span>,<span class="number">256</span>,<span class="number">150</span>,<span class="number">320</span>,<span class="number">32</span>,<span class="number">128</span>,<span class="number">128</span>)</span><br><span class="line">        self.maxpool4 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,ceil_mode = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block5</span></span><br><span class="line">        self.inception5a = inception_block(<span class="number">832</span>,<span class="number">256</span>,<span class="number">160</span>,<span class="number">320</span>,<span class="number">32</span>,<span class="number">128</span>,<span class="number">128</span>)</span><br><span class="line">        self.inception5b = inception_block(<span class="number">832</span>,<span class="number">384</span>,<span class="number">192</span>,<span class="number">384</span>,<span class="number">48</span>,<span class="number">128</span>,<span class="number">128</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#clf</span></span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)) <span class="comment">#我需要的输出的特征图尺寸是多少</span></span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.4</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">1024</span>,num_classes)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#auxclf</span></span><br><span class="line">        self.aux1 = aux_clf_block(<span class="number">512</span>, num_classes) <span class="comment">#4a</span></span><br><span class="line">        self.aux2 = aux_clf_block(<span class="number">528</span>, num_classes) <span class="comment">#4d</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="comment">#block1</span></span><br><span class="line">        x = self.maxpool1(self.conv1(x))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block2</span></span><br><span class="line">        x = self.maxpool2(self.conv3(self.conv2(x)))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block3</span></span><br><span class="line">        x = self.inception3a(x)</span><br><span class="line">        x = self.inception3b(x)</span><br><span class="line">        x = self.maxpool3(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block4</span></span><br><span class="line">        x = self.inception4a(x)</span><br><span class="line">        aux1 = self.aux1(x)</span><br><span class="line">        </span><br><span class="line">        x = self.inception4b(x)</span><br><span class="line">        x = self.inception4c(x)</span><br><span class="line">        x = self.inception4d(x)</span><br><span class="line">        aux2 = self.aux2(x)</span><br><span class="line">        </span><br><span class="line">        x = self.inception4e(x)</span><br><span class="line">        x = self.maxpool4(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block5</span></span><br><span class="line">        x = self.inception5a(x)</span><br><span class="line">        x = self.inception5b(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#clf</span></span><br><span class="line">        x = self.avgpool(x) <span class="comment">#在这个全局平均池化之后，特征图尺寸就变成了1x1</span></span><br><span class="line">        x = torch.flatten(x,<span class="number">1</span>)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x, aux2, aux1</span><br><span class="line">data = torch.ones(<span class="number">10</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">net = GoogLeNet(num_classes=<span class="number">1000</span>)</span><br><span class="line">fc2, fc1, fc0 = net(data)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [fc2, fc1, fc0]:</span><br><span class="line">    <span class="built_in">print</span>(i.shape)</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">torch.Size([10, 1000])</span></span><br><span class="line"><span class="string">torch.Size([10, 1000])</span></span><br><span class="line"><span class="string">torch.Size([10, 1000])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">summary(net,(<span class="number">10</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>),device=<span class="string">&quot;cpu&quot;</span>,depth=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="5-4-ResNet—2015"><a href="#5-4-ResNet—2015" class="headerlink" title="5.4 ResNet—2015"></a>5.4 ResNet—2015</h4><ul><li><p><strong>困难</strong>：</p><ul><li>网络能够达到的<strong>最大深度依然很浅</strong></li><li>深度网络的<strong>训练难度太大</strong>，深度网络会出现<strong>精度平缓甚至退化</strong>的现象。深层网络中的函数关系本质上就比浅层网络中的函数关系更复杂、更难拟合（fit）</li></ul></li><li><p><strong>思路</strong>：</p><ul><li>残差F(x)为0，此时x=H(x)</li><li><code>拟合0与x的关系</code>，比拟合一个<code>未知的函数H(x)与x的关系</code>容易。</li></ul></li></ul><ul><li><strong>残差块H(x)</strong>：快捷连接/跳跃连接x+普通卷积层F(x) </li><li><strong>残差网络</strong>：普通卷积层和全局平均池化层中间插入数个残差单元的网络<ul><li><strong>五种</strong>：18、34、50、101、152</li><li><strong>模式</strong>：7x7开头、3x3(s=2)的重叠池化层、数个重复的残差单元、全局平均池化层、线性层、Softmax</li><li><strong>规定</strong>：对于34层以下的残差网络，每个残差单元至少有两个卷积层；50层以上的残差网络，每个残差单元有三个卷积层（瓶颈架构—-为降低参数量而存在）</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/ResNet%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E5%9B%BE.png" alt=""></p><ul><li><p>padding</p><p>残差单元中，7/3、5/2、3/1、1/0保持特征图尺寸不变，并Conv+Batch Normalization+ReLu为一体。</p></li><li><p>stride</p><p>与GoogleNet一样，残差网络的输入是224x224时，特征图总共折半了5次，最终尺寸是7x7</p><p>我们把这5部分称为Layers，每个Layers包含的残差单元是一个个Block</p><p>只有最初出现了池化层来降低尺寸，说明5次降维认为中的其他4次都是由步长为2的卷积层完成</p><p>conv3_x conv4_x conv5_x这三个Layers，指的是第一个残差单元或瓶颈结构的第一个卷积层</p></li><li><p>跳跃连接上的卷积层</p><p>为了实现残差单元中的加和，在跳跃连接上叶加入核为1x1，步长为2的卷积层缩小x尺寸</p><p>在瓶颈架构中，输出的特征图数量依次是middle_out、middle_out、middle_out*4</p><p>故在跳跃连接上的卷积层要保证被整理为输出的相同结构</p></li><li><p>BN层与ReLU激活函数在哪里？</p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/ResNet-BN%E5%92%8CReLu%E4%BD%8D%E7%BD%AE.png" alt=""></p></li><li><p>参数初始化在哪里实现？</p><p>将残差单元中最后一个卷积层后的BN层上的γ=0，就可以让F(x)的输出结果为0了</p></li></ul><script type="math/tex; mode=display">\text{output} = \frac{x - E[x]}{\sqrt{Var[x] + \epsilon}} * \gamma + \beta</script><ul><li><p>特征图数量繁多、多样地变化</p><p>我们必须使用具有一定通用性的代码来实现不同的层</p><p>保持不变。conv1与conv2_x连接时，一个瓶颈架构内嵌两个卷积层连接时，特征图数量保持不变</p><p>上层输出数量的4倍。内嵌三个卷积层的瓶颈架构中，第三个1x1卷积层的输出特征图数量是4倍</p><p>上层输出数量的1/4倍。一个Layers内，下一个瓶颈架构是上一个瓶颈架构连接的1/4倍</p><p>上层输出数量的1/2倍。不同Layers之间连接时，下一个瓶颈架构是上一个瓶颈架构连接变的1/2倍</p></li></ul><h4 id="5-5-复现ResNet"><a href="#5-5-复现ResNet" class="headerlink" title="5.5 复现ResNet"></a>5.5 复现ResNet</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#basicconv - conv2d + BN + ReLU (→ conv3x3, conv1x1)</span></span><br><span class="line"><span class="comment">#Residual Unit, Bottleneck</span></span><br><span class="line"><span class="comment">#导入需要的库</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Type</span>, <span class="type">Union</span>, <span class="type">List</span>, <span class="type">Optional</span></span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br></pre></td></tr></table></figure><ul><li>架构中的最小单元“3x3卷积层”，以此来构建残差单元和瓶颈架构</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把ReLu写在外面，保证相加后可以ReLu</span></span><br><span class="line"><span class="comment"># 步长不能写死,有缩减有保留，但大部分为1，故写默认为1</span></span><br><span class="line"><span class="comment"># 输入与输出不能写死</span></span><br><span class="line"><span class="comment"># 最后一层的gama和belta为0就可以了</span></span><br><span class="line"><span class="comment"># bn类和conv2d类都不存在的参数：initialzero参数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv3x3</span>(<span class="params">in_, out_, stride=<span class="number">1</span>, initialzero = <span class="literal">False</span></span>):</span><br><span class="line">    bn = nn.BatchNorm2d(out_)</span><br><span class="line">    <span class="comment">#需要进行判断：要对BN进行0初始化吗？</span></span><br><span class="line">    <span class="comment">#最后一层就初始化,不是最后一层就不改变gamma和beta</span></span><br><span class="line">    <span class="keyword">if</span> initialzero == <span class="literal">True</span>:</span><br><span class="line">        nn.init.constant_(bn.weight, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Conv2d(in_, out_</span><br><span class="line">                            , kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>, stride = stride</span><br><span class="line">                            , bias = <span class="literal">False</span>)</span><br><span class="line">                         ,bn)</span><br></pre></td></tr></table></figure><ul><li>架构中的最小单元“1x1卷积层”，以此来构建残差单元和瓶颈架构</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">conv1x1</span>(<span class="params">in_, out_, stride=<span class="number">1</span>, initialzero = <span class="literal">False</span></span>):</span><br><span class="line">    bn = nn.BatchNorm2d(out_)</span><br><span class="line">    <span class="comment">#需要进行判断：要对BN进行0初始化吗？</span></span><br><span class="line">    <span class="comment">#最后一层就初始化,不是最后一层就不改变gamma和beta</span></span><br><span class="line">    <span class="keyword">if</span> initialzero == <span class="literal">True</span>:</span><br><span class="line">        nn.init.constant_(bn.weight, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(nn.Conv2d(in_, out_</span><br><span class="line">                            , kernel_size=<span class="number">1</span>,padding=<span class="number">0</span>, stride = stride</span><br><span class="line">                            , bias = <span class="literal">False</span>)</span><br><span class="line">                         ,bn)</span><br></pre></td></tr></table></figure><ul><li>每个残差单元最后一个卷积层的bn层上，初始化为0</li><li>特征图减半发生在layers与layers之间，且发生在每个layers的第一个残差单元的第一个卷积层上</li><li>相加问题，跳跃连接不是随时都要，只有当需要调整特征图尺寸时候才要，在残差单元中也只有Layers与Layers相连才需要调整尺寸，故forward中要加if，且注意forward中变量要变成属性才能调用</li><li>特征图数量变化，stride=2时，in是out的一半，stride=1时，in和out相等</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualUnit</span>(nn.Module):</span><br><span class="line">    <span class="comment">#这是残差单元类</span></span><br><span class="line">    <span class="comment">#stride1是否等于2呢？如果等于2 - 特征图尺寸会发生变化</span></span><br><span class="line">    <span class="comment">#需要在跳跃链接上增加1x1卷积层来调整特征图尺寸</span></span><br><span class="line">    <span class="comment">#如果stride1等于1，则什么也不需要做</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,out_: <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,stride1: <span class="built_in">int</span> = <span class="number">1</span> <span class="comment">#定义该参数的类型，并且定义默认值</span></span></span><br><span class="line"><span class="params">                 ,in_ : <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span></span><br><span class="line"><span class="params">                </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        self.stride1 = stride1  <span class="comment"># 不定义属性的话捕捉不到stride1参数，</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#当特征图尺寸需要缩小时，卷积层的输出特征图数量out_等于输入特征图数量in_的2被</span></span><br><span class="line">        <span class="comment">#当特征图尺寸不需要缩小时，out_ == in_</span></span><br><span class="line">        <span class="keyword">if</span> stride1 !=<span class="number">1</span>:</span><br><span class="line">            in_ = <span class="built_in">int</span>(out_/<span class="number">2</span>)  <span class="comment"># 仅仅三次</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            in_ = out_</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#拟合部分，输出F(x)</span></span><br><span class="line">        self.fit_ = nn.Sequential(conv3x3(in_,out_,stride=stride1)</span><br><span class="line">                                 ,nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                 ,conv3x3(out_,out_,initialzero=<span class="literal">True</span>)</span><br><span class="line">                                 <span class="comment"># 单纯拟合，还没加和 故没ReLu</span></span><br><span class="line">                                 )</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#跳跃链接，输出x(1x1卷积核之后的x)</span></span><br><span class="line">        self.skipconv = conv1x1(in_,out_,stride = stride1)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#单独定义放在H(x)之后来使用的激活函数ReLU</span></span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        fx = self.fit_(x) <span class="comment">#拟合结果</span></span><br><span class="line">        <span class="keyword">if</span> self.stride1 != <span class="number">1</span>:</span><br><span class="line">            x = self.skipconv(x) <span class="comment">#跳跃链接</span></span><br><span class="line">        hx = self.relu(fx + x)</span><br><span class="line">        <span class="keyword">return</span> hx</span><br></pre></td></tr></table></figure><ul><li>测试残差单元</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data = torch.ones(<span class="number">10</span>,<span class="number">64</span>,<span class="number">56</span>,<span class="number">56</span>) <span class="comment">#特征图尺寸64x64，输入特征图数量64</span></span><br><span class="line">conv3_x_18_0 = ResidualUnit(<span class="number">128</span>,stride1=<span class="number">2</span>) <span class="comment">#特征图尺寸折半，特征图数量加倍</span></span><br><span class="line">conv3_x_18_0(data).shape  <span class="comment"># (10,128,28,28)</span></span><br><span class="line"></span><br><span class="line">conv2_x_18_0 = ResidualUnit(<span class="number">64</span>) <span class="comment">#特征图尺寸不变，特征图数量也不变</span></span><br><span class="line">conv2_x_18_0(data).shape  <span class="comment"># (10,64,56,56)</span></span><br></pre></td></tr></table></figure><ul><li>特征图数量：<ul><li>layers与layers相连时，需要缩小特征图，上层的in等于middle的两倍（stride=2）</li><li>瓶颈与瓶颈相连时，需要缩小特征图，上层的in等于middle的4倍（stride=1)</li><li>conv1与conv2连接时，上层的in等于middle的值(in不是None的时候)</li></ul></li><li>用middle来调整out和in</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    <span class="comment">#是需要将特征图尺寸缩小的场合吗？</span></span><br><span class="line">    <span class="comment">#conv2_x - conv3_x - conv4_x - conv5_x 相互链接的时候</span></span><br><span class="line">    <span class="comment">#每次都需要将特征图尺寸折半，同时卷积层上的middle_out = 1/2in_</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, middle_out</span></span><br><span class="line"><span class="params">                 , stride1: <span class="built_in">int</span> = <span class="number">1</span></span></span><br><span class="line"><span class="params">                 , in_: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        out_ = <span class="number">4</span> * middle_out</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#我希望使用选填参数in_来帮助我们区别，这个架构是不是在conv1的后面</span></span><br><span class="line">        <span class="comment">#如果这个架构不是紧跟在conv1后，就不填写in_</span></span><br><span class="line">        <span class="comment">#如果是跟在conv1后，就填写in_ = 64</span></span><br><span class="line">        <span class="keyword">if</span> in_ == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> stride1 !=<span class="number">1</span>: <span class="comment">#缩小特征图的场合，即这个瓶颈结构是每个layers的第一个瓶颈结构</span></span><br><span class="line">                in_ = middle_out * <span class="number">2</span></span><br><span class="line">                <span class="comment">#不缩小特征图的场合，即这个瓶颈结构不是这个layers的第一个瓶颈结构</span></span><br><span class="line">                <span class="comment">#而是跟在第一个瓶颈结构后的重复的结构</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                in_ = middle_out * <span class="number">4</span></span><br><span class="line">        </span><br><span class="line">        self.fit_ = nn.Sequential(conv1x1(in_,middle_out,stride=stride1)</span><br><span class="line">                                 ,nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                 ,conv3x3(middle_out,middle_out)</span><br><span class="line">                                 ,nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                 ,conv1x1(middle_out,out_,initialzero=<span class="literal">True</span>))</span><br><span class="line">        </span><br><span class="line">        self.skipconv = conv1x1(in_, out_, stride=stride1)</span><br><span class="line">        </span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        fx = self.fit_(x)</span><br><span class="line">        <span class="comment">#跳跃链接</span></span><br><span class="line">        x = self.skipconv(x)</span><br><span class="line">        hx = self.relu(fx + x)</span><br><span class="line">        <span class="keyword">return</span> hx</span><br></pre></td></tr></table></figure><ul><li>测试瓶颈架构</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">data1 = torch.ones(<span class="number">10</span>,<span class="number">64</span>,<span class="number">56</span>,<span class="number">56</span>) <span class="comment">#特征图尺寸56x56，特征图数量64</span></span><br><span class="line"><span class="comment">#  是conv1后紧跟的第一个瓶颈结构</span></span><br><span class="line">conv2_x_101_0 = Bottleneck(in_ = <span class="number">64</span>, middle_out = <span class="number">64</span>)</span><br><span class="line">conv2_x_101_0(data1).shape  <span class="comment"># (10,256,56,56)</span></span><br><span class="line"></span><br><span class="line">data2 = torch.ones(<span class="number">10</span>,<span class="number">256</span>,<span class="number">56</span>,<span class="number">56</span>)</span><br><span class="line"><span class="comment">#不是conv1后紧跟的第一个瓶颈结构，但是需要缩小特征图尺寸</span></span><br><span class="line">conv3_x_101_0 = Bottleneck(middle_out = <span class="number">128</span>, stride1=<span class="number">2</span>)</span><br><span class="line">conv3_x_101_0(data2).shape <span class="comment">#输出翻2倍并缩小特征图尺寸至一半 (10,512,28,28)</span></span><br><span class="line"></span><br><span class="line">data3 = torch.ones(<span class="number">10</span>,<span class="number">512</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line"><span class="comment">#不是conv1后紧跟的第一个瓶颈结构，也不需要缩小特征图尺寸</span></span><br><span class="line">conv3_x_101_1 = Bottleneck(<span class="number">128</span>)</span><br><span class="line">conv3_x_101_1(data3).shape <span class="comment">#输出不变，特征图尺寸也不变 (10,512,128,128)</span></span><br></pre></td></tr></table></figure><ul><li>合并两个类、</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_layers</span>(<span class="params">block: <span class="type">Type</span>[<span class="type">Union</span>[ResidualUnit, Bottleneck]]</span></span><br><span class="line"><span class="params">                ,middle_out: <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                ,num_blocks: <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                ,afterconv1: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span><br><span class="line">    </span><br><span class="line">    layers = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> afterconv1 == <span class="literal">True</span>:</span><br><span class="line">        layers.append(block(middle_out, in_ = <span class="number">64</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        layers.append(block(middle_out, stride1 = <span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_blocks-<span class="number">1</span>):</span><br><span class="line">        layers.append(block(middle_out))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure><ul><li>测试</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#测试 - 需要分别对残差块和瓶颈架构进行测试，并且需要对conv1后的首个架构，以及中间的架构进行测试</span></span><br><span class="line"><span class="comment">#注意检查：输入的数据结构是否正确，网络能否允许正确的数据结构输入，输入后产出的结构是否正确，包括</span></span><br><span class="line"><span class="comment">#特征图尺寸是否变化、特征图数量是否变化，以及一个layers中所包含的blocks数量是否正确</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#18层网络，紧跟在conv1之后的conv2_x，不改变特征图尺寸，且每层的输出都是64</span></span><br><span class="line">datashape = (<span class="number">10</span>,<span class="number">64</span>,<span class="number">56</span>,<span class="number">56</span>) <span class="comment">#特征图尺寸56x56，特征图数量64</span></span><br><span class="line">conv2_x_18 = make_layers(ResidualUnit, middle_out = <span class="number">64</span>, blocks=<span class="number">2</span>, afterconv1=<span class="literal">True</span>)</span><br><span class="line">summary(conv2_x_18,datashape,depth=<span class="number">1</span>,device=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#34层网络，conv4_x，缩小特征图尺寸，且每层的输出翻倍</span></span><br><span class="line">datashape2 = (<span class="number">10</span>,<span class="number">128</span>,<span class="number">14</span>,<span class="number">14</span>)</span><br><span class="line">conv2_x_34 = make_layers(ResidualUnit, middle_out = <span class="number">256</span>, blocks=<span class="number">6</span>,afterconv1=<span class="literal">False</span>)</span><br><span class="line">summary(conv2_x_34,datashape2,depth=<span class="number">1</span>,device=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#101层网络，紧跟在conv1之后的conv2_x，不改变特征图尺寸，1x1和3x3卷积层的输出是64，整个瓶颈架构的输出是256</span></span><br><span class="line">conv2_x_101 = make_layers(Bottleneck, middle_out = <span class="number">64</span>, blocks=<span class="number">3</span>, afterconv1=<span class="literal">True</span>)</span><br><span class="line">summary(conv2_x_101,datashape,depth=<span class="number">3</span>,device=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#101层网络，conv4_x，缩小特征图尺寸，且每层的输出翻4倍</span></span><br><span class="line">datashape3 = (<span class="number">10</span>,<span class="number">512</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">conv4_x_101 = make_layers(Bottleneck, middle_out = <span class="number">256</span>, blocks=<span class="number">23</span>, afterconv1=<span class="literal">False</span>)</span><br><span class="line">summary(conv4_x_101,datashape3,depth=<span class="number">1</span>,device=<span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>构建残差网络</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,block: <span class="type">Type</span>[<span class="type">Union</span>[ResidualUnit, Bottleneck]]</span></span><br><span class="line"><span class="params">                ,layers: <span class="type">List</span>[<span class="built_in">int</span>]</span></span><br><span class="line"><span class="params">                ,num_classes : <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        block：要使用的用来加深深度的基本架构是？可以选择残差单元或瓶颈结构，两种都带有skip connection</span></span><br><span class="line"><span class="string">        layers：列表，每个层里具体有多少个块呢？可参考网络架构图。例如，34层的残差网络的layers = [3,4,6,3]</span></span><br><span class="line"><span class="string">        num_classes：真实标签含有多少个类别？</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#layer1:卷积+池化的组合</span></span><br><span class="line">        self.layer1 = nn.Sequential(nn.Conv2d(<span class="number">3</span>,<span class="number">64</span></span><br><span class="line">                                              ,kernel_size=<span class="number">7</span>,stride=<span class="number">2</span></span><br><span class="line">                                              ,padding=<span class="number">3</span>,bias = <span class="literal">False</span>)</span><br><span class="line">                                   ,nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">                                   ,nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                   ,nn.MaxPool2d(kernel_size=<span class="number">3</span></span><br><span class="line">                                                 ,stride=<span class="number">2</span></span><br><span class="line">                                                 ,ceil_mode = <span class="literal">True</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#layer2 - layer5:残差块/瓶颈结构</span></span><br><span class="line">        self.layer2_x = make_layers(block,<span class="number">64</span>,layers[<span class="number">0</span>],afterconv1=<span class="literal">True</span>)</span><br><span class="line">        self.layer3_x = make_layers(block,<span class="number">128</span>,layers[<span class="number">1</span>])</span><br><span class="line">        self.layer4_x = make_layers(block,<span class="number">256</span>,layers[<span class="number">2</span>])</span><br><span class="line">        self.layer5_x = make_layers(block,<span class="number">512</span>,layers[<span class="number">3</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#全局平均池化</span></span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#分类</span></span><br><span class="line">        <span class="keyword">if</span> block == ResidualUnit:</span><br><span class="line">            self.fc = nn.Linear(<span class="number">512</span>,num_classes)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.fc = nn.Linear(<span class="number">2048</span>,num_classes)</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.layer1(x) <span class="comment">#layer1，普通卷积+池化的输出</span></span><br><span class="line">        x = self.layer5_x(self.layer4_x(self.layer3_x(self.layer2_x(x))))</span><br><span class="line">        x = self.avgpool(x) <span class="comment">#特征图尺寸1x1 (n_samples, fc, 1, 1)</span></span><br><span class="line">        x = torch.flatten(x,<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br></pre></td></tr></table></figure><ul><li>测试残差网络</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">datashape = (<span class="number">10</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line"></span><br><span class="line">res34 = ResNet(ResidualUnit, layers=[<span class="number">3</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">3</span>], num_classes=<span class="number">1000</span>)</span><br><span class="line">summary(res34,datashape,depth=<span class="number">2</span>,device=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">res101 = ResNet(Bottleneck, layers=[<span class="number">3</span>,<span class="number">4</span>,<span class="number">23</span>,<span class="number">3</span>], num_classes=<span class="number">1000</span>)</span><br><span class="line">summary(res101,datashape,depth=<span class="number">2</span>,device=<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment">#你能数出有多少层吗？一个ResidualUnit中含有2个卷积层，一个Bottleneck中含有3个卷积层，卷积层</span></span><br><span class="line"><span class="comment">#的数目 + 开头的conv1和结尾的线性层，就是整个网络的深度</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(7)-欢迎来到深度学习的世界</title>
      <link href="/Pytorch-7/"/>
      <url>/Pytorch-7/</url>
      
        <content type="html"><![CDATA[<h4 id="1-神经网络-深度学习-机器学习三者关系"><a href="#1-神经网络-深度学习-机器学习三者关系" class="headerlink" title="1. 神经网络/深度学习/机器学习三者关系"></a>1. 神经网络/深度学习/机器学习三者关系</h4><ul><li><p>数学家提出<strong>算法</strong></p></li><li><p>计算机科学家想到让其规模化，提出<strong>机器学习</strong></p></li><li><p>哲学家想到像人类一样学习，提出<strong>人工神经网络</strong></p></li><li><p>神经网络有致命的<strong>弱点</strong>：</p><ul><li>预测效果差(最初只能解决线性问题)</li><li>数据需求大(需要大量的数据去喂养)</li><li>计算时间长</li></ul></li></ul><p>导致其<strong>根本无法到达人类智力水平</strong></p><ul><li>同时期 KNN,决策树等提出，<strong>机器学习其他算法繁荣</strong>，神经网络无人问津。</li><li>新世纪，<strong>算法进步</strong>、<strong>数据量激增</strong>、<strong>算力提升</strong>三者释放了神经网络的潜力。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Performance-Data.png" alt="image-20240103191554847"></p><ul><li>Hinton 提出为神经网络的相关研究提出<strong>深度学习</strong>一词</li></ul><h4 id="2-机器学习中的基本概念"><a href="#2-机器学习中的基本概念" class="headerlink" title="2. 机器学习中的基本概念"></a>2. 机器学习中的基本概念</h4><ul><li><strong>样本、特征、标签</strong></li><li>图片中 3 维以外即<strong>索引</strong></li><li><strong>分类</strong>是离散的，<strong>回归</strong>是连续的</li><li>有标签的任务称为<strong>有监督学习</strong>(KNN,SVM,线性回归,逻辑回归)；</li><li>无标签的任务称为<strong>无监督学习</strong>(聚类分析,协同分析,自编码器)，<strong>辅助</strong>作用</li><li>其他还有<strong>半监督学习</strong>，<strong>强化学习</strong>等</li></ul><h4 id="3-模型评判标准"><a href="#3-模型评判标准" class="headerlink" title="3. 模型评判标准"></a>3. 模型评判标准</h4><ul><li>模型预测效果</li><li>运算速度</li><li>可解释性</li><li>服务于业务</li></ul><h4 id="4-Pytorch-优势"><a href="#4-Pytorch-优势" class="headerlink" title="4. Pytorch 优势"></a>4. Pytorch 优势</h4><ul><li>承受大数据</li><li>调整 NN 架构</li><li>API 简化</li><li><p>服务工业 jit</p></li><li><p>组成</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Pytorch%E7%BB%84%E6%88%901.png" alt="Pytorch组成1"></p><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Pytorch%E7%BB%84%E6%88%902.png" alt="Pytorch组成2"></p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Git指令</title>
      <link href="/Git/"/>
      <url>/Git/</url>
      
        <content type="html"><![CDATA[<h4 id="1-查看文件"><a href="#1-查看文件" class="headerlink" title="1. 查看文件"></a>1. 查看文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">dir</span></span><br></pre></td></tr></table></figure><blockquote><p>可在文件夹中查看所有的文件名</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Git%E6%9F%A5%E7%9C%8B%E6%96%87%E4%BB%B6" alt="image-20240103135110537"></p><h4 id="2-修改文件"><a href="#2-修改文件" class="headerlink" title="2. 修改文件"></a>2. 修改文件</h4><h5 id="2-1-拉取最新的代码"><a href="#2-1-拉取最新的代码" class="headerlink" title="2.1 拉取最新的代码"></a>2.1 拉取最新的代码</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git pull</span><br></pre></td></tr></table></figure><h5 id="2-2-重命名文件"><a href="#2-2-重命名文件" class="headerlink" title="2.2 重命名文件"></a>2.2 重命名文件</h5><h6 id="命令行方法"><a href="#命令行方法" class="headerlink" title="命令行方法"></a>命令行方法</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> A.png B.png  <span class="comment"># A改为B</span></span><br></pre></td></tr></table></figure><h6 id="本地文件夹"><a href="#本地文件夹" class="headerlink" title="本地文件夹"></a>本地文件夹</h6><p>直接重命名文件即可。</p><h5 id="2-3-将改动添加到暂存区"><a href="#2-3-将改动添加到暂存区" class="headerlink" title="2.3 将改动添加到暂存区"></a>2.3 将改动添加到暂存区</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add -A</span><br></pre></td></tr></table></figure><h5 id="2-4-提交更改"><a href="#2-4-提交更改" class="headerlink" title="2.4 提交更改"></a>2.4 提交更改</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m <span class="string">&quot;Renamed image file&quot;</span></span><br></pre></td></tr></table></figure><h5 id="2-5-推送到远程仓库"><a href="#2-5-推送到远程仓库" class="headerlink" title="2.5 推送到远程仓库"></a>2.5 推送到远程仓库</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git push</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>查看版本</title>
      <link href="/Related%20versions/"/>
      <url>/Related%20versions/</url>
      
        <content type="html"><![CDATA[<h4 id="1-Pytorch版本"><a href="#1-Pytorch版本" class="headerlink" title="1. Pytorch版本"></a>1. Pytorch版本</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch   </span><br><span class="line"><span class="built_in">print</span>(torch.__version__)  <span class="comment"># 注意是双下划线</span></span><br></pre></td></tr></table></figure><h4 id="2-torchvision版本"><a href="#2-torchvision版本" class="headerlink" title="2. torchvision版本"></a>2. torchvision版本</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="built_in">print</span>(torchvision.__version__)</span><br></pre></td></tr></table></figure><h4 id="3-cuda目前版本"><a href="#3-cuda目前版本" class="headerlink" title="3. cuda目前版本"></a>3. cuda目前版本</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda)  <span class="comment">#注意是双下划线</span></span><br></pre></td></tr></table></figure><h4 id="4-cuda最高版本"><a href="#4-cuda最高版本" class="headerlink" title="4. cuda最高版本"></a>4. cuda最高版本</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/查看cuda最高版本.png" alt="查看cuda最高版本"></p><h4 id="5-cudnn版本"><a href="#5-cudnn版本" class="headerlink" title="5. cudnn版本"></a>5. cudnn版本</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.backends.cudnn.version())  <span class="comment">#注意是双下划线</span></span><br></pre></td></tr></table></figure><h4 id="6-查看显卡"><a href="#6-查看显卡" class="headerlink" title="6. 查看显卡"></a>6. 查看显卡</h4><h5 id="6-1-查看显卡数量"><a href="#6-1-查看显卡数量" class="headerlink" title="6.1 查看显卡数量"></a>6.1 查看显卡数量</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.device_count()  <span class="comment"># 查看GPU数量</span></span><br></pre></td></tr></table></figure><h5 id="6-2-查看是否有可用GPU"><a href="#6-2-查看是否有可用GPU" class="headerlink" title="6.2 查看是否有可用GPU"></a>6.2 查看是否有可用GPU</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.is_available()  <span class="comment"># 查看是否有可用GPU</span></span><br></pre></td></tr></table></figure><h5 id="6-3-查看GPU算力"><a href="#6-3-查看GPU算力" class="headerlink" title="6.3 查看GPU算力"></a>6.3 查看GPU算力</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.get_device_capability(device)  <span class="comment"># 查看指定GPU算力，参数device是数字，从0开始</span></span><br><span class="line"><span class="comment">#例如有两张显卡</span></span><br><span class="line">torch.cuda.get_device_capability(<span class="number">0</span>)  <span class="comment"># 查看第0张显卡</span></span><br><span class="line">torch.cuda.get_device_capability(<span class="number">1</span>)  <span class="comment"># 查看第1张显卡</span></span><br></pre></td></tr></table></figure><h5 id="6-4-查看GPU名称"><a href="#6-4-查看GPU名称" class="headerlink" title="6.4 查看GPU名称"></a>6.4 查看GPU名称</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.get_device_name(device)  <span class="comment"># 查看指定GPU名称，参数device是数字，从0开始</span></span><br><span class="line"><span class="comment">#例如有两张显卡</span></span><br><span class="line">torch.cuda.get_device_name(<span class="number">0</span>)  <span class="comment"># 查看第0张显卡名称</span></span><br><span class="line">torch.cuda.get_device_name(<span class="number">1</span>)  <span class="comment"># 查看第1张显卡名称</span></span><br></pre></td></tr></table></figure><h5 id="6-5-查看当前GPU"><a href="#6-5-查看当前GPU" class="headerlink" title="6.5 查看当前GPU"></a>6.5 查看当前GPU</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看当前使用的显卡，返回的是显卡的序号</span></span><br><span class="line">torch.cuda.current_device()</span><br></pre></td></tr></table></figure><h5 id="6-6-其他命令"><a href="#6-6-其他命令" class="headerlink" title="6.6 其他命令"></a>6.6 其他命令</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.empty_cache() <span class="comment"># 清空程序占用的GPU资源</span></span><br><span class="line"></span><br><span class="line">torch.cuda.manual_seed(seed) <span class="comment"># 设置随机种子</span></span><br><span class="line"></span><br><span class="line">torch.cuda.manual_seed_all(seed) <span class="comment"># 设置随机种子</span></span><br></pre></td></tr></table></figure><h4 id="7-指定显卡"><a href="#7-指定显卡" class="headerlink" title="7. 指定显卡"></a>7. 指定显卡</h4><h5 id="7-1在Pytorch程序里面指定显卡"><a href="#7-1在Pytorch程序里面指定显卡" class="headerlink" title="7.1在Pytorch程序里面指定显卡"></a>7.1在Pytorch程序里面指定显卡</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置torch的调用显卡（有效）</span></span><br><span class="line">torch.cuda.set_device(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出当前卡的信息</span></span><br><span class="line">A=torch.arange(<span class="number">12</span>,dtype=torch.float32)</span><br><span class="line">B=A.cuda()</span><br><span class="line"><span class="built_in">print</span>(B.device)  <span class="comment"># cuda:0</span></span><br></pre></td></tr></table></figure><h5 id="7-2在os里面指定显卡"><a href="#7-2在os里面指定显卡" class="headerlink" title="7.2在os里面指定显卡"></a>7.2在os里面指定显卡</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&quot;2,1,3,4&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;torch.cuda.device_count() &#123;&#125;&quot;</span>.<span class="built_in">format</span>(torch.cuda.device_count()))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> others </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(11)-优化SGD</title>
      <link href="/Pytorch-11/"/>
      <url>/Pytorch-11/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br></pre></td></tr></table></figure><h3 id="1-神经网络的复杂性与优化"><a href="#1-神经网络的复杂性与优化" class="headerlink" title="1. 神经网络的复杂性与优化"></a>1. 神经网络的复杂性与优化</h3><ul><li>神经网络的<strong>函数复杂度</strong>使得求导过程复杂。</li><li>面对<strong>成千上万的权重 (w)</strong>，逐个计算梯度不现实。</li><li>优化算法核心：<strong>逐步迭代</strong>至最小值。</li></ul><h3 id="2-梯度下降的方向与距离"><a href="#2-梯度下降的方向与距离" class="headerlink" title="2. 梯度下降的方向与距离"></a>2. 梯度下降的方向与距离</h3><ul><li><strong>方向</strong>：梯度下降的反方向。</li><li><strong>距离</strong>：步长乘以梯度向量的大小。</li><li>每个坐标点的梯度方向独一无二。</li></ul><p>梯度下降公式：</p><script type="math/tex; mode=display">    w_{(t+1)} = w_{(t)} - \eta \frac{\partial L}{\partial w}</script><ul><li>梯度下降的反方向总比损失函数低一个维度。</li></ul><h3 id="3-反向传播的实现"><a href="#3-反向传播的实现" class="headerlink" title="3. 反向传播的实现"></a>3. 反向传播的实现</h3><ul><li>在 PyTorch 中实现反向传播。</li><li>输出 <code>y</code> 需为向量形式。</li><li>当使用交叉熵损失 (<code>CrossEntropyLoss</code>) 类时：<ul><li>该类已内置 <code>sigmoid</code> 功能。</li><li>从 <code>forward</code> 函数的输出层中去除 <code>sigmoid</code>。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">X = torch.rand((<span class="number">500</span>,<span class="number">20</span>),dtype=torch.float32) * <span class="number">100</span></span><br><span class="line">y = torch.randint(low=<span class="number">0</span>,high=<span class="number">3</span>,size=(<span class="number">500</span>,),dtype=torch.float32)  <span class="comment"># 向量形式</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_features=<span class="number">10</span>,out_features=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__() <span class="comment">#super(请查找这个类的父类，请使用找到的父类替换现在的类)</span></span><br><span class="line">        self.linear1 = nn.Linear(in_features,<span class="number">13</span>,bias=<span class="literal">True</span>) <span class="comment">#输入层不用写，这里是隐藏层的第一层</span></span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">13</span>,<span class="number">8</span>,bias=<span class="literal">True</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">8</span>,out_features,bias=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        z1 = self.linear1(x)  <span class="comment"># Hidden1-1</span></span><br><span class="line">        sigma1 = torch.relu(z1)  <span class="comment"># Hidden1-2</span></span><br><span class="line">        z2 = self.linear2(sigma1)  <span class="comment"># Hidden2-1</span></span><br><span class="line">        sigma2 = torch.sigmoid(z2)  <span class="comment"># Hidden2-1</span></span><br><span class="line">        z3 = self.output(sigma2)  <span class="comment"># Output</span></span><br><span class="line">        <span class="comment">#sigma3 = F.softmax(z3,dim=1)  </span></span><br><span class="line">        <span class="keyword">return</span> z3</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算数据大小</span></span><br><span class="line">input_ = X.shape[<span class="number">1</span>] <span class="comment">#特征的数目</span></span><br><span class="line">output_ = <span class="built_in">len</span>(y.unique()) <span class="comment">#分类的数目</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#实例化神经网络类</span></span><br><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">net = Model(in_features=input_, out_features=output_)</span><br><span class="line"></span><br><span class="line"><span class="comment">#前向传播</span></span><br><span class="line">zhat = net.forward(X)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义损失函数</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment">#对打包好的CorssEnrtopyLoss而言，只需要输入zhat</span></span><br><span class="line">loss = criterion(zhat,y.long())</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br><span class="line">loss.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#net.linear1.weight.grad 可打印梯度</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>tensor(1.1057, grad_fn=&lt;NllLossBackward0&gt;)</code></pre><h3 id="4-移动坐标点"><a href="#4-移动坐标点" class="headerlink" title="4.移动坐标点"></a>4.移动坐标点</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">dw = net.linear1.weight.grad</span><br><span class="line">w = net.linear1.weight.data</span><br><span class="line"></span><br><span class="line"><span class="comment">#对任意w可以有</span></span><br><span class="line">w-=lr*dw</span><br></pre></td></tr></table></figure><h3 id="5-动量法Momentum"><a href="#5-动量法Momentum" class="headerlink" title="5.动量法Momentum"></a>5.动量法Momentum</h3><script type="math/tex; mode=display">v(t) = \gamma v(t-1) - \eta \frac{\partial L}{\partial w}</script><script type="math/tex; mode=display">w(t+1) = w(t) + v(t)</script><script type="math/tex; mode=display">\begin{align*}t &= 0 \quad v(0) = 0 \\w(1)=initiate \\t &= 1 \quad v(1) = r v(0) - (\eta \frac{\partial L}{\partial w})_0 \\w(2) &= w(1) + v(1) = w(1) - (\eta \frac{\partial L}{\partial w})_0 \\t &= 2 \quad v(2) = r v(1) - (\eta \frac{\partial L}{\partial w})_1 = - (\eta \frac{\partial L}{\partial w})_1 - r(\eta \frac{\partial L}{\partial w})_0 \\w(3) &= w(2) + v(2) = w(2) - (\eta \frac{\partial L}{\partial w})_1 - r(\eta \frac{\partial L}{\partial w})_0 \\t &= 3 \quad v(3) = r v(2) - (\eta \frac{\partial L}{\partial w})_2 = - (\eta \frac{\partial L}{\partial w})_2 - r(\eta \frac{\partial L}{\partial w})_1 - r^2(\eta \frac{\partial L}{\partial w})_0 \\w(4) &= w(3) + v(3) = w(3) - (\eta \frac{\partial L}{\partial w})_2 - r(\eta \frac{\partial L}{\partial w})_1 - r^2(\eta \frac{\partial L}{\partial w})_0 \\\end{align*}</script><ul><li>梯度<code>下降的方向</code>有了<code>惯性</code></li><li>上一步的梯度向量与现在这一点的梯度向量以加权的方式求和，求解出真实梯度向量</li></ul><h4 id="tensor实现动量法"><a href="#tensor实现动量法" class="headerlink" title="tensor实现动量法"></a>tensor实现动量法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#恢复小步长</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line"></span><br><span class="line">dw = net.linear1.weight.grad</span><br><span class="line">w = net.linear1.weight.data</span><br><span class="line">v = torch.zeros(dw.shape[<span class="number">0</span>],dw.shape[<span class="number">1</span>]) <span class="comment"># v和w的形状相同</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#==========分割cell，不然重复运行的时候w会每次都被覆盖掉=============</span></span><br><span class="line"><span class="comment">#对任意w可以有</span></span><br><span class="line">v = gamma * v - lr * dw</span><br><span class="line">w += v</span><br><span class="line"></span><br><span class="line"><span class="comment">#不难发现，当加入gamma之后，即便是较小的步长，也可以让w发生变化</span></span><br></pre></td></tr></table></figure><h4 id="torch-optim实现动量法"><a href="#torch-optim实现动量法" class="headerlink" title="torch.optim实现动量法"></a>torch.optim<code>实现</code>动量法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#确定数据、确定优先需要设置的值</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">gamma = <span class="number">0.9</span></span><br><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">X = torch.rand((<span class="number">500</span>,<span class="number">20</span>),dtype=torch.float32) * <span class="number">100</span></span><br><span class="line">y = torch.randint(low=<span class="number">0</span>,high=<span class="number">3</span>,size=(<span class="number">500</span>,<span class="number">1</span>),dtype=torch.float32)</span><br><span class="line">input_ = X.shape[<span class="number">1</span>] <span class="comment">#特征的数目</span></span><br><span class="line">output_ = <span class="built_in">len</span>(y.unique()) <span class="comment">#分类的数目</span></span><br><span class="line"><span class="comment">#定义神经网路的架构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_features=<span class="number">10</span>,out_features=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Model,self).__init__() <span class="comment">#super(请查找这个类的父类，请使用找到的父类替换现在的类)</span></span><br><span class="line">        self.linear1 = nn.Linear(in_features,<span class="number">13</span>,bias=<span class="literal">True</span>) <span class="comment">#输入层不用写，这里是隐藏层的第一层</span></span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">13</span>,<span class="number">8</span>,bias=<span class="literal">True</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">8</span>,out_features,bias=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">        z1 = self.linear1(x)</span><br><span class="line">        sigma1 = torch.relu(z1)</span><br><span class="line">        z2 = self.linear2(sigma1)</span><br><span class="line">        sigma2 = torch.sigmoid(z2)</span><br><span class="line">        z3 = self.output(sigma2)</span><br><span class="line">        <span class="comment">#sigma3 = F.softmax(z3,dim=1)</span></span><br><span class="line">        <span class="keyword">return</span> z3</span><br><span class="line"><span class="comment">#实例化神经网络，调用优化算法需要的参数</span></span><br><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">net = Model(in_features=input_, out_features=output_)</span><br><span class="line">net.parameters()</span><br><span class="line">    </span><br><span class="line"><span class="comment">#定义损失函数</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment">#定义优化算法</span></span><br><span class="line">opt = optim.SGD(net.parameters() <span class="comment">#要优化的参数是哪些？</span></span><br><span class="line">               , lr=lr <span class="comment">#学习率</span></span><br><span class="line">               , momentum = gamma <span class="comment">#动量参数</span></span><br><span class="line">               )</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="神经网络迭代过程简述"><a href="#神经网络迭代过程简述" class="headerlink" title="神经网络迭代过程简述"></a>神经网络迭代过程简述</h4><ul><li><p><strong>迭代流程</strong>包括以下步骤：</p><ol><li><strong>正向传播</strong>：计算网络的输出。</li><li><strong>计算损失</strong>：根据输出和真实标签计算损失函数。</li><li><strong>反向传播</strong>：通过网络传递损失的梯度。</li><li><strong>更新权重 (w)</strong>：根据梯度调整网络参数。</li><li><strong>清除梯度</strong>：为下一次迭代准备。</li></ol></li><li><p>总结：<strong>“正损反更清”</strong>，即正向传播、计算损失、反向传播、更新权重和清除梯度。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">zhat = net.forward(X) <span class="comment">#向前传播</span></span><br><span class="line">loss = criterion(zhat,y.reshape(<span class="number">500</span>).long()) <span class="comment">#损失函数值</span></span><br><span class="line">loss.backward(retain_graph=<span class="literal">True</span>) <span class="comment">#反向传播</span></span><br><span class="line">opt.step() <span class="comment">#更新权重w，从这一瞬间开始，坐标点就发生了变化，所有的梯度必须重新计算</span></span><br><span class="line">opt.zero_grad() <span class="comment">#清除原来储存好的，基于上一个坐标点计算的梯度，为下一次计算梯度腾出空间</span></span><br><span class="line"><span class="built_in">print</span>(loss)</span><br><span class="line"><span class="built_in">print</span>(net.linear1.weight.data[<span class="number">0</span>][:<span class="number">10</span>])</span><br><span class="line"><span class="comment"># 这样每执行依次loss变小</span></span><br></pre></td></tr></table></figure><pre><code>tensor(1.1057, grad_fn=&lt;NllLossBackward0&gt;)tensor([ 0.1365, -0.1346,  0.2128, -0.1776, -0.0682, -0.1541,  0.1724,  0.0839,        -0.1115, -0.1729])</code></pre><h3 id="6-Batch-Size-与-Epoches-在迭代中的作用"><a href="#6-Batch-Size-与-Epoches-在迭代中的作用" class="headerlink" title="6.Batch Size 与 Epoches 在迭代中的作用"></a>6.Batch Size 与 Epoches 在迭代中的作用</h3><ul><li><p><strong>小批量梯度下降的必要性</strong></p><ul><li><strong>正向传播</strong>：大量数据在正向传播中导致计算速度缓慢。</li><li><strong>反向传播</strong>：采用动量法加速，但若数据量大，仍影响效率。</li><li><strong>Mini-batch</strong>：更易找到全局最小值，提升训练效率。</li></ul></li><li><p><strong>不同梯度下降方法的对比</strong></p><ul><li><strong>传统梯度下降</strong>：相同数据集，仅小范围内权重更新。快速接近最小值，但易陷入局部最优。</li><li><strong>随机梯度下降</strong>：每次一个样本，高随机性，计算不稳定。</li><li><strong>Mini-batch SGD</strong>：结合两者优点，数据集和权重均有变化，路径曲折但有效。</li></ul></li><li><p><strong>优势</strong></p><ul><li>减少计算开销，提高训练效率。</li><li>在寻找最优解过程中，避免局部最优的困扰。</li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/SGD.png" alt="SGD"></p><h3 id="7-使用-TensorDataset-与-DataLoader-处理数据"><a href="#7-使用-TensorDataset-与-DataLoader-处理数据" class="headerlink" title="7. 使用 TensorDataset 与 DataLoader 处理数据"></a>7. 使用 TensorDataset 与 DataLoader 处理数据</h3><ul><li><p><strong>TensorDataset 用于数据打包</strong></p><ul><li>功能：将数据打包成元组格式。</li><li>要求：数据的首维度大小需保持一致。</li><li>使用示例：通过 <code>td[0]</code> 访问数据。</li></ul></li><li><p><strong>DataLoader 用于数据分批处理</strong></p><ul><li>功能：切分数据为小批量，便于训练。</li><li>特点：支持任意形式的张量或数组。</li><li>主要参数：<ul><li><code>dataset</code>：包含所有数据的列表，通过 <code>dl.dataset[0]</code> 访问。</li><li><code>batch_size</code>（简写为 <code>bs</code>）：定义每批数据的大小。</li><li>通过 <code>len(dl)</code> 计算得到总批次（即 <code>m/bs</code>）。</li></ul></li></ul></li><li><p><strong>特别说明</strong></p><ul><li>若数据集已经以特征张量和标签的形式组合在一起，可跳过此步骤。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">TensorDataset:打包操作</span></span><br><span class="line"><span class="string">DataLoader：切割操作</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">a = torch.randn(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.randn(<span class="number">2</span>,<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">c = torch.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> TensorDataset(a,b,c):<span class="comment"># 元组</span></span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line"><span class="keyword">for</span> x,y,z <span class="keyword">in</span> DataLoader(TensorDataset(a,b,c),batch_size=<span class="number">2</span>,drop_last=<span class="literal">True</span>): <span class="comment"># 列表且升维</span></span><br><span class="line">    <span class="built_in">print</span>(x)</span><br><span class="line">    <span class="built_in">print</span>(x.shape)</span><br><span class="line">    <span class="built_in">print</span>(y.shape)</span><br><span class="line">    <span class="built_in">print</span>(z.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># TensorDataset是元组！</span></span><br><span class="line"><span class="comment"># DataLoader转换为列表！</span></span><br><span class="line"><span class="comment"># batch_size是在每一个列表上升维！</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># c = torch.randn(300,1)</span></span><br><span class="line"><span class="comment"># TensorDataset(a,c)[0] </span></span><br><span class="line"><span class="comment"># 报错</span></span><br></pre></td></tr></table></figure><pre><code>(tensor([[-0.3513, -0.3595,  0.9324],        [-1.1048, -0.8469,  1.4757]]), tensor([[ 0.4467,  1.2014, -0.5548,  0.7919],        [ 0.1874, -0.7003,  0.4888, -0.8637]]), tensor([0.1421]))(tensor([[ 1.8659, -1.3644, -0.3450],        [ 1.7672,  1.4598, -0.3842]]), tensor([[-0.4296, -0.5561, -1.4006,  0.3309],        [-0.6369, -0.2273, -0.6020, -0.3844]]), tensor([-0.1834]))tensor([[[-0.3513, -0.3595,  0.9324],         [-1.1048, -0.8469,  1.4757]],        [[ 1.8659, -1.3644, -0.3450],         [ 1.7672,  1.4598, -0.3842]]])torch.Size([2, 2, 3])torch.Size([2, 2, 4])torch.Size([2, 1])</code></pre><ul><li>对于mini-batch SGD,我们一般这样使用TensorDataset和DataLoader</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">X = torch.rand((<span class="number">50000</span>,<span class="number">20</span>),dtype=torch.float32) * <span class="number">100</span> <span class="comment">#要进行迭代了，增加样本数量</span></span><br><span class="line">y = torch.randint(low=<span class="number">0</span>,high=<span class="number">3</span>,size=(<span class="number">50000</span>,<span class="number">1</span>),dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">4</span></span><br><span class="line">bs = <span class="number">4000</span></span><br><span class="line"></span><br><span class="line">data = TensorDataset(X,y)</span><br><span class="line">batchdata = DataLoader(data, batch_size=bs, shuffle = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">len</span>(batchdata) <span class="comment">#查看具体被分了多少个batch 50000 / 14 = 13(12.5)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#可以使用.datasets查看数据集相关的属性</span></span><br><span class="line"><span class="built_in">print</span>(batchdata.dataset[<span class="number">49999</span>]) <span class="comment">#查看其中一个样本 列表[tensor([500,20]),tensor([1])]</span></span><br><span class="line"><span class="built_in">print</span>(batchdata.batch_size) <span class="comment">#查看其中一个样本 列表[tensor([500,20]),tensor([1])]</span></span><br><span class="line"><span class="keyword">for</span> batch_idx, (x,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(batchdata):</span><br><span class="line">    <span class="comment">#sigma = net(x)</span></span><br><span class="line">    <span class="comment">#loss = lossfn(sigma, y)</span></span><br><span class="line">    <span class="comment">#loss.backward()</span></span><br><span class="line">    <span class="comment">#opt.step()</span></span><br><span class="line">    <span class="comment">#opt.zero_grad()</span></span><br><span class="line">    <span class="built_in">print</span>(batch_idx)</span><br><span class="line">    <span class="built_in">print</span>(x.shape)</span><br><span class="line">    <span class="built_in">print</span>(y.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># print(x,y)</span></span><br><span class="line">    <span class="keyword">if</span> batch_idx == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">break</span> <span class="comment">#为了演示用，所以打断，在正常的循环里是不会打断的</span></span><br></pre></td></tr></table></figure><pre><code>(tensor([61.8071, 94.8606, 77.5624,  4.6598, 50.3617, 10.5395, 37.8427, 88.2321,        55.0742, 11.2930, 35.5145, 37.8230, 83.2974, 16.3466, 91.3999, 76.7965,        71.2780, 36.9534,  1.4939, 31.9203]), tensor([1.]))40000torch.Size([4000, 20])torch.Size([4000, 1])1torch.Size([4000, 20])torch.Size([4000, 1])</code></pre><h3 id="8-在Mini-Fashion上实现神经网络的学习流程"><a href="#8-在Mini-Fashion上实现神经网络的学习流程" class="headerlink" title="8.在Mini-Fashion上实现神经网络的学习流程"></a>8.在Mini-Fashion上实现神经网络的学习流程</h3><ul><li>在<code>Linux/UNIX</code>中,推荐使用<code>/</code>,<code>Python解释器</code>也推荐使用<code>/</code></li><li>在Windows中,推荐使用\</li><li>以下三个表示等价<ul><li>root=<code>&#39;../lesson 11/&#39;</code></li><li>root=<code>&#39;..\\lesson 11\\&#39;</code></li><li>root=<code>r&#39;..\lesson 11\\&#39;</code><ul><li><code>./</code>表示当前py文件的<code>子目录</code>下</li><li><code>../</code>表示当前py文件的<code>子目录的父目录</code>下</li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入库，设置各种初始值</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment">#确定数据、确定优先需要设置的值</span></span><br><span class="line">lr = <span class="number">0.15</span></span><br><span class="line">gamma = <span class="number">0.8</span></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line">bs = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#导入数据</span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line">mnist = torchvision.datasets.FashionMNIST(root=<span class="string">&#x27;../lesson 11/&#x27;</span>,</span><br><span class="line">                                          train=<span class="literal">True</span>,</span><br><span class="line">                                          download=<span class="literal">True</span>,</span><br><span class="line">                                          transform=transforms.ToTensor())</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1. 将一个 PIL 图片或者一个 NumPy ndarray 转换为 FloatTensor。</span></span><br><span class="line"><span class="string">2. 把图片的像素值从 [0, 255] 范围线性缩放到 [0.0, 1.0] 范围。即原先的整数类型像素值被缩放到浮点数，并且归一化到0到1之间的范围。</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 这里mnist是一个Dataset的对象</span></span><br></pre></td></tr></table></figure><ul><li><code>mnist</code> 数据集是 <code>Dataset</code> 类的一个实例，主要包含以下特点：<ul><li>数据结构：每个元素是一个元组，包含特征和标签。</li><li><code>data</code> 属性：特征张量的形状为 <code>torch.Size([60000, 28, 28])</code>，表示有 60000 个样本，每个样本为 28x28 的图像。</li><li><code>targets</code> 属性：标签张量的形状为 <code>torch.Size([60000])</code>，表示有 60000 个标签。</li><li><code>classes</code> 属性：类别列表，包括 <code>[&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;]</code>，共 10 类。</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看特征张量</span></span><br><span class="line"><span class="built_in">print</span>(mnist.data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看标签</span></span><br><span class="line"><span class="built_in">print</span>(mnist.targets.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看标签的类别</span></span><br><span class="line"><span class="built_in">print</span>(mnist.classes)</span><br></pre></td></tr></table></figure><pre><code>torch.Size([60000, 28, 28])torch.Size([60000])[&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;]</code></pre><ul><li>查看图像的模样</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.imshow(mnist[<span class="number">1</span>][<span class="number">0</span>].view((<span class="number">28</span>, <span class="number">28</span>)).numpy());</span><br><span class="line"><span class="comment"># mnist[1][0]表示第一个元组的特征张量(1,28,28) </span></span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/shirt.png" alt="shirt"></p><ul><li>分割batch</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">batchdata = DataLoader(mnist,batch_size=bs, shuffle = <span class="literal">True</span>)</span><br><span class="line"><span class="comment">#总共多少个batch?</span></span><br><span class="line"><span class="built_in">len</span>(batchdata) <span class="comment"># 60000/128=469(ceil)</span></span><br><span class="line"><span class="comment">#查看会放入进行迭代的数据结构</span></span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> batchdata:</span><br><span class="line">    <span class="built_in">print</span>(x.shape)  <span class="comment"># torch.Size([128, 1, 28, 28])</span></span><br><span class="line">    <span class="built_in">print</span>(y.shape)  <span class="comment"># torch.Size([128])</span></span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">input_ = mnist.data[<span class="number">0</span>].numel() <span class="comment">#特征的数目，一般是第一维之外的所有维度相乘的数</span></span><br><span class="line"><span class="comment"># mnist[0][0].numel()</span></span><br><span class="line"><span class="comment">#.data相当于第二个[0]把特征给跑去了</span></span><br><span class="line"><span class="comment">#[0]相当于第一个[0]即元组的第一个部分</span></span><br><span class="line">output_ = <span class="built_in">len</span>(mnist.targets.unique()) <span class="comment">#分类的数目</span></span><br></pre></td></tr></table></figure><pre><code>torch.Size([128, 1, 28, 28])torch.Size([128])</code></pre><ul><li>定义神经网络架构</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_features=<span class="number">10</span>,out_features=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__() </span><br><span class="line">        <span class="comment"># self.normalize = nn.BatchNorm2d(num_features=1)</span></span><br><span class="line">        self.linear1 = nn.Linear(in_features,<span class="number">128</span>,bias=<span class="literal">False</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">128</span>,out_features,bias=<span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x = self.normalize(x)</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">        <span class="comment">#需要对数据的结构进行一个改变，这里的“-1”代表，我不想算，请pytorch帮我计算</span></span><br><span class="line">        sigma1 = torch.relu(self.linear1(x))</span><br><span class="line">        z2 = self.output(sigma1)</span><br><span class="line">        sigma2 = F.log_softmax(z2,dim=<span class="number">1</span>)  <span class="comment"># 为了得准确率</span></span><br><span class="line">        <span class="keyword">return</span> sigma2</span><br></pre></td></tr></table></figure><ul><li>定义训练函数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">net,batchdata,lr=<span class="number">0.01</span>,epochs=<span class="number">5</span>,gamma=<span class="number">0</span></span>):</span><br><span class="line">    criterion = nn.NLLLoss() <span class="comment">#定义损失函数</span></span><br><span class="line">    opt = optim.SGD(net.parameters(), lr=lr,momentum=gamma) <span class="comment">#定义优化算法</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    samples = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> batch_idx, (x,y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(batchdata):</span><br><span class="line">            y = y.view(x.shape[<span class="number">0</span>])</span><br><span class="line">            sigma = net.forward(x)  <span class="comment"># 正</span></span><br><span class="line">            loss = criterion(sigma,y)  <span class="comment"># 损</span></span><br><span class="line">            loss.backward()  <span class="comment"># 反</span></span><br><span class="line">            opt.step()  <span class="comment"># 更</span></span><br><span class="line">            opt.zero_grad()  <span class="comment"># 清</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">#求解准确率</span></span><br><span class="line">            yhat = torch.<span class="built_in">max</span>(sigma,dim=<span class="number">1</span>)[<span class="number">1</span>] <span class="comment"># 取下标即预测标签</span></span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            (values, indices) = ([0.7], [2]) 最大值和索引</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            correct += torch.<span class="built_in">sum</span>(yhat == y)</span><br><span class="line">            samples += x.shape[<span class="number">0</span>]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> (batch_idx+<span class="number">1</span>) % <span class="number">125</span> == <span class="number">0</span> <span class="keyword">or</span> batch_idx == <span class="built_in">len</span>(batchdata)-<span class="number">1</span>:</span><br><span class="line">                <span class="comment"># 468没被125整除,,要让m/m等于1加上batch_idx == len(batchdata)-1</span></span><br><span class="line">                <span class="comment"># 125 250 375 468故每个epoch打印四次</span></span><br><span class="line">                <span class="comment"># 最后一下没够125，所以没打印100%，但是确实是训练了，只是没够125</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Epoch&#123;&#125;:[&#123;&#125;/&#123;&#125;(&#123;:.0f&#125;%)]\tLoss:&#123;:.6f&#125;\t Accuracy:&#123;:.3f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                    epoch+<span class="number">1</span></span><br><span class="line">                   ,samples</span><br><span class="line">                   ,<span class="built_in">len</span>(batchdata.dataset)*epochs</span><br><span class="line">                   ,<span class="number">100</span>*samples/(<span class="built_in">len</span>(batchdata.dataset)*epochs)</span><br><span class="line">                   ,loss.data.item()</span><br><span class="line">                   ,<span class="built_in">float</span>(correct*<span class="number">100</span>)/samples))</span><br></pre></td></tr></table></figure><ul><li>进行训练和评估</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#实例化神经网络，调用优化算法需要的参数</span></span><br><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">net = Model(in_features=input_, out_features=output_)</span><br><span class="line">fit(net,batchdata,lr=lr,epochs=epochs,gamma=gamma)</span><br></pre></td></tr></table></figure><pre><code>Epoch1:[16000/600000(3%)]    Loss:0.445854     Accuracy:71.844Epoch1:[32000/600000(5%)]    Loss:0.541044     Accuracy:76.766Epoch1:[48000/600000(8%)]    Loss:0.420771     Accuracy:78.817Epoch1:[60000/600000(10%)]    Loss:0.456890     Accuracy:79.743Epoch2:[76000/600000(13%)]    Loss:0.520517     Accuracy:80.801Epoch2:[92000/600000(15%)]    Loss:0.335469     Accuracy:81.486Epoch2:[108000/600000(18%)]    Loss:0.390427     Accuracy:82.020Epoch2:[120000/600000(20%)]    Loss:0.464855     Accuracy:82.355Epoch3:[136000/600000(23%)]    Loss:0.382425     Accuracy:82.770Epoch3:[152000/600000(25%)]    Loss:0.388381     Accuracy:83.201Epoch3:[168000/600000(28%)]    Loss:0.456149     Accuracy:83.518Epoch3:[180000/600000(30%)]    Loss:0.336247     Accuracy:83.714Epoch4:[196000/600000(33%)]    Loss:0.400932     Accuracy:84.001Epoch4:[212000/600000(35%)]    Loss:0.280075     Accuracy:84.251Epoch4:[228000/600000(38%)]    Loss:0.329395     Accuracy:84.461Epoch4:[240000/600000(40%)]    Loss:0.368081     Accuracy:84.579Epoch5:[256000/600000(43%)]    Loss:0.491687     Accuracy:84.777Epoch5:[272000/600000(45%)]    Loss:0.201454     Accuracy:84.953Epoch5:[288000/600000(48%)]    Loss:0.199246     Accuracy:85.079Epoch5:[300000/600000(50%)]    Loss:0.222263     Accuracy:85.179Epoch6:[316000/600000(53%)]    Loss:0.357817     Accuracy:85.341Epoch6:[332000/600000(55%)]    Loss:0.386327     Accuracy:85.465Epoch6:[348000/600000(58%)]    Loss:0.365660     Accuracy:85.593Epoch6:[360000/600000(60%)]    Loss:0.302036     Accuracy:85.690Epoch7:[376000/600000(63%)]    Loss:0.270010     Accuracy:85.834Epoch7:[392000/600000(65%)]    Loss:0.289248     Accuracy:85.949Epoch7:[408000/600000(68%)]    Loss:0.262059     Accuracy:86.047Epoch7:[420000/600000(70%)]    Loss:0.302930     Accuracy:86.116Epoch8:[436000/600000(73%)]    Loss:0.255372     Accuracy:86.247Epoch8:[452000/600000(75%)]    Loss:0.323171     Accuracy:86.354Epoch8:[468000/600000(78%)]    Loss:0.223748     Accuracy:86.437Epoch8:[480000/600000(80%)]    Loss:0.403298     Accuracy:86.499Epoch9:[496000/600000(83%)]    Loss:0.297387     Accuracy:86.578Epoch9:[512000/600000(85%)]    Loss:0.356909     Accuracy:86.666Epoch9:[528000/600000(88%)]    Loss:0.306189     Accuracy:86.757Epoch9:[540000/600000(90%)]    Loss:0.389048     Accuracy:86.804Epoch10:[556000/600000(93%)]    Loss:0.337781     Accuracy:86.882Epoch10:[572000/600000(95%)]    Loss:0.256883     Accuracy:86.969Epoch10:[588000/600000(98%)]    Loss:0.188146     Accuracy:87.044Epoch10:[600000/600000(100%)]    Loss:0.293160     Accuracy:87.087</code></pre><h3 id="9-数据处理总结"><a href="#9-数据处理总结" class="headerlink" title="9. 数据处理总结"></a>9. 数据处理总结</h3><h4 id="TensorDataset-的使用"><a href="#TensorDataset-的使用" class="headerlink" title="TensorDataset 的使用"></a>TensorDataset 的使用</h4><ul><li>实例化：<code>td = TensorDataset(a, b, c)</code>，形成元组 <code>(tensor(), tensor(), tensor())</code>。</li><li>访问元素：使用 <code>td[0]</code> 可访问每个元组；适用于 <code>for</code> 循环遍历。</li><li>图片数据处理：可通过 <code>td.data</code>、<code>td.classes</code>、<code>td.targets</code> 进行访问。<ul><li>也是minist等data类的类型</li></ul></li></ul><h4 id="DataLoader-的应用"><a href="#DataLoader-的应用" class="headerlink" title="DataLoader 的应用"></a>DataLoader 的应用</h4><ul><li>实例化：<code>dl = DataLoader(td, batch_size)</code>，形成列表。</li><li>遍历数据：<ul><li>使用 <code>for x, y in dl: print(i)</code> 遍历；列表格式 <code>[tensor(), tensor(), tensor()]</code>。</li><li>对于图片数据，<code>x.shape</code> 可能为 <code>[batch_size, 1, 28, 28]</code> 或 <code>[batch_size, 28, 28]</code>。</li></ul></li><li>注意事项：<ul><li>不能直接使用 <code>dl[0]</code> 访问元素，只能<code>dl.dataset[0]</code>。</li><li><code>dl.dataset</code> 包含所有数据列表；<code>len(dl)</code> 表示总批次数（即 <code>m/batch_size</code>）；<code>dl.batch_size</code> 表示每批数据的大小。</li><li><code>batch_size</code> 增加了数据列表的一个维度。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(10)-训练损失</title>
      <link href="/Pytorch-10/"/>
      <url>/Pytorch-10/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br></pre></td></tr></table></figure><h3 id="1-训练目标与流程"><a href="#1-训练目标与流程" class="headerlink" title="1. 训练目标与流程"></a>1. 训练目标与流程</h3><div class="table-container"><table><thead><tr><th>步骤</th><th>描述</th></tr></thead><tbody><tr><td><strong>目标</strong></td><td>找到参数 <code>w</code> 使预测与真实值最接近。</td></tr><tr><td><strong>流程</strong></td><td>模型 (Model) + 损失 (Loss) + 优化器 (Optim) ⇒ 求解 <code>w</code></td></tr><tr><td><strong>损失</strong></td><td><code>L(w)</code></td></tr><tr><td><strong>数学工具</strong></td><td>- 化为凸函数：拉格朗日变换<br>- 最小化 <code>L(w)</code> 时的 <code>w</code>：梯度下降法为代表</td></tr></tbody></table></div><h3 id="2-回归与误差分析"><a href="#2-回归与误差分析" class="headerlink" title="2. 回归与误差分析"></a>2. 回归与误差分析</h3><div class="table-container"><table><thead><tr><th>类型</th><th>描述</th></tr></thead><tbody><tr><td><strong>回归</strong></td><td>误差平方和SSE</td></tr><tr><td><strong>MSE</strong></td><td><code>MSE = SSE/m</code></td></tr><tr><td><strong>顺序</strong></td><td><code>(yhat, y)</code> 的顺序</td></tr><tr><td><strong>reduction 参数</strong></td><td>sum/mean(default)/None</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> MSELoss</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">420</span>)</span><br><span class="line">yhat = torch.randn(size=(<span class="number">50</span>,),dtype=torch.float32)</span><br><span class="line">y = torch.randn(size=(<span class="number">50</span>,),dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">criterion = MSELoss(reduction = <span class="string">&#x27;sum&#x27;</span>)</span><br><span class="line">loss = criterion(yhat,y)</span><br><span class="line">loss</span><br></pre></td></tr></table></figure><pre><code>tensor(124.3458)</code></pre><h3 id="3-分类：二分类交叉熵损失函数-对数损失"><a href="#3-分类：二分类交叉熵损失函数-对数损失" class="headerlink" title="3.分类：二分类交叉熵损失函数(对数损失)"></a>3.分类：二分类交叉熵损失函数(对数损失)</h3><ul><li>除非特别声明，不然提到<strong>交叉熵损失</strong>均为<code>多分类</code></li><li><p>极大似然估计：寻找相应的权重w，使得目标事件的发生概率最大，就是极大似然估计的基本方法。</p><ul><li><code>构筑对数似然函数:</code>用于评估目标事件发生的概率，该函数<strong>被设计成</strong>目标事件发生时，概率最大。</li><li><code>对整体取对数:</code>叫做对数似然函数</li><li><code>在对数似然函数上对w求导:</code>并使导数为0，对权重求解。</li></ul></li><li><p>由对数似然函数的概念，可将其定义为：</p></li></ul><script type="math/tex; mode=display">P(\hat{y}_i | x_i, w) = P_1^{y_i} * P_0^{1-y_i}</script><ul><li><strong>为了达成让模型拟合好，损失小的目的，我们每时每刻都希望其为1，也就是说每时每刻都在追求其最大值</strong></li><li>对于多样本而言，推导如下：</li></ul><script type="math/tex; mode=display">\begin{align*}\ln P & = \ln \prod_{i=1}^{m} (\sigma_i^{y_i} \cdot (1 - \sigma_i)^{1-y_i}) \\      & = \sum_{i=1}^{m} \ln(\sigma_i^{y_i} \cdot (1 - \sigma_i)^{1-y_i}) \\      & = \sum_{i=1}^{m} (\ln \sigma_i^{y_i} + \ln(1 - \sigma_i)^{1-y_i}) \\      & = \sum_{i=1}^{m} (y_i \cdot \ln(\sigma_i) + (1 - y_i) \cdot \ln(1 - \sigma_i))\end{align*}</script><ul><li>故对多样本二分类而言，损失函数如下：</li></ul><script type="math/tex; mode=display">L(w) = -\sum_{i=1}^{m} \left( y_i \cdot \ln(\sigma_i) + (1 - y_i) \cdot \ln(1 - \sigma_i) \right)</script><h4 id="3-1-tensor实现二分类交叉熵损失函数"><a href="#3-1-tensor实现二分类交叉熵损失函数" class="headerlink" title="3.1 tensor实现二分类交叉熵损失函数"></a>3.1 tensor实现二分类交叉熵损失函数</h4><ul><li>除了<code>普通的加减乘除</code>，其余<code>均用 torch</code> 运算，<code>比 python 快！</code><ul><li>即sum函数用torch.sum，不要用普通sum</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">m = <span class="number">3</span>*<span class="built_in">pow</span>(<span class="number">10</span>,<span class="number">3</span>)</span><br><span class="line">torch.random.manual_seed(<span class="number">420</span>)</span><br><span class="line">X = torch.rand((m,<span class="number">4</span>),dtype=torch.float32)  <span class="comment"># Input</span></span><br><span class="line">w = torch.rand((<span class="number">4</span>,<span class="number">1</span>),dtype=torch.float32)</span><br><span class="line">y = torch.randint(low=<span class="number">0</span>,high=<span class="number">2</span>,size=(m,<span class="number">1</span>),dtype=torch.float32)</span><br><span class="line">zhat = torch.mm(X,w)  <span class="comment"># Hidden1-1</span></span><br><span class="line">sigma = torch.sigmoid(zhat)  <span class="comment"># Hidden1-2</span></span><br><span class="line">loss = -(<span class="number">1</span>/m)*torch.<span class="built_in">sum</span>(y*torch.log(sigma) + (<span class="number">1</span>-y)*torch.log(<span class="number">1</span>-sigma))</span><br><span class="line">loss</span><br></pre></td></tr></table></figure><pre><code>tensor([0.7962])</code></pre><h4 id="3-2-torch实现二分类交叉熵损失函数"><a href="#3-2-torch实现二分类交叉熵损失函数" class="headerlink" title="3.2 torch实现二分类交叉熵损失函数"></a>3.2 torch实现二分类交叉熵损失函数</h4><div class="table-container"><table><thead><tr><th>类别</th><th>描述</th></tr></thead><tbody><tr><td><code>BCEWithLogitsLoss</code></td><td>- 输入：<code>zhat</code>, <code>y</code><br> - 目的：缩小精度误差</td></tr><tr><td><code>BCELoss</code></td><td>- 输入：<code>sigma</code>, <code>y</code><br> - 目的：监控准确率</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.BCEWithLogitsLoss(reduction = <span class="string">&quot;mean&quot;</span>)</span><br><span class="line">loss = criterion(zhat, y)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br><span class="line">criterion2 = nn.BCELoss(reduction = <span class="string">&quot;mean&quot;</span>) <span class="comment">#实例化</span></span><br><span class="line">loss = criterion2(sigma,y)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br></pre></td></tr></table></figure><pre><code>tensor(0.7962)tensor(0.7962)</code></pre><h3 id="4-分类任务中的多分类交叉熵损失函数"><a href="#4-分类任务中的多分类交叉熵损失函数" class="headerlink" title="4. 分类任务中的多分类交叉熵损失函数"></a>4. 分类任务中的多分类交叉熵损失函数</h3><p>在 PyTorch 中处理多分类问题时，常用的方法是结合 <code>nn.LogSoftmax(dim=1)</code> 和 <code>nn.NLLLoss()</code> 来计算交叉熵损失。</p><ul><li><strong>关键步骤</strong>:<ol><li><strong>LogSoftmax 应用</strong>:<ul><li>应用于模型输出，使用 <code>nn.LogSoftmax(dim=1)</code>。</li><li><code>dim=1</code> 确保沿着正确的维度（特征维度）应用 Softmax。</li></ul></li><li><strong>负对数似然函数（Negative Log Likelihood function）</strong>:<ul><li>使用 <code>nn.NLLLoss()</code> 计算损失。</li><li>确保标签张量 <code>y</code> 使用 <code>.long()</code> 进行类型转换，以匹配损失函数的要求。</li></ul></li><li>即，在计算损失函数时，我们<strong>不再需要使用单独的softmax函数</strong>了。</li><li>推导过程：</li></ol></li></ul><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/BCELoss.png" alt="Alt text"></p><h4 id="4-1-PyTorch-nn-LogSoftmax-dim-1-和nn-NLLLoss"><a href="#4-1-PyTorch-nn-LogSoftmax-dim-1-和nn-NLLLoss" class="headerlink" title="4.1 PyTorch(nn.LogSoftmax(dim=1)和nn.NLLLoss())"></a>4.1 PyTorch(<code>nn.LogSoftmax(dim=1)</code>和<code>nn.NLLLoss()</code>)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">N = <span class="number">3</span>*<span class="built_in">pow</span>(<span class="number">10</span>,<span class="number">2</span>) </span><br><span class="line">torch.random.manual_seed(<span class="number">420</span>) </span><br><span class="line">X = torch.rand((N,<span class="number">4</span>),dtype=torch.float32) </span><br><span class="line">w = torch.rand((<span class="number">4</span>,<span class="number">3</span>),dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = torch.randint(low=<span class="number">0</span>,high=<span class="number">3</span>,size=(N,),dtype=torch.float32)</span><br><span class="line">zhat = torch.mm(X,w)</span><br><span class="line"><span class="comment">#从这里开始调用 softmax 和 NLLLoss</span></span><br><span class="line">logsm = nn.LogSoftmax(dim=<span class="number">1</span>) <span class="comment">#实例化</span></span><br><span class="line">logsigma = logsm(zhat)</span><br><span class="line">criterion = nn.NLLLoss() <span class="comment">#实例化</span></span><br><span class="line"><span class="comment">#由于交叉熵损失需要将标签转化为独热形式，因此不接受浮点数作为标签的输入</span></span><br><span class="line"><span class="comment">#对 NLLLoss 而言，需要输入 logsigma</span></span><br><span class="line">criterion(logsigma,y.long())</span><br></pre></td></tr></table></figure><pre><code>tensor(1.1591, grad_fn=&lt;NllLossBackward0&gt;)</code></pre><h4 id="4-2-PyTorch-nn-CrossEntropyLoss"><a href="#4-2-PyTorch-nn-CrossEntropyLoss" class="headerlink" title="4.2 PyTorch(nn.CrossEntropyLoss())"></a>4.2 PyTorch(<code>nn.CrossEntropyLoss()</code>)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">criterion(zhat,y.long())</span><br></pre></td></tr></table></figure><pre><code>tensor(1.1591, grad_fn=&lt;NllLossBackward0&gt;)</code></pre><h3 id="5-PyTorch-中的分类损失函数选择"><a href="#5-PyTorch-中的分类损失函数选择" class="headerlink" title="5. PyTorch 中的分类损失函数选择"></a>5. PyTorch 中的分类损失函数选择</h3><p>在 PyTorch 中，根据分类任务的类型（二分类或多分类），有不同的损失函数选择：</p><ul><li><p><strong>二分类任务</strong>:</p><ul><li><strong>不含激活函数的输出</strong>:<ul><li>使用 <code>BCELoss()</code>。</li><li>应用场景：模型输入 <code>sigma</code>, 真实标签 <code>y</code>。</li></ul></li><li><strong>含激活函数的输出</strong>:<ul><li>使用 <code>BCEWithLogitsLoss()</code>。</li><li>应用场景：模型输入 <code>zhat</code>, 真实标签 <code>y</code>。</li></ul></li></ul></li><li><p><strong>多分类任务</strong>:</p><ul><li><strong>含激活函数</strong>:<ul><li>使用 <code>CrossEntropyLoss()</code>。</li><li>应用场景：模型输出 <code>zhat</code>, 真实标签 <code>y.long()</code>（确保标签为长整型）。</li></ul></li><li><strong>含对数激活函数</strong>:<ul><li>首先应用 <code>LogSoftmax(dim=1)</code> 于模型输出 <code>logsigma</code>。</li><li>然后使用 <code>NLLLoss(logsigma,y.long())</code> 计算损失。</li><li>应用场景：处理后的 <code>logsigma</code>, 真实标签 <code>y.long()</code>。</li></ul></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(8)-单层神经网络</title>
      <link href="/Pytorch-8/"/>
      <url>/Pytorch-8/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure><h3 id="1-单层线性回归"><a href="#1-单层线性回归" class="headerlink" title="1.单层线性回归"></a>1.单层线性回归</h3><ul><li><code>tensor</code>实现神经网络的正向传播<ul><li><code>特征张量x</code>最好设置为<code>float32</code>规避bug</li><li><code>标签张量z</code>最好设置为<code>二维</code>，当<code>有bug再view</code><ul><li>pytorch中的nn.层中最后输出的是二维结果</li></ul></li><li><code>规避精度</code>问题：<code>torch.allclose()</code></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32) </span><br><span class="line">z=torch.tensor([[-<span class="number">0.2</span>],[-<span class="number">0.05</span>],[-<span class="number">0.05</span>],[<span class="number">0.1</span>]])</span><br><span class="line">w=torch.tensor([-<span class="number">0.2</span>,<span class="number">0.15</span>,<span class="number">0.15</span>])</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">LinearR</span>(<span class="params">X,w</span>):</span><br><span class="line">    zhat=torch.mv(X,w)</span><br><span class="line">    <span class="keyword">return</span> zhat</span><br><span class="line">zhat =LinearR(x,w)</span><br><span class="line"><span class="built_in">print</span>(zhat)</span><br><span class="line"><span class="built_in">print</span>(torch.allclose(zhat,z.view(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure><pre><code>tensor([-0.2000, -0.0500, -0.0500,  0.1000])True</code></pre><ul><li><code>torch.nn.Linear</code>实现神经网络的正向传播<ul><li>torch.nn.Linear(<code>上一层神经元的个数</code>,<code>这一层神经元的个数</code>)</li><li>所有<code>nn.module的层</code>都会实例化<code>带梯度的w和b</code></li><li><code>zhat</code>计算结果为<code>二维</code></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line">torch.manual_seed(<span class="number">420</span>)  <span class="comment"># 控制随机性</span></span><br><span class="line">output = torch.nn.Linear(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 不要bias：`output=torch.nn.Linear(2,1,bias=False)`</span></span><br><span class="line">zhat = output(X)</span><br><span class="line"><span class="built_in">print</span>(zhat)</span><br><span class="line"><span class="built_in">print</span>(output.weight)</span><br><span class="line"><span class="built_in">print</span>(output.bias)</span><br></pre></td></tr></table></figure><pre><code>tensor([[0.6730],        [1.1048],        [0.2473],        [0.6792]], grad_fn=&lt;AddmmBackward0&gt;)Parameter containing:tensor([[ 0.4318, -0.4256]], requires_grad=True)Parameter containing:tensor([0.6730], requires_grad=True)</code></pre><h3 id="2-单层逻辑回归"><a href="#2-单层逻辑回归" class="headerlink" title="2.单层逻辑回归"></a>2.单层逻辑回归</h3><ul><li><p><code>sigmoid函数</code>：对数几率回归</p><ul><li>设置<code>阈值</code>为0.5(默认),可得<code>预测</code>为0/1</li><li>化<code>回归</code>为<code>分类</code></li></ul></li><li><p><code>tensor</code>实现二分类神经网络的正向传播</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)  <span class="comment"># Input</span></span><br><span class="line">w=torch.tensor([-<span class="number">0.2</span>,<span class="number">0.15</span>,<span class="number">0.15</span>])</span><br><span class="line">andgate=torch.tensor([[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">0</span>],[<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">LogisticR</span>(<span class="params">x,W</span>): </span><br><span class="line">    zhat=torch.mv(x,W)  <span class="comment"># Hidden1-1</span></span><br><span class="line">    sigma=torch.sigmoid(zhat)  <span class="comment"># Hidden1-2</span></span><br><span class="line">    andhat=torch.tensor([<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> sigma &gt;=<span class="number">0.5</span>],dtype=torch.float32)  <span class="comment"># Output</span></span><br><span class="line">    <span class="keyword">return</span> sigma,andhat</span><br><span class="line">sigma,andhat=LogisticR(x,w)</span><br><span class="line"><span class="built_in">print</span>(sigma)</span><br><span class="line"><span class="built_in">print</span>(andgate)</span><br></pre></td></tr></table></figure><pre><code>tensor([0.4502, 0.4875, 0.4875, 0.5250])tensor([[0.],        [0.],        [0.],        [1.]])</code></pre><ul><li><code>torch.functional</code>实现<code>二分类神经网络</code>的<code>正向传播</code><ul><li><code>relu</code>/<code>sigmoid</code>重要所以<code>F有</code>，<code>本质</code>还是<code>torch.</code></li><li><code>tanh</code>/<code>sign</code>还行所以<code>torch.</code></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">X=torch.tensor([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)   <span class="comment"># Input</span></span><br><span class="line">torch.random.manual_seed(<span class="number">420</span>)</span><br><span class="line">output=torch.nn.Linear(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">zhat=output(X)   <span class="comment"># Hidden1-1</span></span><br><span class="line">sigma=F.sigmoid(zhat)  <span class="comment"># Hidden1-2</span></span><br><span class="line">andhat = [<span class="built_in">int</span> (x) <span class="keyword">for</span> x <span class="keyword">in</span> sigma &gt; <span class="number">0.5</span>]  <span class="comment"># Output</span></span><br><span class="line"><span class="built_in">print</span>(zhat)</span><br><span class="line"><span class="built_in">print</span>(sigma)</span><br><span class="line"><span class="built_in">print</span>(andhat)</span><br></pre></td></tr></table></figure><pre><code>tensor([[0.6730],        [1.1048],        [0.2473],        [0.6792]], grad_fn=&lt;AddmmBackward0&gt;)tensor([[0.6622],        [0.7512],        [0.5615],        [0.6636]], grad_fn=&lt;SigmoidBackward0&gt;)[1, 1, 1, 1]</code></pre><h3 id="3-Softmax回归"><a href="#3-Softmax回归" class="headerlink" title="3.Softmax回归"></a>3.Softmax回归</h3><ul><li>假设<code>三分类</code>，故Linear(2,<code>3</code>)</li><li>一般在<code>输出层</code>使用</li><li>本质也是<code>torch.</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X=torch.tensor([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)   <span class="comment"># Input</span></span><br><span class="line">torch.random.manual_seed(<span class="number">420</span>)</span><br><span class="line">output=torch.nn.Linear(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">zhat=output(X)   <span class="comment"># Hidden1-1</span></span><br><span class="line">sigma = F.softmax(zhat,dim=<span class="number">1</span>)  <span class="comment"># Output</span></span><br><span class="line"><span class="built_in">print</span>(zhat)</span><br><span class="line"><span class="built_in">print</span>(sigma)</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.5453,  0.2653, -0.3527],        [ 0.9772,  0.9382, -0.5684],        [ 0.1197, -0.2964, -0.8400],        [ 0.5516,  0.3765, -1.0557]], grad_fn=&lt;AddmmBackward0&gt;)tensor([[0.4623, 0.3494, 0.1883],        [0.4598, 0.4422, 0.0980],        [0.4896, 0.3229, 0.1875],        [0.4902, 0.4115, 0.0983]], grad_fn=&lt;SoftmaxBackward0&gt;)</code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(9)-单层到多层</title>
      <link href="/Pytorch-9/"/>
      <url>/Pytorch-9/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure><h3 id="1-数据划分与阶跃函数的关系"><a href="#1-数据划分与阶跃函数的关系" class="headerlink" title="1. 数据划分与阶跃函数的关系"></a>1. 数据划分与阶跃函数的关系</h3><p>在构建神经网络的 <strong>“与门”</strong> 逻辑时，我们通过应用 <code>sigmoid(wx+b)</code> 函数并设置阈值为 <code>0.5</code> 来决定输出 <code>yhat</code>。这实质上引入了阶跃函数的概念，即：</p><ul><li>当 <code>wx+b</code> 大于 0 时，输出为 <code>1</code>；小于 0 时，输出为 <code>0</code>。这样的映射关系等价于阶跃函数。</li><li><code>wx+b=0</code> 的解析线定义了数据的<strong>二元分类界限</strong>，即数据划分线，可由 <code>x2=(-b-w1x1)/w2</code> 表示。</li><li>总结而言，<strong>阶跃函数等同于使用 <code>sigmoid(wx+b)</code> 以 <code>0.5</code> 为阈值进行数据划分</strong>。</li></ul><h3 id="2-或门"><a href="#2-或门" class="headerlink" title="2.或门"></a>2.或门</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">OR</span>(<span class="params">X</span>):</span><br><span class="line">    w = torch.tensor([-<span class="number">0.08</span>, <span class="number">0.15</span>,<span class="number">0.15</span>], dtype = torch.float32)</span><br><span class="line">    zhat = torch.mv(X,w)</span><br><span class="line">    <span class="comment"># sigma  = F.sigmoid(zhat)</span></span><br><span class="line">    yhat = torch.tensor([<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> zhat &gt; <span class="number">0</span>], dtype=torch.float32) <span class="comment">#化sigmoid和0.5为阶跃</span></span><br><span class="line">    <span class="keyword">return</span> yhat</span><br><span class="line">OR(X)</span><br></pre></td></tr></table></figure><pre><code>tensor([0., 1., 1., 1.])</code></pre><h3 id="3-非与门"><a href="#3-非与门" class="headerlink" title="3.非与门"></a>3.非与门</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X = torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">NAND</span>(<span class="params">X</span>):</span><br><span class="line">    w = torch.tensor([<span class="number">0.23</span>,-<span class="number">0.15</span>,-<span class="number">0.15</span>], dtype = torch.float32) <span class="comment">#和与门、或门都不同的权重</span></span><br><span class="line">    zhat = torch.mv(X,w)</span><br><span class="line">    yhat = torch.tensor([<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> zhat &gt;= <span class="number">0</span>],dtype=torch.float32)</span><br><span class="line">    <span class="keyword">return</span> yhat</span><br><span class="line">NAND(X)</span><br></pre></td></tr></table></figure><pre><code>tensor([1., 1., 1., 0.])</code></pre><h3 id="4-与门、或门、非与门的总结"><a href="#4-与门、或门、非与门的总结" class="headerlink" title="4. 与门、或门、非与门的总结"></a>4. 与门、或门、非与门的总结</h3><ul><li><p><strong>直线拟合</strong>：</p><ul><li>与门、或门、非与门的逻辑可以通过线性回归模型中的直线进行拟合，而异或门由于其输出不是线性可分的，不能仅使用直线来拟合。</li></ul></li><li><p><strong>从直线到曲线</strong>：</p><ul><li>当我们不能用直线来划分数据时，决策边界需要从<code>直线演变为曲线</code>。这通常意味着需要从<code>单层网络转变为多层网络</code>来实现更复杂的决策边界。</li></ul></li><li><p><strong>逻辑门总结</strong>：</p><ul><li><strong>与门</strong> (<code>AND gate</code>)：可以通过 <code>sigmoid</code> 函数配合 <code>0.5</code> 的阈值来实现。</li><li><strong>或门</strong> (<code>OR gate</code>)：可以通过 <code>sign</code> 函数来实现，其输出取决于输入值的符号。</li><li><strong>非与门</strong> (<code>NAND gate</code>)：同样可以通过 <code>sign</code> 函数来实现，其输出是与门的逆逻辑。</li><li>输出层的激活函数 <code>g(z)</code> 对网络的性能并不产生影响。</li></ul></li></ul><h3 id="5-异或门"><a href="#5-异或门" class="headerlink" title="5. 异或门"></a>5. 异或门</h3><p><img src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/NOR.png" alt="NOR"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">AND</span>(<span class="params">X</span>):</span><br><span class="line">    w=torch.tensor([-<span class="number">0.2</span>,<span class="number">0.15</span>,<span class="number">0.15</span>])</span><br><span class="line">    zhat = torch.mv(X,w)</span><br><span class="line">    <span class="comment"># yhat = F.sigmoid(zhat) #原始AND的h(z)是sigmoid</span></span><br><span class="line">    yhat = torch.tensor([<span class="built_in">int</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> zhat&gt;<span class="number">0</span>],dtype=torch.float32) <span class="comment">#换阶跃效果不变，其与下面等效</span></span><br><span class="line">    <span class="comment"># yhat = torch.tensor([int(x) for x in F.sigmoid(zhat)&gt;0.5],dtype=torch.float32)与上面等效</span></span><br><span class="line">    <span class="keyword">return</span> yhat</span><br><span class="line">X = torch.tensor([[<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">XOR</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="comment">#输入层：</span></span><br><span class="line">    input_1 = X</span><br><span class="line">    <span class="comment">#中间层：</span></span><br><span class="line">    sigma_nand = NAND(input_1)</span><br><span class="line">    sigma_or = OR(input_1)</span><br><span class="line">    x0 = torch.tensor([[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>],[<span class="number">1</span>]],dtype=torch.float32)</span><br><span class="line">    <span class="comment">#输出层：</span></span><br><span class="line">    input_2 = torch.cat((x0,sigma_nand.view(<span class="number">4</span>,<span class="number">1</span>),sigma_or.view(<span class="number">4</span>,<span class="number">1</span>)),dim=<span class="number">1</span>)</span><br><span class="line">    y_and = AND(input_2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_and</span><br><span class="line">XOR(X)</span><br></pre></td></tr></table></figure><pre><code>tensor([0., 1., 1., 0.])</code></pre><h3 id="6-神经网络的正向传播"><a href="#6-神经网络的正向传播" class="headerlink" title="6.神经网络的正向传播"></a>6.神经网络的正向传播</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调用必要库</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 确定数据</span></span><br><span class="line">torch.random.manual_seed(<span class="number">420</span>)</span><br><span class="line">X = torch.rand((<span class="number">500</span>,<span class="number">20</span>), dtype=torch.float32)</span><br><span class="line">y = torch.randint(low=<span class="number">0</span>,high=<span class="number">3</span>,size=(<span class="number">500</span>,<span class="number">1</span>), dtype=torch.float32)</span><br><span class="line"><span class="comment"># 确定网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_features, out_features</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model,self).__init__()</span><br><span class="line">        self.linear1 = nn.Linear(in_features,<span class="number">13</span>,bias=<span class="literal">True</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">13</span>,<span class="number">8</span>,bias=<span class="literal">True</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">8</span>,out_features,bias=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>): <span class="comment">#神经网络的向前传播</span></span><br><span class="line">        z1 = self.linear1(x)</span><br><span class="line">        sigma1 = torch.relu(z1)</span><br><span class="line">        z2 = self.linear2(sigma1)</span><br><span class="line">        sigma2 = torch.sigmoid(z2)</span><br><span class="line">        z3 = self.output(sigma2)</span><br><span class="line">        sigma3 = F.softmax(z3,dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> sigma3</span><br><span class="line"><span class="comment">#确定特征数目和分类数目</span></span><br><span class="line">input_ = X.shape[<span class="number">1</span>]</span><br><span class="line">output_ = <span class="built_in">len</span>(y.unique())</span><br><span class="line"><span class="comment">#实例化神经网络</span></span><br><span class="line">torch.random.manual_seed(<span class="number">420</span>)</span><br><span class="line">net = Model(in_features = input_, out_features = output_)</span><br><span class="line"><span class="comment">#前向传播</span></span><br><span class="line">net.forward(X) <span class="comment">#向前传播</span></span><br><span class="line"><span class="comment"># net(X) #或者这样 执行init以下的所有函数</span></span><br></pre></td></tr></table></figure><pre><code>tensor([[0.4140, 0.3496, 0.2365],        [0.4210, 0.3454, 0.2336],        [0.4011, 0.3635, 0.2355],        ...,        [0.4196, 0.3452, 0.2352],        [0.4153, 0.3455, 0.2392],        [0.4153, 0.3442, 0.2405]], grad_fn=&lt;SoftmaxBackward0&gt;)</code></pre><p>在 PyTorch 中处理矩阵维度转换的逻辑通常涉及以下步骤：</p><ol><li><p><strong>权重矩阵转置</strong>：</p><ul><li>如果权重矩阵 <code>W</code> 的初始维度是 <code>(20, 13)</code>，为了进行矩阵乘法 <code>WX</code>，我们需要将其转置为 <code>(13, 20)</code>。</li></ul></li><li><p><strong>特征矩阵转置</strong>：</p><ul><li>同样地，如果特征矩阵 <code>X</code> 的初始维度是 <code>(500, 20)</code>，为了与转置后的权重矩阵 <code>W</code> 相乘，我们需要将 <code>X</code> 转置为 <code>(20, 500)</code>。</li></ul></li><li><p><strong>输出层处理</strong>：</p><ul><li>假设输出层 <code>output</code> 的维度是 <code>(3, 500)</code>，通过 <code>softmax</code> 函数处理后，我们得到一个 <code>(500, 3)</code> 的矩阵，其中每一行代表一个样本的类别概率。</li></ul></li><li><p><strong>偏差向量维度</strong>：</p><ul><li>偏差 <code>bias</code> 的维度大小通常与权重矩阵的输出层维度一致，例如 <code>tensor([13])</code>。</li></ul></li></ol><p>注意点：</p><ul><li><strong>权重矩阵的转置</strong>：<ul><li>在矩阵乘法中，权重矩阵的行和列需要与相应的特征矩阵的维度匹配。这通常意味着要进行转置操作。</li></ul></li></ul><h3 id="7-从nn-module中调用的方法"><a href="#7-从nn-module中调用的方法" class="headerlink" title="7.从nn.module中调用的方法"></a>7.从nn.module中调用的方法</h3><p>当操作 PyTorch 中的神经网络时，有几个常用的方法可以管理网络的训练和资源分配：</p><div class="table-container"><table><thead><tr><th>方法</th><th>描述</th></tr></thead><tbody><tr><td><code>net.training</code></td><td>检查网络是否处于训练模式。</td></tr><tr><td><code>net.cuda()</code></td><td>将整个网络迁移到 GPU 上，以利用其计算资源。</td></tr><tr><td><code>net.cpu()</code></td><td>将整个网络迁移到 CPU 上，通常用于推理或当 GPU 资源不可用时。</td></tr><tr><td><code>net.apply()</code></td><td>对神经网络中的所有层进行一致的操作，例如初始化权重。</td></tr><tr><td><code>net.parameters()</code></td><td>用于迭代网络中所有的参数，常用于优化过程中更新权重。</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.train()</span><br></pre></td></tr></table></figure><pre><code>Model(  (linear1): Linear(in_features=20, out_features=13, bias=True)  (linear2): Linear(in_features=13, out_features=8, bias=True)  (output): Linear(in_features=8, out_features=3, bias=True))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.cuda()</span><br></pre></td></tr></table></figure><pre><code>Model(  (linear1): Linear(in_features=20, out_features=13, bias=True)  (linear2): Linear(in_features=13, out_features=8, bias=True)  (output): Linear(in_features=8, out_features=3, bias=True))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.cpu()</span><br></pre></td></tr></table></figure><pre><code>Model(  (linear1): Linear(in_features=20, out_features=13, bias=True)  (linear2): Linear(in_features=13, out_features=8, bias=True)  (output): Linear(in_features=8, out_features=3, bias=True))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">initial_0</span>(<span class="params">m</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(m) == nn.Linear:</span><br><span class="line">      m.weight.data.fill_(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">net.apply(initial_0)</span><br></pre></td></tr></table></figure><pre><code>Model(  (linear1): Linear(in_features=20, out_features=13, bias=True)  (linear2): Linear(in_features=13, out_features=8, bias=True)  (output): Linear(in_features=8, out_features=3, bias=True))</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for param in net.parameters():</span></span><br><span class="line"><span class="comment">#  print(param)</span></span><br><span class="line"><span class="comment">#  将每层的参数都打印出来</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(6)-张量可微性</title>
      <link href="/Pytorch-6/"/>
      <url>/Pytorch-6/</url>
      
        <content type="html"><![CDATA[<h3 id="1-可微分性相关属性"><a href="#1-可微分性相关属性" class="headerlink" title="1.可微分性相关属性"></a>1.可微分性相关属性</h3><ul><li>张量<code>y</code>具有一个<code>grad_fn</code>属性，可以<code>查看该属性</code>:<code>&lt;PowBackward0&gt;</code><ul><li>保存了一种<code>y-x</code>的函数关系<code>pow</code></li><li>由<code>可微分张量</code>创建而来</li></ul></li><li>张量<code>y</code>还包含<code>由x计算</code>得出的<code>结果</code>:<code>4.</code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建可微分张量x</span></span><br><span class="line">x = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>) </span><br><span class="line"><span class="built_in">print</span>(x)  <span class="comment">## tensor(2., requires_grad=True)</span></span><br><span class="line"><span class="comment"># 构建函数关系</span></span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)  <span class="comment">## tensor(4., grad_fn=&lt;PowBackward0&gt;)</span></span><br><span class="line"><span class="comment"># 查看grad_fn属性</span></span><br><span class="line"><span class="built_in">print</span>(y.grad_fn)  <span class="comment">## &lt;PowBackward0 object at 0x00000154FAE1E160&gt;</span></span><br></pre></td></tr></table></figure></li><li><code>x</code>作为<code>初始张量</code>，<code>没有</code>grad_fn属性</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.grad_fn   <span class="comment"># 不返回任何东西                                              </span></span><br></pre></td></tr></table></figure><ul><li>可微分张量具有<code>传递性</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y.requires_grad</span><br></pre></td></tr></table></figure><pre><code>True</code></pre><ul><li><strong>注意</strong><code>grad_fn</code>属性是<code>保存直接y-x</code>的关系:比如这里的<code>add</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">z = y + <span class="number">1</span></span><br><span class="line">z </span><br></pre></td></tr></table></figure><pre><code>tensor(5., grad_fn=&lt;AddBackward0&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br><span class="line"><span class="built_in">print</span>(z.grad_fn)</span><br></pre></td></tr></table></figure><pre><code>True&lt;AddBackward0 object at 0x00000154FAE595B0&gt;</code></pre><h3 id="2-反向传播与梯度计算"><a href="#2-反向传播与梯度计算" class="headerlink" title="2. 反向传播与梯度计算"></a>2. 反向传播与梯度计算</h3><ul><li>某个可微分张量的<code>导数值</code>，存在其<code>grad属性</code>中</li><li>执行<code>反向传播</code>才可查看<code>grad</code>，不然<code>不返回</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.grad</span><br></pre></td></tr></table></figure><pre><code>不返回</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">z.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure><pre><code>tensor(4.)</code></pre><ul><li><code>一张计算图</code>反向传播<code>仅能计算一次</code>，<code>backward</code>再次调用将<code>报错</code>！</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#z.backward() # 报错</span></span><br></pre></td></tr></table></figure><ul><li>可以用<code>retain_graph=True</code>可<code>不报错</code>，但会<code>累加梯度</code>(这段代码没累加)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">z = y+<span class="number">1</span></span><br><span class="line">z.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure><pre><code>tensor(4.)</code></pre><ul><li><code>y</code>上也可<code>反向传播</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">z = y+<span class="number">1</span></span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure><pre><code>tensor(4.)</code></pre><ul><li>无论何时，<strong>仅能计算叶节点的导数值</strong></li><li><code>中间节点不会保存梯度</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#y.grad() # 报错！</span></span><br></pre></td></tr></table></figure><ul><li>若<code>想保存</code>，可以使用<code>retain_grad()</code>方法</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line">y.retain_grad()</span><br><span class="line">z = y+<span class="number">1</span></span><br><span class="line">z.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="built_in">print</span>(y.grad)</span><br></pre></td></tr></table></figure><pre><code>tensor(4.)tensor(1.)</code></pre><h3 id="3-阻止计算图跟踪"><a href="#3-阻止计算图跟踪" class="headerlink" title="3.阻止计算图跟踪"></a>3.阻止计算图跟踪</h3><ul><li><code>with torch.no_grad()</code>:阻止计算图跟踪</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x ** <span class="number">2</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    z = y+<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"><span class="built_in">print</span>(z.requires_grad)</span><br></pre></td></tr></table></figure><pre><code>tensor(5.)False</code></pre><h3 id="4-创建一个不可导张量"><a href="#4-创建一个不可导张量" class="headerlink" title="4.创建一个不可导张量"></a>4.创建一个不可导张量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">2.</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y1 = x ** <span class="number">2</span></span><br><span class="line">y2 = y1.detach()</span><br><span class="line">z = y2**<span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y1)</span><br><span class="line"><span class="built_in">print</span>(y2)</span><br><span class="line"><span class="built_in">print</span>(z)</span><br></pre></td></tr></table></figure><pre><code>tensor(4., grad_fn=&lt;PowBackward0&gt;)tensor(4.)tensor(16.)</code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>python</title>
      <link href="/2023-12-18-python/"/>
      <url>/2023-12-18-python/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch(1)-基础与张量转换</title>
      <link href="/Pytorch-1/"/>
      <url>/Pytorch-1/</url>
      
        <content type="html"><![CDATA[<h3 id="1-导入库和版本"><a href="#1-导入库和版本" class="headerlink" title="1.导入库和版本"></a>1.导入库和版本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)</span><br></pre></td></tr></table></figure><p>   2.1.1+cu121</p><blockquote><p>cu121表示 PyTorch 是为了与 CUDA 12.1 版本兼容而编译的</p></blockquote><h3 id="2-张量的创建方法"><a href="#2-张量的创建方法" class="headerlink" title="2.张量的创建方法"></a>2.张量的创建方法</h3><ul><li>包括<code>列表</code>、<code>元组</code>、<code>数组</code>的创建</li><li><code>列表/元组</code>的创建方法默认是<strong>int64/float32</strong></li><li><code>数组</code>的创建方法默认是<strong>int32/float64</strong>，且后面会附带上dtype的类型</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">t1=torch.tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">t2=torch.tensor((<span class="number">1</span>,<span class="number">2</span>)) </span><br><span class="line">t3=torch.tensor(<span class="number">1</span>+<span class="number">2j</span>)</span><br><span class="line">t4=torch.tensor([<span class="literal">True</span>,<span class="literal">True</span>])</span><br><span class="line">t5=torch.tensor(np.array([<span class="number">1</span>,<span class="number">2</span>]))  <span class="comment"># 后面会附带类型</span></span><br><span class="line"><span class="built_in">print</span>(t1)  <span class="comment"># tensor([1, 2])</span></span><br><span class="line"><span class="built_in">print</span>(t2)  <span class="comment"># tensor([1, 2])</span></span><br><span class="line"><span class="built_in">print</span>(t3)  <span class="comment"># tensor(1.+2.j)</span></span><br><span class="line"><span class="built_in">print</span>(t4)  <span class="comment"># tensor([True, True])</span></span><br><span class="line"><span class="built_in">print</span>(t5)  <span class="comment"># tensor([1, 2], dtype=torch.int32)</span></span><br></pre></td></tr></table></figure><h3 id="3-torch数据类型大全——10种"><a href="#3-torch数据类型大全——10种" class="headerlink" title="3. torch数据类型大全——10种"></a>3. torch数据类型大全——10种</h3><div class="table-container"><table><thead><tr><th style="text-align:center">Torch Type</th><th style="text-align:center">Alias(显示转换指令)</th></tr></thead><tbody><tr><td style="text-align:center"><strong><code>torch.float64</code></strong></td><td style="text-align:center"><strong><code>torch.double</code></strong></td></tr><tr><td style="text-align:center"><code>torch.float32</code></td><td style="text-align:center"><code>torch.float</code></td></tr><tr><td style="text-align:center"><code>torch.float16</code></td><td style="text-align:center"><code>torch.half</code></td></tr><tr><td style="text-align:center"><strong><code>torch.int64</code></strong></td><td style="text-align:center"><strong><code>torch.long</code></strong></td></tr><tr><td style="text-align:center"><code>torch.int32</code></td><td style="text-align:center"><code>torch.int</code></td></tr><tr><td style="text-align:center"><code>torch.int16</code></td><td style="text-align:center"><code>torch.short</code></td></tr><tr><td style="text-align:center"><code>torch.uint8</code></td><td style="text-align:center"></td></tr><tr><td style="text-align:center"><code>torch.int8</code></td><td style="text-align:center"></td></tr><tr><td style="text-align:center"><strong><code>torch.bool</code></strong></td><td style="text-align:center"></td></tr><tr><td style="text-align:center"><code>torch.complex64</code></td></tr></tbody></table></div><h3 id="4-隐式转换"><a href="#4-隐式转换" class="headerlink" title="4. 隐式转换"></a>4. 隐式转换</h3><ul><li>浮点型&gt;整数型&gt;布尔型</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.tensor([<span class="number">1.1</span>,<span class="number">2.7</span>],dtype = torch.uint8))  <span class="comment"># tensor([1, 2], dtype=torch.uint8)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.tensor([<span class="number">1.1</span>,<span class="number">2</span>]).dtype)   <span class="comment"># torch.float32</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor([<span class="number">2</span>,<span class="literal">True</span>]).dtype)  <span class="comment"># torch.int64</span></span><br></pre></td></tr></table></figure><p>​    </p><h3 id="5-显示转换（方法）"><a href="#5-显示转换（方法）" class="headerlink" title="5. 显示转换（方法）"></a>5. 显示转换（方法）</h3><ul><li><code>不改变</code>原数据类型</li><li>检查类型用<code>dtype</code></li><li><code>half</code>/<code>float</code>/<code>double</code>/<code>short</code>/<code>int</code>/<code>long</code>指令</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(t.<span class="built_in">float</span>())  <span class="comment"># tensor([1., 2.])</span></span><br><span class="line"><span class="built_in">print</span>(t.dtype)    <span class="comment"># torch.int64</span></span><br></pre></td></tr></table></figure><p>​    </p><h3 id="6-一维度情况的维度形变"><a href="#6-一维度情况的维度形变" class="headerlink" title="6. 一维度情况的维度形变"></a>6. 一维度情况的维度形变</h3><p>在PyTorch中，<code>shape</code>与<code>size()</code>用于查看张量的<code>形状</code>。以下是一些常用的属性和方法：</p><div class="table-container"><table><thead><tr><th style="text-align:center">属性/方法</th><th style="text-align:center">作用</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center"><code>ndim</code></td><td style="text-align:center">维度</td><td style="text-align:center">查看张量的维度数</td></tr><tr><td style="text-align:center"><code>shape</code></td><td style="text-align:center">形状</td><td style="text-align:center">查看张量的形状，与<code>size()</code>相同</td></tr><tr><td style="text-align:center"><code>size()</code></td><td style="text-align:center">形状</td><td style="text-align:center">查看张量的形状，与<code>shape</code>相同</td></tr><tr><td style="text-align:center"><code>numel()</code></td><td style="text-align:center">元素数量</td><td style="text-align:center">查看张量的元素总数</td></tr><tr><td style="text-align:center"><code>len()</code></td><td style="text-align:center">元素数量</td><td style="text-align:center">在一维中与<code>numel()</code>相同</td></tr></tbody></table></div><ul><li><strong>注意</strong>：在一维张量中，<code>numel()</code> 和 <code>len()</code> 返回相同的结果，都表示张量的元素数量。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;ndim:\t&#x27;</span>,torch.tensor([<span class="number">1</span>,<span class="number">2</span>]).ndim)          <span class="comment">#   ndim: 1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;shape:\t&#x27;</span>,torch.tensor([<span class="number">1</span>,<span class="number">2</span>]).shape)        <span class="comment">#   shape: torch.Size([2])</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;size():\t&#x27;</span>,torch.tensor([<span class="number">1</span>,<span class="number">2</span>]).size())      <span class="comment">#   size(): torch.Size([2])</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;numel():&#x27;</span>,torch.tensor([<span class="number">1</span>,<span class="number">2</span>]).numel())      <span class="comment">#   numel(): 2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;len():\t&#x27;</span>,<span class="built_in">len</span>(torch.tensor([<span class="number">1</span>,<span class="number">2</span>])))         <span class="comment">#   len(): 2    </span></span><br></pre></td></tr></table></figure><h3 id="7-二维度情况的维度形变"><a href="#7-二维度情况的维度形变" class="headerlink" title="7. 二维度情况的维度形变"></a>7. 二维度情况的维度形变</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;ndim:\t&#x27;</span>,torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).ndim)          <span class="comment">#   ndim: 2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;shape:\t&#x27;</span>,torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).shape)        <span class="comment">#   shape: torch.Size([2, 2]) </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;size():\t&#x27;</span>,torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).size())      <span class="comment">#   size(): torch.Size([2, 2])</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;numel():&#x27;</span>,torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).numel())      <span class="comment">#   numel(): 4</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;len():\t&#x27;</span>,<span class="built_in">len</span>(torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])))         <span class="comment">#   len(): 2</span></span><br></pre></td></tr></table></figure><h3 id="8-三维度情况的维度形变"><a href="#8-三维度情况的维度形变" class="headerlink" title="8. 三维度情况的维度形变"></a>8. 三维度情况的维度形变</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;ndim:\t&#x27;</span>,torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>]],[[<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>]]]).ndim)     <span class="comment"># ndim:3</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;shape:\t&#x27;</span>,torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>]],[[<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>]]]).shape)   <span class="comment"># shape:torch.Size([2, 2, 3]) </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;size():\t&#x27;</span>,torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>]],[[<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>]]]).size()) <span class="comment"># size():torch.Size([2, 2, 3])</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;numel():&#x27;</span>,torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>]],[[<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>]]]).numel()) <span class="comment"># numel():12</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;len():\t&#x27;</span>,<span class="built_in">len</span>(torch.tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>]],[[<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">8</span>]]])))    <span class="comment"># len():2</span></span><br></pre></td></tr></table></figure><h3 id="9-零维张量"><a href="#9-零维张量" class="headerlink" title="9. 零维张量"></a>9. 零维张量</h3><ul><li><strong>零维张量</strong>：PyTorch中的单个数值，具有张量属性（如 <code>torch.tensor(1)</code>）。</li><li><strong>标量</strong>：Python中的单个数值，不具备张量属性。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;ndim:\t&#x27;</span>,torch.tensor(<span class="number">1</span>).ndim)     <span class="comment">#  ndim: 0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;shape:\t&#x27;</span>,torch.tensor(<span class="number">1</span>).shape)   <span class="comment">#  shape: torch.Size([])</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;size():\t&#x27;</span>,torch.tensor(<span class="number">1</span>).size()) <span class="comment">#  size(): torch.Size([]) </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;numel():&#x27;</span>,torch.tensor(<span class="number">1</span>).numel()) <span class="comment">#  numel(): 1</span></span><br><span class="line"><span class="comment"># print(&#x27;len():\t&#x27;,len(torch.tensor(1))) 报错，零维张量不能len()</span></span><br></pre></td></tr></table></figure><h3 id="10-张量的形变"><a href="#10-张量的形变" class="headerlink" title="10.张量的形变"></a>10.张量的形变</h3><ul><li><code>N维、0维</code>flatten()后被变为<code>1维</code>，保证只有一个<code>[]</code></li><li>flatten()和reshape()均<code>不会对原张量造成影响</code></li><li>reshape(4),reshape(4,)<code>均为1维</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).flatten())     <span class="comment">#  tensor([1, 2, 3, 4])</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor(<span class="number">1</span>).flatten())                 <span class="comment">#  tensor([1])</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).reshape(<span class="number">4</span>))    <span class="comment">#  tensor([1, 2, 3, 4])</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).reshape(<span class="number">4</span>,))   <span class="comment">#  tensor([1, 2, 3, 4])</span></span><br><span class="line"><span class="built_in">print</span>(torch.tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]).reshape(<span class="number">1</span>,<span class="number">4</span>))  <span class="comment">#  tensor([[1, 2, 3, 4]])</span></span><br></pre></td></tr></table></figure><h3 id="11-特殊张量的创建"><a href="#11-特殊张量的创建" class="headerlink" title="11.特殊张量的创建"></a>11.特殊张量的创建</h3><ul><li><code>1/0/empty</code>只填size参数时,无括号/圆括号/方括号均可</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.zeros(<span class="number">2</span>,<span class="number">3</span>))   <span class="comment"># tensor([[0., 0., 0.],[0., 0., 0.]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.ones((<span class="number">2</span>,<span class="number">3</span>)))  <span class="comment"># tensor([[1., 1., 1.],[1., 1., 1.]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.empty([<span class="number">2</span>,<span class="number">3</span>])) <span class="comment"># tensor([[1., 1., 1.],[1., 1., 1.]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.full((<span class="number">2</span>,<span class="number">3</span>),<span class="number">1</span>))<span class="comment"># tensor([[1, 1, 1],[1, 1, 1]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.eye(<span class="number">3</span>))       <span class="comment"># tensor([[1., 0., 0.],[0., 1., 0.],[0., 0., 1.]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.diag(torch.tensor([<span class="number">1</span>,<span class="number">2</span>])))  <span class="comment"># tensor([[1, 0], [0, 2]])</span></span><br></pre></td></tr></table></figure><h3 id="12-随机张量的创建"><a href="#12-随机张量的创建" class="headerlink" title="12.随机张量的创建"></a>12.随机张量的创建</h3><div class="table-container"><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">描述</th></tr></thead><tbody><tr><td style="text-align:center"><code>rand()</code></td><td style="text-align:center">0-1均匀分布</td></tr><tr><td style="text-align:center"><code>randn()</code></td><td style="text-align:center">标准正态分布</td></tr><tr><td style="text-align:center"><code>normal()</code></td><td style="text-align:center">服从指定正态分布</td></tr><tr><td style="text-align:center"><code>randint()</code></td><td style="text-align:center">整数随机采样</td></tr><tr><td style="text-align:center"><code>arange</code></td><td style="text-align:center">生成数列(间隔)</td></tr><tr><td style="text-align:center"><code>linspace</code></td><td style="text-align:center">生成数列(数量)</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.rand(<span class="number">2</span>,<span class="number">3</span>))  <span class="comment"># tensor([[0.7878, 0.6142, 0.6865], [0.1155, 0.4820, 0.1763]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.randn(<span class="number">2</span>,<span class="number">3</span>)) <span class="comment"># tensor([[ 0.0174,  1.1351, -0.1324],[-0.2343, -0.0504,  0.1417]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.normal(<span class="number">2</span>,<span class="number">3</span>,size=(<span class="number">2</span>,<span class="number">2</span>))) <span class="comment"># tensor([[1.1347, 1.6557], [3.1312, 2.9998]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.randint(<span class="number">1</span>,<span class="number">10</span>,size=[<span class="number">2</span>,<span class="number">4</span>])) <span class="comment"># tensor([[5, 2, 3, 8],[2, 8, 8, 5]])</span></span><br><span class="line"><span class="built_in">print</span>(torch.arange(<span class="number">0</span>,<span class="number">5</span>,<span class="number">3</span>))   <span class="comment"># tensor([0, 3])</span></span><br><span class="line"><span class="built_in">print</span>(torch.linspace(<span class="number">0</span>,<span class="number">5</span>,<span class="number">4</span>)) <span class="comment">#   tensor([0.0000, 1.6667, 3.3333, 5.0000])</span></span><br></pre></td></tr></table></figure><h3 id="13-tensor-numpy-list相互转换"><a href="#13-tensor-numpy-list相互转换" class="headerlink" title="13.tensor/numpy/list相互转换"></a>13.tensor/numpy/list相互转换</h3><ul><li><code>list(t)</code>是转换为<code>零维张量</code></li><li><code>item()</code>要求<strong>单元素张量（tensor）</strong>中提取其数值</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(t.numpy())   <span class="comment"># [1]</span></span><br><span class="line"><span class="built_in">print</span>(np.array(t)) <span class="comment"># [1] </span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(t.tolist())  <span class="comment"># [1]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(t))     <span class="comment"># [tensor(1)]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(t.item())  <span class="comment">#  1</span></span><br></pre></td></tr></table></figure><h3 id="14-张量的浅拷贝与深拷贝"><a href="#14-张量的浅拷贝与深拷贝" class="headerlink" title="14. 张量的浅拷贝与深拷贝"></a>14. 张量的浅拷贝与深拷贝</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line">t2 = t1  <span class="comment"># 赋值/浅拷贝，动一个则变另一个</span></span><br><span class="line">t3=t1.clone()  <span class="comment"># 深拷贝，动一个则另一个不变</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 标签1 </tag>
            
            <tag> 标签A </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第二篇文章</title>
      <link href="/2023-12-17-2/"/>
      <url>/2023-12-17-2/</url>
      
        <content type="html"><![CDATA[<h2 id="这是我的第二篇文章"><a href="#这是我的第二篇文章" class="headerlink" title="这是我的第二篇文章"></a>这是我的第二篇文章</h2>]]></content>
      
      
      <categories>
          
          <category> C </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 标签2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>第三篇文章</title>
      <link href="/2023-12-17-3/"/>
      <url>/2023-12-17-3/</url>
      
        <content type="html"><![CDATA[<h2 id="这是我的第三篇文章"><a href="#这是我的第三篇文章" class="headerlink" title="这是我的第三篇文章"></a>这是我的第三篇文章</h2>]]></content>
      
      
      <categories>
          
          <category> Embedded </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 标签3 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>关于</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<!-- GitCalendar容器 --><div id="gitZone"></div>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/css/custom.css"/>
      <url>/css/custom.css</url>
      
        <content type="html"><![CDATA[:root {  --trans-light: rgba(255, 255, 255, 0.88);  --trans-dark: rgba(25, 25, 25, 0.88);  --border-style: 1px solid rgb(169, 169, 169);  --backdrop-filter: blur(5px) saturate(150%);}/* 页脚与头图透明 */#footer {    background: transparent !important;  }  #page-header {    background: transparent !important;  }    /* 白天模式遮罩透明 */  #footer::before {    background: transparent !important;  }  #page-header::before {    background: transparent !important;  }    /* 夜间模式遮罩透明 */  [data-theme="dark"] #footer::before {    background: transparent !important;  }  [data-theme="dark"] #page-header::before {    background: transparent !important;  }/* 小冰分类分类磁铁黑夜模式适配 *//* 一般状态 */[data-theme="dark"] .magnet_link_context {  background: #1e1e1e;  color: antiquewhite;}/* 鼠标悬浮状态 */[data-theme="dark"] .magnet_link_context:hover {  background: #3ecdf1;  color: #f2f2f2;}@font-face {  /* 为载入的字体取名字(随意) */  font-family: 'YSHST';  /* 字体文件地址(相对或者绝对路径都可以) */  src: url(/font/poppins-black-webfont.woff2);  /* 定义加粗样式(加粗多少) */  font-weight: normal;  /* 定义字体样式(斜体/非斜体) */  font-style: normal;  /* 定义显示样式 */  font-display: block;}/* 翻页按钮居中 */#pagination {  width: 100%;  margin: auto;}/* 一级菜单居中 */#nav .menus_items {  position: absolute !important;  width: fit-content !important;  left: 50% !important;  transform: translateX(-50%) !important;}/* 子菜单横向展示 */#nav .menus_items .menus_item:hover .menus_item_child {  display: flex !important;}/* 这里的2是代表导航栏的第2个元素，即有子菜单的元素，可以按自己需求修改 */.menus_items .menus_item:nth-child(5) .menus_item_child {  left: -38px;}/* 夜间模式菜单栏发光字 */[data-theme="dark"] #nav .site-page,[data-theme="dark"] #nav .menus_items .menus_item .menus_item_child li a {  text-shadow: 0 0 2px var(--theme-color) !important;}/* 手机端适配 */[data-theme="dark"] #sidebar #sidebar-menus .menus_items .site-page {  text-shadow: 0 0 2px var(--theme-color) !important;}/* 闪烁变动颜色连续渐变 *//* 日间模式不生效 */[data-theme="light"] #site-name,[data-theme="light"] #site-title,[data-theme="light"] #site-subtitle,[data-theme="light"] #post-info {  animation: none;}/* 夜间模式生效 */[data-theme="dark"] #site-name,[data-theme="dark"] #site-title {  animation: light_15px 10s linear infinite;}[data-theme="dark"] #site-subtitle {  animation: light_10px 10s linear infinite;}[data-theme="dark"] #post-info {  animation: light_5px 10s linear infinite;}/* 关键帧描述 */@keyframes light_15px {  0% {    text-shadow: #5636ed 0 0 15px;  }  12.5% {    text-shadow: #11ee5e 0 0 15px;  }  25% {    text-shadow: #f14747 0 0 15px;  }  37.5% {    text-shadow: #f1a247 0 0 15px;  }  50% {    text-shadow: #f1ee47 0 0 15px;  }  50% {    text-shadow: #b347f1 0 0 15px;  }  62.5% {    text-shadow: #002afa 0 0 15px;  }  75% {    text-shadow: #ed709b 0 0 15px;  }  87.5% {    text-shadow: #39c5bb 0 0 15px;  }  100% {    text-shadow: #5636ed 0 0 15px;  }}@keyframes light_10px {  0% {    text-shadow: #5636ed 0 0 10px;  }  12.5% {    text-shadow: #11ee5e 0 0 10px;  }  25% {    text-shadow: #f14747 0 0 10px;  }  37.5% {    text-shadow: #f1a247 0 0 10px;  }  50% {    text-shadow: #f1ee47 0 0 10px;  }  50% {    text-shadow: #b347f1 0 0 10px;  }  62.5% {    text-shadow: #002afa 0 0 10px;  }  75% {    text-shadow: #ed709b 0 0 10px;  }  87.5% {    text-shadow: #39c5bb 0 0 10px;  }  100% {    text-shadow: #5636ed 0 0 10px;  }}@keyframes light_5px {  0% {    text-shadow: #5636ed 0 0 5px;  }  12.5% {    text-shadow: #11ee5e 0 0 5px;  }  25% {    text-shadow: #f14747 0 0 5px;  }  37.5% {    text-shadow: #f1a247 0 0 15px;  }  50% {    text-shadow: #f1ee47 0 0 5px;  }  50% {    text-shadow: #b347f1 0 0 5px;  }  62.5% {    text-shadow: #002afa 0 0 5px;  }  75% {    text-shadow: #ed709b 0 0 5px;  }  87.5% {    text-shadow: #39c5bb 0 0 5px;  }  100% {    text-shadow: #5636ed 0 0 5px;  }}/* 背景宇宙星光  */#universe{  display: block;  position: fixed;  margin: 0;  padding: 0;  border: 0;  outline: 0;  left: 0;  top: 0;  width: 100%;  height: 100%;  pointer-events: none;  /* 这个是调置顶的优先级的，-1在文章页下面，背景上面，个人推荐这种 */  z-index: -1;}/* 侧边栏个人信息卡片动态渐变色 */#aside-content > .card-widget.card-info {  background: linear-gradient(    -45deg,    #e8d8b9,    #eccec5,    #a3e9eb,      #bdbdf0,    #eec1ea  );  box-shadow: 0 0 5px rgb(66, 68, 68);  position: relative;  background-size: 400% 400%;  -webkit-animation: Gradient 10s ease infinite;  -moz-animation: Gradient 10s ease infinite;  animation: Gradient 10s ease infinite !important;}@-webkit-keyframes Gradient {  0% {    background-position: 0% 50%;  }  50% {    background-position: 100% 50%;  }  100% {    background-position: 0% 50%;  }}@-moz-keyframes Gradient {  0% {    background-position: 0% 50%;  }  50% {    background-position: 100% 50%;  }  100% {    background-position: 0% 50%;  }}@keyframes Gradient {  0% {    background-position: 0% 50%;  }  50% {    background-position: 100% 50%;  }  100% {    background-position: 0% 50%;  }}/* 黑夜模式适配 */[data-theme="dark"] #aside-content > .card-widget.card-info {  background: #191919ee;}/* 个人信息Follow me按钮 */#aside-content > .card-widget.card-info > #card-info-btn {  background-color: #3eb8be;  border-radius: 8px;}/* 鼠标样式 */#cursor {  position: fixed;  width: 16px;  height: 16px;  /* 这里改变跟随的底色 */  background: rgb(101, 153, 245);  border-radius: 8px;  opacity: 0.25;  z-index: 10086;  pointer-events: none;  transition: 0.2s ease-in-out;  transition-property: background, opacity, transform;}#cursor.hidden {  opacity: 0;}#cursor.hover {  opacity: 0.1;  transform: scale(2.5);  -webkit-transform: scale(2.5);  -moz-transform: scale(2.5);  -ms-transform: scale(2.5);  -o-transform: scale(2.5);}#cursor.active {  opacity: 0.5;  transform: scale(0.5);  -webkit-transform: scale(0.5);  -moz-transform: scale(0.5);  -ms-transform: scale(0.5);  -o-transform: scale(0.5);}/* 首页文章卡片 */#recent-posts > .recent-post-item {  background: var(--trans-light);  backdrop-filter: var(--backdrop-filter);  border-radius: 25px;  border: var(--border-style);}/* 首页侧栏卡片 */#aside-content .card-widget {  background: var(--trans-light);  backdrop-filter: var(--backdrop-filter);  border-radius: 18px;  border: var(--border-style);}/* 文章页、归档页、普通页面 */div#post,div#page,div#archive {  background: var(--trans-light);  backdrop-filter: var(--backdrop-filter);  border: var(--border-style);  border-radius: 20px;}/* 导航栏 */#page-header.nav-fixed #nav {  background: rgba(255, 255, 255, 0.75);  backdrop-filter: var(--backdrop-filter);}[data-theme="dark"] #page-header.nav-fixed #nav {  background: rgba(0, 0, 0, 0.7) !important;}/* 夜间模式遮罩 */[data-theme="dark"] #recent-posts > .recent-post-item,[data-theme="dark"] #aside-content .card-widget,[data-theme="dark"] div#post,[data-theme="dark"] div#archive,[data-theme="dark"] div#page {  background: var(--trans-dark);}/* 夜间模式页脚页头遮罩透明 */[data-theme="dark"] #footer::before {  background: transparent !important;}[data-theme="dark"] #page-header::before {  background: transparent !important;}/* 阅读模式 */.read-mode #aside-content .card-widget {  background: rgba(158, 204, 171, 0.5) !important;}.read-mode div#post {  background: rgba(158, 204, 171, 0.5) !important;}/* 夜间模式下的阅读模式 */[data-theme="dark"] .read-mode #aside-content .card-widget {  background: rgba(25, 25, 25, 0.9) !important;  color: #ffffff;}[data-theme="dark"] .read-mode div#post {  background: rgba(25, 25, 25, 0.9) !important;  color: #ffffff;}/* 文章页H1-H6图标样式效果 *//* 控制风车转动速度 4s那里可以自己调节快慢 */h1::before,h2::before,h3::before,h4::before,h5::before,h6::before {  -webkit-animation: ccc 4s linear infinite;  animation: ccc 4s linear infinite;}/* 控制风车转动方向 -1turn 为逆时针转动，1turn 为顺时针转动，相同数字部分记得统一修改 */@-webkit-keyframes ccc {  0% {    -webkit-transform: rotate(0deg);    transform: rotate(0deg);  }  to {    -webkit-transform: rotate(-1turn);    transform: rotate(-1turn);  }}@keyframes ccc {  0% {    -webkit-transform: rotate(0deg);    transform: rotate(0deg);  }  to {    -webkit-transform: rotate(-1turn);    transform: rotate(-1turn);  }}/* 设置风车颜色 */#content-inner.layout h1::before {  color: #ef50a8;  margin-left: -1.55rem;  font-size: 1.3rem;  margin-top: -0.23rem;}#content-inner.layout h2::before {  color: #fb7061;  margin-left: -1.35rem;  font-size: 1.1rem;  margin-top: -0.12rem;}#content-inner.layout h3::before {  color: #ffbf00;  margin-left: -1.22rem;  font-size: 0.95rem;  margin-top: -0.09rem;}#content-inner.layout h4::before {  color: #a9e000;  margin-left: -1.05rem;  font-size: 0.8rem;  margin-top: -0.09rem;}#content-inner.layout h5::before {  color: #57c850;  margin-left: -0.9rem;  font-size: 0.7rem;  margin-top: 0rem;}#content-inner.layout h6::before {  color: #5ec1e0;  margin-left: -0.9rem;  font-size: 0.66rem;  margin-top: 0rem;}/* s设置风车hover动效 6s那里可以自己调节快慢*/#content-inner.layout h1:hover,#content-inner.layout h2:hover,#content-inner.layout h3:hover,#content-inner.layout h4:hover,#content-inner.layout h5:hover,#content-inner.layout h6:hover {  color: var(--theme-color);}#content-inner.layout h1:hover::before,#content-inner.layout h2:hover::before,#content-inner.layout h3:hover::before,#content-inner.layout h4:hover::before,#content-inner.layout h5:hover::before,#content-inner.layout h6:hover::before {  color: var(--theme-color);  -webkit-animation: ccc 6s linear infinite;  animation: ccc 6s linear infinite;}]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/light.js"/>
      <url>/js/light.js</url>
      
        <content type="html"><![CDATA[// 霓虹灯效果// 颜色数组var arr = ["#39c5bb", "#f14747", "#f1a247", "#f1ee47", "#b347f1", "#1edbff", "#ed709b", "#5636ed"];// 颜色索引var idx = 0;// 切换颜色function changeColor() {    // 仅夜间模式才启用    if (document.getElementsByTagName('html')[0].getAttribute('data-theme') == 'dark') {        if (document.getElementById("site-name"))            document.getElementById("site-name").style.textShadow = arr[idx] + " 0 0 15px";        if (document.getElementById("site-title"))            document.getElementById("site-title").style.textShadow = arr[idx] + " 0 0 15px";        if (document.getElementById("site-subtitle"))            document.getElementById("site-subtitle").style.textShadow = arr[idx] + " 0 0 10px";        if (document.getElementById("post-info"))            document.getElementById("post-info").style.textShadow = arr[idx] + " 0 0 5px";        try {            document.getElementsByClassName("author-info__name")[0].style.textShadow = arr[idx] + " 0 0 12px";            document.getElementsByClassName("author-info__description")[0].style.textShadow = arr[idx] + " 0 0 12px";        } catch {                    }        idx++;        if (idx == 8) {            idx = 0;        }    } else {        // 白天模式恢复默认        if (document.getElementById("site-name"))            document.getElementById("site-name").style.textShadow = "#1e1e1ee0 1px 1px 1px";        if (document.getElementById("site-title"))            document.getElementById("site-title").style.textShadow = "#1e1e1ee0 1px 1px 1px";        if (document.getElementById("site-subtitle"))            document.getElementById("site-subtitle").style.textShadow = "#1e1e1ee0 1px 1px 1px";        if (document.getElementById("post-info"))            document.getElementById("post-info").style.textShadow = "#1e1e1ee0 1px 1px 1px";        try {            document.getElementsByClassName("author-info__name")[0].style.textShadow = "";            document.getElementsByClassName("author-info__description")[0].style.textShadow = "";        } catch {                    }    }}// 开启计时器window.onload = setInterval(changeColor, 1200);]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/runtime.js"/>
      <url>/js/runtime.js</url>
      
        <content type="html"><![CDATA[function createtime() {    var now = new Date(); // 获取当前时间    var startDate = new Date("12/17/2023 21:00:00"); // 网站开始运行的日期，按需更改    var elapsed = now - startDate; // 计算流逝的时间    var seconds = Math.floor(elapsed / 1000);    var minutes = Math.floor(seconds / 60);    var hours = Math.floor(minutes / 60);    var days = Math.floor(hours / 24);    seconds %= 60; // 剩余秒数    minutes %= 60; // 剩余分钟数    hours %= 24; // 剩余小时数    // 确保时间是两位数字格式    var secondsStr = seconds < 10 ? "0" + seconds : seconds;    var minutesStr = minutes < 10 ? "0" + minutes : minutes;    var hoursStr = hours < 10 ? "0" + hours : hours;    // 更新网站内容    var c = `<div style="font-size:13px;font-weight:bold">本站居然运行了 ${days} 天 ${hoursStr} 小时 ${minutesStr} 分 ${secondsStr} 秒</div>`;    document.getElementById("workboard").innerHTML = c;}setInterval(createtime, 1000); // 每秒更新一次]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>分类</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/cursor.js"/>
      <url>/js/cursor.js</url>
      
        <content type="html"><![CDATA[var CURSOR;Math.lerp = (a, b, n) => (1 - n) * a + n * b;const getStyle = (el, attr) => {    try {        return window.getComputedStyle            ? window.getComputedStyle(el)[attr]            : el.currentStyle[attr];    } catch (e) {}    return "";};class Cursor {    constructor() {        this.pos = {curr: null, prev: null};        this.pt = [];        this.create();        this.init();        this.render();    }    move(left, top) {        this.cursor.style["left"] = `${left}px`;        this.cursor.style["top"] = `${top}px`;    }    create() {        if (!this.cursor) {            this.cursor = document.createElement("div");            this.cursor.id = "cursor";            this.cursor.classList.add("hidden");            document.body.append(this.cursor);        }        var el = document.getElementsByTagName('*');        for (let i = 0; i < el.length; i++)            if (getStyle(el[i], "cursor") == "pointer")                this.pt.push(el[i].outerHTML);        document.body.appendChild((this.scr = document.createElement("style")));        // 这里改变鼠标指针的颜色 由svg生成        this.scr.innerHTML = `* {cursor: url("data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 8 8' width='8px' height='8px'><circle cx='4' cy='4' r='4' opacity='1.0' fill='rgb(0, 0, 255)'/></svg>") 4 4, auto}`;    }    refresh() {        this.scr.remove();        this.cursor.classList.remove("hover");        this.cursor.classList.remove("active");        this.pos = {curr: null, prev: null};        this.pt = [];        this.create();        this.init();        this.render();    }    init() {        document.onmouseover  = e => this.pt.includes(e.target.outerHTML) && this.cursor.classList.add("hover");        document.onmouseout   = e => this.pt.includes(e.target.outerHTML) && this.cursor.classList.remove("hover");        document.onmousemove  = e => {(this.pos.curr == null) && this.move(e.clientX - 8, e.clientY - 8); this.pos.curr = {x: e.clientX - 8, y: e.clientY - 8}; this.cursor.classList.remove("hidden");};        document.onmouseenter = e => this.cursor.classList.remove("hidden");        document.onmouseleave = e => this.cursor.classList.add("hidden");        document.onmousedown  = e => this.cursor.classList.add("active");        document.onmouseup    = e => this.cursor.classList.remove("active");    }    render() {        if (this.pos.prev) {            this.pos.prev.x = Math.lerp(this.pos.prev.x, this.pos.curr.x, 0.15);            this.pos.prev.y = Math.lerp(this.pos.prev.y, this.pos.curr.y, 0.15);            this.move(this.pos.prev.x, this.pos.prev.y);        } else {            this.pos.prev = this.pos.curr;        }        requestAnimationFrame(() => this.render());    }}(() => {    CURSOR = new Cursor();    // 需要重新获取列表时，使用 CURSOR.refresh()})();]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>电影</title>
      <link href="/movies/index.html"/>
      <url>/movies/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/js/universe.js"/>
      <url>/js/universe.js</url>
      
        <content type="html"><![CDATA[function dark() {window.requestAnimationFrame=window.requestAnimationFrame||window.mozRequestAnimationFrame||window.webkitRequestAnimationFrame||window.msRequestAnimationFrame;var n,e,i,h,t=.05,s=document.getElementById("universe"),o=!0,a="180,184,240",r="226,225,142",d="226,225,224",c=[];function f(){n=window.innerWidth,e=window.innerHeight,i=.216*n,s.setAttribute("width",n),s.setAttribute("height",e)}function u(){h.clearRect(0,0,n,e);for(var t=c.length,i=0;i<t;i++){var s=c[i];s.move(),s.fadeIn(),s.fadeOut(),s.draw()}}function y(){this.reset=function(){this.giant=m(3),this.comet=!this.giant&&!o&&m(10),this.x=l(0,n-10),this.y=l(0,e),this.r=l(1.1,2.6),this.dx=l(t,6*t)+(this.comet+1-1)*t*l(50,120)+2*t,this.dy=-l(t,6*t)-(this.comet+1-1)*t*l(50,120),this.fadingOut=null,this.fadingIn=!0,this.opacity=0,this.opacityTresh=l(.2,1-.4*(this.comet+1-1)),this.do=l(5e-4,.002)+.001*(this.comet+1-1)},this.fadeIn=function(){this.fadingIn&&(this.fadingIn=!(this.opacity>this.opacityTresh),this.opacity+=this.do)},this.fadeOut=function(){this.fadingOut&&(this.fadingOut=!(this.opacity<0),this.opacity-=this.do/2,(this.x>n||this.y<0)&&(this.fadingOut=!1,this.reset()))},this.draw=function(){if(h.beginPath(),this.giant)h.fillStyle="rgba("+a+","+this.opacity+")",h.arc(this.x,this.y,2,0,2*Math.PI,!1);else if(this.comet){h.fillStyle="rgba("+d+","+this.opacity+")",h.arc(this.x,this.y,1.5,0,2*Math.PI,!1);for(var t=0;t<30;t++)h.fillStyle="rgba("+d+","+(this.opacity-this.opacity/20*t)+")",h.rect(this.x-this.dx/4*t,this.y-this.dy/4*t-2,2,2),h.fill()}else h.fillStyle="rgba("+r+","+this.opacity+")",h.rect(this.x,this.y,this.r,this.r);h.closePath(),h.fill()},this.move=function(){this.x+=this.dx,this.y+=this.dy,!1===this.fadingOut&&this.reset(),(this.x>n-n/4||this.y<0)&&(this.fadingOut=!0)},setTimeout(function(){o=!1},50)}function m(t){return Math.floor(1e3*Math.random())+1<10*t}function l(t,i){return Math.random()*(i-t)+t}f(),window.addEventListener("resize",f,!1),function(){h=s.getContext("2d");for(var t=0;t<i;t++)c[t]=new y,c[t].reset();u()}(),function t(){document.getElementsByTagName('html')[0].getAttribute('data-theme')=='dark'&&u(),window.requestAnimationFrame(t)}()};dark()]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>添加我的QQ</title>
      <link href="/qq/index.html"/>
      <url>/qq/index.html</url>
      
        <content type="html"><![CDATA[<p><img src="../img/QQ.jpg" alt="我的QQ二维码"></p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>音乐</title>
      <link href="/music/index.html"/>
      <url>/music/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>友链</title>
      <link href="/link/index.html"/>
      <url>/link/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
