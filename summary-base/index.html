<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>图像分类经典网络汇总 | Chitose-Blog</title><meta name="author" content="Chitose"><meta name="copyright" content="Chitose"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="前言 在CNN网络结构的演化上，CNN的经典结构始于1998年的LeNet，成于2012年历史性的AlexNet，从此盛于图像相关领域。 发展历史：Lenet --&gt; Alexnet --&gt; ZFnet --&gt; VGG --&gt; NIN --&gt; GoogLeNet --&gt;ResNet--&gt; DenseNet --&gt;ResNeXt ---&gt;">
<meta property="og:type" content="article">
<meta property="og:title" content="图像分类经典网络汇总">
<meta property="og:url" content="https://www.chitose.cn/summary-base/index.html">
<meta property="og:site_name" content="Chitose-Blog">
<meta property="og:description" content="前言 在CNN网络结构的演化上，CNN的经典结构始于1998年的LeNet，成于2012年历史性的AlexNet，从此盛于图像相关领域。 发展历史：Lenet --&gt; Alexnet --&gt; ZFnet --&gt; VGG --&gt; NIN --&gt; GoogLeNet --&gt;ResNet--&gt; DenseNet --&gt;ResNeXt ---&gt;">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_1.png">
<meta property="article:published_time" content="2024-02-18T14:41:28.000Z">
<meta property="article:modified_time" content="2024-02-18T14:41:28.000Z">
<meta property="article:author" content="Chitose">
<meta property="article:tag" content="演示">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_1.png"><link rel="shortcut icon" href="https://www.fomal.cc/favicon.ico"><link rel="canonical" href="https://www.chitose.cn/summary-base/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体中文","cht_to_chs":"你已切换为简体中文","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '图像分类经典网络汇总',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-02-18 22:41:28'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://www.fomal.cc/static/css/runtime.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.css" /><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Face.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">91</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/todolist/"><i class="fa-fw fas fa-link"></i><span> 计划</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_1.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Chitose-Blog"><span class="site-name">Chitose-Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/todolist/"><i class="fa-fw fas fa-link"></i><span> 计划</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">图像分类经典网络汇总</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-02-18T14:41:28.000Z" title="发表于 2024-02-18 22:41:28">2024-02-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-02-18T14:41:28.000Z" title="更新于 2024-02-18 22:41:28">2024-02-18</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/others/">others</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">12.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>63分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="图像分类经典网络汇总"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="前言">前言</h2>
<p>在<a
target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=CNN&amp;spm=1001.2101.3001.7020">CNN</a>网络结构的演化上，CNN的经典结构始于<strong>1998年的LeNet</strong>，成于<strong>2012年历史性的AlexNet</strong>，从此<strong>盛于图像相关领域</strong>。</p>
<p>发展历史：<a
target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=Lenet&amp;spm=1001.2101.3001.7020">Lenet</a>
--&gt; Alexnet --&gt; ZFnet --&gt; VGG --&gt; NIN --&gt; GoogLeNet
--&gt;ResNet--&gt; DenseNet --&gt;ResNeXt ---&gt; EfficientNet</p>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 6%" />
<col style="width: 56%" />
<col style="width: 22%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>神经网络</strong></th>
<th>年份</th>
<th><strong>标签</strong></th>
<th><strong>作者</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>LeNets-5</strong></td>
<td>1998年</td>
<td>CNN开山之作</td>
<td>纽约大学</td>
</tr>
<tr class="even">
<td><strong>AlexNet-8</strong></td>
<td>2012年</td>
<td>深度学习CV领域划时代论文<br/>具有里程碑意义<br/>ImageNet
2012冠军</td>
<td>多伦多大学　Hinton团队</td>
</tr>
<tr class="odd">
<td><strong>ZFNet</strong></td>
<td>2013年</td>
<td>ImageNet
2013冠军<br/>结构与AlexNet类似<br/>反卷积网络可视化图像特征</td>
<td>纽约大学</td>
</tr>
<tr class="even">
<td><strong>NiN</strong></td>
<td>2014年</td>
<td>Network in
Network（网中网）<br/>MLPConv（多层感知机卷积层）<br/>GAP（全局平均池化）</td>
<td>新加坡国立大学</td>
</tr>
<tr class="odd">
<td><strong>VGG-16</strong></td>
<td>2014年</td>
<td>开启3*3卷积堆叠时代<br>ImageNet 2014亚军<br>VGG-16和VGG-19</td>
<td>牛津大学</td>
</tr>
<tr class="even">
<td><strong>InceptionV1</strong></td>
<td>2014.09</td>
<td>Google系列论文开创论文<br>ImageNet 2014冠军<br>Inception模块</td>
<td>谷歌</td>
</tr>
<tr class="odd">
<td><strong>InceptionV2</strong></td>
<td>2015.02</td>
<td>Google系列论文<br/>大卷积由小卷积组成<br/>提出BN</td>
<td>谷歌</td>
</tr>
<tr class="even">
<td><strong>InceptionV3</strong></td>
<td>2015.12</td>
<td>Google系列论文<br/>非对称卷积分解<br/>池化+卷积结合</td>
<td>谷歌</td>
</tr>
<tr class="odd">
<td><strong>ResNet</strong></td>
<td>2015年</td>
<td>最具影响力的卷积神经网络<br>ImageNet 2015冠军<br>残差网络</td>
<td>何凯明团队　微软亚院</td>
</tr>
<tr class="even">
<td><strong>Inception v4</strong></td>
<td>2016.02</td>
<td>解决梯度消散<br/>Inception-ResNet</td>
<td>谷歌</td>
</tr>
<tr class="odd">
<td><strong>Wide ResNet</strong></td>
<td>2016年</td>
<td>增加残差中卷积核数量（宽度）</td>
<td>谢尔盖·扎戈鲁科</td>
</tr>
<tr class="even">
<td><strong>Squeeze Net</strong></td>
<td>2016年</td>
<td>轻量级网络<br>压缩参数量</td>
<td>斯坦福大学　伯克利大学</td>
</tr>
<tr class="odd">
<td><strong>FractalNet</strong></td>
<td>2017年</td>
<td>分形网络</td>
<td>Gustav Larsson团队</td>
</tr>
<tr class="even">
<td><strong>DenseNet</strong></td>
<td>2017年</td>
<td>ImageNet 2016冠军<br>CVPR 2017最佳论文<br>Dense模块</td>
<td>康奈尔大学　清华大学</td>
</tr>
<tr class="odd">
<td><strong>Xception</strong></td>
<td>2017.04</td>
<td>深度可分离卷积<br/>性能优于inceptionv3</td>
<td>谷歌Keras的作者</td>
</tr>
<tr class="even">
<td><strong>MobileNetV1</strong></td>
<td>2017年</td>
<td></td>
<td>谷歌</td>
</tr>
<tr class="odd">
<td><strong>SENet</strong></td>
<td>2017年</td>
<td>ImageNet 2017冠军<br>SE模块</td>
<td>momenta + 牛津大学　胡杰</td>
</tr>
<tr class="even">
<td><strong>ResNext</strong></td>
<td>2017年</td>
<td>何恺明团队对ResNet重大改进<br>Resnet + Inception</td>
<td>Saining Xie团队</td>
</tr>
<tr class="odd">
<td><strong>MobileNetV2</strong></td>
<td>2018年</td>
<td>轻量级<br>Group卷积<br>Depthwise Seperable卷积</td>
<td>谷歌</td>
</tr>
<tr class="even">
<td><strong>NASNet</strong></td>
<td>2018年</td>
<td>神经架构搜索　强化学习</td>
<td>谷歌</td>
</tr>
<tr class="odd">
<td><strong>MobileNetV3</strong></td>
<td>2019年</td>
<td></td>
<td>谷歌</td>
</tr>
</tbody>
</table>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/base-summary.png" /></p>
<h2 id="零-lenet">零 LeNet</h2>
<p><strong>期刊日期：</strong> IEEE</p>
<p><strong>论文名称：</strong>《Gradient-Based Learning Applied to
Document Recognition》</p>
<h3 id="简介">1 简介</h3>
<p>LeNet，CNN的开山之作。</p>
<h3 id="网络结构">2 网络结构</h3>
<p>网络共有<strong>五</strong>层</p>
<p>网络结构为：<strong>卷积-&gt;池化-&gt;卷积-&gt;池化-&gt;全连接x3</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLeNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyLeNet, self).__init__()</span><br><span class="line">        self.features_ = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">6</span>,kernel_size=<span class="number">5</span>,padding=<span class="number">2</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">6</span>,out_channels=<span class="number">16</span>,kernel_size=<span class="number">5</span>,padding=<span class="number">2</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Flatten()</span><br><span class="line">        )</span><br><span class="line">        self.cls_ = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">56</span>*<span class="number">56</span>, <span class="number">120</span>),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.Linear(<span class="number">84</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.features_(x)</span><br><span class="line">        x = self.cls_(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="创新点">3 创新点</h3>
<ul>
<li>定义了<strong>CNN基本框架</strong>：卷积层、池化层、全连接层</li>
<li>定义了<strong>卷积层</strong>（局部连接、权值共享）</li>
<li>用<strong>Tanh</strong>作为非线性激活函数</li>
</ul>
<h2 id="一-alexnet">一 AlexNet</h2>
<p><strong>期刊日期：</strong> NIPS-2012</p>
<p><strong>论文名称：</strong>《ImageNet Classification with Deep
Convolutional Neural Networks》</p>
<h3 id="简介-1">1 简介</h3>
<p>2012年Geoffrey和他学生Alex在ImageNet的竞赛中，刷新了image
classification的记录，一举夺得冠军，自此打开了深度学习的大门，也使得深度学习成为学术界的新宠。这次竞赛中Alex所用的结构就被称为作为AlexNet。</p>
<h3 id="网络结构-1">2 网络结构</h3>
<p>网络共有<strong>八</strong>层</p>
<p>结构为：<strong>卷积-&gt;池化-&gt;卷积-&gt;池化-&gt;卷积x3-&gt;池化-&gt;全连接x3</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyAlexNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyAlexNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>)  <span class="comment"># [None, 3, 224, 224] --&gt; [None, 96, 55, 55]</span></span><br><span class="line">        self.pool1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>)  <span class="comment"># [None, 96, 55, 55] --&gt; [None, 96, 27, 27]</span></span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">96</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)  <span class="comment"># [None, 96, 27, 27] --&gt; [None, 256, 27, 27]</span></span><br><span class="line">        self.pool2 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)  <span class="comment"># [None, 256, 27, 27] --&gt; [None, 256, 13, 13]</span></span><br><span class="line">        self.conv3 = nn.Conv2d(in_channels=<span class="number">256</span>, out_channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)  <span class="comment"># [None, 256, 13, 13] --&gt; [None, 384, 13, 13]</span></span><br><span class="line">        self.conv4 = nn.Conv2d(in_channels=<span class="number">384</span>, out_channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)  <span class="comment"># [None, 384, 13, 13] --&gt; [None, 384, 13, 13]</span></span><br><span class="line">        self.conv5 = nn.Conv2d(in_channels=<span class="number">384</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)  <span class="comment"># [None, 384, 13, 13] --&gt; [None, 256, 13, 13]</span></span><br><span class="line">        self.pool3 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)  <span class="comment"># [None, 256, 13, 13] --&gt; [None, 256, 6, 6]</span></span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">256</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">2048</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">2048</span>, <span class="number">2048</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">2048</span>, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = self.pool1(x)</span><br><span class="line"></span><br><span class="line">        x = F.relu(self.conv2(x))</span><br><span class="line">        x = self.pool2(x)</span><br><span class="line"></span><br><span class="line">        x = F.relu(self.conv3(x))</span><br><span class="line">        x = F.relu(self.conv4(x))</span><br><span class="line">        x = F.relu(self.conv5(x))</span><br><span class="line">        x = self.pool3(x)</span><br><span class="line"></span><br><span class="line">        x = x.view(-<span class="number">1</span>,<span class="number">256</span>*<span class="number">6</span>*<span class="number">6</span>)</span><br><span class="line">        x = F.relu(F.dropout(self.fc1(x), <span class="number">0.5</span>))</span><br><span class="line">        x = F.relu(F.dropout(self.fc2(x), <span class="number">0.5</span>))</span><br><span class="line">        x = F.relu(F.dropout(self.fc3(x), <span class="number">0.5</span>))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x = torch.rand([<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>])</span><br><span class="line">    model = MyAlexNet()</span><br><span class="line">    summary(model, input_size=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br></pre></td></tr></table></figure>
<h3 id="创新点-1">3 创新点</h3>
<ul>
<li>使用<strong>非线性激活函数ReLU</strong>加快了训练速度：AlexNet使用ReLU代替了Sigmoid,其能更快的训练,同时解决sigmoid在训练较深的网络中出现的梯度消失。</li>
<li>使用<strong>多个GPU</strong>并行训练：加速了计算。</li>
<li><strong>LRN局部归一化</strong>：增加了泛化能力，做了平滑处理，提高了1%~2%的识别率,
LRN
对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大（使得重要位置更突出），并抑制其他反馈较小的神经元。</li>
<li><strong>重叠池化</strong>：以前的CNN中普遍使用平均池化层，AlexNet全部使用最大池化层,避免了平均池化层的模糊化的效果,并且步长比池化的核的尺寸小,这样池化层的输出之间有重叠,提升了特征的丰富性。</li>
<li><strong>Dropout：</strong>
避免过拟合，Dropout随机失活，随机忽略一些神经元,以避免过拟合。</li>
<li><strong>数据增广：</strong> 来增加模型泛化能力 256×256×3
--&gt;随机裁剪224×224×3 --&gt;进入网络。</li>
</ul>
<h2 id="二-zfnet">二 ZFNet</h2>
<p><strong>期刊日期：</strong>ECCV-2013</p>
<p><strong>论文名称：</strong>《Visualizing and Understanding
Convolutional Networks》</p>
<h3 id="简介-2">1 简介</h3>
<p>ZFNet（Zeiler &amp; Fergus网络），是由Matthew Zeiler和Rob
Fergus提出的卷积神经网络，主要通过改进AlexNet的结构来增强模型的性能。它在2013年的ImageNet
ILSVRC竞赛中获得了冠军。</p>
<h3 id="网络结构-2">2 网络结构</h3>
<p>网络一共8层，(卷积+ReLu+池化)+(卷积+ReLu+池化)+(卷积+ReLu)+(卷积+ReLu)+(卷积+ReLu+池化)+FCN*3</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ZFNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ZFNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            <span class="comment"># 第一层</span></span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">96</span>, <span class="number">7</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 第二次</span></span><br><span class="line">            nn.Conv2d(<span class="number">96</span>, <span class="number">256</span>, <span class="number">5</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">            <span class="comment"># 第三层</span></span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># 第四层</span></span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># 第五层</span></span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 全连接层</span></span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">256</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            <span class="comment"># 输出层。由于这里使用Fashion-MNIST，所以用类别数为10</span></span><br><span class="line">            nn.Linear(<span class="number">4096</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img</span>):</span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        <span class="comment"># print(feature.shape)</span></span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    datashape = (<span class="number">10</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">    model = ZFNet().to(device)</span><br><span class="line">    <span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line">    summary(model,datashape)</span><br></pre></td></tr></table></figure>
<h3 id="创新点-2">3 创新点</h3>
<ul>
<li>引入了反卷积网络来可视化卷积层的特征映射，这有助于理解卷积网络是如何学习和识别图像特征的。</li>
</ul>
<h2 id="三-vgg">三 VGG</h2>
<p><strong>期刊日期：</strong> ICLR-2015</p>
<p><strong>论文名称：</strong>《Very Deep Convolutional Networks for
Large-Scale Image Recognition》</p>
<h3 id="简介-3">1 简介</h3>
<p>VGG-Net来自 Andrew Zisserman 教授的组
(Oxford)，在AlexNet基础上提出了更深的网络，分别为VGG-16和VGG-19。在2014年的
ILSVRC
图像定位和分类两个问题上分别取得了第一名和第二名。相比于AlexNet，AlexNet只有8层，而VGG有16～19层；AlexNet使用了11x11的卷积核，VGG使用了3x3卷积核和2x2的最大池化层，VGG参数是AlexNet的三倍。为后面的框架提供了方向：加深网络的深度。</p>
<h3 id="网络结构-3">2 网络结构</h3>
<p>VGG经典网络共有<strong>16/19</strong>层</p>
<p>16层网络结构为：<strong>（卷积x2-&gt;池化）-&gt;（卷积x2-&gt;池化）-&gt;（卷积x3-&gt;池化）-&gt;（卷积x3-&gt;池化）-&gt;（卷积x3-&gt;池化）-&gt;全连接x3</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VGG16</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block1</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>,<span class="number">64</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">64</span>,<span class="number">64</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.pool1 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block2</span></span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">64</span>,<span class="number">128</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.conv4 = nn.Conv2d(<span class="number">128</span>,<span class="number">128</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.pool2 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block3</span></span><br><span class="line">        self.conv5 = nn.Conv2d(<span class="number">128</span>,<span class="number">256</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.conv6 = nn.Conv2d(<span class="number">256</span>,<span class="number">256</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.conv7 = nn.Conv2d(<span class="number">256</span>,<span class="number">256</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.pool3 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block4</span></span><br><span class="line">        self.conv8 = nn.Conv2d(<span class="number">256</span>,<span class="number">512</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.conv9 = nn.Conv2d(<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.conv10 = nn.Conv2d(<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.pool4 = nn.MaxPool2d(<span class="number">2</span>)            </span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block5</span></span><br><span class="line">        self.conv11 = nn.Conv2d(<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.conv12 = nn.Conv2d(<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.conv13 = nn.Conv2d(<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.pool5 = nn.MaxPool2d(<span class="number">2</span>)               </span><br><span class="line">        </span><br><span class="line">        <span class="comment">#FC层</span></span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">512</span>*<span class="number">7</span>*<span class="number">7</span>,<span class="number">4096</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">4096</span>,<span class="number">4096</span>)</span><br><span class="line">        self.linear3 = nn.Linear(<span class="number">4096</span>,<span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = self.pool1(F.relu(self.conv2(x)))</span><br><span class="line">        x = F.relu(self.conv3(x))</span><br><span class="line">        x = self.pool2(F.relu(self.conv4(x)))</span><br><span class="line">        x = F.relu(self.conv5(x))</span><br><span class="line">        x = F.relu(self.conv6(x))</span><br><span class="line">        x = self.pool3(F.relu(self.conv7(x)))</span><br><span class="line">        x = F.relu(self.conv8(x))</span><br><span class="line">        x = F.relu(self.conv9(x))</span><br><span class="line">        x = self.pool4(F.relu(self.conv10(x)))</span><br><span class="line">        x = F.relu(self.conv11(x))</span><br><span class="line">        x = F.relu(self.conv12(x))</span><br><span class="line">        x = self.pool5(F.relu(self.conv13(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">512</span>*<span class="number">7</span>*<span class="number">7</span>)</span><br><span class="line">        </span><br><span class="line">        x = F.relu(F.dropout(self.linear1(x),<span class="number">0.5</span>))</span><br><span class="line">        x = F.relu(F.dropout(self.linear2(x),<span class="number">0.5</span>))</span><br><span class="line">        output = F.softmax(self.linear3(x),dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h3 id="创新点-3">3 创新点</h3>
<ul>
<li><p><strong>用三个3×3的卷积核代替7×7的卷积核</strong>，有的FC层还用到了1×1的卷积核以及2×2的池化层。网络更深，增加了CNN对特征的学习能力。</p></li>
<li><p><strong>在更深的结构中没有用到LRN（推翻了Alex）</strong>，避免了部分内存和计算的增加。</p></li>
<li><p><strong>VGG采用的是一种Pre-training的方式</strong>，先训练级别简单（层数较浅）的VGGNet的A级网络，然后使用A网络的权重来初始化后面的复杂模型，加快训练的收敛速度。</p></li>
<li><p><strong>采用了Multi-Scale的方法来训练和预测</strong>。可以增加训练的数据量，防止模型过拟合，提升预测准确率
。</p></li>
</ul>
<h2 id="四-nin">四 NiN</h2>
<p><strong>期刊日期：</strong> arXiv 2014.09</p>
<p><strong>论文名称：</strong> 《Network in Network》</p>
<h3 id="简介-4">1 简介</h3>
<p>由多个堆叠的卷积层和空间池化层组成，通过线性的卷积核+非线性激活函数（ReLU、sigmoid、tanh等）生成特征图。</p>
<h3 id="网络结构-4">2 网络结构</h3>
<p>（卷积+1x1卷积+1x1卷积）x3+GAP <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNiN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNiN, self).__init__()</span><br><span class="line">        self.features_ = nn.Sequential(</span><br><span class="line">            self.nin_block(<span class="number">3</span>, <span class="number">96</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            self.nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            self.nin_block(<span class="number">256</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>), <span class="comment"># 标签类别数是10</span></span><br><span class="line">            </span><br><span class="line">            nn.AdaptiveAvgPool2d(<span class="number">1</span>),</span><br><span class="line">            nn.Flatten()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">nin_block</span>(<span class="params">self, in_channels, out_channels, kernel_size, stride, padding</span>):</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.features_(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">net = MyNiN()</span><br><span class="line">net.to(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)  <span class="comment"># 把网络移到GPU上，如果有GPU的话</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印模型概要</span></span><br><span class="line">summary(net, input_size=(<span class="number">10</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))  <span class="comment"># 注意这里的input_size是按照CIFAR-10的图像大小设置的</span></span><br></pre></td></tr></table></figure> ### 3 创新点</p>
<ul>
<li>提出了<strong>NIN块(即mlpconv层)</strong>，等价于<strong>普通卷积后跟两个1×1conv(即mlp层)</strong>，其中每个卷积操作后面都有ReLU进行非线性激活。
<ul>
<li>保持特征图的尺寸、把各通道的输入特征图进行线性加权，起到综合各个通道特征图信息的作用。</li>
<li>MLPconv其实就是在常规卷积后面加了N层1X1卷积，</li>
</ul></li>
<li>提出了<strong>GAP(即GlobalAveragePooling)</strong>，等价于把样本求均值映射到特征，因为最后一层的
feature map 数和类别数相同，那么也就是把样本映射到要分类的类别。</li>
</ul>
<h2 id="五-googlenet-inceptionv1">五 GoogLeNet InceptionV1</h2>
<p><strong>期刊日期：</strong> CVPR-2015</p>
<p><strong>论文名称：</strong>《Going deeper with convolutions》</p>
<h3 id="简介-5">1 简介</h3>
<p>GoogLeNet在2014的ImageNet分类任务上击败了VGG夺得冠军，GoogLeNet跟AlexNet,VGG-Nets这种单纯依靠加深网络结构进而改进网络性能的思路不一样，它在加深网络的同时（22层），在同一层中使用了多个不同尺寸的卷积，以获得不同的视野，最后级联（直接叠加通道数量）。引入Inception结构代替了单纯的卷积+激活的传统操作（这思路最早由Network
in Network提出） <img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/incepv1-module.png" />
<img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Googlev1-aux.png" /></p>
<h3 id="网络结构-5">2 网络结构</h3>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/googlev1.png" />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicConv2d</span>(nn.Module):</span><br><span class="line">    <span class="comment">#在这里我们要改掉原来会输入参数默认值的习惯，而使用定义类型的方式，同时将**kwargs也放到init中继承</span></span><br><span class="line">    <span class="comment">#**kwargs代表了“所需要的全部参数”，由于现在的架构变得复杂，我们不太可能将每个需要用的参数都写在定义中</span></span><br><span class="line">    <span class="comment">#因此，我们继承**kwargs来获得所需类的全部参数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span>, out_channels: <span class="built_in">int</span>,**kwargs</span>): </span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, bias=<span class="literal">False</span>, **kwargs) </span><br><span class="line">                                  <span class="comment">#同样写上**kwargs</span></span><br><span class="line">                                 ,nn.BatchNorm2d(out_channels)</span><br><span class="line">                                 ,nn.ReLU(inplace=<span class="literal">True</span>))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span></span><br><span class="line"><span class="params">                 ,in_channels : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch1x1 : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch3x3red : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch3x3 : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch5x5red : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch5x5 : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,pool_proj : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#1x1</span></span><br><span class="line">        self.branch1 = BasicConv2d(in_channels,ch1x1,kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#1x1 + 3x3</span></span><br><span class="line">        self.branch2 = nn.Sequential(BasicConv2d(in_channels, ch3x3red, kernel_size=<span class="number">1</span>)</span><br><span class="line">                                     ,BasicConv2d(ch3x3red, ch3x3, kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>))</span><br><span class="line">        <span class="comment">#1x1 + 5x5</span></span><br><span class="line">        self.branch3 = nn.Sequential(BasicConv2d(in_channels, ch5x5red, kernel_size=<span class="number">1</span>)</span><br><span class="line">                                     ,BasicConv2d(ch5x5red, ch5x5, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>))</span><br><span class="line">        <span class="comment">#pool + 1x1</span></span><br><span class="line">        self.branch4 = nn.Sequential(nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>, padding=<span class="number">1</span>,ceil_mode=<span class="literal">True</span>)  <span class="comment"># 向上取整</span></span><br><span class="line">                                    ,BasicConv2d(in_channels,pool_proj,kernel_size=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        branch1 = self.branch1(x) <span class="comment">#28x28,ch1x1</span></span><br><span class="line">        branch2 = self.branch2(x) <span class="comment">#28x28,ch3x3</span></span><br><span class="line">        branch3 = self.branch3(x) <span class="comment">#28x28,ch5x5</span></span><br><span class="line">        branch4 = self.branch4(x) <span class="comment">#28x28,pool_proj</span></span><br><span class="line">        outputs = [branch1, branch2, branch3, branch4]</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>) <span class="comment">#合并</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AuxClf</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_channels : <span class="built_in">int</span>, num_classes : <span class="built_in">int</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.feature_ = nn.Sequential(nn.AvgPool2d(kernel_size=<span class="number">5</span>,stride=<span class="number">3</span>)</span><br><span class="line">                                     ,BasicConv2d(in_channels,<span class="number">128</span>, kernel_size=<span class="number">1</span>))</span><br><span class="line">        self.clf_ = nn.Sequential(nn.Linear(<span class="number">4</span>*<span class="number">4</span>*<span class="number">128</span>, <span class="number">1024</span>)</span><br><span class="line">                                 ,nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                 ,nn.Dropout(<span class="number">0.7</span>)</span><br><span class="line">                                 ,nn.Linear(<span class="number">1024</span>,num_classes))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.feature_(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>,<span class="number">4</span>*<span class="number">4</span>*<span class="number">128</span>)</span><br><span class="line">        x = self.clf_(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyIncV1</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,num_classes: <span class="built_in">int</span> = <span class="number">1000</span>, blocks = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> blocks <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            blocks = [BasicConv2d, Inception, AuxClf]</span><br><span class="line">        conv_block = blocks[<span class="number">0</span>]</span><br><span class="line">        inception_block = blocks[<span class="number">1</span>]</span><br><span class="line">        aux_clf_block = blocks[<span class="number">2</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block1</span></span><br><span class="line">        self.conv1 = conv_block(<span class="number">3</span>,<span class="number">64</span>,kernel_size=<span class="number">7</span>,stride=<span class="number">2</span>,padding = <span class="number">3</span>)</span><br><span class="line">        self.maxpool1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,ceil_mode = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block2</span></span><br><span class="line">        self.conv2 = conv_block(<span class="number">64</span>,<span class="number">64</span>,kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = conv_block(<span class="number">64</span>,<span class="number">192</span>,kernel_size=<span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        self.maxpool2 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,ceil_mode = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block3</span></span><br><span class="line">        self.inception3a = inception_block(<span class="number">192</span>,<span class="number">64</span>,<span class="number">96</span>,<span class="number">128</span>,<span class="number">16</span>,<span class="number">32</span>,<span class="number">32</span>)</span><br><span class="line">        self.inception3b = inception_block(<span class="number">256</span>,<span class="number">128</span>,<span class="number">128</span>,<span class="number">192</span>,<span class="number">32</span>,<span class="number">96</span>,<span class="number">64</span>)</span><br><span class="line">        self.maxpool3 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,ceil_mode = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block4 </span></span><br><span class="line">        self.inception4a = inception_block(<span class="number">480</span>,<span class="number">192</span>,<span class="number">96</span>,<span class="number">208</span>,<span class="number">16</span>,<span class="number">48</span>,<span class="number">64</span>)</span><br><span class="line">        self.inception4b = inception_block(<span class="number">512</span>,<span class="number">160</span>,<span class="number">112</span>,<span class="number">224</span>,<span class="number">24</span>,<span class="number">64</span>,<span class="number">64</span>)</span><br><span class="line">        self.inception4c = inception_block(<span class="number">512</span>,<span class="number">128</span>,<span class="number">128</span>,<span class="number">256</span>,<span class="number">24</span>,<span class="number">64</span>,<span class="number">64</span>)</span><br><span class="line">        self.inception4d = inception_block(<span class="number">512</span>,<span class="number">112</span>,<span class="number">144</span>,<span class="number">288</span>,<span class="number">32</span>,<span class="number">64</span>,<span class="number">64</span>)</span><br><span class="line">        self.inception4e = inception_block(<span class="number">528</span>,<span class="number">256</span>,<span class="number">150</span>,<span class="number">320</span>,<span class="number">32</span>,<span class="number">128</span>,<span class="number">128</span>)</span><br><span class="line">        self.maxpool4 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,ceil_mode = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block5</span></span><br><span class="line">        self.inception5a = inception_block(<span class="number">832</span>,<span class="number">256</span>,<span class="number">160</span>,<span class="number">320</span>,<span class="number">32</span>,<span class="number">128</span>,<span class="number">128</span>)</span><br><span class="line">        self.inception5b = inception_block(<span class="number">832</span>,<span class="number">384</span>,<span class="number">192</span>,<span class="number">384</span>,<span class="number">48</span>,<span class="number">128</span>,<span class="number">128</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#clf</span></span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)) <span class="comment">#我需要的输出的特征图尺寸是多少</span></span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.4</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">1024</span>,num_classes)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#auxclf</span></span><br><span class="line">        self.aux1 = aux_clf_block(<span class="number">512</span>, num_classes) <span class="comment">#4a</span></span><br><span class="line">        self.aux2 = aux_clf_block(<span class="number">528</span>, num_classes) <span class="comment">#4d</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="comment">#block1</span></span><br><span class="line">        x = self.maxpool1(self.conv1(x))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block2</span></span><br><span class="line">        x = self.maxpool2(self.conv3(self.conv2(x)))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block3</span></span><br><span class="line">        x = self.inception3a(x)</span><br><span class="line">        x = self.inception3b(x)</span><br><span class="line">        x = self.maxpool3(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block4</span></span><br><span class="line">        x = self.inception4a(x)</span><br><span class="line">        aux1 = self.aux1(x)</span><br><span class="line">        </span><br><span class="line">        x = self.inception4b(x)</span><br><span class="line">        x = self.inception4c(x)</span><br><span class="line">        x = self.inception4d(x)</span><br><span class="line">        aux2 = self.aux2(x)</span><br><span class="line">        </span><br><span class="line">        x = self.inception4e(x)</span><br><span class="line">        x = self.maxpool4(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block5</span></span><br><span class="line">        x = self.inception5a(x)</span><br><span class="line">        x = self.inception5b(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#clf</span></span><br><span class="line">        x = self.avgpool(x) <span class="comment">#在这个全局平均池化之后，特征图尺寸就变成了1x1</span></span><br><span class="line">        x = torch.flatten(x,<span class="number">1</span>)  <span class="comment"># 1=2dim 0=1dim</span></span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">               <span class="comment"># ,aux1,aux2</span></span><br></pre></td></tr></table></figure></p>
<h3 id="创新点-4">3 创新点</h3>
<ul>
<li><p>引入了Inception，使用更宽更深的网络，提升网络性能；</p></li>
<li><p>感受野的大小不同，获得不同尺度特征；</p></li>
<li><p>用稀疏连接代替密集连接，减少计算资源需求；</p></li>
<li><p>1x1的卷积核降维和增深，提高计算资源利用率；</p></li>
<li><p>添加两个辅助分类器帮助训练，避免梯度消失；</p></li>
<li><p>后面的全连接层全部替换为简单的全局平均pooling。</p></li>
</ul>
<h2 id="六-googlenet-inceptionv2">六 GoogLeNet InceptionV2</h2>
<p><strong>期刊日期：</strong> 2015-ICML</p>
<p><strong>论文名称:</strong>《Batch Normalization: Accelerating Deep
Network Training by Reducing Internal Covariate Shift》</p>
<h3 id="简介-6">1 简介</h3>
<p>训练DNN网络的一个难点是，在训练时每层输入数据的分布会发生改变，所以需要较低的学习率和精心设置初始化参数。只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。作者把网络中间层在训练过程中，数据分布的改变称之为：“Internal
Covariate Shift”。因此，作者提出对数据做归一化的想法。</p>
<h3 id="网络结构-6">2 网络结构</h3>
<p>略</p>
<h3 id="创新点-5">3 创新点</h3>
<ul>
<li>使用Batch Normalization，加快模型训练速度；</li>
<li>使用两个3x3的卷积代替5x5的大卷积，降低了参数数量并减轻了过拟合；</li>
<li>去除Dropout并减轻L2正则化（因BN已起到正则化的作用）；</li>
</ul>
<h2 id="七-googlenet-inceptionv3">七 GoogLeNet InceptionV3</h2>
<p><strong>期刊日期：</strong> CVPR-2016</p>
<p><strong>论文名称：</strong>《Rethinking the Inception Architecture
for Computer Vision》</p>
<h3 id="简介-7">1 简介</h3>
<p>GoogLeNet经过了Inception V1、Inception
V2（BN）的发展以后，Google的Szegedy等人又对其进行了更深层次的研究和拓展，在本文中，作者提出了当前环境下，网络设计的一些重要准则，并根据这些准则，对原有的GoogLeNet进行了改进，提出了一个更加复杂、性能更好的模型框架：Inception
V3。这篇文章证明了这些改进的有效性，并为以后的网络设计提供了新的思路。</p>
<h3 id="网络结构-7">2 网络结构</h3>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3.png" /></p>
<h4 id="定义基本卷积模块">2.1 定义基本卷积模块</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicConv2d</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span>, out_channels: <span class="built_in">int</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, bias=<span class="literal">False</span>, **kwargs)</span><br><span class="line">                                  <span class="comment"># 同样写上**kwargs</span></span><br><span class="line">                                  , nn.BatchNorm2d(out_channels)</span><br><span class="line">                                  , nn.ReLU(inplace=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4 id="定义inceptiona模块">2.2 定义InceptionA模块</h4>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3-A.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3-AA.png" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionA---&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionA</span>(nn.Module):</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, pool_features, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionA, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch1x1 = conv_block(in_channels, <span class="number">64</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">        self.branch5x5_1 = conv_block(in_channels, <span class="number">48</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch5x5_2 = conv_block(<span class="number">48</span>, <span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">        self.branch3x3dbl_1 = conv_block(in_channels, <span class="number">64</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_2 = conv_block(<span class="number">64</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_3 = conv_block(<span class="number">96</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">        self.branch_pool = conv_block(in_channels, pool_features, kernel_size=<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line"> </span><br><span class="line">        branch5x5 = self.branch5x5_1(x)</span><br><span class="line">        branch5x5 = self.branch5x5_2(branch5x5)</span><br><span class="line"> </span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_1(x)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)</span><br><span class="line"> </span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line"> </span><br><span class="line">        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="定义inceptionb模块">2.2 定义InceptionB模块</h4>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3-B.png" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionB---&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionB</span>(nn.Module):</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionB, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch3x3 = conv_block(in_channels, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">        self.branch3x3dbl_1 = conv_block(in_channels, <span class="number">64</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_2 = conv_block(<span class="number">64</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_3 = conv_block(<span class="number">96</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch3x3 = self.branch3x3(x)</span><br><span class="line"> </span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_1(x)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)</span><br><span class="line"> </span><br><span class="line">        branch_pool = F.max_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">        outputs = [branch3x3, branch3x3dbl, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="定义inceptionc模块">2.3 定义InceptionC模块</h4>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3-C.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3-CC.png" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionC---&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionC</span>(nn.Module):</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, channels_7x7, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionC, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch1x1 = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">        c7 = channels_7x7</span><br><span class="line">        self.branch7x7_1 = conv_block(in_channels, c7, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch7x7_2 = conv_block(c7, c7, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line">        self.branch7x7_3 = conv_block(c7, <span class="number">192</span>, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</span><br><span class="line"> </span><br><span class="line">        self.branch7x7dbl_1 = conv_block(in_channels, c7, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch7x7dbl_2 = conv_block(c7, c7, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</span><br><span class="line">        self.branch7x7dbl_3 = conv_block(c7, c7, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line">        self.branch7x7dbl_4 = conv_block(c7, c7, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</span><br><span class="line">        self.branch7x7dbl_5 = conv_block(c7, <span class="number">192</span>, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line"> </span><br><span class="line">        self.branch_pool = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line"> </span><br><span class="line">        branch7x7 = self.branch7x7_1(x)</span><br><span class="line">        branch7x7 = self.branch7x7_2(branch7x7)</span><br><span class="line">        branch7x7 = self.branch7x7_3(branch7x7)</span><br><span class="line"> </span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_1(x)</span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)</span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)</span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)</span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)</span><br><span class="line"> </span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line"> </span><br><span class="line">        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="定义inceptiond模块">2.4 定义InceptionD模块</h4>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3-D.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3-DD.png" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionD---&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionD</span>(nn.Module):</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionD, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch3x3_1 = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_2 = conv_block(<span class="number">192</span>, <span class="number">320</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">        self.branch7x7x3_1 = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch7x7x3_2 = conv_block(<span class="number">192</span>, <span class="number">192</span>, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line">        self.branch7x7x3_3 = conv_block(<span class="number">192</span>, <span class="number">192</span>, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</span><br><span class="line">        self.branch7x7x3_4 = conv_block(<span class="number">192</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch3x3 = self.branch3x3_1(x)</span><br><span class="line">        branch3x3 = self.branch3x3_2(branch3x3)</span><br><span class="line"> </span><br><span class="line">        branch7x7x3 = self.branch7x7x3_1(x)</span><br><span class="line">        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)</span><br><span class="line">        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)</span><br><span class="line">        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)</span><br><span class="line"> </span><br><span class="line">        branch_pool = F.max_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        outputs = [branch3x3, branch7x7x3, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="定义inceptione模块">2.5 定义InceptionE模块</h4>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3-E.png" /></p>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3-EE.png" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionE---&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionE</span>(nn.Module):</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionE, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch1x1 = conv_block(in_channels, <span class="number">320</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">        self.branch3x3_1 = conv_block(in_channels, <span class="number">384</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_2a = conv_block(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">1</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        self.branch3x3_2b = conv_block(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line"> </span><br><span class="line">        self.branch3x3dbl_1 = conv_block(in_channels, <span class="number">448</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_2 = conv_block(<span class="number">448</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_3a = conv_block(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">1</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        self.branch3x3dbl_3b = conv_block(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line"> </span><br><span class="line">        self.branch_pool = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line"> </span><br><span class="line">        branch3x3 = self.branch3x3_1(x)</span><br><span class="line">        branch3x3 = [</span><br><span class="line">            self.branch3x3_2a(branch3x3),</span><br><span class="line">            self.branch3x3_2b(branch3x3),</span><br><span class="line">        ]</span><br><span class="line">        branch3x3 = torch.cat(branch3x3, <span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_1(x)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)</span><br><span class="line">        branch3x3dbl = [</span><br><span class="line">            self.branch3x3dbl_3a(branch3x3dbl),</span><br><span class="line">            self.branch3x3dbl_3b(branch3x3dbl),</span><br><span class="line">        ]</span><br><span class="line">        branch3x3dbl = torch.cat(branch3x3dbl, <span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line"> </span><br><span class="line">        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="总代码">2.6 总代码</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;-------------------------第一步：定义基础卷积模块-------------------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicConv2d</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span>, out_channels: <span class="built_in">int</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, bias=<span class="literal">False</span>, **kwargs)</span><br><span class="line">                                  <span class="comment"># 同样写上**kwargs</span></span><br><span class="line">                                  , nn.BatchNorm2d(out_channels)</span><br><span class="line">                                  , nn.ReLU(inplace=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;-----------------第二步：定义Inceptionv3模块---------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionA---&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionA</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, pool_features, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionA, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch1x1 = conv_block(in_channels, <span class="number">64</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.branch5x5_1 = conv_block(in_channels, <span class="number">48</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch5x5_2 = conv_block(<span class="number">48</span>, <span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.branch3x3dbl_1 = conv_block(in_channels, <span class="number">64</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_2 = conv_block(<span class="number">64</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_3 = conv_block(<span class="number">96</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.branch_pool = conv_block(in_channels, pool_features, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line"></span><br><span class="line">        branch5x5 = self.branch5x5_1(x)</span><br><span class="line">        branch5x5 = self.branch5x5_2(branch5x5)</span><br><span class="line"></span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_1(x)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)</span><br><span class="line"></span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line"></span><br><span class="line">        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionB---&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionB</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionB, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch3x3 = conv_block(in_channels, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.branch3x3dbl_1 = conv_block(in_channels, <span class="number">64</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_2 = conv_block(<span class="number">64</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_3 = conv_block(<span class="number">96</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch3x3 = self.branch3x3(x)</span><br><span class="line"></span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_1(x)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)</span><br><span class="line"></span><br><span class="line">        branch_pool = F.max_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        outputs = [branch3x3, branch3x3dbl, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionC---&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionC</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, channels_7x7, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionC, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch1x1 = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        c7 = channels_7x7</span><br><span class="line">        self.branch7x7_1 = conv_block(in_channels, c7, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch7x7_2 = conv_block(c7, c7, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line">        self.branch7x7_3 = conv_block(c7, <span class="number">192</span>, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        self.branch7x7dbl_1 = conv_block(in_channels, c7, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch7x7dbl_2 = conv_block(c7, c7, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</span><br><span class="line">        self.branch7x7dbl_3 = conv_block(c7, c7, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line">        self.branch7x7dbl_4 = conv_block(c7, c7, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</span><br><span class="line">        self.branch7x7dbl_5 = conv_block(c7, <span class="number">192</span>, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        self.branch_pool = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line"></span><br><span class="line">        branch7x7 = self.branch7x7_1(x)</span><br><span class="line">        branch7x7 = self.branch7x7_2(branch7x7)</span><br><span class="line">        branch7x7 = self.branch7x7_3(branch7x7)</span><br><span class="line"></span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_1(x)</span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)</span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)</span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)</span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)</span><br><span class="line"></span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line"></span><br><span class="line">        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionD---&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionD</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionD, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch3x3_1 = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_2 = conv_block(<span class="number">192</span>, <span class="number">320</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.branch7x7x3_1 = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch7x7x3_2 = conv_block(<span class="number">192</span>, <span class="number">192</span>, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line">        self.branch7x7x3_3 = conv_block(<span class="number">192</span>, <span class="number">192</span>, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</span><br><span class="line">        self.branch7x7x3_4 = conv_block(<span class="number">192</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch3x3 = self.branch3x3_1(x)</span><br><span class="line">        branch3x3 = self.branch3x3_2(branch3x3)</span><br><span class="line"></span><br><span class="line">        branch7x7x3 = self.branch7x7x3_1(x)</span><br><span class="line">        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)</span><br><span class="line">        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)</span><br><span class="line">        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)</span><br><span class="line"></span><br><span class="line">        branch_pool = F.max_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        outputs = [branch3x3, branch7x7x3, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionE---&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionE</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionE, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch1x1 = conv_block(in_channels, <span class="number">320</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.branch3x3_1 = conv_block(in_channels, <span class="number">384</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_2a = conv_block(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">1</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        self.branch3x3_2b = conv_block(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        self.branch3x3dbl_1 = conv_block(in_channels, <span class="number">448</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_2 = conv_block(<span class="number">448</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_3a = conv_block(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">1</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        self.branch3x3dbl_3b = conv_block(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        self.branch_pool = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line"></span><br><span class="line">        branch3x3 = self.branch3x3_1(x)</span><br><span class="line">        branch3x3 = [</span><br><span class="line">            self.branch3x3_2a(branch3x3),</span><br><span class="line">            self.branch3x3_2b(branch3x3),</span><br><span class="line">        ]</span><br><span class="line">        branch3x3 = torch.cat(branch3x3, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_1(x)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)</span><br><span class="line">        branch3x3dbl = [</span><br><span class="line">            self.branch3x3dbl_3a(branch3x3dbl),</span><br><span class="line">            self.branch3x3dbl_3b(branch3x3dbl),</span><br><span class="line">        ]</span><br><span class="line">        branch3x3dbl = torch.cat(branch3x3dbl, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line"></span><br><span class="line">        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;-------------------第三步：定义辅助分类器InceptionAux-----------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionAux</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, num_classes, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionAux, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.conv0 = conv_block(in_channels, <span class="number">128</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.conv1 = conv_block(<span class="number">128</span>, <span class="number">768</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">768</span>, num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># N x 768 x 17 x 17</span></span><br><span class="line">        x = F.avg_pool2d(x, kernel_size=<span class="number">5</span>, stride=<span class="number">3</span>)</span><br><span class="line">        <span class="comment"># N x 768 x 5 x 5</span></span><br><span class="line">        x = self.conv0(x)</span><br><span class="line">        <span class="comment"># N x 128 x 5 x 5</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="comment"># N x 768 x 1 x 1</span></span><br><span class="line">        <span class="comment"># Adaptive average pooling</span></span><br><span class="line">        x = F.adaptive_avg_pool2d(x, (<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># N x 768 x 1 x 1</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># N x 768</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="comment"># N x 1000</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;-----------------------第四步：搭建GoogLeNet网络--------------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GoogLeNet</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span>, aux_logits=<span class="literal">True</span>, transform_input=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 inception_blocks=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GoogLeNet, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> inception_blocks <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            inception_blocks = [</span><br><span class="line">                BasicConv2d, InceptionA, InceptionB, InceptionC,</span><br><span class="line">                InceptionD, InceptionE, InceptionAux</span><br><span class="line">            ]</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(inception_blocks) == <span class="number">7</span></span><br><span class="line">        conv_block = inception_blocks[<span class="number">0</span>]</span><br><span class="line">        inception_a = inception_blocks[<span class="number">1</span>]</span><br><span class="line">        inception_b = inception_blocks[<span class="number">2</span>]</span><br><span class="line">        inception_c = inception_blocks[<span class="number">3</span>]</span><br><span class="line">        inception_d = inception_blocks[<span class="number">4</span>]</span><br><span class="line">        inception_e = inception_blocks[<span class="number">5</span>]</span><br><span class="line">        inception_aux = inception_blocks[<span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">        self.aux_logits = aux_logits</span><br><span class="line">        self.transform_input = transform_input</span><br><span class="line">        self.Conv2d_1a_3x3 = conv_block(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.Conv2d_1b_3x3 = conv_block(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.Conv2d_1c_3x3 = conv_block(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.Conv2d_2a_1x1 = conv_block(<span class="number">64</span>, <span class="number">80</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.Conv2d_2b_3x3 = conv_block(<span class="number">80</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.Mixed_A1 = inception_a(<span class="number">192</span>, pool_features=<span class="number">32</span>)</span><br><span class="line">        self.Mixed_A2 = inception_a(<span class="number">256</span>, pool_features=<span class="number">64</span>)</span><br><span class="line">        self.Mixed_A3 = inception_a(<span class="number">288</span>, pool_features=<span class="number">64</span>)</span><br><span class="line">        self.Mixed_B1 = inception_b(<span class="number">288</span>)</span><br><span class="line">        self.Mixed_C1 = inception_c(<span class="number">768</span>, channels_7x7=<span class="number">128</span>)</span><br><span class="line">        self.Mixed_C2 = inception_c(<span class="number">768</span>, channels_7x7=<span class="number">160</span>)</span><br><span class="line">        self.Mixed_C3 = inception_c(<span class="number">768</span>, channels_7x7=<span class="number">160</span>)</span><br><span class="line">        self.Mixed_C4 = inception_c(<span class="number">768</span>, channels_7x7=<span class="number">192</span>)</span><br><span class="line">        <span class="keyword">if</span> aux_logits:</span><br><span class="line">            self.AuxLogits = inception_aux(<span class="number">768</span>, num_classes)</span><br><span class="line">        self.Mixed_D1 = inception_d(<span class="number">768</span>)</span><br><span class="line">        self.Mixed_E1 = inception_e(<span class="number">1280</span>)</span><br><span class="line">        self.Mixed_E2 = inception_e(<span class="number">2048</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">2048</span>, num_classes)</span><br><span class="line">        self._initialize_weights()  <span class="comment"># 在模型构建完毕后调用初始化函数</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;输入(229,229,3)的数据，首先归一化输入，经过5个卷积，2个最大池化层。&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># N x 3 x 299 x 299</span></span><br><span class="line">        x = self.Conv2d_1a_3x3(x)</span><br><span class="line">        <span class="comment"># N x 32 x 149 x 149</span></span><br><span class="line">        x = self.Conv2d_1b_3x3(x)</span><br><span class="line">        <span class="comment"># N x 32 x 147 x 147</span></span><br><span class="line">        x = self.Conv2d_1c_3x3(x)</span><br><span class="line">        <span class="comment"># N x 64 x 147 x 147</span></span><br><span class="line">        x = F.max_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># N x 64 x 73 x 73</span></span><br><span class="line">        x = self.Conv2d_2a_1x1(x)</span><br><span class="line">        <span class="comment"># N x 80 x 73 x 73</span></span><br><span class="line">        x = self.Conv2d_2b_3x3(x)</span><br><span class="line">        <span class="comment"># N x 192 x 71 x 71</span></span><br><span class="line">        x = F.max_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;然后经过</span></span><br><span class="line"><span class="string">        3个InceptionA结构，1个InceptionB，3个InceptionC，1个InceptionD，2个InceptionE，</span></span><br><span class="line"><span class="string">        其中InceptionA，辅助分类器AuxLogits以经过最后一个InceptionC的输出为输入。&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># 35 x 35 x 192</span></span><br><span class="line">        x = self.Mixed_A1(x)  <span class="comment"># InceptionA(192, pool_features=32)  224+32</span></span><br><span class="line">        <span class="comment"># 35 x 35 x 256</span></span><br><span class="line">        x = self.Mixed_A2(x)  <span class="comment"># InceptionA(256, pool_features=64)  224+64</span></span><br><span class="line">        <span class="comment"># 35 x 35 x 288</span></span><br><span class="line">        x = self.Mixed_A3(x)  <span class="comment"># InceptionA(288, pool_features=64)  224+64</span></span><br><span class="line">        <span class="comment"># 35 x 35 x 288</span></span><br><span class="line">        x = self.Mixed_B1(x)  <span class="comment"># InceptionB(288)  384+96+288</span></span><br><span class="line">        <span class="comment"># 17 x 17 x 768</span></span><br><span class="line">        x = self.Mixed_C1(x)  <span class="comment"># InceptionC(768, channels_7x7=128)  invariant,768</span></span><br><span class="line">        <span class="comment"># 17 x 17 x 768</span></span><br><span class="line">        x = self.Mixed_C2(x)  <span class="comment"># InceptionC(768, channels_7x7=160)  invariant,768</span></span><br><span class="line">        <span class="comment"># 17 x 17 x 768</span></span><br><span class="line">        x = self.Mixed_C3(x)  <span class="comment"># InceptionC(768, channels_7x7=160)  invariant,768</span></span><br><span class="line">        <span class="comment"># 17 x 17 x 768</span></span><br><span class="line">        x = self.Mixed_C4(x)  <span class="comment"># InceptionC(768, channels_7x7=192)  invariant,768</span></span><br><span class="line">        <span class="comment"># 17 x 17 x 768</span></span><br><span class="line">        <span class="keyword">if</span> self.training <span class="keyword">and</span> self.aux_logits:</span><br><span class="line">            aux_defined = <span class="literal">True</span></span><br><span class="line">            aux = self.AuxLogits(x)  <span class="comment"># InceptionAux(768, num_classes)</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            aux_defined = <span class="literal">False</span></span><br><span class="line">            aux = <span class="literal">None</span>  <span class="comment"># Initialize aux even if not used</span></span><br><span class="line">        <span class="comment"># 17 x 17 x 768</span></span><br><span class="line">        x = self.Mixed_D1(x)  <span class="comment"># InceptionD(768) half,512</span></span><br><span class="line">        <span class="comment"># 8 x 8 x 1280</span></span><br><span class="line">        x = self.Mixed_E1(x)  <span class="comment"># InceptionE(1280)  invariant,2048</span></span><br><span class="line">        <span class="comment"># 8 x 8 x 2048</span></span><br><span class="line">        x = self.Mixed_E2(x)  <span class="comment"># InceptionE(2048)  invariant,2048</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;进入分类部分。</span></span><br><span class="line"><span class="string">        经过平均池化层+dropout+打平+全连接层输出&#x27;&#x27;&#x27;</span></span><br><span class="line">        x = F.adaptive_avg_pool2d(x, (<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># N x 2048 x 1 x 1</span></span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        <span class="comment"># N x 2048 x 1 x 1</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)  <span class="comment"># Flatten（）就是将2D的特征图压扁为1D的特征向量，是展平操作，进入全连接层之前使用,类才能写进nn.Sequential</span></span><br><span class="line">        <span class="comment"># N x 2048</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="comment"># N x 1000 (num_classes)</span></span><br><span class="line">        <span class="keyword">return</span> x, aux, aux_defined</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x, aux, aux_defined = self._forward(x)</span><br><span class="line">        <span class="keyword">if</span> aux_defined:</span><br><span class="line">            <span class="keyword">return</span> x, aux</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;-----------------------第五步：网络结构参数初始化--------------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 目的：使网络更好收敛，准确率更高</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_initialize_weights</span>(<span class="params">self</span>):  <span class="comment"># 将各种初始化方法定义为一个initialize_weights()的函数并在模型初始后进行使用。</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 遍历网络中的每一层</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="comment"># isinstance(object, type)，如果指定的对象拥有指定的类型，则isinstance()函数返回True</span></span><br><span class="line"></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;如果是卷积层Conv2d&#x27;&#x27;&#x27;</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                <span class="comment"># Kaiming正态分布方式的权重初始化</span></span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">                <span class="string">&#x27;&#x27;&#x27;判断是否有偏置：&#x27;&#x27;&#x27;</span></span><br><span class="line">                <span class="comment"># 如果偏置不是0，将偏置置成0，对偏置进行初始化</span></span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="comment"># torch.nn.init.constant_(tensor, val)，初始化整个矩阵为常数val</span></span><br><span class="line">                    nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">                <span class="string">&#x27;&#x27;&#x27;如果是全连接层&#x27;&#x27;&#x27;</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                <span class="comment"># init.normal_(tensor, mean=0.0, std=1.0)，使用从正态分布中提取的值填充输入张量</span></span><br><span class="line">                <span class="comment"># 参数：tensor：一个n维Tensor，mean：正态分布的平均值，std：正态分布的标准差</span></span><br><span class="line">                nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;---------------------------------------显示网络结构-------------------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    net = GoogLeNet(<span class="number">1000</span>).cuda()</span><br><span class="line">    <span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"></span><br><span class="line">    summary(net, (<span class="number">10</span>, <span class="number">3</span>, <span class="number">299</span>, <span class="number">299</span>))</span><br></pre></td></tr></table></figure>
<h3 id="创新点-6">3 创新点</h3>
<ul>
<li><p>卷积核分解：1.将大的卷积核分解成小的。2.非对称分解（<code>n*n</code>的卷积核替换成
<code>1*n</code> 和<code>n*1</code>的卷积核堆叠）</p></li>
<li><p>并行执行（卷积C+池化P），再进行feature map的堆叠</p></li>
<li><p>标签平滑（LSR）进行模型正则化</p></li>
<li><p>纠正了辅助分类器的作用，靠近输入辅助分类器（加不加没有影响），但是靠近输出的辅助分类器（加上BN后）可以起到正则化的作用。</p></li>
</ul>
<h2 id="八-resnet">八 ResNet</h2>
<p><strong>期刊日期：</strong> CVPR-2016</p>
<p><strong>论文名称：</strong>《Deep Residual Learning for Image
Recognition》</p>
<h3 id="简介-8">1 简介</h3>
<p>2015年何恺明等大神推出的ResNet相当经典，在深度学习发展历程中具有里程碑的意义。在ISLVRC和COCO上横扫所有选手，获得冠军，同时也在2016CVPR获得best
paper。ResNet在网络结构上做了大创新，首次提出了残差的思想（跨层连接），不再是简单的堆积层数，解决了网络过深而导致的梯度消失的问题，为更深的网络提供了有力的方向。</p>
<h3 id="网络结构-8">2 网络结构</h3>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/rresnet.png" />
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Type</span>, <span class="type">Union</span>, <span class="type">List</span>, <span class="type">Optional</span></span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv3x3</span>(<span class="params">in_, out_, stride=<span class="number">1</span>, initial_zero=<span class="literal">False</span></span>):</span><br><span class="line">    bn = nn.BatchNorm2d(out_)</span><br><span class="line">    <span class="keyword">if</span> initial_zero:</span><br><span class="line">        nn.init.constant_(bn.weight, <span class="number">0</span>)  <span class="comment"># 使用nn.init.constant_并直接作用于bn.weight</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_, out_, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">        bn</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv1x1</span>(<span class="params">in_, out_, stride=<span class="number">1</span>, initial_zero=<span class="literal">False</span></span>):</span><br><span class="line">    bn = nn.BatchNorm2d(out_)</span><br><span class="line">    <span class="keyword">if</span> initial_zero:</span><br><span class="line">        nn.init.constant_(bn.weight, <span class="number">0</span>)  <span class="comment"># 使用nn.init.constant_并直接作用于bn.weight</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_, out_, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">        bn</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualUnit</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, out_: <span class="built_in">int</span>, stride1: <span class="built_in">int</span> = <span class="number">1</span>, in_: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()  <span class="comment"># 修正了这里</span></span><br><span class="line"></span><br><span class="line">        self.stride1 = stride1</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> stride1 != <span class="number">1</span>:  <span class="comment"># 零号的RU</span></span><br><span class="line">            in_ = <span class="built_in">int</span>(out_ / <span class="number">2</span>)  <span class="comment"># 零号</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            in_ = out_  <span class="comment"># 除了零号的RU</span></span><br><span class="line">        <span class="comment"># 拟合部分，输出F(x)</span></span><br><span class="line">        self.fit_ = nn.Sequential(</span><br><span class="line">            conv3x3(in_, out_, stride=stride1),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            conv3x3(out_, out_, initial_zero=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.skipconv = conv1x1(in_, out_, stride=stride1)</span><br><span class="line">        <span class="comment"># 单独定义放在H(x)之后来使用的激活函数ReLu</span></span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        fx = self.fit_(x)  <span class="comment"># 拟合结果</span></span><br><span class="line">        <span class="keyword">if</span> self.stride1 == <span class="number">2</span>:</span><br><span class="line">            x = self.skipconv(x)  <span class="comment"># 跳跃连接</span></span><br><span class="line">        hx = self.relu(fx + x)</span><br><span class="line">        <span class="keyword">return</span> hx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BottleNeck</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, middle_out, stride1: <span class="built_in">int</span> = <span class="number">1</span>, in_: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        out_ = <span class="number">4</span> * middle_out</span><br><span class="line">        <span class="keyword">if</span> in_ == <span class="literal">None</span>:  <span class="comment"># 仅为conv2的零号BN服务</span></span><br><span class="line">            <span class="keyword">if</span> stride1 != <span class="number">1</span>:  <span class="comment"># 除了conv2的零号BN</span></span><br><span class="line">                in_ = middle_out * <span class="number">2</span>  <span class="comment"># 零号</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                in_ = middle_out * <span class="number">4</span>  <span class="comment"># 除了零号的BN</span></span><br><span class="line">        self.fit_ = nn.Sequential(</span><br><span class="line">            conv1x1(in_, middle_out, stride=stride1),  <span class="comment"># 无论是RU还是BN,第一层的stride都要带上</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            conv3x3(middle_out, middle_out),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            conv1x1(middle_out, out_, initial_zero=<span class="literal">True</span>)  <span class="comment"># 无论RU还是BN，最后一层的zero都是True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.skipconv = conv1x1(in_, out_, stride=stride1)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        fx = self.fit_(x)</span><br><span class="line"></span><br><span class="line">        x = self.skipconv(x)</span><br><span class="line">        hx = self.relu(fx + x)</span><br><span class="line">        <span class="keyword">return</span> hx</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mak_layers</span>(<span class="params">block: <span class="type">Type</span>[<span class="type">Union</span>[ResidualUnit, BottleNeck]],middle_out:<span class="built_in">int</span>,num_blocks: <span class="built_in">int</span>, afterconv1: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span><br><span class="line">    <span class="comment"># 当你参数要填入类，需要用Type,有两种类，故Union</span></span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">if</span> afterconv1:</span><br><span class="line">        layers.append(block(<span class="number">64</span>,in_=<span class="number">64</span>)) <span class="comment"># 定死了</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        layers.append(block(middle_out,stride1=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_blocks-<span class="number">1</span>):</span><br><span class="line">        layers.append(block(middle_out))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,block: <span class="type">Type</span>[<span class="type">Union</span>[ResidualUnit, BottleNeck]],layers:<span class="type">List</span>[<span class="built_in">int</span>],num_classes:<span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.divisor = <span class="number">512</span> <span class="keyword">if</span> block == ResidualUnit <span class="keyword">else</span> <span class="number">2048</span>  <span class="comment"># 用于全连接层的除数</span></span><br><span class="line">        <span class="comment"># 卷积+池化</span></span><br><span class="line">        self.layer1 = nn.Sequential(nn.Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">64</span>,kernel_size=<span class="number">7</span>,stride=<span class="number">2</span>,padding=<span class="number">3</span>),</span><br><span class="line">                                    nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">                                    nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                                    nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,ceil_mode=<span class="literal">True</span>))</span><br><span class="line">        <span class="comment"># Layer2-Layer5 残差块/瓶颈结构</span></span><br><span class="line">        self.layer2_x=mak_layers(block,<span class="number">64</span>,layers[<span class="number">0</span>],<span class="literal">True</span>)</span><br><span class="line">        self.layer3_x=mak_layers(block,<span class="number">128</span>,layers[<span class="number">1</span>],<span class="literal">False</span>)</span><br><span class="line">        self.layer4_x=mak_layers(block,<span class="number">256</span>,layers[<span class="number">2</span>],<span class="literal">False</span>)</span><br><span class="line">        self.layer5_x=mak_layers(block,<span class="number">512</span>,layers[<span class="number">3</span>],<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 全局平均池化</span></span><br><span class="line">        self.avgpool =nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分类</span></span><br><span class="line">        self.fc = nn.Linear(self.divisor,num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2_x(x)</span><br><span class="line">        x = self.layer3_x(x)</span><br><span class="line">        x = self.layer4_x(x)</span><br><span class="line">        x = self.layer5_x(x)</span><br><span class="line">        x = self.avgpool(x) <span class="comment"># (n_samples,fc,1,1)</span></span><br><span class="line">        x = torch.flatten(x,<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure> ### 3 创新点</p>
<ul>
<li>实现了超深的网络结构（突破1000层）</li>
<li>提出了残差网络，将x直接传递到后面的层，使得网络可以很容易的学习恒等变换，从而解决网络退化的问题，同时也使得学习效率更高。</li>
<li>使用BatchNormalization加速训练（丢弃dropout）</li>
</ul>
<h2 id="九-resnext">九 ResNext</h2>
<p><strong>期刊日期：</strong>CVPR-2017</p>
<p><strong>论文名称：</strong>《Aggregated Residual Transformations for
Deep Neural Networks》</p>
<h3 id="简介-9">1 简介</h3>
<p>ResNeXt是何恺明大神的又一经典之作。提出了一种用于图像分类的简单、高度模块化的网络架构。
这个网络可以被解释为 VGG、ResNet 和 Inception
的结合体，它通过重复多个block（如在 VGG
中）块组成，每个block块聚合了多种转换（如
Inception），同时考虑到跨层连接（来自
ResNet）。该网络通过反复堆叠Building Block实现，Building
Block则通过聚集简洁的卷积模块来实现。并且提出一个与网络宽度和深度类似作用的参数，用来衡量网络大小，称之为Cardinality基数。在ILSVRC
2016分类任务获得了第二名。</p>
<h3 id="网络结构-9">2 网络结构</h3>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/resnext-t1.png" /></p>
<ul>
<li>ResNet的conv2_x的输入和middle改一下</li>
<li>ResNet的每大层的middle改一下</li>
<li>3x3改为groups为32</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Type</span>, <span class="type">Union</span>, <span class="type">List</span>, <span class="type">Optional</span></span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv3x3</span>(<span class="params">in_, out_, stride=<span class="number">1</span>, initial_zero=<span class="literal">False</span></span>):</span><br><span class="line">    bn = nn.BatchNorm2d(out_)</span><br><span class="line">    <span class="keyword">if</span> initial_zero:</span><br><span class="line">        nn.init.constant_(bn.weight, <span class="number">0</span>)  <span class="comment"># 使用nn.init.constant_并直接作用于bn.weight</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_, out_, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>,groups=<span class="number">32</span>),</span><br><span class="line">        bn</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv1x1</span>(<span class="params">in_, out_, stride=<span class="number">1</span>, initial_zero=<span class="literal">False</span></span>):</span><br><span class="line">    bn = nn.BatchNorm2d(out_)</span><br><span class="line">    <span class="keyword">if</span> initial_zero:</span><br><span class="line">        nn.init.constant_(bn.weight, <span class="number">0</span>)  <span class="comment"># 使用nn.init.constant_并直接作用于bn.weight</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_, out_, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">        bn</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualUnit</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, out_: <span class="built_in">int</span>, stride1: <span class="built_in">int</span> = <span class="number">1</span>, in_: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()  <span class="comment"># 修正了这里</span></span><br><span class="line"></span><br><span class="line">        self.stride1 = stride1</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> stride1 != <span class="number">1</span>:  <span class="comment"># 零号的RU</span></span><br><span class="line">            in_ = <span class="built_in">int</span>(out_ / <span class="number">2</span>)  <span class="comment"># 零号</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            in_ = out_  <span class="comment"># 除了零号的RU</span></span><br><span class="line">        <span class="comment"># 拟合部分，输出F(x)</span></span><br><span class="line">        self.fit_ = nn.Sequential(</span><br><span class="line">            conv3x3(in_, out_, stride=stride1),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            conv3x3(out_, out_, initial_zero=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.skipconv = conv1x1(in_, out_, stride=stride1)</span><br><span class="line">        <span class="comment"># 单独定义放在H(x)之后来使用的激活函数ReLu</span></span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        fx = self.fit_(x)  <span class="comment"># 拟合结果</span></span><br><span class="line">        <span class="keyword">if</span> self.stride1 == <span class="number">2</span>:</span><br><span class="line">            x = self.skipconv(x)  <span class="comment"># 跳跃连接</span></span><br><span class="line">        hx = self.relu(fx + x)</span><br><span class="line">        <span class="keyword">return</span> hx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BottleNeck</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, middle_out, stride1: <span class="built_in">int</span> = <span class="number">1</span>, in_: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        out_ = <span class="number">2</span> * middle_out</span><br><span class="line">        <span class="keyword">if</span> in_ == <span class="literal">None</span>:  <span class="comment"># 仅为conv2的零号BN服务</span></span><br><span class="line">            <span class="keyword">if</span> stride1 != <span class="number">1</span>:  <span class="comment"># 除了conv2的零号BN</span></span><br><span class="line">                in_ = middle_out  <span class="comment"># 零号</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                in_ = middle_out * <span class="number">2</span>   <span class="comment"># 除了零号的BN</span></span><br><span class="line">        self.fit_ = nn.Sequential(</span><br><span class="line">            conv1x1(in_, middle_out, stride=stride1),  <span class="comment"># 无论是RU还是BN,第一层的stride都要带上</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            conv3x3(middle_out, middle_out),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            conv1x1(middle_out, out_, initial_zero=<span class="literal">True</span>)  <span class="comment"># 无论RU还是BN，最后一层的zero都是True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.skipconv = conv1x1(in_, out_, stride=stride1)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        fx = self.fit_(x)</span><br><span class="line"></span><br><span class="line">        x = self.skipconv(x)</span><br><span class="line">        hx = self.relu(fx + x)</span><br><span class="line">        <span class="keyword">return</span> hx</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mak_layers</span>(<span class="params">block: <span class="type">Type</span>[<span class="type">Union</span>[ResidualUnit, BottleNeck]],middle_out:<span class="built_in">int</span>,num_blocks: <span class="built_in">int</span>, afterconv1: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span><br><span class="line">    <span class="comment"># 当你参数要填入类，需要用Type,有两种类，故Union</span></span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">if</span> afterconv1:</span><br><span class="line">        layers.append(block(<span class="number">128</span>,in_=<span class="number">64</span>)) <span class="comment"># 定死了</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        layers.append(block(middle_out,stride1=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_blocks-<span class="number">1</span>):</span><br><span class="line">        layers.append(block(middle_out))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResNeXt</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,block: <span class="type">Type</span>[<span class="type">Union</span>[ResidualUnit, BottleNeck]],layers:<span class="type">List</span>[<span class="built_in">int</span>],num_classes:<span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.divisor = <span class="number">512</span> <span class="keyword">if</span> block == ResidualUnit <span class="keyword">else</span> <span class="number">2048</span>  <span class="comment"># 用于全连接层的除数</span></span><br><span class="line">        <span class="comment"># 卷积+池化</span></span><br><span class="line">        self.layer1 = nn.Sequential(nn.Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">64</span>,kernel_size=<span class="number">7</span>,stride=<span class="number">2</span>,padding=<span class="number">3</span>),</span><br><span class="line">                                    nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">                                    nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                                    nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,ceil_mode=<span class="literal">True</span>))</span><br><span class="line">        <span class="comment"># Layer2-Layer5 残差块/瓶颈结构</span></span><br><span class="line">        self.layer2_x=mak_layers(block,<span class="number">128</span>,layers[<span class="number">0</span>],<span class="literal">True</span>)</span><br><span class="line">        self.layer3_x=mak_layers(block,<span class="number">256</span>,layers[<span class="number">1</span>],<span class="literal">False</span>)</span><br><span class="line">        self.layer4_x=mak_layers(block,<span class="number">512</span>,layers[<span class="number">2</span>],<span class="literal">False</span>)</span><br><span class="line">        self.layer5_x=mak_layers(block,<span class="number">1024</span>,layers[<span class="number">3</span>],<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 全局平均池化</span></span><br><span class="line">        self.avgpool =nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分类</span></span><br><span class="line">        self.fc = nn.Linear(self.divisor,num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2_x(x)</span><br><span class="line">        x = self.layer3_x(x)</span><br><span class="line">        x = self.layer4_x(x)</span><br><span class="line">        x = self.layer5_x(x)</span><br><span class="line">        x = self.avgpool(x) <span class="comment"># (n_samples,fc,1,1)</span></span><br><span class="line">        x = torch.flatten(x,<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    layers=[<span class="number">3</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">3</span>]</span><br><span class="line">    resnext = ResNeXt(BottleNeck,layers,<span class="number">1000</span>).to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">    <span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line">    datashape = (<span class="number">100</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">    summary(resnext,datashape)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="创新点-7">3 创新点</h3>
<ul>
<li><p>提炼VGG、ResNet和Inception系列的优秀思想。</p></li>
<li><p><strong>分支同构，</strong>处理相同尺寸的特征图时，采用同样大小、数量的卷积核，网络结构简明，模块化需要手动调节的超参数少。</p></li>
<li><p>提出<strong>cardinality</strong>来衡量模型复杂度，实验表明cardinality比模型深度、宽度更高效。</p></li>
<li><p><strong>总结：</strong>用分组卷积带来的参数量的减少，换来了在残差块中更宽的通道</p></li>
</ul>
<h2 id="十-xception">十 Xception</h2>
<h3 id="简介-10">1 简介</h3>
<p>如果说<strong>ResNeXt</strong>是<a
target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=ResNet&amp;spm=1001.2101.3001.7020">ResNet</a>的增强版，那毫无疑问，<strong>Xception</strong>就是<a
target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=Inception&amp;spm=1001.2101.3001.7020">Inception</a>家族的增强版。这个网络在Inception
v3的基础上，把Inception模块替换为深度可分离卷积，然后结合ResNet的跳跃连接，提出了Xception</p>
<h3 id="网络结构-10">2 网络结构</h3>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/xception-f5.png" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ConvBN</span>(<span class="params">in_channels,out_channels,kernel_size,stride</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=kernel_size,stride=stride,padding=<span class="number">0</span> <span class="keyword">if</span> kernel_size==<span class="number">1</span> <span class="keyword">else</span> (kernel_size-<span class="number">1</span>)//<span class="number">2</span>),</span><br><span class="line">        nn.BatchNorm2d(out_channels),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ConvBNRelu</span>(<span class="params">in_channels,out_channels,kernel_size,stride</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        ConvBN(in_channels, out_channels, kernel_size, stride),</span><br><span class="line">        nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">SeparableConvolution</span>(<span class="params">in_channels, out_channels</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>,padding=<span class="number">1</span>,groups=in_channels),</span><br><span class="line">        nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>),</span><br><span class="line">    )</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">SeparableConvolutionRelu</span>(<span class="params">in_channels, out_channels</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        SeparableConvolution(in_channels, out_channels),</span><br><span class="line">        nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ReluSeparableConvolution</span>(<span class="params">in_channels, out_channels</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line">        SeparableConvolution(in_channels, out_channels)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EntryBottleneck</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, first_relu=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(EntryBottleneck, self).__init__()</span><br><span class="line">        mid_channels = out_channels</span><br><span class="line"></span><br><span class="line">        self.shortcut = ConvBN(in_channels=in_channels,out_channels=out_channels,kernel_size=<span class="number">1</span>,stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.bottleneck = nn.Sequential(</span><br><span class="line">            ReluSeparableConvolution(in_channels=in_channels,out_channels=mid_channels) <span class="keyword">if</span> first_relu <span class="keyword">else</span> SeparableConvolution(in_channels=in_channels,out_channels=mid_channels),</span><br><span class="line">            ReluSeparableConvolution(in_channels=mid_channels, out_channels=out_channels),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = self.shortcut(x)</span><br><span class="line">        x = self.bottleneck(x)</span><br><span class="line">        <span class="keyword">return</span> out+x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MiddleBottleneck</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(MiddleBottleneck, self).__init__()</span><br><span class="line">        mid_channels = out_channels</span><br><span class="line"></span><br><span class="line">        self.bottleneck = nn.Sequential(</span><br><span class="line">            ReluSeparableConvolution(in_channels=in_channels,out_channels=mid_channels),</span><br><span class="line">            ReluSeparableConvolution(in_channels=mid_channels, out_channels=mid_channels),</span><br><span class="line">            ReluSeparableConvolution(in_channels=mid_channels, out_channels=out_channels),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = self.bottleneck(x)</span><br><span class="line">        <span class="keyword">return</span> out+x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ExitBottleneck</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(ExitBottleneck, self).__init__()</span><br><span class="line">        mid_channels = in_channels</span><br><span class="line"></span><br><span class="line">        self.shortcut = ConvBN(in_channels=in_channels,out_channels=out_channels,kernel_size=<span class="number">1</span>,stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.bottleneck = nn.Sequential(</span><br><span class="line">            ReluSeparableConvolution(in_channels=in_channels,out_channels=mid_channels),</span><br><span class="line">            ReluSeparableConvolution(in_channels=mid_channels, out_channels=out_channels),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = self.shortcut(x)</span><br><span class="line">        x = self.bottleneck(x)</span><br><span class="line">        <span class="keyword">return</span> out+x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Xception</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Xception, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.entryFlow = nn.Sequential(</span><br><span class="line">            ConvBNRelu(in_channels=<span class="number">3</span>, out_channels=<span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">            ConvBNRelu(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>),</span><br><span class="line">            EntryBottleneck(in_channels=<span class="number">64</span>, out_channels=<span class="number">128</span>, first_relu=<span class="literal">False</span>),</span><br><span class="line">            EntryBottleneck(in_channels=<span class="number">128</span>, out_channels=<span class="number">256</span>, first_relu=<span class="literal">True</span>),</span><br><span class="line">            EntryBottleneck(in_channels=<span class="number">256</span>, out_channels=<span class="number">728</span>, first_relu=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line">        self.middleFlow = nn.Sequential(</span><br><span class="line">            MiddleBottleneck(in_channels=<span class="number">728</span>,out_channels=<span class="number">728</span>),</span><br><span class="line">            MiddleBottleneck(in_channels=<span class="number">728</span>, out_channels=<span class="number">728</span>),</span><br><span class="line">            MiddleBottleneck(in_channels=<span class="number">728</span>, out_channels=<span class="number">728</span>),</span><br><span class="line">            MiddleBottleneck(in_channels=<span class="number">728</span>, out_channels=<span class="number">728</span>),</span><br><span class="line">            MiddleBottleneck(in_channels=<span class="number">728</span>, out_channels=<span class="number">728</span>),</span><br><span class="line">            MiddleBottleneck(in_channels=<span class="number">728</span>, out_channels=<span class="number">728</span>),</span><br><span class="line">            MiddleBottleneck(in_channels=<span class="number">728</span>, out_channels=<span class="number">728</span>),</span><br><span class="line">            MiddleBottleneck(in_channels=<span class="number">728</span>, out_channels=<span class="number">728</span>),</span><br><span class="line">        )</span><br><span class="line">        self.exitFlow = nn.Sequential(</span><br><span class="line">            ExitBottleneck(in_channels=<span class="number">728</span>, out_channels=<span class="number">1024</span>),</span><br><span class="line">            SeparableConvolutionRelu(in_channels=<span class="number">1024</span>, out_channels=<span class="number">1536</span>),</span><br><span class="line">            SeparableConvolutionRelu(in_channels=<span class="number">1536</span>, out_channels=<span class="number">2048</span>),</span><br><span class="line">            nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.linear = nn.Linear(<span class="number">2048</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.entryFlow(x)</span><br><span class="line">        x = self.middleFlow(x)</span><br><span class="line">        x = self.exitFlow(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = Xception().to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">    <span class="built_in">input</span> = torch.randn(<span class="number">1</span>,<span class="number">3</span>,<span class="number">299</span>,<span class="number">299</span>).to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line">    summary(model,<span class="built_in">input</span>.shape)</span><br><span class="line"></span><br><span class="line">    output = model(<span class="built_in">input</span>)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="创新点-8">3 创新点</h3>
<ul>
<li><strong>深度可分离卷积（Depthwise Separable
Convolution）:DW+PW</strong>
<ul>
<li>在inceptionv1v2v3中是先PW后DW</li>
</ul></li>
<li><strong>模块化结构</strong></li>
<li><strong>跨通道相关性（PW）和空间相关性（DW）的分离</strong></li>
</ul>
<h2 id="十一-densenet">十一 DenseNet</h2>
<p>期刊日期：CVPR-2017</p>
<p>论文名称：《Densely Connected Convolutional Networks》</p>
<h3 id="简介-11">1 简介</h3>
<p>DenseNet是一种卷积网络架构，其特点是层间密集连接，每层接收前所有层的特征图作为输入。这种设计改善了梯度流、减少了参数数量，提升了特征重用，从而增强了网络的学习能力和效率。</p>
<h3 id="网络结构-11">2 网络结构</h3>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/densenet-t1.png" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;PyTorch Version: &quot;</span>,torch.__version__)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Torchvision Version: &quot;</span>,torchvision.__version__)</span><br><span class="line"></span><br><span class="line">__all__ = [<span class="string">&#x27;DenseNet121&#x27;</span>, <span class="string">&#x27;DenseNet169&#x27;</span>,<span class="string">&#x27;DenseNet201&#x27;</span>,<span class="string">&#x27;DenseNet264&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">1. bn_size 论文默认为4</span></span><br><span class="line"><span class="string">2. 压缩因子θ=0.5 在过渡层体现</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Conv1</span>(<span class="params">in_channels, out_channels</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="literal">False</span>),</span><br><span class="line">        nn.BatchNorm2d(out_channels),</span><br><span class="line">        nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">_TransitionLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels</span>):</span><br><span class="line">        <span class="built_in">super</span>(_TransitionLayer, self).__init__()</span><br><span class="line">        self.transition_layer = nn.Sequential(</span><br><span class="line">            nn.BatchNorm2d(in_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.AvgPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.transition_layer(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">_DenseLayer</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 64+32*i 32 4 0</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, growth_rate, bn_size, drop_rate=<span class="number">0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(_DenseLayer, self).__init__()</span><br><span class="line">        self.drop_rate = drop_rate</span><br><span class="line">        self.dense_layer = nn.Sequential(</span><br><span class="line">            nn.BatchNorm2d(in_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            <span class="comment"># growth_rate：增长率。一层产生多少个特征图</span></span><br><span class="line">            nn.Conv2d(in_channels=in_channels, out_channels=bn_size * growth_rate, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(bn_size * growth_rate),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(in_channels=bn_size * growth_rate, out_channels=growth_rate, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">        )</span><br><span class="line">        self.dropout = nn.Dropout(p=self.drop_rate)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = self.dense_layer(x)</span><br><span class="line">        <span class="keyword">if</span> self.drop_rate &gt; <span class="number">0</span>:</span><br><span class="line">            y = self.dropout(y)</span><br><span class="line">        <span class="keyword">return</span> torch.cat([x, y], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DenseBlock</span>(nn.Module):</span><br><span class="line">    <span class="comment">#  6 64 32 4 0</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_layers, inplances, growth_rate, bn_size , drop_rate=<span class="number">0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DenseBlock, self).__init__()</span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_layers):</span><br><span class="line">            layers.append(_DenseLayer(inplances + i * growth_rate, growth_rate, bn_size, drop_rate))</span><br><span class="line">        self.layers = nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.layers(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DenseNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, init_channels=<span class="number">64</span>, growth_rate=<span class="number">32</span>, blocks=[<span class="number">6</span>, <span class="number">12</span>, <span class="number">24</span>, <span class="number">16</span>],num_classes=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DenseNet, self).__init__()</span><br><span class="line">        bn_size = <span class="number">4</span></span><br><span class="line">        drop_rate = <span class="number">0</span></span><br><span class="line">        self.conv1 = Conv1(in_channels=<span class="number">3</span>, out_channels=init_channels)</span><br><span class="line"></span><br><span class="line">        num_features = init_channels</span><br><span class="line">        self.layer1 = DenseBlock(num_layers=blocks[<span class="number">0</span>], inplances=num_features, growth_rate=growth_rate, bn_size=bn_size, drop_rate=drop_rate)</span><br><span class="line">        num_features = num_features + blocks[<span class="number">0</span>] * growth_rate</span><br><span class="line">        self.transition1 = _TransitionLayer(in_channels=num_features, out_channels=num_features // <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        num_features = num_features // <span class="number">2</span></span><br><span class="line">        self.layer2 = DenseBlock(num_layers=blocks[<span class="number">1</span>], inplances=num_features, growth_rate=growth_rate, bn_size=bn_size, drop_rate=drop_rate)</span><br><span class="line">        num_features = num_features + blocks[<span class="number">1</span>] * growth_rate</span><br><span class="line">        self.transition2 = _TransitionLayer(in_channels=num_features, out_channels=num_features // <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        num_features = num_features // <span class="number">2</span></span><br><span class="line">        self.layer3 = DenseBlock(num_layers=blocks[<span class="number">2</span>], inplances=num_features, growth_rate=growth_rate, bn_size=bn_size, drop_rate=drop_rate)</span><br><span class="line">        num_features = num_features + blocks[<span class="number">2</span>] * growth_rate</span><br><span class="line">        self.transition3 = _TransitionLayer(in_channels=num_features, out_channels=num_features // <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        num_features = num_features // <span class="number">2</span></span><br><span class="line">        self.layer4 = DenseBlock(num_layers=blocks[<span class="number">3</span>], inplances=num_features, growth_rate=growth_rate, bn_size=bn_size, drop_rate=drop_rate)</span><br><span class="line">        num_features = num_features + blocks[<span class="number">3</span>] * growth_rate</span><br><span class="line"></span><br><span class="line">        self.avgpool = nn.AvgPool2d(<span class="number">7</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.fc = nn.Linear(num_features, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.transition1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.transition2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.transition3(x)</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line"></span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">DenseNet121</span>():</span><br><span class="line">    <span class="keyword">return</span> DenseNet(init_channels=<span class="number">64</span>, growth_rate=<span class="number">32</span>, blocks=[<span class="number">6</span>, <span class="number">12</span>, <span class="number">24</span>, <span class="number">16</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">DenseNet169</span>():</span><br><span class="line">    <span class="keyword">return</span> DenseNet(init_channels=<span class="number">64</span>, growth_rate=<span class="number">32</span>, blocks=[<span class="number">6</span>, <span class="number">12</span>, <span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">DenseNet201</span>():</span><br><span class="line">    <span class="keyword">return</span> DenseNet(init_channels=<span class="number">64</span>, growth_rate=<span class="number">32</span>, blocks=[<span class="number">6</span>, <span class="number">12</span>, <span class="number">48</span>, <span class="number">32</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">DenseNet264</span>():</span><br><span class="line">    <span class="keyword">return</span> DenseNet(init_channels=<span class="number">64</span>, growth_rate=<span class="number">32</span>, blocks=[<span class="number">6</span>, <span class="number">12</span>, <span class="number">64</span>, <span class="number">48</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># model = torchvision.models.densenet121()</span></span><br><span class="line">    model = DenseNet121().to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">    <span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>).to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line">    summary(model,<span class="built_in">input</span>.shape)</span><br><span class="line"></span><br><span class="line">    out = model(<span class="built_in">input</span>)</span><br><span class="line">    <span class="built_in">print</span>(out.shape)</span><br></pre></td></tr></table></figure>
<h3 id="创新点-9">3 创新点</h3>
<ul>
<li><p><strong>密集连接（Dense Connectivity）：</strong>
DenseNet的核心特征是其密集连接模式，每一层都与前面所有层直接相连。这意味着第<code>l</code>层的输入不仅是第<code>l-1</code>层的输出，而且还包括所有之前层的输出。这种密集连接的设计促进了特征的重用，减少了参数数量，并改善了梯度的反向传播。</p></li>
<li><p><strong>特征复用（Feature Reuse）：</strong>
由于每一层都可以直接访问前面所有层的特征图，因此在网络内部实现了高效的特征复用。这降低了冗余信息，并减少了模型所需的学习参数数量。</p></li>
<li><p><strong>改进的信息流（Improved Information Flow）：</strong>
密集连接保证了信息在网络中的流动更加顺畅。这有助于解决深层网络中的梯度消失问题，使网络训练变得更加稳定和高效。</p></li>
<li><p><strong>参数高效：</strong>
尽管DenseNet拥有广泛的连接，但其模型参数相对较少。这得益于特征重用和每层仅添加少量特征图的策略，使得DenseNet在保持轻量级的同时仍能保持高性能。</p></li>
<li><p><strong>卓越的泛化能力：</strong>
DenseNet展示了在多个标准数据集上的卓越性能，包括但不限于ImageNet、CIFAR-10和CIFAR-100。它的结构对于图像识别任务表现出了极好的泛化能力。</p></li>
</ul>
<h2 id="十二senet">十二、SENet</h2>
<p>期刊日期：CVPR-2018</p>
<p>论文名称：《Squeeze-and-Excitation Networks》</p>
<h3 id="简介-12">1 简介</h3>
<p>SENet通过引入Squeeze-and-Excitation（SE）块改进了传统卷积网络，这些块通过学习特征通道之间的关系，动态调整通道的特征响应，从而增强了网络对重要特征的捕捉能力。</p>
<h3 id="网络结构-12">2 网络结构</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">__all__=[<span class="string">&#x27;SE_ResNet50&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Conv1</span>(<span class="params">in_planes, places, stride=<span class="number">2</span></span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels=in_planes,out_channels=places,kernel_size=<span class="number">7</span>,stride=stride,padding=<span class="number">3</span>, bias=<span class="literal">False</span>),</span><br><span class="line">        nn.BatchNorm2d(places),</span><br><span class="line">        nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SE_Module</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel,ratio = <span class="number">16</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SE_Module, self).__init__()</span><br><span class="line">        self.squeeze = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.excitation = nn.Sequential(</span><br><span class="line">                nn.Linear(in_features=channel, out_features=channel // ratio),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                nn.Linear(in_features=channel // ratio, out_features=channel),</span><br><span class="line">                nn.Sigmoid()</span><br><span class="line">            )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        b, c, _, _ = x.size()</span><br><span class="line">        y = self.squeeze(x).view(b, c)</span><br><span class="line">        z = self.excitation(y).view(b, c, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x * z.expand_as(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SE_ResNetBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_places,places, stride=<span class="number">1</span>,downsampling=<span class="literal">False</span>, expansion = <span class="number">4</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SE_ResNetBlock,self).__init__()</span><br><span class="line">        self.expansion = expansion</span><br><span class="line">        self.downsampling = downsampling</span><br><span class="line"></span><br><span class="line">        self.bottleneck = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=in_places,out_channels=places,kernel_size=<span class="number">1</span>,stride=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(places),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(in_channels=places, out_channels=places, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(places),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(in_channels=places, out_channels=places*self.expansion, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(places*self.expansion),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsampling:</span><br><span class="line">            self.downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channels=in_places, out_channels=places*self.expansion, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(places*self.expansion)</span><br><span class="line">            )</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x</span><br><span class="line">        out = self.bottleneck(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.downsampling:</span><br><span class="line">            residual = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out += residual</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SE_ResNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,blocks, num_classes=<span class="number">1000</span>, expansion = <span class="number">4</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SE_ResNet,self).__init__()</span><br><span class="line">        self.expansion = expansion</span><br><span class="line"></span><br><span class="line">        self.conv1 = Conv1(in_planes = <span class="number">3</span>, places= <span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">        self.layer1 = self.make_layer(in_places = <span class="number">64</span>, places= <span class="number">64</span>, block=blocks[<span class="number">0</span>], stride=<span class="number">1</span>)</span><br><span class="line">        self.layer2 = self.make_layer(in_places = <span class="number">256</span>,places=<span class="number">128</span>, block=blocks[<span class="number">1</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self.make_layer(in_places=<span class="number">512</span>,places=<span class="number">256</span>, block=blocks[<span class="number">2</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self.make_layer(in_places=<span class="number">1024</span>,places=<span class="number">512</span>, block=blocks[<span class="number">3</span>], stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.avgpool = nn.AvgPool2d(<span class="number">7</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">2048</span>,num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">make_layer</span>(<span class="params">self, in_places, places, block, stride</span>):</span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(SE_ResNetBlock(in_places, places,stride, downsampling =<span class="literal">True</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, block):</span><br><span class="line">            layers.append(SE_ResNetBlock(places*self.expansion, places))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line"></span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">SE_ResNet50</span>():</span><br><span class="line">    <span class="keyword">return</span> SE_ResNet([<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = SE_ResNet50().to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">    <span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line">    summary(model,<span class="built_in">input</span>.shape)</span><br></pre></td></tr></table></figure>
<h3 id="创新点-10">3 创新点</h3>
<ul>
<li><p><strong>特征自适应重标定：</strong>
SENet通过SE块对卷积层的特征通道进行动态重标定，增强了模型的表示能力。这种重标定使网络能够强化有用的特征并抑制不太重要的特征。</p></li>
<li><p><strong>压缩（Squeeze）操作：</strong>
在每个SE块中，全局平均池化被用于压缩空间维度，从而生成每个通道的全局特征描述。这帮助网络捕捉通道之间的全局信息。</p></li>
<li><p><strong>激励（Excitation）操作：</strong>
压缩后的特征通过两层全连接网络（其中间层使用ReLU激活函数，最后一层使用sigmoid激活函数），以学习通道间的非线性关系和特征的重要性。</p></li>
<li><p><strong>重标定特征响应：</strong>
通过乘以学习到的重标定权重，SE块输出一个调整过的特征图，这些特征图在随后被送入网络的下一层。</p></li>
<li><p><strong>模块化和兼容性：</strong>
SE块可以轻松地集成到各种现有的卷积网络架构中（如ResNet、Inception等），增强它们的学习能力而几乎不影响网络的复杂度。</p></li>
</ul>
<h2 id="十三-mobilenetv1">十三 MobileNetV1</h2>
<p><strong>期刊日期：</strong>CVPR-2017</p>
<p><strong>论文名称：</strong>《MobileNets: Efficient Convolutional
Neural Networks for Mobile Vision Applications》</p>
<h3 id="简介-13">1 简介</h3>
<p>MobileNetV1是一种轻量级深度神经网络，采用<strong>深度可分离卷积</strong>优化模型尺寸和计算效率，实现快速、高效的移动和嵌入式设备视觉任务处理。</p>
<h3 id="网络结构-13">2 网络结构</h3>
<p>明白bottleneck的结构即可通透。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/m1-t1.png" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">BottleneckV1</span>(<span class="params">in_channels, out_channels, stride</span>):</span><br><span class="line">  <span class="keyword">return</span>  nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_channels=in_channels,out_channels=in_channels,kernel_size=<span class="number">3</span>,stride=stride,padding=<span class="number">1</span>,groups=in_channels),</span><br><span class="line">        nn.BatchNorm2d(in_channels),</span><br><span class="line">        nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line">        nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>),</span><br><span class="line">        nn.BatchNorm2d(out_channels),</span><br><span class="line">        nn.ReLU6(inplace=<span class="literal">True</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MobileNetV1</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MobileNetV1, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.first_conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">32</span>,kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">32</span>),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.bottleneck = nn.Sequential(</span><br><span class="line">            BottleneckV1(<span class="number">32</span>, <span class="number">64</span>, stride=<span class="number">1</span>),</span><br><span class="line">            BottleneckV1(<span class="number">64</span>, <span class="number">128</span>, stride=<span class="number">2</span>),</span><br><span class="line">            BottleneckV1(<span class="number">128</span>, <span class="number">128</span>, stride=<span class="number">1</span>),</span><br><span class="line">            BottleneckV1(<span class="number">128</span>, <span class="number">256</span>, stride=<span class="number">2</span>),</span><br><span class="line">            BottleneckV1(<span class="number">256</span>, <span class="number">256</span>, stride=<span class="number">1</span>),</span><br><span class="line">            BottleneckV1(<span class="number">256</span>, <span class="number">512</span>, stride=<span class="number">2</span>),</span><br><span class="line">            BottleneckV1(<span class="number">512</span>, <span class="number">512</span>, stride=<span class="number">1</span>),</span><br><span class="line">            BottleneckV1(<span class="number">512</span>, <span class="number">512</span>, stride=<span class="number">1</span>),</span><br><span class="line">            BottleneckV1(<span class="number">512</span>, <span class="number">512</span>, stride=<span class="number">1</span>),</span><br><span class="line">            BottleneckV1(<span class="number">512</span>, <span class="number">512</span>, stride=<span class="number">1</span>),</span><br><span class="line">            BottleneckV1(<span class="number">512</span>, <span class="number">512</span>, stride=<span class="number">1</span>),</span><br><span class="line">            BottleneckV1(<span class="number">512</span>, <span class="number">1024</span>, stride=<span class="number">2</span>),</span><br><span class="line">            BottleneckV1(<span class="number">1024</span>, <span class="number">1024</span>, stride=<span class="number">1</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.avg_pool = nn.AvgPool2d(kernel_size=<span class="number">7</span>,stride=<span class="number">1</span>)</span><br><span class="line">        self.linear = nn.Linear(in_features=<span class="number">1024</span>,out_features=num_classes)</span><br><span class="line">        self.dropout = nn.Dropout(p=<span class="number">0.2</span>)</span><br><span class="line">        <span class="comment"># self.softmax = nn.Softmax(dim=1)</span></span><br><span class="line"></span><br><span class="line">        self.init_params()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_params</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight)</span><br><span class="line">                nn.init.constant_(m.bias,<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear) <span class="keyword">or</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.first_conv(x)</span><br><span class="line">        x = self.bottleneck(x)</span><br><span class="line">        x = self.avg_pool(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>),-<span class="number">1</span>)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="comment"># out = self.softmax(x)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = MobileNetV1().to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">    <span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line">    summary(model,<span class="built_in">input</span>.shape)</span><br></pre></td></tr></table></figure>
<h3 id="创新点-11">3 创新点</h3>
<ul>
<li>深度可分离卷积技术：DW+PW</li>
<li>轻量模型的两个参数</li>
</ul>
<h2 id="十四-mobilenetv2">十四 MobileNetV2</h2>
<p><strong>期刊日期：</strong>CVPR-2018</p>
<p><strong>论文名称：</strong>《MobileNetV2: Inverted Residuals and
Linear Bottlenecks》</p>
<h3 id="简介-14">1 简介</h3>
<p>MobileNetV2是一种轻量级深度神经网络架构，专为移动和嵌入式设备上的高效视觉识别而设计。</p>
<h3 id="网络结构-14">2 网络结构</h3>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/m2-t2.png" /></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Conv3x3BNReLU</span>(<span class="params">in_channels,out_channels,stride,groups</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, groups=groups),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Conv1x1BNReLU</span>(<span class="params">in_channels,out_channels</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU6(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Conv1x1BN</span>(<span class="params">in_channels,out_channels</span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InvertedResidual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, out_channels, stride, expansion_factor=<span class="number">6</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InvertedResidual, self).__init__()</span><br><span class="line">        self.stride = stride</span><br><span class="line">        mid_channels = (in_channels * expansion_factor)</span><br><span class="line"></span><br><span class="line">        self.bottleneck = nn.Sequential(</span><br><span class="line">            Conv1x1BNReLU(in_channels, mid_channels),</span><br><span class="line">            Conv3x3BNReLU(mid_channels, mid_channels, stride,groups=mid_channels),</span><br><span class="line">            Conv1x1BN(mid_channels, out_channels)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.stride == <span class="number">1</span>:</span><br><span class="line">            self.shortcut = Conv1x1BN(in_channels, out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        out = self.bottleneck(x)</span><br><span class="line">        out = (out+self.shortcut(x)) <span class="keyword">if</span> self.stride==<span class="number">1</span> <span class="keyword">else</span> out</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MobileNetV2</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MobileNetV2,self).__init__()</span><br><span class="line"></span><br><span class="line">        self.first_conv = Conv3x3BNReLU(<span class="number">3</span>,<span class="number">32</span>,<span class="number">2</span>,groups=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.layer1 = self.make_layer(in_channels=<span class="number">32</span>, out_channels=<span class="number">16</span>, stride=<span class="number">1</span>, block_num=<span class="number">1</span>)</span><br><span class="line">        self.layer2 = self.make_layer(in_channels=<span class="number">16</span>, out_channels=<span class="number">24</span>, stride=<span class="number">2</span>, block_num=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self.make_layer(in_channels=<span class="number">24</span>, out_channels=<span class="number">32</span>, stride=<span class="number">2</span>, block_num=<span class="number">3</span>)</span><br><span class="line">        self.layer4 = self.make_layer(in_channels=<span class="number">32</span>, out_channels=<span class="number">64</span>, stride=<span class="number">2</span>, block_num=<span class="number">4</span>)</span><br><span class="line">        self.layer5 = self.make_layer(in_channels=<span class="number">64</span>, out_channels=<span class="number">96</span>, stride=<span class="number">1</span>, block_num=<span class="number">3</span>)</span><br><span class="line">        self.layer6 = self.make_layer(in_channels=<span class="number">96</span>, out_channels=<span class="number">160</span>, stride=<span class="number">2</span>, block_num=<span class="number">3</span>)</span><br><span class="line">        self.layer7 = self.make_layer(in_channels=<span class="number">160</span>, out_channels=<span class="number">320</span>, stride=<span class="number">1</span>, block_num=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.last_conv = Conv1x1BNReLU(<span class="number">320</span>,<span class="number">1280</span>)</span><br><span class="line">        self.avgpool = nn.AvgPool2d(kernel_size=<span class="number">7</span>,stride=<span class="number">1</span>)</span><br><span class="line">        self.dropout = nn.Dropout(p=<span class="number">0.2</span>)</span><br><span class="line">        self.linear = nn.Linear(in_features=<span class="number">1280</span>,out_features=num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">make_layer</span>(<span class="params">self, in_channels, out_channels, stride, block_num</span>):</span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(InvertedResidual(in_channels, out_channels, stride))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, block_num):</span><br><span class="line">            layers.append(InvertedResidual(out_channels,out_channels,<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">init_params</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear) <span class="keyword">or</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.first_conv(x)</span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line">        x = self.layer5(x)</span><br><span class="line">        x = self.layer6(x)</span><br><span class="line">        x = self.layer7(x)</span><br><span class="line">        x = self.last_conv(x)</span><br><span class="line">        x = self.avgpool(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>),-<span class="number">1</span>)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        out = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = MobileNetV2()</span><br><span class="line">    <span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">    model1 = torchvision.models.MobileNetV2()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line">    summary(model,<span class="built_in">input</span>.shape)</span><br></pre></td></tr></table></figure>
<h3 id="创新点-12">3 创新点</h3>
<ul>
<li>采用<strong>倒置残差结构</strong>以减少计算量</li>
<li><strong>线性瓶颈层</strong>以优化表示能力</li>
<li>以及<strong>ReLU6激活函数</strong>以提高低精度计算的鲁棒性。</li>
</ul>
<h2 id="十五-fcn">十五 FCN</h2>
<p>期刊日期：</p>
<p>论文名称：</p>
<h3 id="简介-15">1 简介</h3>
<h3 id="网络结构-15">2 网络结构</h3>
<h3 id="创新点-13">3创新点</h3>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://www.chitose.cn">Chitose</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://www.chitose.cn/summary-base/">https://www.chitose.cn/summary-base/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.chitose.cn" target="_blank">Chitose-Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_1.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/segmentation-introduction/" title="语义分割篇章前言（数据集、评价指标）"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_3.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">语义分割篇章前言（数据集、评价指标）</div></div></a></div><div class="next-post pull-right"><a href="/sever/" title="sever"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_9.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">sever</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Face.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Chitose</div><div class="author-info__description">Hahaha</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">91</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/chitose-r"><i class="fab fa-github"></i><span>🛴/前往小家..</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/chitose-r" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:171450290@qq.com" target="_blank" title="Email"><i class="fa-solid fa-square-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="/qq/" target="_blank" title="QQ"><i class="fab fa-qq" style="color: #12b7f5;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%B6-lenet"><span class="toc-text">零 LeNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80-alexnet"><span class="toc-text">一 AlexNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B-1"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-1"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9-1"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C-zfnet"><span class="toc-text">二 ZFNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B-2"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-2"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9-2"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89-vgg"><span class="toc-text">三 VGG</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B-3"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-3"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9-3"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B-nin"><span class="toc-text">四 NiN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B-4"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-4"><span class="toc-text">2 网络结构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94-googlenet-inceptionv1"><span class="toc-text">五 GoogLeNet InceptionV1</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B-5"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-5"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9-4"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD-googlenet-inceptionv2"><span class="toc-text">六 GoogLeNet InceptionV2</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B-6"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-6"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9-5"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83-googlenet-inceptionv3"><span class="toc-text">七 GoogLeNet InceptionV3</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B-7"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-7"><span class="toc-text">2 网络结构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E5%9F%BA%E6%9C%AC%E5%8D%B7%E7%A7%AF%E6%A8%A1%E5%9D%97"><span class="toc-text">2.1 定义基本卷积模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89inceptiona%E6%A8%A1%E5%9D%97"><span class="toc-text">2.2 定义InceptionA模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89inceptionb%E6%A8%A1%E5%9D%97"><span class="toc-text">2.2 定义InceptionB模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89inceptionc%E6%A8%A1%E5%9D%97"><span class="toc-text">2.3 定义InceptionC模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89inceptiond%E6%A8%A1%E5%9D%97"><span class="toc-text">2.4 定义InceptionD模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89inceptione%E6%A8%A1%E5%9D%97"><span class="toc-text">2.5 定义InceptionE模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E4%BB%A3%E7%A0%81"><span class="toc-text">2.6 总代码</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9-6"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AB-resnet"><span class="toc-text">八 ResNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B-8"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-8"><span class="toc-text">2 网络结构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B9%9D-resnext"><span class="toc-text">九 ResNext</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B-9"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-9"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9-7"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%81-xception"><span class="toc-text">十 Xception</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B-10"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-10"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9-8"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%81%E4%B8%80-densenet"><span class="toc-text">十一 DenseNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B-11"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-11"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9-9"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%81%E4%BA%8Csenet"><span class="toc-text">十二、SENet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B-12"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-12"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9-10"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%81%E4%B8%89-mobilenetv1"><span class="toc-text">十三 MobileNetV1</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B-13"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-13"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9-11"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%81%E5%9B%9B-mobilenetv2"><span class="toc-text">十四 MobileNetV2</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B-14"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-14"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9-12"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%81%E4%BA%94-fcn"><span class="toc-text">十五 FCN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B-15"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-15"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E6%96%B0%E7%82%B9-13"><span class="toc-text">3创新点</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By Chitose</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"></div><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script defer src="/js/cursor.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Python/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🥩 Python (30)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/C/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🕶️ C (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Embedded/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">💳 Embedded (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Pytorch/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📯 Pytorch (9)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Paper/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📰 Paper (28)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/others/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🤡 others (16)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/segmentation/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🔪 segmentation (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item" style="visibility: hidden"></div><div class="magnet_item" style="visibility: hidden"></div><a class="magnet_link_more"  href="https://www.chitose.cn/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(33.333333333333336% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v6.2.0" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly_v4.3.1" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" data-title="本站采用多线部署，主线路托管于Vercel" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-Vercel-brightgreen?style=flat&amp;logo=Vercel" alt=""/></a><a class="github-badge" target="_blank" href="https://dashboard.4everland.org/" style="margin-inline:5px" data-title="本站采用多线部署，备用线路托管于4EVERLAND" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-4EVERLAND-22DDDD?style=flat&amp;logo=IPFS" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="/js/runtime.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023-12-17-2/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_10.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023-12-17-2/" alt="">第二篇文章</a><div class="blog-slider__text">这是第二篇文章</div><a class="blog-slider__button" href="2023-12-17-2/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Pytorch-6/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_3.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-19</span><a class="blog-slider__title" href="Pytorch-6/" alt="">Pytorch(6)-张量可微性</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="Pytorch-6/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023-12-17-3/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_8.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023-12-17-3/" alt="">第三篇文章</a><div class="blog-slider__text">这是第三篇文章</div><a class="blog-slider__button" href="2023-12-17-3/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><script data-pjax>
  function butterfly_clock_anzhiyu_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock_anzhiyu')
    if(parent_div_git) {
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var qweather_key = '6be604177b8a4c3e97c78a352ee324f7';
  var gaud_map_key = '17b299fafade134736e6a1d4acb5ef18';
  var baidu_ak_key = 'undefined';
  var flag = 0;
  var clock_rectangle = '113.34532,23.15624';
  var clock_default_rectangle_enable = 'false';

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_anzhiyu_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_anzhiyu_injector_config();
  }
  </script><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script data-pjax src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body></html>