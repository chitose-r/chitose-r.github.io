<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>图像分类经典网络汇总 | Chitose-Blog</title><meta name="author" content="Chitose"><meta name="copyright" content="Chitose"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="前言 在CNN网络结构的演化上，CNN的经典结构始于1998年的LeNet，成于2012年历史性的AlexNet，从此盛于图像相关领域。 发展历史：Lenet --&gt; Alexnet --&gt; ZFnet --&gt; VGG --&gt; NIN --&gt; GoogLeNet --&gt;ResNet–&gt; DenseNet --&gt;ResNeXt —&gt; Effici">
<meta property="og:type" content="article">
<meta property="og:title" content="图像分类经典网络汇总">
<meta property="og:url" content="https://www.chitose.cn/summary-base/index.html">
<meta property="og:site_name" content="Chitose-Blog">
<meta property="og:description" content="前言 在CNN网络结构的演化上，CNN的经典结构始于1998年的LeNet，成于2012年历史性的AlexNet，从此盛于图像相关领域。 发展历史：Lenet --&gt; Alexnet --&gt; ZFnet --&gt; VGG --&gt; NIN --&gt; GoogLeNet --&gt;ResNet–&gt; DenseNet --&gt;ResNeXt —&gt; Effici">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_6.png">
<meta property="article:published_time" content="2024-02-18T14:41:28.000Z">
<meta property="article:modified_time" content="2024-02-18T14:41:28.000Z">
<meta property="article:author" content="Chitose">
<meta property="article:tag" content="演示">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_6.png"><link rel="shortcut icon" href="https://www.fomal.cc/favicon.ico"><link rel="canonical" href="https://www.chitose.cn/summary-base/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体中文","cht_to_chs":"你已切换为简体中文","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '图像分类经典网络汇总',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-02-18 22:41:28'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.css" /><link rel="stylesheet" href="https://www.fomal.cc/static/css/runtime.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Face.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">87</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/todolist/"><i class="fa-fw fas fa-link"></i><span> 计划</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_6.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Chitose-Blog"><span class="site-name">Chitose-Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/todolist/"><i class="fa-fw fas fa-link"></i><span> 计划</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">图像分类经典网络汇总</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-02-18T14:41:28.000Z" title="发表于 2024-02-18 22:41:28">2024-02-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-02-18T14:41:28.000Z" title="更新于 2024-02-18 22:41:28">2024-02-18</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/others/">others</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>38分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="图像分类经典网络汇总"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="前言">前言</h2>
<p>在<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=CNN&amp;spm=1001.2101.3001.7020">CNN</a>网络结构的演化上，CNN的经典结构始于<strong>1998年的LeNet</strong>，成于<strong>2012年历史性的AlexNet</strong>，从此<strong>盛于图像相关领域</strong>。</p>
<p>发展历史：<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=Lenet&amp;spm=1001.2101.3001.7020">Lenet</a> --&gt; Alexnet --&gt; ZFnet --&gt; VGG --&gt; NIN --&gt; GoogLeNet --&gt;ResNet–&gt; DenseNet --&gt;ResNeXt —&gt; EfficientNet</p>
<table>
<thead>
<tr>
<th><strong>神经网络</strong></th>
<th>年份</th>
<th><strong>标签</strong></th>
<th><strong>作者</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>LeNets-5</strong></td>
<td>1998年</td>
<td>CNN开山之作</td>
<td>纽约大学</td>
</tr>
<tr>
<td><strong>AlexNet-8</strong></td>
<td>2012年</td>
<td>深度学习CV领域划时代论文<br/>具有里程碑意义<br/>ImageNet 2012冠军</td>
<td>多伦多大学　Hinton团队</td>
</tr>
<tr>
<td><strong>ZFNet</strong></td>
<td>2013年</td>
<td>ImageNet 2013冠军<br/>结构与AlexNet类似</td>
<td>纽约大学</td>
</tr>
<tr>
<td><strong>NiN</strong></td>
<td>2014年</td>
<td>Network in Network（网中网）<br/>MLPConv（多层感知机卷积层）<br/>GAP（全局平均池化）</td>
<td>新加坡国立大学</td>
</tr>
<tr>
<td><strong>VGG-16</strong></td>
<td>2014年</td>
<td>开启3*3卷积堆叠时代<br>ImageNet 2014亚军<br>VGG-16和VGG-19</td>
<td>牛津大学</td>
</tr>
<tr>
<td><strong>InceptionV1</strong></td>
<td>2014.09</td>
<td>Google系列论文开创论文<br>ImageNet 2014冠军<br>Inception模块</td>
<td>谷歌</td>
</tr>
<tr>
<td><strong>InceptionV2</strong></td>
<td>2015.02</td>
<td>Google系列论文<br/>大卷积由小卷积组成<br/>提出BN</td>
<td>谷歌</td>
</tr>
<tr>
<td><strong>InceptionV3</strong></td>
<td>2015.12</td>
<td>Google系列论文<br/>非对称卷积分解<br/>池化+卷积结合</td>
<td>谷歌</td>
</tr>
<tr>
<td><strong>ResNet</strong></td>
<td>2015年</td>
<td>最具影响力的卷积神经网络<br>ImageNet 2015冠军<br>残差网络</td>
<td>何凯明团队　微软亚院</td>
</tr>
<tr>
<td><strong>Inception v4</strong></td>
<td>2016.02</td>
<td>解决梯度消散<br/>Inception-ResNet</td>
<td>谷歌</td>
</tr>
<tr>
<td><strong>Wide ResNet</strong></td>
<td>2016年</td>
<td>增加残差中卷积核数量（宽度）</td>
<td>谢尔盖·扎戈鲁科</td>
</tr>
<tr>
<td><strong>Squeeze Net</strong></td>
<td>2016年</td>
<td>轻量级网络<br>压缩参数量</td>
<td>斯坦福大学　伯克利大学</td>
</tr>
<tr>
<td><strong>DenseNet</strong></td>
<td>2017年</td>
<td>ImageNet 2016冠军<br>CVPR 2017最佳论文<br>Dense模块</td>
<td>康奈尔大学　清华大学</td>
</tr>
<tr>
<td><strong>SENet</strong></td>
<td>2017年</td>
<td>ImageNet 2017冠军<br>SE模块</td>
<td>momenta + 牛津大学　胡杰</td>
</tr>
<tr>
<td><strong>ResNext</strong></td>
<td>2017年</td>
<td>何恺明团队对ResNet重大改进<br>Resnet + Inception</td>
<td>Saining Xie团队</td>
</tr>
<tr>
<td><strong>FractalNet</strong></td>
<td>2017年</td>
<td>分形网络</td>
<td>Gustav Larsson团队</td>
</tr>
<tr>
<td><strong>MobileNets</strong></td>
<td>2017年</td>
<td>轻量级<br>Group卷积<br>Depthwise Seperable卷积</td>
<td>谷歌</td>
</tr>
<tr>
<td><strong>NASNet</strong></td>
<td>2018年</td>
<td>神经架构搜索　强化学习</td>
<td>谷歌</td>
</tr>
</tbody>
</table>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/base-summary.png" alt=""></p>
<h2 id="零-LeNet">零 LeNet</h2>
<p><strong>期刊日期：</strong> IEEE</p>
<p><strong>论文名称：</strong>《Gradient-Based Learning Applied to Document Recognition》</p>
<h3 id="1-简介">1 简介</h3>
<p>LeNet，CNN的开山之作。</p>
<h3 id="2-网络结构">2 网络结构</h3>
<p>网络共有<strong>五</strong>层</p>
<p>网络结构为：<strong>卷积-&gt;池化-&gt;卷积-&gt;池化-&gt;全连接x3</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyLeNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyLeNet, self).__init__()</span><br><span class="line">        self.features_ = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">6</span>,kernel_size=<span class="number">5</span>,padding=<span class="number">2</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(in_channels=<span class="number">6</span>,out_channels=<span class="number">16</span>,kernel_size=<span class="number">5</span>,padding=<span class="number">2</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">            nn.Flatten()</span><br><span class="line">        )</span><br><span class="line">        self.cls_ = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">56</span>*<span class="number">56</span>, <span class="number">120</span>),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.Linear(<span class="number">84</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.features_(x)</span><br><span class="line">        x = self.cls_(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="3-创新点">3 创新点</h3>
<ul>
<li>定义了<strong>CNN基本框架</strong>：卷积层、池化层、全连接层</li>
<li>定义了<strong>卷积层</strong>（局部连接、权值共享）</li>
<li>用<strong>Tanh</strong>作为非线性激活函数</li>
</ul>
<h2 id="一-AlexNet">一 AlexNet</h2>
<p><strong>期刊日期：</strong> NIPS-2012</p>
<p><strong>论文名称：</strong>《ImageNet Classification with Deep Convolutional Neural Networks》</p>
<h3 id="1-简介-2">1 简介</h3>
<p>2012年Geoffrey和他学生Alex在ImageNet的竞赛中，刷新了image classification的记录，一举夺得冠军，自此打开了深度学习的大门，也使得深度学习成为学术界的新宠。这次竞赛中Alex所用的结构就被称为作为AlexNet。</p>
<h3 id="2-网络结构-2">2 网络结构</h3>
<p>网络共有<strong>八</strong>层</p>
<p>结构为：<strong>卷积-&gt;池化-&gt;卷积-&gt;池化-&gt;卷积x3-&gt;池化-&gt;全连接x3</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyAlexNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyAlexNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">2</span>)  <span class="comment"># [None, 3, 224, 224] --&gt; [None, 96, 55, 55]</span></span><br><span class="line">        self.pool1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>)  <span class="comment"># [None, 96, 55, 55] --&gt; [None, 96, 27, 27]</span></span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=<span class="number">96</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)  <span class="comment"># [None, 96, 27, 27] --&gt; [None, 256, 27, 27]</span></span><br><span class="line">        self.pool2 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)  <span class="comment"># [None, 256, 27, 27] --&gt; [None, 256, 13, 13]</span></span><br><span class="line">        self.conv3 = nn.Conv2d(in_channels=<span class="number">256</span>, out_channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)  <span class="comment"># [None, 256, 13, 13] --&gt; [None, 384, 13, 13]</span></span><br><span class="line">        self.conv4 = nn.Conv2d(in_channels=<span class="number">384</span>, out_channels=<span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)  <span class="comment"># [None, 384, 13, 13] --&gt; [None, 384, 13, 13]</span></span><br><span class="line">        self.conv5 = nn.Conv2d(in_channels=<span class="number">384</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)  <span class="comment"># [None, 384, 13, 13] --&gt; [None, 256, 13, 13]</span></span><br><span class="line">        self.pool3 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)  <span class="comment"># [None, 256, 13, 13] --&gt; [None, 256, 6, 6]</span></span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">256</span> * <span class="number">6</span> * <span class="number">6</span>, <span class="number">2048</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">2048</span>, <span class="number">2048</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">2048</span>, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = self.pool1(x)</span><br><span class="line"></span><br><span class="line">        x = F.relu(self.conv2(x))</span><br><span class="line">        x = self.pool2(x)</span><br><span class="line"></span><br><span class="line">        x = F.relu(self.conv3(x))</span><br><span class="line">        x = F.relu(self.conv4(x))</span><br><span class="line">        x = F.relu(self.conv5(x))</span><br><span class="line">        x = self.pool3(x)</span><br><span class="line"></span><br><span class="line">        x = x.view(-<span class="number">1</span>,<span class="number">256</span>*<span class="number">6</span>*<span class="number">6</span>)</span><br><span class="line">        x = F.relu(F.dropout(self.fc1(x), <span class="number">0.5</span>))</span><br><span class="line">        x = F.relu(F.dropout(self.fc2(x), <span class="number">0.5</span>))</span><br><span class="line">        x = F.relu(F.dropout(self.fc3(x), <span class="number">0.5</span>))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x = torch.rand([<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>])</span><br><span class="line">    model = MyAlexNet()</span><br><span class="line">    summary(model, input_size=(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))</span><br></pre></td></tr></table></figure>
<h3 id="3-创新点-2">3 创新点</h3>
<ul>
<li>使用<strong>非线性激活函数ReLU</strong>加快了训练速度：AlexNet使用ReLU代替了Sigmoid,其能更快的训练,同时解决sigmoid在训练较深的网络中出现的梯度消失。</li>
<li>使用<strong>多个GPU</strong>并行训练：加速了计算。</li>
<li><strong>LRN局部归一化</strong>：增加了泛化能力，做了平滑处理，提高了1%~2%的识别率, LRN 对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大（使得重要位置更突出），并抑制其他反馈较小的神经元。</li>
<li><strong>重叠池化</strong>：以前的CNN中普遍使用平均池化层，AlexNet全部使用最大池化层,避免了平均池化层的模糊化的效果,并且步长比池化的核的尺寸小,这样池化层的输出之间有重叠,提升了特征的丰富性。</li>
<li><strong>Dropout：</strong> 避免过拟合，Dropout随机失活，随机忽略一些神经元,以避免过拟合。</li>
<li><strong>数据增广：</strong> 来增加模型泛化能力 256×256×3 --&gt;随机裁剪224×224×3 --&gt;进入网络。</li>
</ul>
<h2 id="二-VGG">二 VGG</h2>
<p><strong>期刊日期：</strong> ICLR-2015</p>
<p><strong>论文名称：</strong>《Very Deep Convolutional Networks for Large-Scale Image Recognition》</p>
<h3 id="1-简介-3">1 简介</h3>
<p>VGG-Net来自 Andrew Zisserman 教授的组 (Oxford)，在AlexNet基础上提出了更深的网络，分别为VGG-16和VGG-19。在2014年的 ILSVRC 图像定位和分类两个问题上分别取得了第一名和第二名。相比于AlexNet，AlexNet只有8层，而VGG有16～19层；AlexNet使用了11x11的卷积核，VGG使用了3x3卷积核和2x2的最大池化层，VGG参数是AlexNet的三倍。为后面的框架提供了方向：加深网络的深度。</p>
<h3 id="2-网络结构-3">2 网络结构</h3>
<p>VGG经典网络共有<strong>16/19</strong>层</p>
<p>16层网络结构为：<strong>（卷积x2-&gt;池化）-&gt;（卷积x2-&gt;池化）-&gt;（卷积x3-&gt;池化）-&gt;（卷积x3-&gt;池化）-&gt;（卷积x3-&gt;池化）-&gt;全连接x3</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VGG16</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block1</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>,<span class="number">64</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">64</span>,<span class="number">64</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.pool1 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block2</span></span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">64</span>,<span class="number">128</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.conv4 = nn.Conv2d(<span class="number">128</span>,<span class="number">128</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.pool2 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block3</span></span><br><span class="line">        self.conv5 = nn.Conv2d(<span class="number">128</span>,<span class="number">256</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.conv6 = nn.Conv2d(<span class="number">256</span>,<span class="number">256</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.conv7 = nn.Conv2d(<span class="number">256</span>,<span class="number">256</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.pool3 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block4</span></span><br><span class="line">        self.conv8 = nn.Conv2d(<span class="number">256</span>,<span class="number">512</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.conv9 = nn.Conv2d(<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.conv10 = nn.Conv2d(<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.pool4 = nn.MaxPool2d(<span class="number">2</span>)            </span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block5</span></span><br><span class="line">        self.conv11 = nn.Conv2d(<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.conv12 = nn.Conv2d(<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.conv13 = nn.Conv2d(<span class="number">512</span>,<span class="number">512</span>,<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.pool5 = nn.MaxPool2d(<span class="number">2</span>)               </span><br><span class="line">        </span><br><span class="line">        <span class="comment">#FC层</span></span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">512</span>*<span class="number">7</span>*<span class="number">7</span>,<span class="number">4096</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">4096</span>,<span class="number">4096</span>)</span><br><span class="line">        self.linear3 = nn.Linear(<span class="number">4096</span>,<span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = self.pool1(F.relu(self.conv2(x)))</span><br><span class="line">        x = F.relu(self.conv3(x))</span><br><span class="line">        x = self.pool2(F.relu(self.conv4(x)))</span><br><span class="line">        x = F.relu(self.conv5(x))</span><br><span class="line">        x = F.relu(self.conv6(x))</span><br><span class="line">        x = self.pool3(F.relu(self.conv7(x)))</span><br><span class="line">        x = F.relu(self.conv8(x))</span><br><span class="line">        x = F.relu(self.conv9(x))</span><br><span class="line">        x = self.pool4(F.relu(self.conv10(x)))</span><br><span class="line">        x = F.relu(self.conv11(x))</span><br><span class="line">        x = F.relu(self.conv12(x))</span><br><span class="line">        x = self.pool5(F.relu(self.conv13(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">512</span>*<span class="number">7</span>*<span class="number">7</span>)</span><br><span class="line">        </span><br><span class="line">        x = F.relu(F.dropout(self.linear1(x),<span class="number">0.5</span>))</span><br><span class="line">        x = F.relu(F.dropout(self.linear2(x),<span class="number">0.5</span>))</span><br><span class="line">        output = F.softmax(self.linear3(x),dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h3 id="3-创新点-3">3 创新点</h3>
<ul>
<li>
<p><strong>用三个3×3的卷积核代替7×7的卷积核</strong>，有的FC层还用到了1×1的卷积核以及2×2的池化层。网络更深，增加了CNN对特征的学习能力。</p>
</li>
<li>
<p><strong>在更深的结构中没有用到LRN（推翻了Alex）</strong>，避免了部分内存和计算的增加。</p>
</li>
<li>
<p><strong>VGG采用的是一种Pre-training的方式</strong>，先训练级别简单（层数较浅）的VGGNet的A级网络，然后使用A网络的权重来初始化后面的复杂模型，加快训练的收敛速度。</p>
</li>
<li>
<p><strong>采用了Multi-Scale的方法来训练和预测</strong>。可以增加训练的数据量，防止模型过拟合，提升预测准确率 。</p>
</li>
</ul>
<h2 id="三-NiN">三 NiN</h2>
<p><strong>期刊日期：</strong> arXiv 2014.09</p>
<p><strong>论文名称：</strong> 《Network in Network》</p>
<h3 id="1-简介-4">1 简介</h3>
<p>由多个堆叠的卷积层和空间池化层组成，通过线性的卷积核+非线性激活函数（ReLU、sigmoid、tanh等）生成特征图。</p>
<h3 id="2-网络结构-4">2 网络结构</h3>
<p>（卷积+1x1卷积+1x1卷积）x3+GAP</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNiN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNiN, self).__init__()</span><br><span class="line">        self.features_ = nn.Sequential(</span><br><span class="line">            self.nin_block(<span class="number">3</span>, <span class="number">96</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            self.nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            self.nin_block(<span class="number">256</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>), <span class="comment"># 标签类别数是10</span></span><br><span class="line">            </span><br><span class="line">            nn.AdaptiveAvgPool2d(<span class="number">1</span>),</span><br><span class="line">            nn.Flatten()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">nin_block</span>(<span class="params">self, in_channels, out_channels, kernel_size, stride, padding</span>):</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.features_(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型实例</span></span><br><span class="line">net = MyNiN()</span><br><span class="line">net.to(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)  <span class="comment"># 把网络移到GPU上，如果有GPU的话</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印模型概要</span></span><br><span class="line">summary(net, input_size=(<span class="number">10</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>))  <span class="comment"># 注意这里的input_size是按照CIFAR-10的图像大小设置的</span></span><br></pre></td></tr></table></figure>
<h3 id="3-创新点-4">3 创新点</h3>
<ul>
<li>提出了<strong>NIN块(即mlpconv层)</strong>，等价于<strong>普通卷积后跟两个1×1conv(即mlp层)</strong>，其中每个卷积操作后面都有ReLU进行非线性激活。
<ul>
<li>保持特征图的尺寸、把各通道的输入特征图进行线性加权，起到综合各个通道特征图信息的作用。</li>
<li>MLPconv其实就是在常规卷积后面加了N层1X1卷积，</li>
</ul>
</li>
<li>提出了<strong>GAP(即GlobalAveragePooling)</strong>，等价于把样本求均值映射到特征，因为最后一层的 feature map 数和类别数相同，那么也就是把样本映射到要分类的类别。</li>
</ul>
<h2 id="四-GoogLeNet-InceptionV1">四 GoogLeNet InceptionV1</h2>
<p><strong>期刊日期：</strong> CVPR-2015</p>
<p><strong>论文名称：</strong>《Going deeper with convolutions》</p>
<h3 id="1-简介-5">1 简介</h3>
<p>GoogLeNet在2014的ImageNet分类任务上击败了VGG夺得冠军，GoogLeNet跟AlexNet,VGG-Nets这种单纯依靠加深网络结构进而改进网络性能的思路不一样，它在加深网络的同时（22层），在同一层中使用了多个不同尺寸的卷积，以获得不同的视野，最后级联（直接叠加通道数量）。引入Inception结构代替了单纯的卷积+激活的传统操作（这思路最早由Network in Network提出）<br>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/incepv1-module.png" alt=""><br>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Googlev1-aux.png" alt=""></p>
<h3 id="2-网络结构-5">2 网络结构</h3>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/googlev1.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicConv2d</span>(nn.Module):</span><br><span class="line">    <span class="comment">#在这里我们要改掉原来会输入参数默认值的习惯，而使用定义类型的方式，同时将**kwargs也放到init中继承</span></span><br><span class="line">    <span class="comment">#**kwargs代表了“所需要的全部参数”，由于现在的架构变得复杂，我们不太可能将每个需要用的参数都写在定义中</span></span><br><span class="line">    <span class="comment">#因此，我们继承**kwargs来获得所需类的全部参数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span>, out_channels: <span class="built_in">int</span>,**kwargs</span>): </span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, bias=<span class="literal">False</span>, **kwargs) </span><br><span class="line">                                  <span class="comment">#同样写上**kwargs</span></span><br><span class="line">                                 ,nn.BatchNorm2d(out_channels)</span><br><span class="line">                                 ,nn.ReLU(inplace=<span class="literal">True</span>))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Inception</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span></span><br><span class="line"><span class="params">                 ,in_channels : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch1x1 : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch3x3red : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch3x3 : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch5x5red : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,ch5x5 : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                 ,pool_proj : <span class="built_in">int</span></span></span><br><span class="line"><span class="params">                </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#1x1</span></span><br><span class="line">        self.branch1 = BasicConv2d(in_channels,ch1x1,kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#1x1 + 3x3</span></span><br><span class="line">        self.branch2 = nn.Sequential(BasicConv2d(in_channels, ch3x3red, kernel_size=<span class="number">1</span>)</span><br><span class="line">                                     ,BasicConv2d(ch3x3red, ch3x3, kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>))</span><br><span class="line">        <span class="comment">#1x1 + 5x5</span></span><br><span class="line">        self.branch3 = nn.Sequential(BasicConv2d(in_channels, ch5x5red, kernel_size=<span class="number">1</span>)</span><br><span class="line">                                     ,BasicConv2d(ch5x5red, ch5x5, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>))</span><br><span class="line">        <span class="comment">#pool + 1x1</span></span><br><span class="line">        self.branch4 = nn.Sequential(nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>, padding=<span class="number">1</span>,ceil_mode=<span class="literal">True</span>)  <span class="comment"># 向上取整</span></span><br><span class="line">                                    ,BasicConv2d(in_channels,pool_proj,kernel_size=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        branch1 = self.branch1(x) <span class="comment">#28x28,ch1x1</span></span><br><span class="line">        branch2 = self.branch2(x) <span class="comment">#28x28,ch3x3</span></span><br><span class="line">        branch3 = self.branch3(x) <span class="comment">#28x28,ch5x5</span></span><br><span class="line">        branch4 = self.branch4(x) <span class="comment">#28x28,pool_proj</span></span><br><span class="line">        outputs = [branch1, branch2, branch3, branch4]</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>) <span class="comment">#合并</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AuxClf</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,in_channels : <span class="built_in">int</span>, num_classes : <span class="built_in">int</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.feature_ = nn.Sequential(nn.AvgPool2d(kernel_size=<span class="number">5</span>,stride=<span class="number">3</span>)</span><br><span class="line">                                     ,BasicConv2d(in_channels,<span class="number">128</span>, kernel_size=<span class="number">1</span>))</span><br><span class="line">        self.clf_ = nn.Sequential(nn.Linear(<span class="number">4</span>*<span class="number">4</span>*<span class="number">128</span>, <span class="number">1024</span>)</span><br><span class="line">                                 ,nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">                                 ,nn.Dropout(<span class="number">0.7</span>)</span><br><span class="line">                                 ,nn.Linear(<span class="number">1024</span>,num_classes))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.feature_(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>,<span class="number">4</span>*<span class="number">4</span>*<span class="number">128</span>)</span><br><span class="line">        x = self.clf_(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyIncV1</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,num_classes: <span class="built_in">int</span> = <span class="number">1000</span>, blocks = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> blocks <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            blocks = [BasicConv2d, Inception, AuxClf]</span><br><span class="line">        conv_block = blocks[<span class="number">0</span>]</span><br><span class="line">        inception_block = blocks[<span class="number">1</span>]</span><br><span class="line">        aux_clf_block = blocks[<span class="number">2</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block1</span></span><br><span class="line">        self.conv1 = conv_block(<span class="number">3</span>,<span class="number">64</span>,kernel_size=<span class="number">7</span>,stride=<span class="number">2</span>,padding = <span class="number">3</span>)</span><br><span class="line">        self.maxpool1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,ceil_mode = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block2</span></span><br><span class="line">        self.conv2 = conv_block(<span class="number">64</span>,<span class="number">64</span>,kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = conv_block(<span class="number">64</span>,<span class="number">192</span>,kernel_size=<span class="number">3</span>, padding = <span class="number">1</span>)</span><br><span class="line">        self.maxpool2 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,ceil_mode = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block3</span></span><br><span class="line">        self.inception3a = inception_block(<span class="number">192</span>,<span class="number">64</span>,<span class="number">96</span>,<span class="number">128</span>,<span class="number">16</span>,<span class="number">32</span>,<span class="number">32</span>)</span><br><span class="line">        self.inception3b = inception_block(<span class="number">256</span>,<span class="number">128</span>,<span class="number">128</span>,<span class="number">192</span>,<span class="number">32</span>,<span class="number">96</span>,<span class="number">64</span>)</span><br><span class="line">        self.maxpool3 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,ceil_mode = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block4 </span></span><br><span class="line">        self.inception4a = inception_block(<span class="number">480</span>,<span class="number">192</span>,<span class="number">96</span>,<span class="number">208</span>,<span class="number">16</span>,<span class="number">48</span>,<span class="number">64</span>)</span><br><span class="line">        self.inception4b = inception_block(<span class="number">512</span>,<span class="number">160</span>,<span class="number">112</span>,<span class="number">224</span>,<span class="number">24</span>,<span class="number">64</span>,<span class="number">64</span>)</span><br><span class="line">        self.inception4c = inception_block(<span class="number">512</span>,<span class="number">128</span>,<span class="number">128</span>,<span class="number">256</span>,<span class="number">24</span>,<span class="number">64</span>,<span class="number">64</span>)</span><br><span class="line">        self.inception4d = inception_block(<span class="number">512</span>,<span class="number">112</span>,<span class="number">144</span>,<span class="number">288</span>,<span class="number">32</span>,<span class="number">64</span>,<span class="number">64</span>)</span><br><span class="line">        self.inception4e = inception_block(<span class="number">528</span>,<span class="number">256</span>,<span class="number">150</span>,<span class="number">320</span>,<span class="number">32</span>,<span class="number">128</span>,<span class="number">128</span>)</span><br><span class="line">        self.maxpool4 = nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,ceil_mode = <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block5</span></span><br><span class="line">        self.inception5a = inception_block(<span class="number">832</span>,<span class="number">256</span>,<span class="number">160</span>,<span class="number">320</span>,<span class="number">32</span>,<span class="number">128</span>,<span class="number">128</span>)</span><br><span class="line">        self.inception5b = inception_block(<span class="number">832</span>,<span class="number">384</span>,<span class="number">192</span>,<span class="number">384</span>,<span class="number">48</span>,<span class="number">128</span>,<span class="number">128</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#clf</span></span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)) <span class="comment">#我需要的输出的特征图尺寸是多少</span></span><br><span class="line">        self.dropout = nn.Dropout(<span class="number">0.4</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">1024</span>,num_classes)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#auxclf</span></span><br><span class="line">        self.aux1 = aux_clf_block(<span class="number">512</span>, num_classes) <span class="comment">#4a</span></span><br><span class="line">        self.aux2 = aux_clf_block(<span class="number">528</span>, num_classes) <span class="comment">#4d</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="comment">#block1</span></span><br><span class="line">        x = self.maxpool1(self.conv1(x))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block2</span></span><br><span class="line">        x = self.maxpool2(self.conv3(self.conv2(x)))</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block3</span></span><br><span class="line">        x = self.inception3a(x)</span><br><span class="line">        x = self.inception3b(x)</span><br><span class="line">        x = self.maxpool3(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block4</span></span><br><span class="line">        x = self.inception4a(x)</span><br><span class="line">        aux1 = self.aux1(x)</span><br><span class="line">        </span><br><span class="line">        x = self.inception4b(x)</span><br><span class="line">        x = self.inception4c(x)</span><br><span class="line">        x = self.inception4d(x)</span><br><span class="line">        aux2 = self.aux2(x)</span><br><span class="line">        </span><br><span class="line">        x = self.inception4e(x)</span><br><span class="line">        x = self.maxpool4(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#block5</span></span><br><span class="line">        x = self.inception5a(x)</span><br><span class="line">        x = self.inception5b(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#clf</span></span><br><span class="line">        x = self.avgpool(x) <span class="comment">#在这个全局平均池化之后，特征图尺寸就变成了1x1</span></span><br><span class="line">        x = torch.flatten(x,<span class="number">1</span>)  <span class="comment"># 1=2dim 0=1dim</span></span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">               <span class="comment"># ,aux1,aux2</span></span><br></pre></td></tr></table></figure>
<h3 id="3-创新点-5">3 创新点</h3>
<ul>
<li>
<p>引入了Inception，使用更宽更深的网络，提升网络性能；</p>
</li>
<li>
<p>感受野的大小不同，获得不同尺度特征；</p>
</li>
<li>
<p>用稀疏连接代替密集连接，减少计算资源需求；</p>
</li>
<li>
<p>1x1的卷积核降维和增深，提高计算资源利用率；</p>
</li>
<li>
<p>添加两个辅助分类器帮助训练，避免梯度消失；</p>
</li>
<li>
<p>后面的全连接层全部替换为简单的全局平均pooling。</p>
</li>
</ul>
<h2 id="五-GoogLeNet-InceptionV2">五 GoogLeNet InceptionV2</h2>
<p><strong>期刊日期：</strong> 2015-ICML</p>
<p><strong>论文名称:</strong>《Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift》</p>
<h3 id="1-简介-6">1 简介</h3>
<p>训练DNN网络的一个难点是，在训练时每层输入数据的分布会发生改变，所以需要较低的学习率和精心设置初始化参数。只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。作者把网络中间层在训练过程中，数据分布的改变称之为：“Internal  Covariate Shift”。因此，作者提出对数据做归一化的想法。</p>
<h3 id="2-网络结构-6">2 网络结构</h3>
<p>略</p>
<h3 id="3-创新点-6">3 创新点</h3>
<ul>
<li>使用Batch Normalization，加快模型训练速度；</li>
<li>使用两个3x3的卷积代替5x5的大卷积，降低了参数数量并减轻了过拟合；</li>
<li>去除Dropout并减轻L2正则化（因BN已起到正则化的作用）；</li>
</ul>
<h2 id="六-GoogLeNet-InceptionV3">六 GoogLeNet InceptionV3</h2>
<p><strong>期刊日期：</strong> CVPR-2016</p>
<p><strong>论文名称：</strong>《Rethinking the Inception Architecture for Computer Vision》</p>
<h3 id="1-简介-7">1 简介</h3>
<p>GoogLeNet经过了Inception V1、Inception V2（BN）的发展以后，Google的Szegedy等人又对其进行了更深层次的研究和拓展，在本文中，作者提出了当前环境下，网络设计的一些重要准则，并根据这些准则，对原有的GoogLeNet进行了改进，提出了一个更加复杂、性能更好的模型框架：Inception V3。这篇文章证明了这些改进的有效性，并为以后的网络设计提供了新的思路。</p>
<h3 id="2-网络结构-7">2 网络结构</h3>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3.png" alt=""></p>
<h4 id="2-1-定义基本卷积模块">2.1 定义基本卷积模块</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicConv2d</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span>, out_channels: <span class="built_in">int</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, bias=<span class="literal">False</span>, **kwargs)</span><br><span class="line">                                  <span class="comment"># 同样写上**kwargs</span></span><br><span class="line">                                  , nn.BatchNorm2d(out_channels)</span><br><span class="line">                                  , nn.ReLU(inplace=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4 id="2-2-定义InceptionA模块">2.2 定义InceptionA模块</h4>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3-A.png" alt=""></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3-AA.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionA---&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionA</span>(nn.Module):</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, pool_features, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionA, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch1x1 = conv_block(in_channels, <span class="number">64</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">        self.branch5x5_1 = conv_block(in_channels, <span class="number">48</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch5x5_2 = conv_block(<span class="number">48</span>, <span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">        self.branch3x3dbl_1 = conv_block(in_channels, <span class="number">64</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_2 = conv_block(<span class="number">64</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_3 = conv_block(<span class="number">96</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">        self.branch_pool = conv_block(in_channels, pool_features, kernel_size=<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line"> </span><br><span class="line">        branch5x5 = self.branch5x5_1(x)</span><br><span class="line">        branch5x5 = self.branch5x5_2(branch5x5)</span><br><span class="line"> </span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_1(x)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)</span><br><span class="line"> </span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line"> </span><br><span class="line">        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="2-2-定义InceptionB模块">2.2 定义InceptionB模块</h4>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3-B.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionB---&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionB</span>(nn.Module):</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionB, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch3x3 = conv_block(in_channels, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">        self.branch3x3dbl_1 = conv_block(in_channels, <span class="number">64</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_2 = conv_block(<span class="number">64</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_3 = conv_block(<span class="number">96</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch3x3 = self.branch3x3(x)</span><br><span class="line"> </span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_1(x)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)</span><br><span class="line"> </span><br><span class="line">        branch_pool = F.max_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">        outputs = [branch3x3, branch3x3dbl, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="2-3-定义InceptionC模块">2.3 定义InceptionC模块</h4>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3-C.png" alt=""></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3-CC.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionC---&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionC</span>(nn.Module):</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, channels_7x7, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionC, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch1x1 = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">        c7 = channels_7x7</span><br><span class="line">        self.branch7x7_1 = conv_block(in_channels, c7, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch7x7_2 = conv_block(c7, c7, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line">        self.branch7x7_3 = conv_block(c7, <span class="number">192</span>, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</span><br><span class="line"> </span><br><span class="line">        self.branch7x7dbl_1 = conv_block(in_channels, c7, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch7x7dbl_2 = conv_block(c7, c7, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</span><br><span class="line">        self.branch7x7dbl_3 = conv_block(c7, c7, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line">        self.branch7x7dbl_4 = conv_block(c7, c7, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</span><br><span class="line">        self.branch7x7dbl_5 = conv_block(c7, <span class="number">192</span>, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line"> </span><br><span class="line">        self.branch_pool = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line"> </span><br><span class="line">        branch7x7 = self.branch7x7_1(x)</span><br><span class="line">        branch7x7 = self.branch7x7_2(branch7x7)</span><br><span class="line">        branch7x7 = self.branch7x7_3(branch7x7)</span><br><span class="line"> </span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_1(x)</span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)</span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)</span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)</span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)</span><br><span class="line"> </span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line"> </span><br><span class="line">        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="2-4-定义InceptionD模块">2.4 定义InceptionD模块</h4>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3-D.png" alt=""></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3-DD.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionD---&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionD</span>(nn.Module):</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionD, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch3x3_1 = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_2 = conv_block(<span class="number">192</span>, <span class="number">320</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">        self.branch7x7x3_1 = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch7x7x3_2 = conv_block(<span class="number">192</span>, <span class="number">192</span>, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line">        self.branch7x7x3_3 = conv_block(<span class="number">192</span>, <span class="number">192</span>, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</span><br><span class="line">        self.branch7x7x3_4 = conv_block(<span class="number">192</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch3x3 = self.branch3x3_1(x)</span><br><span class="line">        branch3x3 = self.branch3x3_2(branch3x3)</span><br><span class="line"> </span><br><span class="line">        branch7x7x3 = self.branch7x7x3_1(x)</span><br><span class="line">        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)</span><br><span class="line">        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)</span><br><span class="line">        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)</span><br><span class="line"> </span><br><span class="line">        branch_pool = F.max_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        outputs = [branch3x3, branch7x7x3, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="2-5-定义InceptionE模块">2.5 定义InceptionE模块</h4>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3-E.png" alt=""></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/GooV3-EE.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionE---&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionE</span>(nn.Module):</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionE, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch1x1 = conv_block(in_channels, <span class="number">320</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">        self.branch3x3_1 = conv_block(in_channels, <span class="number">384</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_2a = conv_block(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">1</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        self.branch3x3_2b = conv_block(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line"> </span><br><span class="line">        self.branch3x3dbl_1 = conv_block(in_channels, <span class="number">448</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_2 = conv_block(<span class="number">448</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_3a = conv_block(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">1</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        self.branch3x3dbl_3b = conv_block(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line"> </span><br><span class="line">        self.branch_pool = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line"> </span><br><span class="line">        branch3x3 = self.branch3x3_1(x)</span><br><span class="line">        branch3x3 = [</span><br><span class="line">            self.branch3x3_2a(branch3x3),</span><br><span class="line">            self.branch3x3_2b(branch3x3),</span><br><span class="line">        ]</span><br><span class="line">        branch3x3 = torch.cat(branch3x3, <span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_1(x)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)</span><br><span class="line">        branch3x3dbl = [</span><br><span class="line">            self.branch3x3dbl_3a(branch3x3dbl),</span><br><span class="line">            self.branch3x3dbl_3b(branch3x3dbl),</span><br><span class="line">        ]</span><br><span class="line">        branch3x3dbl = torch.cat(branch3x3dbl, <span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line"> </span><br><span class="line">        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="2-6-总代码">2.6 总代码</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;-------------------------第一步：定义基础卷积模块-------------------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicConv2d</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels: <span class="built_in">int</span>, out_channels: <span class="built_in">int</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, bias=<span class="literal">False</span>, **kwargs)</span><br><span class="line">                                  <span class="comment"># 同样写上**kwargs</span></span><br><span class="line">                                  , nn.BatchNorm2d(out_channels)</span><br><span class="line">                                  , nn.ReLU(inplace=<span class="literal">True</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;-----------------第二步：定义Inceptionv3模块---------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionA---&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionA</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, pool_features, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionA, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch1x1 = conv_block(in_channels, <span class="number">64</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.branch5x5_1 = conv_block(in_channels, <span class="number">48</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch5x5_2 = conv_block(<span class="number">48</span>, <span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.branch3x3dbl_1 = conv_block(in_channels, <span class="number">64</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_2 = conv_block(<span class="number">64</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_3 = conv_block(<span class="number">96</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.branch_pool = conv_block(in_channels, pool_features, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line"></span><br><span class="line">        branch5x5 = self.branch5x5_1(x)</span><br><span class="line">        branch5x5 = self.branch5x5_2(branch5x5)</span><br><span class="line"></span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_1(x)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)</span><br><span class="line"></span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line"></span><br><span class="line">        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionB---&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionB</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionB, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch3x3 = conv_block(in_channels, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.branch3x3dbl_1 = conv_block(in_channels, <span class="number">64</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_2 = conv_block(<span class="number">64</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_3 = conv_block(<span class="number">96</span>, <span class="number">96</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch3x3 = self.branch3x3(x)</span><br><span class="line"></span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_1(x)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)</span><br><span class="line"></span><br><span class="line">        branch_pool = F.max_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        outputs = [branch3x3, branch3x3dbl, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionC---&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionC</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, channels_7x7, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionC, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch1x1 = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        c7 = channels_7x7</span><br><span class="line">        self.branch7x7_1 = conv_block(in_channels, c7, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch7x7_2 = conv_block(c7, c7, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line">        self.branch7x7_3 = conv_block(c7, <span class="number">192</span>, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        self.branch7x7dbl_1 = conv_block(in_channels, c7, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch7x7dbl_2 = conv_block(c7, c7, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</span><br><span class="line">        self.branch7x7dbl_3 = conv_block(c7, c7, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line">        self.branch7x7dbl_4 = conv_block(c7, c7, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</span><br><span class="line">        self.branch7x7dbl_5 = conv_block(c7, <span class="number">192</span>, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">        self.branch_pool = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line"></span><br><span class="line">        branch7x7 = self.branch7x7_1(x)</span><br><span class="line">        branch7x7 = self.branch7x7_2(branch7x7)</span><br><span class="line">        branch7x7 = self.branch7x7_3(branch7x7)</span><br><span class="line"></span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_1(x)</span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)</span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)</span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)</span><br><span class="line">        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)</span><br><span class="line"></span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line"></span><br><span class="line">        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionD---&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionD</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionD, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch3x3_1 = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_2 = conv_block(<span class="number">192</span>, <span class="number">320</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        self.branch7x7x3_1 = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch7x7x3_2 = conv_block(<span class="number">192</span>, <span class="number">192</span>, kernel_size=(<span class="number">1</span>, <span class="number">7</span>), padding=(<span class="number">0</span>, <span class="number">3</span>))</span><br><span class="line">        self.branch7x7x3_3 = conv_block(<span class="number">192</span>, <span class="number">192</span>, kernel_size=(<span class="number">7</span>, <span class="number">1</span>), padding=(<span class="number">3</span>, <span class="number">0</span>))</span><br><span class="line">        self.branch7x7x3_4 = conv_block(<span class="number">192</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch3x3 = self.branch3x3_1(x)</span><br><span class="line">        branch3x3 = self.branch3x3_2(branch3x3)</span><br><span class="line"></span><br><span class="line">        branch7x7x3 = self.branch7x7x3_1(x)</span><br><span class="line">        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)</span><br><span class="line">        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)</span><br><span class="line">        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)</span><br><span class="line"></span><br><span class="line">        branch_pool = F.max_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        outputs = [branch3x3, branch7x7x3, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;---InceptionE---&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionE</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionE, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.branch1x1 = conv_block(in_channels, <span class="number">320</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.branch3x3_1 = conv_block(in_channels, <span class="number">384</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3_2a = conv_block(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">1</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        self.branch3x3_2b = conv_block(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        self.branch3x3dbl_1 = conv_block(in_channels, <span class="number">448</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_2 = conv_block(<span class="number">448</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3dbl_3a = conv_block(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">1</span>, <span class="number">3</span>), padding=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        self.branch3x3dbl_3b = conv_block(<span class="number">384</span>, <span class="number">384</span>, kernel_size=(<span class="number">3</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        self.branch_pool = conv_block(in_channels, <span class="number">192</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line"></span><br><span class="line">        branch3x3 = self.branch3x3_1(x)</span><br><span class="line">        branch3x3 = [</span><br><span class="line">            self.branch3x3_2a(branch3x3),</span><br><span class="line">            self.branch3x3_2b(branch3x3),</span><br><span class="line">        ]</span><br><span class="line">        branch3x3 = torch.cat(branch3x3, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_1(x)</span><br><span class="line">        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)</span><br><span class="line">        branch3x3dbl = [</span><br><span class="line">            self.branch3x3dbl_3a(branch3x3dbl),</span><br><span class="line">            self.branch3x3dbl_3b(branch3x3dbl),</span><br><span class="line">        ]</span><br><span class="line">        branch3x3dbl = torch.cat(branch3x3dbl, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        branch_pool = F.avg_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line"></span><br><span class="line">        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        outputs = self._forward(x)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;-------------------第三步：定义辅助分类器InceptionAux-----------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InceptionAux</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channels, num_classes, conv_block=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(InceptionAux, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> conv_block <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            conv_block = BasicConv2d</span><br><span class="line">        self.conv0 = conv_block(in_channels, <span class="number">128</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.conv1 = conv_block(<span class="number">128</span>, <span class="number">768</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">768</span>, num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># N x 768 x 17 x 17</span></span><br><span class="line">        x = F.avg_pool2d(x, kernel_size=<span class="number">5</span>, stride=<span class="number">3</span>)</span><br><span class="line">        <span class="comment"># N x 768 x 5 x 5</span></span><br><span class="line">        x = self.conv0(x)</span><br><span class="line">        <span class="comment"># N x 128 x 5 x 5</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="comment"># N x 768 x 1 x 1</span></span><br><span class="line">        <span class="comment"># Adaptive average pooling</span></span><br><span class="line">        x = F.adaptive_avg_pool2d(x, (<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># N x 768 x 1 x 1</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># N x 768</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="comment"># N x 1000</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;-----------------------第四步：搭建GoogLeNet网络--------------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GoogLeNet</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes=<span class="number">1000</span>, aux_logits=<span class="literal">True</span>, transform_input=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">                 inception_blocks=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(GoogLeNet, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> inception_blocks <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            inception_blocks = [</span><br><span class="line">                BasicConv2d, InceptionA, InceptionB, InceptionC,</span><br><span class="line">                InceptionD, InceptionE, InceptionAux</span><br><span class="line">            ]</span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">len</span>(inception_blocks) == <span class="number">7</span></span><br><span class="line">        conv_block = inception_blocks[<span class="number">0</span>]</span><br><span class="line">        inception_a = inception_blocks[<span class="number">1</span>]</span><br><span class="line">        inception_b = inception_blocks[<span class="number">2</span>]</span><br><span class="line">        inception_c = inception_blocks[<span class="number">3</span>]</span><br><span class="line">        inception_d = inception_blocks[<span class="number">4</span>]</span><br><span class="line">        inception_e = inception_blocks[<span class="number">5</span>]</span><br><span class="line">        inception_aux = inception_blocks[<span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">        self.aux_logits = aux_logits</span><br><span class="line">        self.transform_input = transform_input</span><br><span class="line">        self.Conv2d_1a_3x3 = conv_block(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.Conv2d_1b_3x3 = conv_block(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.Conv2d_1c_3x3 = conv_block(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.Conv2d_2a_1x1 = conv_block(<span class="number">64</span>, <span class="number">80</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.Conv2d_2b_3x3 = conv_block(<span class="number">80</span>, <span class="number">192</span>, kernel_size=<span class="number">3</span>)</span><br><span class="line">        self.Mixed_A1 = inception_a(<span class="number">192</span>, pool_features=<span class="number">32</span>)</span><br><span class="line">        self.Mixed_A2 = inception_a(<span class="number">256</span>, pool_features=<span class="number">64</span>)</span><br><span class="line">        self.Mixed_A3 = inception_a(<span class="number">288</span>, pool_features=<span class="number">64</span>)</span><br><span class="line">        self.Mixed_B1 = inception_b(<span class="number">288</span>)</span><br><span class="line">        self.Mixed_C1 = inception_c(<span class="number">768</span>, channels_7x7=<span class="number">128</span>)</span><br><span class="line">        self.Mixed_C2 = inception_c(<span class="number">768</span>, channels_7x7=<span class="number">160</span>)</span><br><span class="line">        self.Mixed_C3 = inception_c(<span class="number">768</span>, channels_7x7=<span class="number">160</span>)</span><br><span class="line">        self.Mixed_C4 = inception_c(<span class="number">768</span>, channels_7x7=<span class="number">192</span>)</span><br><span class="line">        <span class="keyword">if</span> aux_logits:</span><br><span class="line">            self.AuxLogits = inception_aux(<span class="number">768</span>, num_classes)</span><br><span class="line">        self.Mixed_D1 = inception_d(<span class="number">768</span>)</span><br><span class="line">        self.Mixed_E1 = inception_e(<span class="number">1280</span>)</span><br><span class="line">        self.Mixed_E2 = inception_e(<span class="number">2048</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">2048</span>, num_classes)</span><br><span class="line">        self._initialize_weights()  <span class="comment"># 在模型构建完毕后调用初始化函数</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;输入(229,229,3)的数据，首先归一化输入，经过5个卷积，2个最大池化层。&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># N x 3 x 299 x 299</span></span><br><span class="line">        x = self.Conv2d_1a_3x3(x)</span><br><span class="line">        <span class="comment"># N x 32 x 149 x 149</span></span><br><span class="line">        x = self.Conv2d_1b_3x3(x)</span><br><span class="line">        <span class="comment"># N x 32 x 147 x 147</span></span><br><span class="line">        x = self.Conv2d_1c_3x3(x)</span><br><span class="line">        <span class="comment"># N x 64 x 147 x 147</span></span><br><span class="line">        x = F.max_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># N x 64 x 73 x 73</span></span><br><span class="line">        x = self.Conv2d_2a_1x1(x)</span><br><span class="line">        <span class="comment"># N x 80 x 73 x 73</span></span><br><span class="line">        x = self.Conv2d_2b_3x3(x)</span><br><span class="line">        <span class="comment"># N x 192 x 71 x 71</span></span><br><span class="line">        x = F.max_pool2d(x, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>)</span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;然后经过</span></span><br><span class="line"><span class="string">        3个InceptionA结构，1个InceptionB，3个InceptionC，1个InceptionD，2个InceptionE，</span></span><br><span class="line"><span class="string">        其中InceptionA，辅助分类器AuxLogits以经过最后一个InceptionC的输出为输入。&#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="comment"># 35 x 35 x 192</span></span><br><span class="line">        x = self.Mixed_A1(x)  <span class="comment"># InceptionA(192, pool_features=32)  224+32</span></span><br><span class="line">        <span class="comment"># 35 x 35 x 256</span></span><br><span class="line">        x = self.Mixed_A2(x)  <span class="comment"># InceptionA(256, pool_features=64)  224+64</span></span><br><span class="line">        <span class="comment"># 35 x 35 x 288</span></span><br><span class="line">        x = self.Mixed_A3(x)  <span class="comment"># InceptionA(288, pool_features=64)  224+64</span></span><br><span class="line">        <span class="comment"># 35 x 35 x 288</span></span><br><span class="line">        x = self.Mixed_B1(x)  <span class="comment"># InceptionB(288)  384+96+288</span></span><br><span class="line">        <span class="comment"># 17 x 17 x 768</span></span><br><span class="line">        x = self.Mixed_C1(x)  <span class="comment"># InceptionC(768, channels_7x7=128)  invariant,768</span></span><br><span class="line">        <span class="comment"># 17 x 17 x 768</span></span><br><span class="line">        x = self.Mixed_C2(x)  <span class="comment"># InceptionC(768, channels_7x7=160)  invariant,768</span></span><br><span class="line">        <span class="comment"># 17 x 17 x 768</span></span><br><span class="line">        x = self.Mixed_C3(x)  <span class="comment"># InceptionC(768, channels_7x7=160)  invariant,768</span></span><br><span class="line">        <span class="comment"># 17 x 17 x 768</span></span><br><span class="line">        x = self.Mixed_C4(x)  <span class="comment"># InceptionC(768, channels_7x7=192)  invariant,768</span></span><br><span class="line">        <span class="comment"># 17 x 17 x 768</span></span><br><span class="line">        <span class="keyword">if</span> self.training <span class="keyword">and</span> self.aux_logits:</span><br><span class="line">            aux_defined = <span class="literal">True</span></span><br><span class="line">            aux = self.AuxLogits(x)  <span class="comment"># InceptionAux(768, num_classes)</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            aux_defined = <span class="literal">False</span></span><br><span class="line">            aux = <span class="literal">None</span>  <span class="comment"># Initialize aux even if not used</span></span><br><span class="line">        <span class="comment"># 17 x 17 x 768</span></span><br><span class="line">        x = self.Mixed_D1(x)  <span class="comment"># InceptionD(768) half,512</span></span><br><span class="line">        <span class="comment"># 8 x 8 x 1280</span></span><br><span class="line">        x = self.Mixed_E1(x)  <span class="comment"># InceptionE(1280)  invariant,2048</span></span><br><span class="line">        <span class="comment"># 8 x 8 x 2048</span></span><br><span class="line">        x = self.Mixed_E2(x)  <span class="comment"># InceptionE(2048)  invariant,2048</span></span><br><span class="line"></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;进入分类部分。</span></span><br><span class="line"><span class="string">        经过平均池化层+dropout+打平+全连接层输出&#x27;&#x27;&#x27;</span></span><br><span class="line">        x = F.adaptive_avg_pool2d(x, (<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># N x 2048 x 1 x 1</span></span><br><span class="line">        x = F.dropout(x, training=self.training)</span><br><span class="line">        <span class="comment"># N x 2048 x 1 x 1</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)  <span class="comment"># Flatten（）就是将2D的特征图压扁为1D的特征向量，是展平操作，进入全连接层之前使用,类才能写进nn.Sequential</span></span><br><span class="line">        <span class="comment"># N x 2048</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="comment"># N x 1000 (num_classes)</span></span><br><span class="line">        <span class="keyword">return</span> x, aux, aux_defined</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x, aux, aux_defined = self._forward(x)</span><br><span class="line">        <span class="keyword">if</span> aux_defined:</span><br><span class="line">            <span class="keyword">return</span> x, aux</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;-----------------------第五步：网络结构参数初始化--------------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 目的：使网络更好收敛，准确率更高</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_initialize_weights</span>(<span class="params">self</span>):  <span class="comment"># 将各种初始化方法定义为一个initialize_weights()的函数并在模型初始后进行使用。</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 遍历网络中的每一层</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="comment"># isinstance(object, type)，如果指定的对象拥有指定的类型，则isinstance()函数返回True</span></span><br><span class="line"></span><br><span class="line">            <span class="string">&#x27;&#x27;&#x27;如果是卷积层Conv2d&#x27;&#x27;&#x27;</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                <span class="comment"># Kaiming正态分布方式的权重初始化</span></span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">                <span class="string">&#x27;&#x27;&#x27;判断是否有偏置：&#x27;&#x27;&#x27;</span></span><br><span class="line">                <span class="comment"># 如果偏置不是0，将偏置置成0，对偏置进行初始化</span></span><br><span class="line">                <span class="keyword">if</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                    <span class="comment"># torch.nn.init.constant_(tensor, val)，初始化整个矩阵为常数val</span></span><br><span class="line">                    nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">                <span class="string">&#x27;&#x27;&#x27;如果是全连接层&#x27;&#x27;&#x27;</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                <span class="comment"># init.normal_(tensor, mean=0.0, std=1.0)，使用从正态分布中提取的值填充输入张量</span></span><br><span class="line">                <span class="comment"># 参数：tensor：一个n维Tensor，mean：正态分布的平均值，std：正态分布的标准差</span></span><br><span class="line">                nn.init.normal_(m.weight, <span class="number">0</span>, <span class="number">0.01</span>)</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;---------------------------------------显示网络结构-------------------------------&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    net = GoogLeNet(<span class="number">1000</span>).cuda()</span><br><span class="line">    <span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"></span><br><span class="line">    summary(net, (<span class="number">10</span>, <span class="number">3</span>, <span class="number">299</span>, <span class="number">299</span>))</span><br></pre></td></tr></table></figure>
<h3 id="3-创新点-7">3 创新点</h3>
<ul>
<li>
<p>卷积核分解：1.将大的卷积核分解成小的。2.非对称分解（<code>n*n</code>的卷积核替换成 <code>1*n</code> 和<code>n*1</code>的卷积核堆叠）</p>
</li>
<li>
<p>并行执行（卷积C+池化P），再进行feature map的堆叠</p>
</li>
<li>
<p>标签平滑（LSR）进行模型正则化</p>
</li>
<li>
<p>纠正了辅助分类器的作用，靠近输入辅助分类器（加不加没有影响），但是靠近输出的辅助分类器（加上BN后）可以起到正则化的作用。</p>
</li>
</ul>
<h2 id="七-ResNet">七 ResNet</h2>
<p><strong>期刊日期：</strong> CVPR-2016</p>
<p><strong>论文名称：</strong>《Deep Residual Learning for Image Recognition》</p>
<h3 id="1-简介-8">1 简介</h3>
<p>2015年何恺明等大神推出的ResNet相当经典，在深度学习发展历程中具有里程碑的意义。在ISLVRC和COCO上横扫所有选手，获得冠军，同时也在2016CVPR获得best paper。ResNet在网络结构上做了大创新，首次提出了残差的思想（跨层连接），不再是简单的堆积层数，解决了网络过深而导致的梯度消失的问题，为更深的网络提供了有力的方向。</p>
<h3 id="2-网络结构-8">2 网络结构</h3>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/rresnet.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Type</span>, <span class="type">Union</span>, <span class="type">List</span>, <span class="type">Optional</span></span><br><span class="line"><span class="keyword">from</span> torchinfo <span class="keyword">import</span> summary</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv3x3</span>(<span class="params">in_, out_, stride=<span class="number">1</span>, initial_zero=<span class="literal">False</span></span>):</span><br><span class="line">    bn = nn.BatchNorm2d(out_)</span><br><span class="line">    <span class="keyword">if</span> initial_zero:</span><br><span class="line">        nn.init.constant_(bn.weight, <span class="number">0</span>)  <span class="comment"># 使用nn.init.constant_并直接作用于bn.weight</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_, out_, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">        bn</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv1x1</span>(<span class="params">in_, out_, stride=<span class="number">1</span>, initial_zero=<span class="literal">False</span></span>):</span><br><span class="line">    bn = nn.BatchNorm2d(out_)</span><br><span class="line">    <span class="keyword">if</span> initial_zero:</span><br><span class="line">        nn.init.constant_(bn.weight, <span class="number">0</span>)  <span class="comment"># 使用nn.init.constant_并直接作用于bn.weight</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Conv2d(in_, out_, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">        bn</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualUnit</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, out_: <span class="built_in">int</span>, stride1: <span class="built_in">int</span> = <span class="number">1</span>, in_: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()  <span class="comment"># 修正了这里</span></span><br><span class="line"></span><br><span class="line">        self.stride1 = stride1</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> stride1 != <span class="number">1</span>:  <span class="comment"># 零号的RU</span></span><br><span class="line">            in_ = <span class="built_in">int</span>(out_ / <span class="number">2</span>)  <span class="comment"># 零号</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            in_ = out_  <span class="comment"># 除了零号的RU</span></span><br><span class="line">        <span class="comment"># 拟合部分，输出F(x)</span></span><br><span class="line">        self.fit_ = nn.Sequential(</span><br><span class="line">            conv3x3(in_, out_, stride=stride1),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            conv3x3(out_, out_, initial_zero=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.skipconv = conv1x1(in_, out_, stride=stride1)</span><br><span class="line">        <span class="comment"># 单独定义放在H(x)之后来使用的激活函数ReLu</span></span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        fx = self.fit_(x)  <span class="comment"># 拟合结果</span></span><br><span class="line">        <span class="keyword">if</span> self.stride1 == <span class="number">2</span>:</span><br><span class="line">            x = self.skipconv(x)  <span class="comment"># 跳跃连接</span></span><br><span class="line">        hx = self.relu(fx + x)</span><br><span class="line">        <span class="keyword">return</span> hx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BottleNeck</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, middle_out, stride1: <span class="built_in">int</span> = <span class="number">1</span>, in_: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        out_ = <span class="number">4</span> * middle_out</span><br><span class="line">        <span class="keyword">if</span> in_ == <span class="literal">None</span>:  <span class="comment"># 仅为conv2的零号BN服务</span></span><br><span class="line">            <span class="keyword">if</span> stride1 != <span class="number">1</span>:  <span class="comment"># 除了conv2的零号BN</span></span><br><span class="line">                in_ = middle_out * <span class="number">2</span>  <span class="comment"># 零号</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                in_ = middle_out * <span class="number">4</span>  <span class="comment"># 除了零号的BN</span></span><br><span class="line">        self.fit_ = nn.Sequential(</span><br><span class="line">            conv1x1(in_, middle_out, stride=stride1),  <span class="comment"># 无论是RU还是BN,第一层的stride都要带上</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            conv3x3(middle_out, middle_out),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            conv1x1(middle_out, out_, initial_zero=<span class="literal">True</span>)  <span class="comment"># 无论RU还是BN，最后一层的zero都是True</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.skipconv = conv1x1(in_, out_, stride=stride1)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        fx = self.fit_(x)</span><br><span class="line"></span><br><span class="line">        x = self.skipconv(x)</span><br><span class="line">        hx = self.relu(fx + x)</span><br><span class="line">        <span class="keyword">return</span> hx</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mak_layers</span>(<span class="params">block: <span class="type">Type</span>[<span class="type">Union</span>[ResidualUnit, BottleNeck]],middle_out:<span class="built_in">int</span>,num_blocks: <span class="built_in">int</span>, afterconv1: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span><br><span class="line">    <span class="comment"># 当你参数要填入类，需要用Type,有两种类，故Union</span></span><br><span class="line">    layers = []</span><br><span class="line">    <span class="keyword">if</span> afterconv1:</span><br><span class="line">        layers.append(block(<span class="number">64</span>,in_=<span class="number">64</span>)) <span class="comment"># 定死了</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        layers.append(block(middle_out,stride1=<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_blocks-<span class="number">1</span>):</span><br><span class="line">        layers.append(block(middle_out))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,block: <span class="type">Type</span>[<span class="type">Union</span>[ResidualUnit, BottleNeck]],layers:<span class="type">List</span>[<span class="built_in">int</span>],num_classes:<span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.divisor = <span class="number">512</span> <span class="keyword">if</span> block == ResidualUnit <span class="keyword">else</span> <span class="number">2048</span>  <span class="comment"># 用于全连接层的除数</span></span><br><span class="line">        <span class="comment"># 卷积+池化</span></span><br><span class="line">        self.layer1 = nn.Sequential(nn.Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">64</span>,kernel_size=<span class="number">7</span>,stride=<span class="number">2</span>,padding=<span class="number">3</span>),</span><br><span class="line">                                    nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">                                    nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                                    nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>,ceil_mode=<span class="literal">True</span>))</span><br><span class="line">        <span class="comment"># Layer2-Layer5 残差块/瓶颈结构</span></span><br><span class="line">        self.layer2_x=mak_layers(block,<span class="number">64</span>,layers[<span class="number">0</span>],<span class="literal">True</span>)</span><br><span class="line">        self.layer3_x=mak_layers(block,<span class="number">128</span>,layers[<span class="number">1</span>],<span class="literal">False</span>)</span><br><span class="line">        self.layer4_x=mak_layers(block,<span class="number">256</span>,layers[<span class="number">2</span>],<span class="literal">False</span>)</span><br><span class="line">        self.layer5_x=mak_layers(block,<span class="number">512</span>,layers[<span class="number">3</span>],<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 全局平均池化</span></span><br><span class="line">        self.avgpool =nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分类</span></span><br><span class="line">        self.fc = nn.Linear(self.divisor,num_classes)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2_x(x)</span><br><span class="line">        x = self.layer3_x(x)</span><br><span class="line">        x = self.layer4_x(x)</span><br><span class="line">        x = self.layer5_x(x)</span><br><span class="line">        x = self.avgpool(x) <span class="comment"># (n_samples,fc,1,1)</span></span><br><span class="line">        x = torch.flatten(x,<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="3-创新点-8">3 创新点</h3>
<ul>
<li>实现了超深的网络结构（突破1000层）</li>
<li>提出了残差网络，将x直接传递到后面的层，使得网络可以很容易的学习恒等变换，从而解决网络退化的问题，同时也使得学习效率更高。</li>
<li>使用BatchNormalization加速训练（丢弃dropout）</li>
</ul>
<h2 id="八-ResNext">八 ResNext</h2>
<h2 id="九-Xception">九 Xception</h2>
<h3 id="1-简介-9">1 简介</h3>
<h3 id="2-网络结构-9">2 网络结构</h3>
<h3 id="3-创新点-9">3 创新点</h3>
<h2 id="十、DenseNet">十、DenseNet</h2>
<h3 id="1-简介-10">1 简介</h3>
<h3 id="2-网络结构-10">2 网络结构</h3>
<h3 id="3-创新点-10">3 创新点</h3>
<h3 id="十一、SENet">十一、SENet</h3>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://www.chitose.cn">Chitose</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://www.chitose.cn/summary-base/">https://www.chitose.cn/summary-base/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.chitose.cn" target="_blank">Chitose-Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_6.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/segmentation-introduction/" title="语义分割篇章前言（数据集、评价指标）"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_3.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">语义分割篇章前言（数据集、评价指标）</div></div></a></div><div class="next-post pull-right"><a href="/sever/" title="sever"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_9.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">sever</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Face.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Chitose</div><div class="author-info__description">Hahaha</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">87</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/chitose-r"><i class="fab fa-github"></i><span>🛴/前往小家..</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/chitose-r" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:171450290@qq.com" target="_blank" title="Email"><i class="fa-solid fa-square-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="/qq/" target="_blank" title="QQ"><i class="fab fa-qq" style="color: #12b7f5;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-text">前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%B6-LeNet"><span class="toc-text">零 LeNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%AE%80%E4%BB%8B"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%88%9B%E6%96%B0%E7%82%B9"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80-AlexNet"><span class="toc-text">一 AlexNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%AE%80%E4%BB%8B-2"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-2"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%88%9B%E6%96%B0%E7%82%B9-2"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C-VGG"><span class="toc-text">二 VGG</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%AE%80%E4%BB%8B-3"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-3"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%88%9B%E6%96%B0%E7%82%B9-3"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89-NiN"><span class="toc-text">三 NiN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%AE%80%E4%BB%8B-4"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-4"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%88%9B%E6%96%B0%E7%82%B9-4"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B-GoogLeNet-InceptionV1"><span class="toc-text">四 GoogLeNet InceptionV1</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%AE%80%E4%BB%8B-5"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-5"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%88%9B%E6%96%B0%E7%82%B9-5"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94-GoogLeNet-InceptionV2"><span class="toc-text">五 GoogLeNet InceptionV2</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%AE%80%E4%BB%8B-6"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-6"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%88%9B%E6%96%B0%E7%82%B9-6"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD-GoogLeNet-InceptionV3"><span class="toc-text">六 GoogLeNet InceptionV3</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%AE%80%E4%BB%8B-7"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-7"><span class="toc-text">2 网络结构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E5%AE%9A%E4%B9%89%E5%9F%BA%E6%9C%AC%E5%8D%B7%E7%A7%AF%E6%A8%A1%E5%9D%97"><span class="toc-text">2.1 定义基本卷积模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E5%AE%9A%E4%B9%89InceptionA%E6%A8%A1%E5%9D%97"><span class="toc-text">2.2 定义InceptionA模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E5%AE%9A%E4%B9%89InceptionB%E6%A8%A1%E5%9D%97"><span class="toc-text">2.2 定义InceptionB模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-%E5%AE%9A%E4%B9%89InceptionC%E6%A8%A1%E5%9D%97"><span class="toc-text">2.3 定义InceptionC模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-%E5%AE%9A%E4%B9%89InceptionD%E6%A8%A1%E5%9D%97"><span class="toc-text">2.4 定义InceptionD模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5-%E5%AE%9A%E4%B9%89InceptionE%E6%A8%A1%E5%9D%97"><span class="toc-text">2.5 定义InceptionE模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-6-%E6%80%BB%E4%BB%A3%E7%A0%81"><span class="toc-text">2.6 总代码</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%88%9B%E6%96%B0%E7%82%B9-7"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83-ResNet"><span class="toc-text">七 ResNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%AE%80%E4%BB%8B-8"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-8"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%88%9B%E6%96%B0%E7%82%B9-8"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AB-ResNext"><span class="toc-text">八 ResNext</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B9%9D-Xception"><span class="toc-text">九 Xception</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%AE%80%E4%BB%8B-9"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-9"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%88%9B%E6%96%B0%E7%82%B9-9"><span class="toc-text">3 创新点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%81%E3%80%81DenseNet"><span class="toc-text">十、DenseNet</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%AE%80%E4%BB%8B-10"><span class="toc-text">1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-10"><span class="toc-text">2 网络结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%88%9B%E6%96%B0%E7%82%B9-10"><span class="toc-text">3 创新点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%81%E4%B8%80%E3%80%81SENet"><span class="toc-text">十一、SENet</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By Chitose</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"></div><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script defer src="/js/cursor.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Python/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🥩 Python (30)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/C/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🕶️ C (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Embedded/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">💳 Embedded (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Pytorch/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📯 Pytorch (9)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Paper/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📰 Paper (22)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/others/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🤡 others (16)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/segmentation/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🔪 segmentation (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item" style="visibility: hidden"></div><div class="magnet_item" style="visibility: hidden"></div><a class="magnet_link_more"  href="https://www.chitose.cn/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(33.333333333333336% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>
  function butterfly_clock_anzhiyu_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock_anzhiyu')
    if(parent_div_git) {
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var qweather_key = '6be604177b8a4c3e97c78a352ee324f7';
  var gaud_map_key = '17b299fafade134736e6a1d4acb5ef18';
  var baidu_ak_key = 'undefined';
  var flag = 0;
  var clock_rectangle = '113.34532,23.15624';
  var clock_default_rectangle_enable = 'false';

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_anzhiyu_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_anzhiyu_injector_config();
  }
  </script><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script data-pjax src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.js"></script><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v6.2.0" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly_v4.3.1" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" data-title="本站采用多线部署，主线路托管于Vercel" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-Vercel-brightgreen?style=flat&amp;logo=Vercel" alt=""/></a><a class="github-badge" target="_blank" href="https://dashboard.4everland.org/" style="margin-inline:5px" data-title="本站采用多线部署，备用线路托管于4EVERLAND" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-4EVERLAND-22DDDD?style=flat&amp;logo=IPFS" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="/js/runtime.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023-12-17-2/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_7.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023-12-17-2/" alt="">第二篇文章</a><div class="blog-slider__text">这是第二篇文章</div><a class="blog-slider__button" href="2023-12-17-2/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Pytorch-6/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_5.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-19</span><a class="blog-slider__title" href="Pytorch-6/" alt="">Pytorch(6)-张量可微性</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="Pytorch-6/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023-12-17-3/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_10.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023-12-17-3/" alt="">第三篇文章</a><div class="blog-slider__text">这是第三篇文章</div><a class="blog-slider__button" href="2023-12-17-3/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body></html>