<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>VGG-paper | Chitose-Blog</title><meta name="author" content="Chitose"><meta name="copyright" content="Chitose"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Very Deep Convolutional Networks for Large-Scale Image Recognition 用于大规模图像识别的深度卷积网络 摘要在这项工作中，我们研究了卷积网络深度在大规模的图像识别环境下对准确性的影响。我们的主要贡献是使用非常小的（3×3）卷积滤波器架构对网络深度的增加进行了全面评估，这表明通过将深度推到16-19加权层可以实现对现有技术配置的显著改进">
<meta property="og:type" content="article">
<meta property="og:title" content="VGG-paper">
<meta property="og:url" content="https://www.chitose.cn/VGG-paper/index.html">
<meta property="og:site_name" content="Chitose-Blog">
<meta property="og:description" content="Very Deep Convolutional Networks for Large-Scale Image Recognition 用于大规模图像识别的深度卷积网络 摘要在这项工作中，我们研究了卷积网络深度在大规模的图像识别环境下对准确性的影响。我们的主要贡献是使用非常小的（3×3）卷积滤波器架构对网络深度的增加进行了全面评估，这表明通过将深度推到16-19加权层可以实现对现有技术配置的显著改进">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_4.png">
<meta property="article:published_time" content="2024-02-06T13:40:50.000Z">
<meta property="article:modified_time" content="2024-02-14T03:24:40.000Z">
<meta property="article:author" content="Chitose">
<meta property="article:tag" content="演示">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_4.png"><link rel="shortcut icon" href="https://www.fomal.cc/favicon.ico"><link rel="canonical" href="https://www.chitose.cn/VGG-paper/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体中文","cht_to_chs":"你已切换为简体中文","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'VGG-paper',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-02-14 11:24:40'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.css" /><link rel="stylesheet" href="https://www.fomal.cc/static/css/runtime.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Face.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">70</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/todolist/"><i class="fa-fw fas fa-link"></i><span> 计划</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_4.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Chitose-Blog"><span class="site-name">Chitose-Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/todolist/"><i class="fa-fw fas fa-link"></i><span> 计划</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">VGG-paper</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-02-06T13:40:50.000Z" title="发表于 2024-02-06 21:40:50">2024-02-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-02-14T03:24:40.000Z" title="更新于 2024-02-14 11:24:40">2024-02-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>27分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="VGG-paper"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition"><a href="#Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition" class="headerlink" title="Very Deep Convolutional Networks for Large-Scale Image Recognition"></a>Very Deep Convolutional Networks for Large-Scale Image Recognition</h2><blockquote>
<h1 id="用于大规模图像识别的深度卷积网络"><a href="#用于大规模图像识别的深度卷积网络" class="headerlink" title="用于大规模图像识别的深度卷积网络"></a>用于大规模图像识别的深度卷积网络</h1></blockquote>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在这项工作中，我们研究了卷积网络<strong>深度</strong>在<strong>大规模的图像识别</strong>环境下对准确性的影响。我们的主要贡献是<strong>使用非常小的（3×3）卷积滤波器架构</strong>对网络深度的增加进行了全面评估，这表明通过将深度推到<strong>16-19加权层</strong>可以实现对现有技术配置的<strong>显著改进</strong>。这些发现是我们的ImageNet Challenge 2014提交的基础，我们的团队在定位和分类过程中分别获得了第一名和第二名。我们还表明，我们的表示对于其他数据集泛化的很好，在其它数据集上取得了最好的结果。我们使我们的<strong>两个性能最好的ConvNet模型</strong>可公开获得，以便进一步研究计算机视觉中深度视觉表示的使用。</p>
<blockquote>
<h3 id="本文研究的问题"><a href="#本文研究的问题" class="headerlink" title="本文研究的问题"></a>本文研究的问题</h3><p>本文研究了在大规模图像识别中，<strong>卷积网络深度对其识别精度的影响</strong>。</p>
<h3 id="本文主要贡献"><a href="#本文主要贡献" class="headerlink" title="本文主要贡献"></a>本文主要贡献</h3><p>我们的主要贡献是使用具有非常小（3 ×3）卷积滤波器的架构对于增加了深度的网络的全面评估，这表明将通过将深度推到<strong>16-19个权重层</strong>可以实现对现有技术配置的显著改进。</p>
</blockquote>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p><strong>卷积网络</strong>（ConvNets）近来在大规模图像和视频识别方面取得了巨大成功（Krizhevsky-<strong>AlexNet</strong>等，2012；Zeiler＆Fergus-<strong>ZFNet</strong>，2013；Sermanet-<strong>Overfeat</strong>等，2014；Simonyan＆Zisserman-<strong>VGG</strong>，2014）由于大的公开<strong>图像</strong>存储库，例如ImageNet，以及<strong>高性能计算系统</strong>的出现，例如GPU或大规模分布式集群（Dean等，2012），使这成为可能。特别是，在深度视觉识别架构的进步中，ImageNet大型视觉识别挑战（ILSVRC）（Russakovsky等，2014）发挥了重要作用，它已经成为几代大规模图像分类系统的<strong>测试台</strong>，从高维度浅层特征编码（Perronnin等，2010）（ILSVRC-2011的获胜者）到深层ConvNets（Krizhevsky等，2012）（ILSVRC-2012的获奖者）。</p>
<ul>
<li>算法、算力、数据，人工智能的三驾马车</li>
<li>Imagenet的重要性——-‘测试台’</li>
</ul>
<p>随着ConvNets在计算机视觉领域越来越商品化，为了达到更好的准确性，已经进行了许多尝试来改进Krizhevsky等人（2012）最初的架构。例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了<strong>更小的感受窗口尺寸</strong>和更小的第一卷积层步长。另一条改进措施在整个图像和多个尺度上对网络进行<strong>密集地训练和测试</strong>（Sermanet等，2014；Howard，2014）。在本文中，我们解决了ConvNet架构设计的另一个重要方面——<strong>深度。</strong>为此，我们修正了架构的其它参数，并通过添加更多的卷积层来稳定地增加网络的深度，这是可行的，因为在所有层中使用非常小的（3×3）卷积滤波器。</p>
<p>因此，我们提出了更为精确的ConvNet架构，不仅可以在ILSVRC分类和定位任务上取得的最佳的准确性，而且还适用于其它的图像识别数据集，它们可以获得优异的性能，即使使用相对简单流程的一部分（例如，通过线性SVM分类深度特征而不进行微调）。我们发布了两款表现最好的模型1，以便进一步研究。</p>
<p>本文的其余部分组织如下。在第2节，我们描述了我们的ConvNet配置。图像分类训练和评估的细节在第3节，并在第4节中在ILSVRC分类任务上对配置进行了比较。第5节总结了论文。为了完整起见，我们还将在附录A中描述和评估我们的ILSVRC-2014目标定位系统，并在附录B中讨论了非常深的特征在其它数据集上的泛化。最后，附录C包含了主要的论文修订列表。</p>
<blockquote>
<h3 id="介绍卷积网络（ConvNets）的发展和应用"><a href="#介绍卷积网络（ConvNets）的发展和应用" class="headerlink" title="介绍卷积网络（ConvNets）的发展和应用"></a>介绍卷积网络（ConvNets）的发展和应用</h3><p><strong>应用方面:</strong> 最近在大规模图像和视频识别方面取得了巨大成功（Overfeat）。</p>
<p><strong>原因:</strong> 这是由于大型公共图像存储库，如ImageNet和高性能计算系统，例如GPU或大规模分布式集群（Tensorflow）。特别是，ImageNet大规模视觉识别挑战赛（ILSVRC）（Russakovsky et al.，2014）在深度视觉识别体系结构的发展中发挥了重要作用，它是几代大型图像分类系统的试验台，从高维浅特征编码（Perronnin等人，2010年）（ILSVRC-2011的获胜者）到深度转换（Alexnet等人，2012年）（ILSVRC-2012的获胜者）。</p>
<h3 id="之前对ConvNets精度提高的尝试"><a href="#之前对ConvNets精度提高的尝试" class="headerlink" title="之前对ConvNets精度提高的尝试"></a>之前对ConvNets精度提高的尝试</h3><p> 1.利用了较小的接受窗口大小和第一卷积层的较小步幅 <strong>（处理更小的细节）</strong></p>
<p> 2.改进涉及在整个图像和多个尺度上密集地训练和测试网络 <strong>（修改输入网络的数据）</strong></p>
<h3 id="本文对ConvNets精度提高的改进方法和结果"><a href="#本文对ConvNets精度提高的改进方法和结果" class="headerlink" title="本文对ConvNets精度提高的改进方法和结果"></a>本文对ConvNets精度提高的改进方法和结果</h3><p><strong>方法:</strong> 在本文中，我们将讨论ConvNet架构设计的另一个重要方面——<strong>深度</strong>。为此，我们确定了体系结构的其他参数，并通过添加更多卷积层来稳步增加网络的深度，这是可行的，因为在所有层中使用了非常小的（3×3）卷积核。</p>
<p><strong>结果:</strong> 我们提出了更精确的ConvNet体系结构，不仅在ILSVRC分类和定位任务上达到了最先进的精度，而且还适用于其他图像识别数据集，即使仅作为特征提取器，它们也能获得优异的性能（例如，由线性支持向量机分类的深度特征，无需微调）。为了便于进一步研究，<strong>我们发布了两个性能最好的模型(VGG16和VGG19)。</strong></p>
</blockquote>
<h2 id="2-ConvNet配置"><a href="#2-ConvNet配置" class="headerlink" title="2 ConvNet配置"></a>2 ConvNet配置</h2><p>为了衡量ConvNet深度在公平环境中所带来的改进，我们所有的ConvNet层配置都使用相同的规则，灵感来自Ciresan等（2011）；Krizhevsky等人（2012年）。在本节中，我们首先描述我们的ConvNet配置的通用设计（第2.1节），然后详细说明评估中使用的具体配置（第2.2节）。最后，我们的设计选择将在2.3节进行讨论并与现有技术进行比较。</p>
<blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/VGG-2struc.png" alt=""></p>
</blockquote>
<h3 id="2-1Architecture—结构"><a href="#2-1Architecture—结构" class="headerlink" title="2.1Architecture—结构"></a>2.1Architecture—结构</h3><p>在训练期间，我们的ConvNet的输入是固定大小的224×224 RGB图像。我们唯一的预处理是从每个像素中<strong>减去在训练集上计算的RGB均值。</strong>图像通过一堆卷积（conv.）层，我们使用感受野很小的滤波器：<strong>3×3（这是捕获左/右，上/下，中心概念的最小尺寸）</strong>。在其中一种配置中，我们还使用了<strong>1×1卷积滤波器</strong>，可以看作输入通道的线性变换（后面是非线性）。卷积步长固定为1个像素；卷积层输入的空间填充要满足卷积之后保留空间分辨率，即<strong>3×3卷积层的填充为1个像素</strong>。空间池化由五个最大池化层进行，这些层在一些卷积层之后（不是所有的卷积层之后都是最大池化）。在2×2像素窗口上进行最大池化，步长为2。</p>
<p>一堆卷积层（在不同架构中具有不同深度）之后是三个全连接（FC）层：前两个每个都有4096个通道，第三个执行1000维ILSVRC分类，因此包含1000个通道（一个通道对应一个类别）。最后一层是soft-max层。所有网络中全连接层的配置是相同的。</p>
<p>所有隐藏层都配备了修正（ReLU（Krizhevsky等，2012））非线性。我们注意到，我们的网络（除了一个）都不包含局部响应规范化（LRN）（Krizhevsky等，2012）：将在第4节看到，<strong>这种规范化并不能提高在ILSVRC数据集上的性能，但增加了内存消耗和计算时间。</strong>在应用的地方，LRN层的参数是（Krizhevsky等，2012）的参数。</p>
<blockquote>
<h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p><strong>1.输入：</strong> 一个固定大小的224*224RGB图像</p>
<p><strong>2.唯一预处理：</strong> 将输入的224×224×3通道的像素值，减去平均RGB值，然后进行训练</p>
<p><strong>3.卷积核：</strong> ①使用最小尺寸的卷积核3×3这个尺寸也是能捕捉上下左右和中间方位的最小尺寸</p>
<p>​      ②有些卷积层中还使用了1×1大小的卷积核（FC层之间的），可看作是输入通道的线性变换</p>
<blockquote>
<ul>
<li><strong>为什么3个3x3的卷积核可以代替7x7的卷积？</strong></li>
</ul>
<p><strong>①得到更好的拟合效果。</strong> 3个3x3的卷积，使用了3个ReLU函数非线性校正，增加了非线性表达能力，比一个ReLU的单层Layer更具有识别能力，使得分割平面更具有可分性</p>
<p><strong>②减少网络参数个数。</strong> 对于C个通道的卷积核，7×7×C×C 比3×3×C×C的参数量大。</p>
<p>● <strong>1x1卷积核的作用？</strong></p>
<p>①在不影响感受野的情况下，<strong>增加模型的非线性性</strong></p>
<p>②1x1卷积相当于线性变换，<strong>非线性激活函数起到非线性作用</strong></p>
</blockquote>
<p><strong>4.卷积步幅 ：</strong> 卷积步幅固定为1像素，3×3卷积层的填充设置为1个像素。</p>
<p><strong>5.池化层：</strong> 池化层采用空间池化，空间池化有五个最大池化层，他们跟在一些卷积层之后，但是也不是所有的卷积层后都跟最大池化。最大池化层使用2×2像素，步幅为2。</p>
<p><strong>6.卷积层：</strong> 一个卷积层（在不同的体系结构中具有不同的深度）之后是三个完全连接的（FC）层：前两个层各有4096个通道，第三个层执行1000路ILSVRC分类，因此包含1000个通道（每类一个）（1000分类对应ImageNet-1K）。最后一层是softmax层，用来分类。在所有网络中，完全连接的层的配置都是相同的。</p>
<p><strong>7.隐藏层：</strong> 所有隐藏层都有ReLU非线性函数，网络除了第一个其他都不包含局部响应归一化（LRN）（原因：论文中，作者指出，虽然LRN在AlexNet对最终结果起到了作用，但在VGG网络中没有效果，并且该操作会增加内存和计算，从而作者在更深的网络结构中，没有使用该操作）</p>
</blockquote>
<h3 id="2-2-配置"><a href="#2-2-配置" class="headerlink" title="2.2 配置"></a>2.2 配置</h3><p>本文中评估的ConvNet配置在表1中列出，每列一个。接下来我们将按网站名称（A-E）来提及网络。所有配置都遵循2.1节提出的通用设计，并且仅是深度不同：从网络A中的11个加权层（8个卷积层和3个FC层）到网络E中的19个加权层（16个卷积层和3个FC层）。卷积层的宽度（通道数）相当小，从第一层中的64开始，然后在每个最大池化层之后增加2倍，直到达到512。</p>
<h3 id="2-3-讨论"><a href="#2-3-讨论" class="headerlink" title="2.3 讨论"></a>2.3 讨论</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/VGG-2.3.png" alt=""></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/VGG-Table1.png" alt=""></p>
<blockquote>
<p>表1：ConvNet配置（列中显示）。随着更多层的添加，配置的深度从左侧（A）增加到右侧（E），新增加的层以粗体显示。卷积层参数表示为“conv&lt;感受野大小&gt;-&lt;通道数&gt;”。因简洁起见，未显示ReLU激活函数。</p>
</blockquote>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/VGG-Table2.png" alt=""></p>
<blockquote>
<p>表2：参数数量（以百万为单位）</p>
<p>1.这个表的对比是一个对照实验组。</p>
<p>2.我们可以看到第一个A组有11层(8卷积层+3全连接层，只算带权重的层 所以池化层不算)。另一个A组加上LRN。D,E组就是在上文中见到的VGG16和VGG19。C和D都是16层，他俩的区别是C组使用了1×1的卷积核，而D组都是3 × 3 卷积核。</p>
<p>3.图中加粗的就是增加的或者修改的层。</p>
<p>4.这里面是有Relu函数的，作者没有标明出来。</p>
<h3 id="对比第一个卷积层感受野大小："><a href="#对比第一个卷积层感受野大小：" class="headerlink" title="对比第一个卷积层感受野大小："></a>对比第一个卷积层感受野大小：</h3><p>本文章中的网络配置与前面一些表现非常好的网络配置比较不一样（例如AlexNet和OverFeat），VGG在整个网络使用了<strong>非常小的感受野</strong>而不是在第一个卷积层使用相对来说比较大的卷积层（AlexNet中卷积核大小为11×11步长为4，OverFeat中卷积核大小为7×7步长为2）</p>
<h3 id="好处"><a href="#好处" class="headerlink" title="好处"></a>好处</h3><p>通过对卷积层作用的评估，可以很容易看出来<strong>两个3 × 3的卷积层可以起到一个5 × 5 卷积层的作用，三个3×3的卷积层可以起到一个7 × 7卷积层的作用。</strong></p>
<h3 id="为什么有这样的作用？"><a href="#为什么有这样的作用？" class="headerlink" title="为什么有这样的作用？"></a>为什么有这样的作用？</h3><p>1.合并了三个非线性ReLU层，不是只用一个，<strong>使得决策函数更具有判别性</strong>。</p>
<p>2.<strong>减少了参数数量</strong>。</p>
</blockquote>
<h2 id="3-分类框架"><a href="#3-分类框架" class="headerlink" title="3 分类框架"></a>3 分类框架</h2><h3 id="3-1-训练"><a href="#3-1-训练" class="headerlink" title="3.1 训练"></a>3.1 训练</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/VGG-3.1.png" alt=""></p>
<blockquote>
<h3 id="1-步骤和参数"><a href="#1-步骤和参数" class="headerlink" title="(1) 步骤和参数"></a>(1) 步骤和参数</h3><p>卷积网络训练步骤大致跟随AlexNet，除了从多尺度训练图像中对输入进行采样，也就是说通过使用小批量梯度下降法优化多项式逻辑回归来进行训练。同时添加了L2正则项进行正则化，使用Dropout减少全连接层过拟合，并且应用了学习率退火，具体参数设置如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>参数名</strong></th>
<th>大小</th>
</tr>
</thead>
<tbody>
<tr>
<td>batch size（批量大小）</td>
<td>256</td>
</tr>
<tr>
<td>momentum（动量）</td>
<td>0.9</td>
</tr>
<tr>
<td>weight decay（权重衰减）</td>
<td>0.0005</td>
</tr>
<tr>
<td>dropout ratio（随机失活率）</td>
<td>0.5</td>
</tr>
<tr>
<td>learning rate（学习率）</td>
<td>0.01</td>
</tr>
<tr>
<td>迭代步数</td>
<td>370K</td>
</tr>
<tr>
<td>epoches(轮数)</td>
<td>75</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>● 学习率退火：</p>
<p>当验证集的准确率停止提升的时候除以10，学习率总共下降了三次，一共经过了74个Epochs </p>
</blockquote>
<h4 id="网络能更快收敛的原因？（与AlexNet相比，VGGnet有更多的参数和更大的深度）"><a href="#网络能更快收敛的原因？（与AlexNet相比，VGGnet有更多的参数和更大的深度）" class="headerlink" title="网络能更快收敛的原因？（与AlexNet相比，VGGnet有更多的参数和更大的深度）"></a>网络能更快收敛的原因？（与AlexNet相比，VGGnet有更多的参数和更大的深度）</h4><p><strong>①较大深度和较小的卷积核所施加的隐式正则化</strong></p>
<p><strong>②某些层的预初始化</strong></p>
<blockquote>
<p><strong>正则化相关问题</strong></p>
<p>● <strong>正则化</strong></p>
<p>旨在更好实现模型泛化的补充技术，即在测试集上得到更好的表现。</p>
<p>● <strong>L1和L2正则化</strong></p>
<p>L1正则化会让特征变得稀疏，起到特征选择的作用；</p>
<p>L2正则化会让模型变得更简单，防止过拟合，而不会起到特征选择的作用。</p>
<p>● <strong>隐式正则化</strong></p>
<p>没有直接对模型进行正则化约束，但间接获取更好的泛化能力。</p>
<p>1、数据标准化，平滑优化目标函数曲面； 2、数据增强，扩大数据集规模； 3、随机梯度下降算法</p>
</blockquote>
<h3 id="（2）初始化权重"><a href="#（2）初始化权重" class="headerlink" title="（2）初始化权重"></a>（2）初始化权重</h3><h4 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h4><p>因为深度网络中梯度的不稳定性，错误的初始化会阻碍学习。</p>
<h4 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h4><p>开始训练配置A，通过随机初始化进行训练，然后对更深层次网络结构训练。将前四层卷积层和后三个FC层都按照A网络的初始化配置设置，中间层接着随机初始化。没有减少预初始化层的学习率，而是让它们随着学习的进程发生改变。</p>
<h4 id="采样参数"><a href="#采样参数" class="headerlink" title="采样参数"></a>采样参数</h4><p>对于随机初始化，我们从一个均值为0方差为0.01的正态分布中对权重进行采样，偏置初始化为0。</p>
<h4 id="证明结论"><a href="#证明结论" class="headerlink" title="证明结论"></a>证明结论</h4><p>即使随机初始化所有的层，模型也能训练的很好。</p>
<h4 id="后期发现"><a href="#后期发现" class="headerlink" title="后期发现"></a>后期发现</h4><p>使用随机初始化程序可以在不进行预训练的情况下初始化权重。</p>
<h3 id="（3）数据增光"><a href="#（3）数据增光" class="headerlink" title="（3）数据增光"></a>（3）数据增光</h3><h4 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h4><p>对图片进行缩放后裁剪出224 × 224大小的图片，然后进行<strong>随机水平翻转</strong>和<strong>随机的RGB通道切换。</strong></p>
<h4 id="训练尺度S"><a href="#训练尺度S" class="headerlink" title="训练尺度S"></a>训练尺度S</h4><p>训练用到的224 × 224的图片是从缩放后的原始图片中裁剪出来的，而缩放不仅仅可以缩小也可以放大，记图片缩放后的最短边的长度为S，也称为训练尺度(training scale)。</p>
<p><strong>S的取值：</strong> 当裁剪尺寸固定为224*224时，原则上S可以取不小于224的任何值：对于S=224来说，裁剪将会捕获整个的图像统计数据，将会完整横跨训练图像的最小边。对于S ≫ 224，裁剪将会对应于图像的一小部分，包括一个小对象或者对象的一部分。</p>
<p><strong>有两种设置训练尺度S的办法</strong></p>
<p>1、使用固定的S（单尺度的训练），本文使用了两种大小：256(应用于AlexNet，ZFNet，Sermanet)和384。</p>
<p>2、使用变化的S（多尺度的训练），给定一个S变化的范围<a href="文章中使用的范围是[ 256 , 512 ]"> Smin , Smax </a>，使其在这个范围中随机选值来缩放图片。</p>
</blockquote>
<h3 id="3-2-测试"><a href="#3-2-测试" class="headerlink" title="3.2 测试"></a>3.2 测试</h3><p>在测试时，给出训练的ConvNet和输入图像，它按以下方式分类。首先，将其等轴地归一化到预定义的最小图像边，表示为Q（我们也将其称为测试尺度）。我们注意到，Q不一定等于训练尺度S（正如我们在第4节中所示，每个S使用Q的几个值会导致性能改进）。然后，网络以类似于（Sermanet等人，2014）的方式密集地应用于归一化的测试图像上。即，全连接层首先被转换成卷积层（第一FC层转换到7×7卷积层，最后两个FC层转换到1×1卷积层）。然后将所得到的全卷积网络应用于整个（未裁剪）图像上。结果是类得分图的通道数等于类别的数量，以及取决于输入图像大小的可变空间分辨率。最后，为了获得图像的类别分数的固定大小的向量，类得分图在空间上平均（和池化）。我们还通过水平翻转图像来增强测试集；将原始图像和翻转图像的soft-max类后验进行平均，以获得图像的最终分数。</p>
<p>由于全卷积网络被应用在整个图像上，所以不需要在测试时对采样多个裁剪图像（Krizhevsky等，2012），因为它需要网络重新计算每个裁剪图像，这样效率较低。同时，如Szegedy等人（2014）所做的那样，使用大量的裁剪图像可以提高准确度，因为与全卷积网络相比，它使输入图像的采样更精细。此外，由于不同的卷积边界条件，多裁剪图像评估是密集评估的补充：当将ConvNet应用于裁剪图像时，卷积特征图用零填充，而在密集评估的情况下，相同裁剪图像的填充自然会来自于图像的相邻部分（由于卷积和空间池化），这大大增加了整个网络的感受野，因此捕获了更多的上下文。虽然我们认为在实践中，多裁剪图像的计算时间增加并不足以证明准确性的潜在收益，但作为参考，我们还在每个尺度使用50个裁剪图像（5×5规则网格，2次翻转）评估了我们的网络，在3个尺度上总共150个裁剪图像，与Szegedy等人(2014)在4个尺度上使用的144个裁剪图像。</p>
<blockquote>
<h3 id="方法-3"><a href="#方法-3" class="headerlink" title="方法"></a>方法</h3><p>将<strong>全连接层等效替换为卷积层</strong>进行测试</p>
<h3 id="原因-1"><a href="#原因-1" class="headerlink" title="原因"></a>原因</h3><p>卷积层和全连接层的唯一区别就是卷积层的神经元和输入是局部联系的，并且同一个通道（channel）内的不同神经元共享权值（weight）。卷积层和全连接层的计算实际上相同，因此可以将全连接层转换为卷积层。</p>
<p><strong>改造前：</strong> 假设网络是这样的 N个卷积 -&gt; 全连接 -&gt; 全连接。假如N个卷积之后的数据变成7x7x512 则此时展成向量变成 25088个元素的向量，通过一个4096神经元的全连接层 ，变成4096个元素的向量，再通过一个1000个神经元的全连接，最后输出1000个元素的向量。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/VGG-Ini.png" alt=""></p>
<p><strong>改造后：</strong> 当得到前面的数据 7x7x512之后，不将其展开成向量，而是直接送去有 4096 个 7×7卷积核的卷积层，对比改造之前看，其实就是将改造之前的全连接层里的4096个单神经元变成了 7×7的卷积核，参数依旧是等量的。同理，后面就变成了1000个1×1卷积核的卷积层(实际上论文中是三层全连接层，改为一个7x7卷积层和两个 1×1卷积层，他这里少画了一层1×1的卷积层）<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/VGG-aft.png" alt=""></p>
<h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><p>1.输入图像的大小不再受限制，因此可以高效地对图像作滑动窗式预测。</p>
<p>2.而且全连接层的计算量比较大，等效卷积层的计算量减小了，这样既达到了目的又十分高效。</p>
</blockquote>
<h3 id="3-3-实现细节"><a href="#3-3-实现细节" class="headerlink" title="3.3 实现细节"></a>3.3 实现细节</h3><p>我们的实现来源于公开的C++ Caffe工具箱（Jia，2013）（2013年12月推出），但包含了一些重大的修改，使我们能够对安装在单个系统中的多个GPU进行训练和评估，也能训练和评估在多个尺度上（如上所述）的全尺寸（未裁剪）图像。多GPU训练利用数据并行性，通过将每批训练图像分成几个GPU批次，每个GPU并行处理。在计算GPU批次梯度之后，将其平均以获得完整批次的梯度。梯度计算在GPU之间是同步的，所以结果与在单个GPU上训练完全一样。</p>
<p>最近提出了更加复杂的加速ConvNet训练的方法（Krizhevsky，2014），它们对网络的不同层之间采用模型和数据并行，我们发现我们概念上更简单的方案与使用单个GPU相比，在现有的4-GPU系统上已经提供了3.75倍的加速。在配备四个NVIDIA Titan Black GPU的系统上，根据架构训练单个网络需要2-3周时间。</p>
<blockquote>
<p><strong>1.多个GPU上训练：</strong> 实现使用的是基于C++的Caffe toolbox，并且还做了一系列的优化，可以在单个系统中安装的多个GPU上执行训练和评估，以及训练和评估多尺度的全尺寸（未裁剪）图像。</p>
<p><strong>2.分为多个GPU批次：</strong> 通过将每批训练图像分成几个GPU批次，在每个GPU上并行处理来进行，计算GPU批次梯度后，对它们进行平均可以获得完整批次的梯度。因此结果与在单个GPU上训练的结果是一样是的，但是可以节约很多时间。</p>
<p><strong>3.花费的时间：</strong> 该模型在配备有4个NVIDIA Titan Black GPU的机器上训练一个架构需要花费2-3周的时间</p>
</blockquote>
<h2 id="4-分类实验"><a href="#4-分类实验" class="headerlink" title="4 分类实验"></a>4 分类实验</h2><h3 id="4-1-数据集：ILSVRC-2012"><a href="#4-1-数据集：ILSVRC-2012" class="headerlink" title="4.1 数据集：ILSVRC-2012"></a>4.1 数据集：ILSVRC-2012</h3><p><strong>数据集</strong> 在本节中，将展示我们提出的卷积网络架构在ILSVRC-2012数据集(用于ILSVRC2012-2014挑战)上获得的图像分类结果。数据集包括1000个类的图像，并分为三组：训练（130万张图像）、验证(50K张图像)和测试(100K个保留类标签的图像)。分类性能通过两个指标进行评估： the top-1 and top-5 error。前者是一种多类分类误差，即错误分类图像的比例；后者是ILSVRC中使用的主要评价标准，并且计算图像gt 不在top-5 predicted categories的比例。</p>
<p>对于大多数实验，我们使用验证集作为测试集。某些实验也在测试集上进行，并作为ILSVRC-2014竞赛的“VGG”团队参赛作品提交给官方。</p>
<blockquote>
<p><strong>分类：</strong> 这个数据集有1000个类，包含三个部分：训练集(1.3M)，验证集(50K)，测试集(100K)，可以通过两个指标来评估准确率：top-1和 top-5误差。</p>
</blockquote>
<h3 id="4-2-单尺度评价（测试角度）"><a href="#4-2-单尺度评价（测试角度）" class="headerlink" title="4.2 单尺度评价（测试角度）"></a>4.2 单尺度评价（测试角度）</h3><p>我们首先用章节2.2中描述的网络配置在单一尺度上评估单个ConvNet模型的性能。.测试图像大小设置如下：Q=S,固定的S，Q=0.5(Smin+Smax)，S∈[Smin，Smax]。结果如表3所示。首先，我们注意到使用局部响应归一化(A-LRN网络)在没有任何归一化层的改进模型A。因此，我们不在更深层次的架构中使用标准化(B-E)。</p>
<p>其次，我们观察到分类误差随着ConvNet深度的增加而减小：从A的11层减少到E的19层。值得注意的是，尽管有相同的深度，配置C(它包含三个1×1conv层)，性能比配置D差，后者使用3×3conv。贯穿整个网络的图层。这表明，虽然额外的非线性确实有帮助(C比B好),但是通过使用大的卷积核的来捕获空间上下文也很重要。(D优于C)。当深度达到19层时，我们的体系结构的错误率达到饱和，但更深的模型可能有利于更大的数据集。我们还比较了B结构，在浅层把每对3×3conv换成1个5×5conv层(它有相同的感受野。)浅层网的前1误差比B（(on a center crop）高7%，这证实了具有小滤波器的深网优于大滤波器的浅层网。<br>最后，在训练时的尺度变化(S∈[256；512])明显比训练固定最小边的图像(S=256或S=384)，即使在测试时使用单一的尺度。这证实了通过尺度变化增强训练集确实有助于捕获多尺度图像统计数据。<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/VGG-Table3.png" alt=""></p>
<blockquote>
<p>ConvNet在单一测试尺度下的性能。</p>
<h3 id="采用的方法"><a href="#采用的方法" class="headerlink" title="采用的方法"></a>采用的方法</h3><p><strong>训练尺度S：</strong>（1）一种方法是固定S的大小</p>
<p>​           （2）另一种方法是从一定区间内随机取S(测试集记为Q)</p>
<p><strong>具体实现：</strong> 测试时所用的scale固定。这里把训练scale和测试的scale分别用S和Q表示。当S为固定值时，令Q=S固定；当S为[Smin,Smax]浮动时，Q固定为=0.5[Smin + Smax]。</p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>1.测试发现A组和A-LRN组的top1和top5错误率几乎持平，LRN局部响应归一化并没有带来精度提升，故在A-LRN之后的B～E类VGG网络中，都没有使用。</p>
<p>2.变动的S比固定的S准确率高。在训练中，采用浮动尺度效果更好，因为这有助于学习分类目标在不同尺寸下的特征。</p>
<p>3.卷积网络越深，损失越小，效果越好。</p>
<p>4.C优于B，表明增加的非线性ReLU有效。</p>
<p>5.D优于C，表明卷积层3×3对于捕捉空间特征有帮助。</p>
<p>6.E深度达到19层后达到了损失的最低点，达到饱和，更深的层次对精度没有提升，但是对于其他更大型的数据集来说，可能更深的模型效果更好。</p>
<p>7.B和同类型卷积核为5×5的网络进行了对比，发现其top-1错误率比B高7%，表明小尺寸卷积核效果更好。</p>
</blockquote>
<h3 id="4-3-多尺度评价"><a href="#4-3-多尺度评价" class="headerlink" title="4.3 多尺度评价"></a>4.3 多尺度评价</h3><p>在单一尺度上评估了ConvNet模型之后，我们现在评估了测试时尺度抖动的影响。它包括在一个测试图像的几个重新缩放的版本(对应于不同的Q值)上运行一个模型，然后平均得到的类后验。考虑到训练量表和测试量表之间的很大差异会导致性能下降，用固定S训练的模型在三个测试图像大小上进行评估，接近训练一个：Q={S−32，S，S+32}。同时，训练时的尺度抖动允许网络在测试时应用于更广泛的尺度范围，因此使用变量S∈[Smin；Smax]训练的模型在更大的尺寸Q = Smin， 0.5(Smin+Smax) ，Smax范围内进行评估.</p>
<p>结果如表4所示，表明测试时的尺度抖动导致更好的性能（与在单一尺度上评价相同的模型相比，如表3所示）。与之前一样，最深的配置(D和E)表现最好，规模抖动比固定最小边训练更好。我们在验证集上的最佳单网络性能是24.8%/7.5%top1/top5错误（在表4中以粗体突出显示）。在测试集上，配置E达到了7.3%的前5名错误。<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/VGG-Table4.png" alt=""></p>
<blockquote>
<p>表4：ConvNet在多个测试尺度下的性能。</p>
<h3 id="方法-4"><a href="#方法-4" class="headerlink" title="方法"></a>方法</h3><p>multi-scale表示测试时的scale不固定。这里当训练时的S固定时，Q取值是{S-32,S,S+32}这三个值，进行测试过后取平均结果。当S为[Smin,Smax]浮动时,Q取{Smin,0.5(Smin+Smax),Smax}，测试后取平均。</p>
<h3 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h3><p>1.同single scale一样，模型越深，效果越好</p>
<p>2.同深度下，浮动scale效果好于固定scale</p>
</blockquote>
<h3 id="4-4-多裁剪评价"><a href="#4-4-多裁剪评价" class="headerlink" title="4.4 多裁剪评价"></a>4.4 多裁剪评价</h3><p>在表5中，我们将稠密ConvNet评估与多裁剪图像评估进行比较（细节参见第3.2节）。我们还通过平均其soft-max输出来评估两种评估技术的互补性。可以看出，使用多裁剪图像表现比密集评估略好，而且这两种方法确实是互补的，因为它们的组合优于其中的每一种。如上所述，我们假设这是由于卷积边界条件的不同处理。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/VGG-Table5.png" alt=""></p>
<blockquote>
<p>表5：ConvNet评估技术比较。在所有实验中，训练尺度S从[256; 512]中采样，考虑了三种测试尺度Q：{256, 384, 512}。</p>
<h4 id="dense-evaluation-与multi-crop-evaluation两种评估方法的区别"><a href="#dense-evaluation-与multi-crop-evaluation两种评估方法的区别" class="headerlink" title="dense evaluation 与multi-crop evaluation两种评估方法的区别"></a>dense evaluation 与multi-crop evaluation两种评估方法的区别</h4><p><strong>①multi-crop：</strong>即对图像进行多样本的随机裁剪，然后通过网络预测每一个样本的结构，最终对所有结果平均。GoogleNet中使用了很多multi-crop的技巧，可以显著提升精度，因为有更精细的采样。</p>
<p><strong>②dense ：</strong>就是在上章说的将全连接层都变成卷积网络的那种训练方式。</p>
<h4 id="方法-5"><a href="#方法-5" class="headerlink" title="方法"></a>方法</h4><p>在表5中我们比较了密集评估与多裁剪评估。我们还通过平均Softmax输出来评估两种评估技术的互补性。</p>
<h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><p>ConvNet评估技术对比。在所有的实验中训练尺度S是从[256; 512]中抽取的，3个测试尺度Q是从{256，384，512}中抽取的。</p>
<h4 id="结论-2"><a href="#结论-2" class="headerlink" title="结论"></a>结论</h4><p>1.使用multi-crop优于dense</p>
<p>2.这两种方法是互补的</p>
<p>3.multi-crop＋dense方法结合的效果最好</p>
</blockquote>
<h3 id="4-5-ConvNet融合"><a href="#4-5-ConvNet融合" class="headerlink" title="4.5 ConvNet融合"></a>4.5 ConvNet融合</h3><p>最后，我们在表7中与最新技术比较我们的结果。在ILSVRC-2014挑战的分类任务（Russakovsky等，2014）中，我们的“VGG”团队获得了第二名，使用7个模型的组合取得了7.3％测试误差。提交后，我们使用2个模型的组合将错误率降低到6.8％。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/VGG-Table6.png" alt=""></p>
<blockquote>
<p>表6.多ConvNet融合结果</p>
<p><strong>方法</strong><br>到目前为止，我们只计算了一个ConvNet模型的性能。在实验部分，我们通过平均几个模型的soft-max类的后验概率来结合输出。由于模型之间的互补，这种方法提高了性能。</p>
<p><strong>结果</strong><br>结果展示在表6中。到ILSVRC提交的时间，我们仅仅训练了一个单一尺度的网络，和一个多尺度的模型D（仅仅通过微调了全连接层而不是所有层）。</p>
<p><strong>结论</strong><br>1.结合多个模型，最后的结果通过softmax平均再结合判断，由于模型之间的互补性质，这会提高他们的性能。</p>
<p>2.融合之后网络的错误率要比单个网络的错误率低0.几个百分点。</p>
<p>3.使用multi-crop＋dense结合的方法会使得效果更佳。</p>
</blockquote>
<h3 id="4-6-与最新技术的比较"><a href="#4-6-与最新技术的比较" class="headerlink" title="4.6 与最新技术的比较"></a>4.6 与最新技术的比较</h3><p>最后，我们在表7中与最新技术比较我们的结果。在ILSVRC-2014挑战的分类任务（Russakovsky等，2014）中，我们的“VGG”团队获得了第二名，使用7个模型的组合取得了7.3％测试误差。提交后，我们使用2个模型的组合将错误率降低到6.8％。</p>
<p>从表7可以看出，我们非常深的ConvNets显著优于前一代模型，在ILSVRC-2012和ILSVRC-2013竞赛中取得了最好的结果。我们的结果对于分类任务获胜者（GoogLeNet具有6.7％的错误率）也具有竞争力，并且大大优于ILSVRC-2013获胜者Clarifai的提交，其使用外部训练数据取得了11.2％的错误率，没有外部数据则为11.7％。这是非常显著的，考虑到我们最好的结果是仅通过组合两个模型实现的——明显少于大多数ILSVRC提交。在单网络性能方面，我们的架构取得了最好节果（7.0％测试误差），超过单个GoogLeNet 0.9％。值得注意的是，我们并没有偏离LeCun（1989）等人经典的ConvNet架构，但通过大幅增加深度改善了它。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/VGG-Table7.png" alt=""></p>
<blockquote>
<p>表7：在ILSVRC分类任务中与最先进技术的比较。我们的方法标记为“VGG”。仅报告未使用外部训练数据获得的结果。</p>
<p>这部分不重要，作者就是想说我们的模型就是强，你们是弟弟，即使第一名的Googlenet，我们也只和他们差一点点，在雷式对比法中甚至还强些~</p>
</blockquote>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h2><p>在这项工作中，我们评估了非常深的卷积网络（多达19个层）的大尺度图像分类。结果表明，表示深度有利于分类精度，并且可以使用传统的ConvNet大幅增加深度 挑战数据集上实现最先进的性能。在附录中，我们还展示了我们的模型可以很好地推广到广泛的任务和数据集，匹配或优于围绕较不深的图像表示构建的更复杂的识别管道。我们的研究结果再次证实了深度在视觉表征中的重要性。</p>
<blockquote>
<p>1.在这次的工作中，我们评估了大尺度图像分类汇总非常深的卷积神经网络的作用，他展示了模型的深度对于分类准确率的重要性。</p>
<p>2.同时在附录中，我们也展示了我们的模型更广范围的任务以及数据集上得到一个很好的泛化效果。</p>
<p>3.最后，我们的结论再一次证明了<strong>深度在视觉表征中的重要性</strong></p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://www.chitose.cn">Chitose</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://www.chitose.cn/VGG-paper/">https://www.chitose.cn/VGG-paper/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.chitose.cn" target="_blank">Chitose-Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_4.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/Resnet-paper/" title="Resnet-paper"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_6.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Resnet-paper</div></div></a></div><div class="next-post pull-right"><a href="/Inceptionv3-paper/" title="Inceptionv3-paper"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_5.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Inceptionv3-paper</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Face.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Chitose</div><div class="author-info__description">Hahaha</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">70</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/chitose-r"><i class="fab fa-github"></i><span>🛴/前往小家..</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/chitose-r" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:171450290@qq.com" target="_blank" title="Email"><i class="fa-solid fa-square-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="/qq/" target="_blank" title="QQ"><i class="fab fa-qq" style="color: #12b7f5;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition"><span class="toc-text">Very Deep Convolutional Networks for Large-Scale Image Recognition</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%94%A8%E4%BA%8E%E5%A4%A7%E8%A7%84%E6%A8%A1%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C"><span class="toc-text">用于大规模图像识别的深度卷积网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-text">摘要</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E7%A0%94%E7%A9%B6%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text">本文研究的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E4%B8%BB%E8%A6%81%E8%B4%A1%E7%8C%AE"><span class="toc-text">本文主要贡献</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%BC%95%E8%A8%80"><span class="toc-text">1 引言</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%EF%BC%88ConvNets%EF%BC%89%E7%9A%84%E5%8F%91%E5%B1%95%E5%92%8C%E5%BA%94%E7%94%A8"><span class="toc-text">介绍卷积网络（ConvNets）的发展和应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B9%8B%E5%89%8D%E5%AF%B9ConvNets%E7%B2%BE%E5%BA%A6%E6%8F%90%E9%AB%98%E7%9A%84%E5%B0%9D%E8%AF%95"><span class="toc-text">之前对ConvNets精度提高的尝试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E5%AF%B9ConvNets%E7%B2%BE%E5%BA%A6%E6%8F%90%E9%AB%98%E7%9A%84%E6%94%B9%E8%BF%9B%E6%96%B9%E6%B3%95%E5%92%8C%E7%BB%93%E6%9E%9C"><span class="toc-text">本文对ConvNets精度提高的改进方法和结果</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-ConvNet%E9%85%8D%E7%BD%AE"><span class="toc-text">2 ConvNet配置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1Architecture%E2%80%94%E7%BB%93%E6%9E%84"><span class="toc-text">2.1Architecture—结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-text">方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E9%85%8D%E7%BD%AE"><span class="toc-text">2.2 配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E8%AE%A8%E8%AE%BA"><span class="toc-text">2.3 讨论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E6%AF%94%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8D%B7%E7%A7%AF%E5%B1%82%E6%84%9F%E5%8F%97%E9%87%8E%E5%A4%A7%E5%B0%8F%EF%BC%9A"><span class="toc-text">对比第一个卷积层感受野大小：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A5%BD%E5%A4%84"><span class="toc-text">好处</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E8%BF%99%E6%A0%B7%E7%9A%84%E4%BD%9C%E7%94%A8%EF%BC%9F"><span class="toc-text">为什么有这样的作用？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%88%86%E7%B1%BB%E6%A1%86%E6%9E%B6"><span class="toc-text">3 分类框架</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E8%AE%AD%E7%BB%83"><span class="toc-text">3.1 训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%AD%A5%E9%AA%A4%E5%92%8C%E5%8F%82%E6%95%B0"><span class="toc-text">(1) 步骤和参数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E8%83%BD%E6%9B%B4%E5%BF%AB%E6%94%B6%E6%95%9B%E7%9A%84%E5%8E%9F%E5%9B%A0%EF%BC%9F%EF%BC%88%E4%B8%8EAlexNet%E7%9B%B8%E6%AF%94%EF%BC%8CVGGnet%E6%9C%89%E6%9B%B4%E5%A4%9A%E7%9A%84%E5%8F%82%E6%95%B0%E5%92%8C%E6%9B%B4%E5%A4%A7%E7%9A%84%E6%B7%B1%E5%BA%A6%EF%BC%89"><span class="toc-text">网络能更快收敛的原因？（与AlexNet相比，VGGnet有更多的参数和更大的深度）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8D"><span class="toc-text">（2）初始化权重</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8E%9F%E5%9B%A0"><span class="toc-text">原因</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%95-1"><span class="toc-text">方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%87%87%E6%A0%B7%E5%8F%82%E6%95%B0"><span class="toc-text">采样参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%81%E6%98%8E%E7%BB%93%E8%AE%BA"><span class="toc-text">证明结论</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%8E%E6%9C%9F%E5%8F%91%E7%8E%B0"><span class="toc-text">后期发现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%85%89"><span class="toc-text">（3）数据增光</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%95-2"><span class="toc-text">方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%B0%BA%E5%BA%A6S"><span class="toc-text">训练尺度S</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E6%B5%8B%E8%AF%95"><span class="toc-text">3.2 测试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95-3"><span class="toc-text">方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E5%9B%A0-1"><span class="toc-text">原因</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E7%9A%84"><span class="toc-text">目的</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82"><span class="toc-text">3.3 实现细节</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%88%86%E7%B1%BB%E5%AE%9E%E9%AA%8C"><span class="toc-text">4 分类实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%9AILSVRC-2012"><span class="toc-text">4.1 数据集：ILSVRC-2012</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%8D%95%E5%B0%BA%E5%BA%A6%E8%AF%84%E4%BB%B7%EF%BC%88%E6%B5%8B%E8%AF%95%E8%A7%92%E5%BA%A6%EF%BC%89"><span class="toc-text">4.2 单尺度评价（测试角度）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%87%E7%94%A8%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-text">采用的方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-text">结论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E5%A4%9A%E5%B0%BA%E5%BA%A6%E8%AF%84%E4%BB%B7"><span class="toc-text">4.3 多尺度评价</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95-4"><span class="toc-text">方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA-1"><span class="toc-text">结论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E5%A4%9A%E8%A3%81%E5%89%AA%E8%AF%84%E4%BB%B7"><span class="toc-text">4.4 多裁剪评价</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#dense-evaluation-%E4%B8%8Emulti-crop-evaluation%E4%B8%A4%E7%A7%8D%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-text">dense evaluation 与multi-crop evaluation两种评估方法的区别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%95-5"><span class="toc-text">方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C"><span class="toc-text">结果</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA-2"><span class="toc-text">结论</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-ConvNet%E8%9E%8D%E5%90%88"><span class="toc-text">4.5 ConvNet融合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-%E4%B8%8E%E6%9C%80%E6%96%B0%E6%8A%80%E6%9C%AF%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-text">4.6 与最新技术的比较</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E7%BB%93%E8%AE%BA"><span class="toc-text">5 结论</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By Chitose</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"></div><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script defer src="/js/cursor.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_clock_anzhiyu_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock_anzhiyu')
    if(parent_div_git) {
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var qweather_key = '6be604177b8a4c3e97c78a352ee324f7';
  var gaud_map_key = '17b299fafade134736e6a1d4acb5ef18';
  var baidu_ak_key = 'undefined';
  var flag = 0;
  var clock_rectangle = '113.34532,23.15624';
  var clock_default_rectangle_enable = 'false';

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_anzhiyu_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_anzhiyu_injector_config();
  }
  </script><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script data-pjax src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.js"></script><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v6.2.0" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly_v4.3.1" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" data-title="本站采用多线部署，主线路托管于Vercel" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-Vercel-brightgreen?style=flat&amp;logo=Vercel" alt=""/></a><a class="github-badge" target="_blank" href="https://dashboard.4everland.org/" style="margin-inline:5px" data-title="本站采用多线部署，备用线路托管于4EVERLAND" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-4EVERLAND-22DDDD?style=flat&amp;logo=IPFS" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="/js/runtime.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023-12-17-2/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_9.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023-12-17-2/" alt="">第二篇文章</a><div class="blog-slider__text">这是第二篇文章</div><a class="blog-slider__button" href="2023-12-17-2/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Pytorch-6/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_3.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-19</span><a class="blog-slider__title" href="Pytorch-6/" alt="">Pytorch(6)-张量可微性</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="Pytorch-6/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023-12-17-3/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_8.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023-12-17-3/" alt="">第三篇文章</a><div class="blog-slider__text">这是第三篇文章</div><a class="blog-slider__button" href="2023-12-17-3/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body></html>