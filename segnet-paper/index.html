<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>segnet-paper | Chitose-Blog</title><meta name="author" content="Chitose"><meta name="copyright" content="Chitose"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation  SegNet:深度卷积图像的编码器-解码器架构分割  摘要 我们提出了一种新颖且实用的全卷积神经网络架构，用于语义像素级分割，称为SegNet。这个核心可训练分割引擎由一个编码器网络，一个相应的解码器网络，以及一个像素级分">
<meta property="og:type" content="article">
<meta property="og:title" content="segnet-paper">
<meta property="og:url" content="https://www.chitose.cn/segnet-paper/index.html">
<meta property="og:site_name" content="Chitose-Blog">
<meta property="og:description" content="SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation  SegNet:深度卷积图像的编码器-解码器架构分割  摘要 我们提出了一种新颖且实用的全卷积神经网络架构，用于语义像素级分割，称为SegNet。这个核心可训练分割引擎由一个编码器网络，一个相应的解码器网络，以及一个像素级分">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_2.png">
<meta property="article:published_time" content="2024-03-22T04:21:59.000Z">
<meta property="article:modified_time" content="2024-03-22T04:21:59.000Z">
<meta property="article:author" content="Chitose">
<meta property="article:tag" content="演示">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_2.png"><link rel="shortcut icon" href="https://www.fomal.cc/favicon.ico"><link rel="canonical" href="https://www.chitose.cn/segnet-paper/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体中文","cht_to_chs":"你已切换为简体中文","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'segnet-paper',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-03-22 12:21:59'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.css" /><link rel="stylesheet" href="https://www.fomal.cc/static/css/runtime.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Face.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">103</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/todolist/"><i class="fa-fw fas fa-link"></i><span> 计划</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_2.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Chitose-Blog"><span class="site-name">Chitose-Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/todolist/"><i class="fa-fw fas fa-link"></i><span> 计划</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">segnet-paper</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-03-22T04:21:59.000Z" title="发表于 2024-03-22 12:21:59">2024-03-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-03-22T04:21:59.000Z" title="更新于 2024-03-22 12:21:59">2024-03-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/paper/">paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">16.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>50分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="segnet-paper"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="segnet-a-deep-convolutional-encoder-decoder-architecture-for-image-segmentation">SegNet:
A Deep Convolutional Encoder-Decoder Architecture for Image
Segmentation</h1>
<blockquote>
<h1 id="segnet深度卷积图像的编码器-解码器架构分割">SegNet:深度卷积图像的编码器-解码器架构分割</h1>
</blockquote>
<h2 id="摘要">摘要</h2>
<p>我们提出了一种新颖且实用的全卷积神经网络架构，用于语义像素级分割，称为SegNet。这个核心可训练分割引擎由一个编码器网络，一个相应的解码器网络，以及一个像素级分类层组成。编码器网络的架构与VGG16网络中的13个卷积层在拓扑上是相同的【1】。解码器网络的角色是将低分辨率编码器特征图映射到完整输入分辨率的特征图以进行像素级分类。SegNet的新颖之处在于解码器上采样其较低分辨率输入特征图的方式。具体来说，解码器使用在相应编码器的最大池化步骤中计算出的池化索引进行非线性上采样。这消除了学习上采样的需要。上采样的地图是稀疏的，然后与可训练的滤波器卷积，产生密集特征图。我们将我们提出的架构与广泛采用的FCN【2】以及众所周知的DeepLab-LargeFOV【3】、DeconvNet【4】架构进行比较。这一比较揭示了实现良好分割性能所涉及的内存与准确性的权衡。SegNet的设计主要是受到场景理解应用的激励，因此，它在内存和计算时间上进行推断时设计得很高效。它在可训练参数的数量上也显著小于其他竞争架构，可以使用随机梯度下降端到端训练。我们还对SegNet和其他架构进行了控制基准，包括在道路场景和SUN
RGB-D室内场景分割任务上。这些量化评估表明SegNet在推断时间和推断内存方面与其他架构相比具有良好的性能。我们还提供了SegNet的Caffe实现和一个网络演示在
http://mi.eng.cam.ac.uk/projects/segnet/。</p>
<h2 id="引言">1 引言</h2>
<p>语义分割在从场景理解、推断支持关系对象到自动驾驶等广泛的应用中具有广泛的应用。早期依赖于低级视觉线索的方法已经迅速被流行的机器学习算法所取代。特别是深度学习在手写数字识别、语音、分类整个图像和检测图像中的对象方面取得了巨大的成功。现在对于语义像素级标记有了活跃的兴趣。然而，这些最近的方法中有一些试图直接采用为类别预测设计的深度架构到像素级标注。结果虽然非常鼓舞人心，但看起来粗糙。这主要是因为最大池化和子采样减少了特征图分辨率。我们设计SegNet的动机来源于这个需求，需要将低分辨率特征映射到输入分辨率以进行像素级分类。这种映射必须产生对于精确边界定位有用的特征。</p>
<p>我们的架构，SegNet，旨在成为一种高效的像素级语义分割架构。它主要是由道路场景理解应用激励的，这些应用要求能够模拟外观（如道路、建筑物）、形状（如汽车、行人）并理解不同类别如道路和人行道之间的空间关系（上下文）。在典型的道路场景中，大多数像素属于大类别，如道路、建筑，因此网络必须产生平滑的分割。引擎还必须有能力根据它们的形状来划分小尺寸对象。因此，保留在提取的图像表示中的边界信息非常重要。从计算的角度看，网络在内存和推理期间的计算时间上需要高效。使用随机梯度下降（SGD）【17】这样的有效权重更新技术进行端到端的训练能力是一个额外的好处，因为它更容易重复。SegNet的设计基于这些标准的需求而产生。</p>
<p>SegNet中的编码器网络在拓扑上与VGG16【1】中的卷积层相同。我们移除了VGG16的完全连接层，这使得SegNet的编码器网络大大缩小，比许多其他近期的架构【2】【4】【11】【18】更容易训练。SegNet的关键组成部分是解码器网络，它由一系列对应于每个编码器的解码器层次结构组成。这些解码器使用从相应的编码器接收到的最大池化索引来进行非线性上采样其输入特征图。这个想法受到了一个为无监督特征学习设计的架构【19】的启发。在解码过程中重用最大池化索引有几个实际优势：（i）它改善了边界的划分，（ii）它减少了参数数量，使得端到端训练成为可能，（iii）这种上采样形式可以很少的修改就被整合进任何编码器-解码器架构中，如【2】【10】。</p>
<p>本文的主要贡献之一是分析了SegNet解码技术和广泛使用的全卷积网络（FCN）【2】。这是为了传达设计分割架构时涉及的实际权衡。大多数最近的深度分割架构有相同的编码器网络，即VGG16，但在解码器网络的形式、训练和推断方面有所不同。另一个共同特点是它们有数量级达到数百万的可训练参数，因此在执行端到端训练时遇到困难【4】。这些网络的训练难度导致多阶段训练【2】，在预训练的架构如FCN【10】后追加网络，使用支持的辅助工具如区域建议用于推断【4】，分类和分割网络的分离训练【18】以及使用额外的训练数据进行预训练【11】【20】或完整训练【10】。此外，性能增强的后处理技术【3】也很流行。尽管所有这些因素都在挑战性基准测试中提高了性能【21】，但不幸的是很难将它们的定量结果与实现良好性能所需的关键设计因素区分开来。因此，我们分析了这些方法中的一些解码过程，并揭示了它们的优缺点。</p>
<p>我们在两个场景分割任务上评估了SegNet的性能，即CamVid道路场景分割【22】和SUN
RGB-D室内场景分割【23】。Pascal
VOC12【21】一直是分割任务的挑战。然而，这项任务的大多数类别是由高度变化的背景所包围的。这隐含地倾向于用于检测的技术，正如最近的独立分类-分割网络【18】的工作所示，其中分类网络可以用大量弱标记数据进行训练，独立的分割网络性能得到提高。方法【3】也使用了带有独立CRF后处理技术来进行分割的分类网络的特征图。性能也可以通过使用额外的推断辅助工具如区域建议【4】【24】来提升。因此，它不同于场景理解，场景理解的想法是利用物体的共现和其他空间上下文来进行稳健的分割。为了证明SegNet的有效性，我们呈现了一个实时的道路场景分割为11个感兴趣类别的在线演示，用于自动驾驶（见图1中的链接）。一些从Google和SUN
RGB-D数据集【23】中随机抽取的室内测试场景的样本测试结果显示在图1中。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/segnet-f1.png"></p>
<p align="center">
Fig. 1.
SegNet在道路场景和室内场景的预测效果。想要亲自尝试我们的系统，请查看我们的在线网页演示
http://mi.eng.cam.ac.uk/projects/segnet/。
</p>
<p>本文的其余部分组织如下。在第2节，我们回顾了相关的最新文献。在第3节中，我们描述了SegNet架构及其分析。第4节中，我们评估了SegNet在室外和室内场景数据集上的性能。之后是一个关于我们方法的一般讨论，并在第5节中提供了对未来工作的指引。我们在第6节中得出结论。</p>
<h2 id="文献综述">2 文献综述</h2>
<p>语义像素级分割是当前研究的活跃主题，受到了一系列挑战性数据集[21]、[22]、[23]、[25]、[26]的推动。在深度网络出现之前，表现最佳的方法主要依赖于手工设计的特征，这些特征独立地对像素进行分类。通常，一个补丁被输入到一个分类器中，例如随机森林[27]、[28]或提升算法[29]、[30]，以预测中心像素的类别概率。基于外观[27]或基于结构从运动(SfM)和外观[28]、[29]、[30]的特征已经被探索用于CamVid道路场景理解测试[22]。这些分类器产生的每像素噪声预测（通常称为一元项）随后通过使用成对或更高阶的条件随机场(CRF)[29]、[30]进行平滑处理，以提高准确率。最近的方法旨在通过尝试预测补丁中所有像素的标签，而不仅仅是中心像素的标签，以产生高质量的一元项。这提高了基于随机森林的一元项的结果[31]，但结构细腻的类别分类表现不佳。从CamVid视频计算得到的密集深度图也被用作使用随机森林进行分类的输入[32]。另一种方法主张使用一组流行的手工设计特征和时空超像素化的组合，以获得更高的准确度[33]。在CamVid测试[30]上表现最好的技术通过结合对象检测输出与分类器预测在CRF框架中，解决了标签频率之间的不平衡问题。所有这些技术的结果表明，需要改进分类特征。</p>
<p>自NYU数据集[25]发布以来，室内RGBD像素级语义分割也获得了人们的关注。这个数据集显示了深度通道改善分割的有用性。他们的方法使用RGB-SIFT、深度-SIFT和像素位置作为输入到神经网络分类器，以预测像素一元项。然后使用CRF平滑这些噪声一元项。使用更丰富的特征集，包括LBP和区域分割，随后通过CRF获得更高的准确度[34]。在更近期的工作[25]中，类别分割和支持关系同时被推断，使用了基于RGB和深度的线索的组合。另一种方法专注于实时的联合重建和语义分割，其中使用随机森林作为分类器[35]。Gupta等人[36]在进行类别分割之前使用边界检测和层次化分组。所有这些方法的共同特点是使用手工设计的特征对RGB或RGBD图像进行分类。</p>
<p>深度卷积神经网络在对象分类上的成功最近促使研究人员利用其特征学习能力处理结构预测问题，如分割。也有尝试将为对象分类设计的网络应用于分割，特别是通过复制最深层特征在块中以匹配图像尺寸。然而，由此产生的分类是块状的。另一种使用递归神经网络的方法合并了多个低分辨率预测以生成输入图像分辨率预测。这些技术已经是相对于手工制造的特征的进步，但其划定边界的能力是差的。</p>
<p>新的深度架构，特别是为分割设计的，通过学习解码或映射低分辨率图像表示到像素级预测，推动了艺术状态的发展。在所有这些架构中生成这些低分辨率表示的编码器网络是VGG16分类网络，它有13个卷积层和3个全连接层。这个编码器网络的权重通常在大型ImageNet对象分类数据集上预训练。解码器网络在这些架构之间是不同的，负责为每个像素生成多维特征进行分类。</p>
<p>这种架构学习上采样其输入特征图，并将其与相应的编码器特征图结合，以生成输入到下一个解码器。这是一个具有大量可训练参数的架构，编码器网络中有134M，但解码器网络很小，只有0.5M。这个网络的总体大小使其很难进行端到端训练。因此，作者使用了分阶段训练过程。这里每个解码器逐渐加入到一个已经训练过的网络。这个网络增长到不再观察到性能提升为止。这种增长在停止后忽视了高分辨率特征图，这可以导致边缘信息的损失。除了与训练相关的问题外，解码器中特征图的使用使得在测试时它变得非常占用内存。我们将更详细地研究这个网络，因为它是其他最近架构的核心。</p>
<p>通过在FCN后附加一个递归神经网络，并在大数据集上微调，改善了FCN的预测性能。RNN层模仿了CRF的锐利边界划定能力，同时利用了FCN的特征表示力量。它们显示出在FCN-8上有显著的改善，但也显示出当使用更多的训练数据时，这种差异会减小。CRF-RNN的主要优势在于它与像FCN-8这样的架构联合训练时被揭示出来。联合训练有助于的事实也在其他最近的结果中显示出来。有趣的是，去卷积网络的性能显著优于FCN，尽管以更复杂的训练和推理为代价。这提出了一个问题，即CRF-RNN的感知优势是否会随着核心前馈分割引擎的改进而减少。无论如何，CRF-RNN网络都可以附加到任何深度分割架构，包括SegNet。</p>
<p>多尺度深度架构也在被追求【13】【44】。它们分为两种类型：（i）那些使用输入图像在几个尺度和相应深度特征提取网络中，（ii）那些从单一深度架构不同层合并特征图的。共同的想法是使用在多个尺度提取的特征来同时提供局部和全局上下文【46】，并且使用早期编码层的特征图能够保留更多高频细节，导致更清晰的类别边界。由于参数规模，这些架构中的一些难以训练【13】。因此采用了多阶段训练过程，同时进行数据增强。推理过程也因多个卷积路径进行特征提取而成本高昂。其他一些则在它们的多尺度网络中加入了条件随机场（CRF），并且联合训练它们。然而，这些在测试时不是前馈的，并且需要优化以确定最大后验概率（MAP）标签。</p>
<p>近期提出的一些用于分割的深度架构在推理时不是前馈的【4】【3】【18】。它们需要通过CRF【44】【43】或区域提议【4】进行MAP推理或其他辅助方法来进行推理。我们认为通过使用CRF获得的性能增加是由于缺乏在它们核心前馈分割引擎中的良好解码技术。而另一方面，SegNet则使用解码器来获取精确的像素级分类的特征。</p>
<p>最近提出的反卷积网络【4】及其半监督变种解耦网络【18】使用编码器特征图的最大位置（池化指数）来在解码器网络中执行非线性上采样。这些架构的作者，独立于SegNet（最早提交到CVPR
2015【12】），提出了解码器网络中的解码思想。然而，他们的编码器网络由VGG-16网络的全部连接层组成，约占整个网络的90%参数。这使得训练他们的网络变得非常困难，因此需要额外的辅助方法，如使用区域提议来实现训练。而且，在推理中使用这些提议，显著增加了推理时间。从基准测试的角度来看，这也使得在没有其他辅助方法的情况下评估他们架构（编码器-解码器网络）的性能变得困难。在这项工作中，我们放弃了VGG16编码器网络的全部连接层，这使我们能够使用相关的训练集通过随机梯度下降（SGD）优化来训练网络。另一种最近的方法【3】显示了通过显著减少参数数量的好处，不牺牲性能，减少了内存消耗，并且改善了推理时间。</p>
<p>我们的工作是受到Ranzato等人【19】提出的无监督特征学习架构的启发。这个关键的学习模块是一个编码器-解码器网络。编码器由一个滤波器库、逐元素的双曲正切非线性函数和最大池化及子采样组成，以获取特征图。对于每个样本，计算的最大位置的索引会被存储并传递给解码器。解码器使用存储的池化索引来上采样特征图。它使用一个可训练的解码器滤波器库来卷积这个上采样的图，重构输入图像。这个架构被用于分类的无监督预训练。一个有点类似的解码技术被用于可视化已训练的卷积网络【47】进行分类。Ranzato等人的架构主要专注于使用小输入补丁的逐层特征学习。Kavukcuoglu等人【48】将其扩展，接受完整图像尺寸作为输入以学习分层编码器。然而这些方法并没有尝试使用深度编码器-解码器网络进行无监督特征训练，因为它们在每个编码器训练后舍弃了解码器。这里，SegNet与这些架构不同，因为深度编码器-解码器网络是联合训练的，用于一个监督学习任务，因此解码器在测试时是网络的一个整体部分。</p>
<p>其他使用深度网络进行像素级预测的应用包括图像超分辨率【49】和从单一图像的深度图预测【50】。文献【50】中的作者讨论了从低分辨率特征图中学习上采样的需求，这是本文的中心主题。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/segent-f2.png"></p>
<p align="center">
Fig. 2.
SegNet架构的示意图。该架构没有全连接层，因此它完全是卷积的。解码器使用从其编码器传递的池化索引对输入进行上采样，以产生稀疏的特征图。然后，它使用可训练的滤波器组进行卷积，使特征图密集化。最终解码器输出的特征图被送入softmax分类器进行逐像素分类。
</p>
<h2 id="架构">3 架构</h2>
<p>SegNet有一个编码器网络和一个相应的解码器网络，后面是一个最终的像素级分类层。这个架构如图3所示。编码器网络包括13个卷积层，对应于VGG16网络【1】中为对象分类设计的前13个卷积层。因此，我们可以从在大数据集上训练分类的权重开始训练过程【41】。我们也舍弃了全连接层，以保留更高分辨率的特征图在最深层的编码器输出。这也显著减少了SegNet编码器网络中的参数数量（从134M减少到14.7M），与其他最近的架构相比（见表6）。每个编码器层有一个对应的解码器层，因此解码器网络有13层。最终解码器输出被送入一个多类别softmax分类器，以独立地产生每个像素的类别概率。</p>
<figure>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="C:/Users/17145/AppData/Roaming/Typora/typora-user-images/image-20240322155250092.png" alt="image-20240322155250092">
<figcaption aria-hidden="true">image-20240322155250092</figcaption>
</figure>
<p align="center">
Fig. 3.
SegNet和FCN[2]解码器的对比示意图。a，b，c，d对应特征图中的值。SegNet使用最大池化索引进行上采样（无需学习）特征图，并与可训练的解码器滤波器组进行卷积。FCN通过学习对输入特征图进行反卷积并添加相应的编码器特征图来产生解码器输出。该特征图是相应编码器中的最大池化层（包括下采样）的输出。注意，FCN中没有可训练的解码器滤波器。
</p>
<p>每个编码器在编码器网络中执行带滤波器库的空间卷积，生成一组特征图。这些特征图接着被批量标准化【51】【52】。然后应用逐元素的整流线性单元（ReLU）<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="9.669ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4273.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(1407,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1979,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(2368,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(2868,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(3312.7,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(3884.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span>。之后，进行2×2窗口和步长2的最大池化（非重叠窗口），得到的输出经过子采样缩减为原来的二分之一。最大池化用于实现对输入图像中小空间偏移的平移不变性。子采样为特征图中的每个像素提供了大尺寸输入图像的上下文（空间窗口）。虽然多层的最大池化和子采样可以实现更多的平移不变性以便于稳健的分类，相应地也会有特征图的空间分辨率损失。特征图越来越多的损失（边界细节）的图像表示对于分割是不利的，在分割中边界划分至关重要。因此，需要在子采样之前捕获并存储编码器特征图中的边界信息。如果在推理期间内存不受限制，则所有编码器特征图（子采样后）都可以被存储。通常情况下在实际应用中并非如此，因此我们提出了一种更有效的存储这些信息的方法。它仅涉及存储最大池化索引，即每个池化窗口中最大特征值的位置，在每个编码器特征图中都记住了这个位置。原则上，这可以使用每个2×2池化窗口2位来完成，因此与以浮点精度记忆化特征图相比，这种方法存储起来更为高效。我们稍后展示，这种较低的内存存储结果在精度上有轻微的损失，但仍适用于实际应用。</p>
<p>在解码器网络中的相应解码器使用记忆化的最大池化索引来上采样其输入特征图。这一步产生稀疏的特征图。这种SegNet解码技术如图3所示。这些特征图随后与一个可训练的解码器滤波器库卷积，生成密集的特征图。然后对这些图进行批量标准化处理。请注意，与第一个编码器（最接近输入图像）对应的解码器产生一个多通道特征图，尽管其编码器输入有3个通道（RGB）。这与网络中的其他解码器生成与其编码器输入同样大小和通道数的特征图不同。最终解码器的高维特征表示输出被输入到一个可训练的softmax分类器中。这个softmax分类器独立地对每个像素进行分类。softmax分类器的输出是一个K通道的概率图像，其中K是类别的数量。预测的分割对应于每个像素最大概率的类别。</p>
<p>我们在这里补充两个其他架构，DeconvNet【53】和U-Net【16】，它们与SegNet有类似的架构，但也有一些不同。DeconvNet具有更大的参数化，需要更多的计算资源，并且更难进行端到端训练（见表6），这主要是由于使用了全连接层（尽管以卷积方式）。我们将在本文第4节报告与DeconvNet的几个比较。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/segnet-t6.png"></p>
<p align="center">
TABLE 6 对不同深度架构所需的计算时间和硬件资源进行了比较。使用caffe
time命令，基于小批量大小为1且图像分辨率为360×480，计算了10次迭代的平均计算时间要求。我们使用nvidia-smi
unix命令来计算内存消耗。对于训练内存的计算，我们使用了大小为4的小批量；而对于推理内存，批量大小是1。模型大小是硬盘上caffe模型的大小。SegNet在推理模型期间是最节省内存的。
</p>
<p>与SegNet相比，U-Net（为医学成像社区提出）并不重用池化指标，而是转移整个特征图（代价是更多的内存）到对应的解码器并将它们与上采样（通过反卷积）的解码器特征图连接起来。在U-Net的VGG网络架构中没有conv5和max-pool
5块。另一方面，SegNet使用了VGG网络的所有预训练的卷积层权重作为预训练权重。</p>
<h4 id="解码器变种">3.1 解码器变种</h4>
<p>许多分割架构【2】【3】【4】共享相同的编码器网络，它们只在解码器网络的形式上有所不同。在这些中，我们选择比较SegNet解码技术与广泛使用的全卷积网络（FCN）解码技术【2】【10】。</p>
<p>为了分析SegNet并与FCN（解码器变种）的性能进行比较，我们使用了一个较小版本的SegNet，称为SegNet-Basic，它有4个编码器和4个解码器。SegNet-Basic中的所有编码器都执行最大池化和子采样，相应的解码器使用接收到的最大池化指标来上采样其输入。批量标准化在编码器和解码器网络的每个卷积层之后使用。卷积之后没有使用偏置，解码器网络中也没有ReLU非线性存在。此外，所有编码器和解码器层上选择了一个常数的7×7核大小，以提供光滑标记的广泛上下文，即一个像素在最深层特征图（第4层）可以追溯到输入图像中106×106像素的上下文窗口。SegNet-Basic的小尺寸允许我们探索许多不同的变种（解码器）并在合理的时间内对它们进行训练。同样我们创建了一个可比的FCN的版本，FCN-Basic，用于我们的分析，它与SegNet-Basic共享相同的编码器网络，但使用了FCN解码技术（见图3）在其所有解码器中。</p>
<p>图3左侧是SegNet（也就是SegNet-Basic）使用的解码技术，其中在上采样步骤中没有涉及学习。然而，上采样的图通过训练有素的多通道解码器滤波器进行卷积，以密集其稀疏的输入。每个解码器滤波器具有与上采样特征图相同数量的通道。一个较小的变种是其中解码器滤波器是单通道的，即它们仅与其对应的上采样特征图进行卷积。这种变种（SegNet-Basic-SingleChannelDecoder）显著减少了可训练参数和推理时间。</p>
<p>图3右侧的是全卷积网络（FCN，亦称为FCN-Basic）解码技术。FCN模型的重要设计元素是编码器特征图的降维步骤。这压缩了编码器的特征图，然后在相应的解码器中使用。编码器特征图的降维，例如一个有64通道的特征图，是通过使用<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="14.833ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 6556.3 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(722.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(1722.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(2444.7,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(3444.9,0)"><path data-c="36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"></path><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" transform="translate(500,0)"></path></g><g data-mml-node="mo" transform="translate(4667.1,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(5667.3,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g></g></svg></mjx-container></span>的可训练滤波器进行卷积完成的，其中<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.011ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 889 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g></g></svg></mjx-container></span>是类别的数量。压缩的<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.011ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 889 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g></g></svg></mjx-container></span>通道的最终编码器层特征图是解码器网络的输入。在这样一个解码器网络中，上采样是通过使用固定或可训练的多通道上采样核进行反卷积完成的。我们将核的大小设置为<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="5.028ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 2222.4 688"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path></g><g data-mml-node="mo" transform="translate(722.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(1722.4,0)"><path data-c="38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path></g></g></g></svg></mjx-container></span>。这种上采样的方式也被称为反卷积。需要注意的是，与SegNet相比，多通道卷积使用可训练的解码器滤波器进行上采样以密集化特征图的操作是在上采样之后完成的。FCN中上采样的特征图有<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.011ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 889 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g></g></svg></mjx-container></span>个通道。然后它被逐元素地加到相应的分辨率编码器特征图上，以产生输出解码器特征图。上采样核是用双线性插值权重初始化的【2】。</p>
<p>FCN解码器模型在推理期间需要存储编码器特征图，这对嵌入式应用来说可能会很占内存；例如，存储FCN-Basic的第一层64个特征图，在<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="9.553ex" height="1.581ex" role="img" focusable="false" viewBox="0 -677 4222.4 699"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z" transform="translate(500,0)"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1000,0)"></path></g><g data-mml-node="mo" transform="translate(1722.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(2722.4,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" transform="translate(500,0)"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1000,0)"></path></g></g></g></svg></mjx-container></span>分辨率下以32位浮点精度存储大约需要11MB。这可以通过对编码器特征图进行降维来减小，降维到11个特征图大约需要1.9MB的存储空间。另一方面，SegNet几乎不需要存储成本，对于池化指标的存储成本非常小（如果使用2位每个<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="5.028ex" height="1.507ex" role="img" focusable="false" viewBox="0 -666 2222.4 666"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mo" transform="translate(722.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(1722.4,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></svg></mjx-container></span>池化窗口的话，大约为0.17MB）。我们还可以创建一个FCN-Basic的变种，该变种舍弃了编码器特征图加法步骤，只学习上采样核（FCN-Basic-NoAddition）。</p>
<p>除了上述变种外，我们研究了使用固定双线性插值权重进行上采样，因此不需要学习上采样（双线性插值）。在另一个极端，我们可以将每一层的64个编码器特征图添加到SegNet解码器的相应输出特征图中，创建一个更占用内存的SegNet变种（SegNet-Basic-EncoderAddition）。在这里，池化指标被用于上采样，然后进行卷积步骤以密集其稀疏输入。然后这个被逐元素加到相应的编码器特征图上，以产生解码器输出。</p>
<p>另一个更占内存的FCN-Basic变种（FCN-Basic-NoDimReduction）是不对编码器特征图进行降维。这意味着与FCN-Basic不同的是，最终的编码器特征图在传递给解码器网络之前没有被压缩到<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.011ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 889 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g></g></svg></mjx-container></span>个通道。因此，每个解码器结束时的通道数与相应的编码器相同（即64个）。</p>
<p>我们还尝试了其他一些通用变种，其中特征图仅通过复制【7】或使用固定的（而且稀疏的）索引数组进行上采样。这些相比上述变种的表现要差得多。一个在编码器网络中没有最大池化和子采样的变种（解码器是多余的）会消耗更多内存，收敛速度更慢，表现也更差。最后，请注意，为了鼓励我们结果的再现，我们发布了所有变种的Caffe实现。</p>
<h3 id="训练">3.2 训练</h3>
<p>我们使用CamVid道路场景数据集来对解码器变种的性能进行基准测试。该数据集较小，包括367张训练和233张测试的RGB图像（日间和黄昏场景），分辨率为360×480。挑战是要分割11个类别，如道路、建筑、汽车、行人、标志、电线杆、人行道等。我们对RGB输入执行局部对比度标准化【54】。</p>
<p>编码器和解码器的权重都是使用He等人【55】描述的技术初始化的。为了训练所有的变种，我们使用固定学习率0.1和动量0.9【17】的随机梯度下降（SGD），使用我们的Caffe实现对SegNet-Basic【56】进行训练，直到训练损失收敛。在每个epoch之前，训练集被洗牌，每个mini-batch（12张图像）按顺序被选取，确保每个epoch中每张图像只使用一次。我们选择在验证数据集上表现最好的模型。</p>
<p>我们使用交叉熵损失【2】作为训练网络的目标函数。损失是在一个mini-batch中所有像素上求和的。当训练集中每个类别的像素数量变化很大时（例如在CamVid数据集中，道路、天空和建筑的像素占主导），需要根据真实类别不同地权衡损失。这被称为类别平衡。我们使用中位数频率平衡【13】，其中损失函数中分配给一个类别的权重是整个训练集上计算的类别频率中位数除以类别频率。这意味着训练集中较大类别的权重小于1，而最小类别的权重最高。我们还尝试了不进行类别平衡或等价使用自然频率平衡来训练不同的变种。</p>
<h3 id="分析">3.3 分析</h3>
<p>为了比较不同解码器变种的定量性能，我们使用三个常用的性能衡量指标：全局准确率（G），它衡量数据集中被正确分类的像素的百分比；类别平均准确率（C），它是所有类别的预测准确率的平均值；以及均值交并比（mIoU），它是如在Pascal
VOC12挑战【21】中使用的所有类别的平均交并比。mIoU指标比类别平均准确率更严格，因为它惩罚假阳性预测。然而，mIoU指标并不是直接通过类别平衡的交叉熵损失优化的。</p>
<p>均值交并比（mIoU）指标，也称为杰卡德指数，是基准测试中最常用的。然而，Csurka等人【57】指出，这个指标并不总是与人类对高质量分割的定性评价（排名）相对应。他们通过示例表明，mIoU倾向于区域平滑性，并没有评估边界准确性，这一点也被FCN的作者最近提及【58】。因此他们建议，用基于伯克利轮廓匹配得分的边界测量来补充mIoU指标，该得分通常用于评估无监督图像分割质量【59】。Csurka等人【57】简单扩展了这一指标应用于语义分割，并表明结合使用轮廓准确性和mIoU指标与人类对分割输出的排名更为吻合。</p>
<p>关键的想法是计算一个语义轮廓得分来评估F1测量【59】，它涉及计算预测和真实类边界之间的精确度和召回率值，给定一个像素容忍距离。我们使用图像对角线的0.75%作为容忍距离。在地面真实测试图像中存在的每个类别的F1测量被平均，以产生一个图像F1测量。然后我们计算整个测试集平均值，即边界F1测量（BF），通过平均图像F1测量得到。</p>
<p>我们在每1000次迭代后测试每个架构变体，在CamVid验证集上进行优化，直到训练损失收敛。以12个图像的训练小批量大小，这相当于每33个周期（通过训练集）测试一次。我们选择了在验证集上评估全局准确率最高的迭代。我们在保留的CamVid测试集上报告了此时点的所有三种性能指标。虽然我们在训练变体时使用了类别平衡，但仍然很重要的是要实现高的全局准确率，以结果整体上平滑的分割。另一个原因是分割对自动驾驶的贡献主要是用于划分如道路、建筑、人行道、天空等类别。这些类别在图像中占据了大多数像素，高全局准确率对应于这些重要类别的良好分割。我们还观察到，当类别平均值最高时报告数值性能往往对应于低全局准确率，这表明了感知上嘈杂的分割输出。</p>
<p>在表1中，我们报告了我们分析的数值结果。我们还展示了可训练参数的大小和最高分辨率特征图或池化指标存储内存，即第一层特征图在最大池化和子采样后。我们展示了使用我们的Caffe实现在NVIDIA
Titan GPU上使用cuDNN
v3加速的360×480输入进行一次前向传播的平均时间，平均是基于50次测量得出的。我们注意到SegNet变体中的上采样层没有使用cuDNN加速进行优化。我们展示了所选迭代中所有变体在测试和训练中的结果，这些结果也被列在没有类别平衡（自然频率）的情况下。下面我们分析了带有类别平衡的结果。</p>
<figure>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/segnet-t1.png" alt="image-20240322155342818">
<figcaption aria-hidden="true">image-20240322155342818</figcaption>
</figure>
<p align="center">
TABLE 1
对解码器变体的比较。我们使用全局（G）、类别平均（C）、交并比平均值（mIoU）和语义轮廓度量（BF）对性能进行量化。测试和训练的准确率以百分比形式展示，包括自然频率和中位数频率平衡的训练损失函数。SegNet-Basic的性能与FCN-Basic相当，但仅需存储最大池化索引，因此在推理期间更加内存高效。请注意，所报告的理论内存要求仅基于第一层编码器特征图的大小。FCN-Basic、SegNet-Basic、SegNet-Basic-EncoderAddition都有高BF得分，这表明使用编码器特征图中的信息对于更好的类别轮廓划分是必需的。具有较大解码器和全面使用编码器特征图的网络性能最好，尽管它们在推理时间和内存效率方面是最低的。
</p>
<p>在表1中，我们报告了我们分析的数值结果。我们还展示了可训练参数的大小和最高分辨率特征图或池化指标存储内存，即，第一层特征图经过最大池化和子采样后的情况。我们展示了使用我们的Caffe实现在NVIDIA
Titan GPU上使用cuDNN
v3加速的360×480输入进行一次前向传播的平均时间，这个平均值是基于50次测量得出的。我们注意到SegNet变种中的上采样层并没有使用cuDNN加速进行优化。我们展示了在选定迭代中所有变种的测试和训练结果，这些结果也在没有类别平衡（自然频率）的情况下列出。下面我们将分析带有类别平衡的结果。</p>
<p>从表1中，我们看到，基于双线性插值的上采样在所有准确性度量上表现最差。所有其他方法，无论是用于上采样的学习（FCN-Basic及其变体）还是上采样后学习解码器滤波器（SegNet-Basic及其变体），都表现得明显更好。这强调了学习分割解码器的必要性。这也得到了其他作者在比较FCN与SegNet类型解码技术时收集的实验证据的支持【4】。</p>
<p>当我们比较SegNet-Basic和FCN-Basic时，我们看到它们在这项测试中的所有准确性度量上表现得同样好。不同的是，SegNet在推理期间使用的内存更少，因为它只存储最大池化指标。另一方面，FCN-Basic存储了完整的编码器特征图，这消耗了更多的内存（多11倍）。SegNet-Basic在每个解码器层都有64个特征图。相比之下，FCN-Basic使用了降维，解码器层的特征图更少（11个）。这减少了解码器网络中的卷积数量，因此FCN-Basic在推理（前向传播）过程中更快。从另一个角度来看，SegNet-Basic的解码器网络总体上比FCN-Basic大。这赋予了它更多的灵活性，因此在相同迭代次数下实现了比FCN-Basic更高的训练准确性。总体来看，我们看到当推理时内存受限但可以在某种程度上妥协时，SegNet-Basic在推理时间内存上有优势。</p>
<p>SegNet-Basic在解码器方面与FCN-Basic-NoAddition最为相似，尽管SegNet的解码器更大。两者都通过学习执行反卷积（如在FCN-Basic-NoAddition中）或首先上采样然后通过训练有素的解码器滤波器进行卷积，来生成密集的特征图。由于其更大的解码器尺寸，SegNet-Basic的性能更为优越。FCN-Basic-NoAddition的准确率也低于FCN-Basic。这表明捕获编码器特征图中的信息对于提高性能至关重要。特别是，请注意这两个变种之间的BF（边界F1度量）测量值大幅下降，这也可以解释为什么SegNet-Basic比FCN-Basic-NoAddition性能更好的部分原因。</p>
<p>FCN-Basic-NoAddition-NoDimReduction模型的大小略大于SegNet-Basic，因为最终的编码器特征图并未压缩以匹配类别数<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.011ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 889 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g></g></svg></mjx-container></span>。这使得就模型大小而言，它与SegNet-Basic相当。这个FCN变种的性能不如SegNet-Basic，其训练准确性在相同训练周期数下也较低。这显示出使用更大的解码器还不够，还需要捕捉编码器特征图信息以更好地学习，特别是细粒度的轮廓信息（注意BF测量的下降）。此外，也很有趣地看到，与更大的模型如FCN-Basic-NoDimReduction相比，SegNet-Basic具有竞争力的训练准确性。</p>
<p>另一个有趣的比较是FCN-Basic-NoAddition与SegNet-Basic-SingleChannelDecoder之间的比较，表明使用最大池化指数进行上采样和整体较大的解码器带来了更好的性能。这也证明了SegNet是一个优秀的分割架构，特别是在需要在存储成本、准确性与推理时间之间找到折衷时。在最佳情况下，当内存和推理时间不受限制时，如FCN-Basic-NoDimReduction和SegNet-EncoderAddition等较大模型的性能都更优。</p>
<p>最后两列的表1显示了没有类别平衡（自然频率）时的结果。在这里，我们观察到对所有变种的结果都较差，特别是对于类别平均准确率和mIoU指标。由于场景的大部分是由天空、道路和建筑像素占主导，所以不加权时全局准确性是最高的。除此之外，从对各种变种的比较分析中得出的所有推断都适用于自然频率平衡，包括BF测量的趋势。SegNet-Basic的表现与FCN-Basic一样好，并且比更大的FCN-Basic-NoAddition-NoDimReduction表现得更好。但较大但效率较低的模型，如FCN-Basic-NoDimReduction和SegNet-EncoderAddition的表现比其他变种好。</p>
<p>我们现在可以用以下几点总结上述分析： 1)
当编码器特征图完整存储时，性能最好。这在语义轮廓划分度量（BF）中反映得最清晰。
2)
如果在推理期间内存受限，则可以存储和使用压缩形式的编码器特征图（维度减少、最大池化索引），并与合适的解码器（例如SegNet类型）一起使用，以提高性能。
3) 更大的解码器会提升给定编码器网络的性能。</p>
<h2 id="基准测试">4 基准测试</h2>
<p>我们使用我们的Caffe实现量化SegNet在两个场景分割基准测试中的性能。第一个任务是当前实际感兴趣的道路场景分割，它与各种自动驾驶相关问题有关。第二个任务是室内场景分割，它立即引起了几个增强现实（AR）应用的兴趣。两个任务的输入RGB图像分辨率均为360×480。</p>
<p>我们将SegNet与其他一些广泛采用的深度架构进行了基准测试，例如FCN【2】、DeepLab-LargFOV【3】和DeconvNet【4】。我们的目标是理解当端到端在相同数据集上训练时这些架构的性能。为了实现端到端训练，我们在每个卷积层后添加了批量标准化层【51】。对于DeepLab-LargeFOV，我们将最大池化3的步幅从1改为1，以实现45×60的最终预测分辨率。我们限制了DeconvNet的全连接层特征大小为1024，以便使用与其他模型相同的批量大小进行训练。DeepLab-LargeFOV的作者也报告说，通过减小全连接层的尺寸，性能损失很小。</p>
<p>为了进行受控基准测试，我们使用了相同的SGD求解器【17】，学习率固定为<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="4.495ex" height="2.003ex" role="img" focusable="false" viewBox="0 -863.3 1986.7 885.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"></path></g><g data-mml-node="TeXAtom" transform="translate(1033,393.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(778,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g></g></g></g></g></svg></mjx-container></span>​​，动量为0.9。优化在数据集上进行了超过100个epoch，直到不再观察到性能提升。为了防止所有模型中更深的卷积层过拟合，我们在所有模型中添加了0.5的dropout（参见http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html，例如caffe
prototxt）。对于有11个类别的道路场景，我们使用了5的mini-batch大小，对于有37个类别的室内场景，我们使用了4的mini-batch大小。</p>
<h3 id="道路场景分割">4.1 道路场景分割</h3>
<p>许多道路场景数据集可用于语义解析【22】【26】【60】【61】。我们选择使用CamVid数据集【22】来基准测试SegNet，因为它包含视频序列。这使我们能够将我们提出的架构与使用运动和结构【28】【29】【30】以及视频片段【33】的方法进行比较。我们还结合了【22】【26】【60】【61】形成一组3433张图像，以对SegNet进行额外的基准测试。对于道路场景分割的网页演示（见脚注3），我们将CamVid测试集的图像加入到这个更大的数据集中。这里，我们要指出，对SegNet和本文中使用的其他竞争架构在道路场景上进行了另一个最近的独立分割基准测试【62】。然而，这个基准测试没有受控制，意味着每种架构都是用不同的配方训练的，输入分辨率和有时验证集包括在内是变化的。因此，我们相信我们更受控的基准测试可以用来补充他们的工作。</p>
<p>SegNet预测与其他深度架构的定性比较可以在图4中看到。定性结果展示了所提出架构在道路场景中对较小类别进行分割的能力，同时生成整体场景的平滑分割。确实，在受控的基准设置下，SegNet表现出优越的性能，与一些较大模型相比。DeepLab-LargeFOV是最高效的模型，通过CRF后处理可以产生竞争性结果，尽管较小的类别丢失了。带有学习反卷积的FCN明显优于固定双线性上采样。DeconvNet是最大的模型，也是训练最低效的。其预测不保留小类别。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/segnet-f4.png"></p>
<p align="center">
Fig. 4.
在CamVid白天和黄昏测试样本上的结果。SegNet显示出优越的性能，特别是在边界划分能力上，与其他在受控环境下训练的较大模型相比较。DeepLab-LargeFOV是最高效的模型，并且使用CRF后处理可以产生竞争性的结果，尽管丢失了较小的类别。FCN通过学习反卷积明显更好。DeconvNet是训练时间最长的最大模型，但其预测丢失了小类别。请注意，这些结果对应于表3中mIoU准确率最高的模型。
</p>
<p>我们还使用这个基准首先将SegNet与包括随机森林[27]、[29]结合CRF基方法的几个非深度学习方法进行比较。这样做是为了给用户提供一个观点，了解使用深度网络相比于传统特征工程技术取得的准确性提高。</p>
<p>表2的结果显示SegNet-Basic、SegNet获得了竞争性结果，与使用CRF的方法相比。这展示了深度架构提取输入图像中有意义的特征并将其映射到准确和平滑的类别分割标签的能力。最有趣的结果是，在使用大型训练数据集（结合[22]、[26]、[60]、[61]）训练时，在类平均和mIOU指标上获得的巨大性能提升。相应地，SegNet的定性结果（见图4）明显优于其他方法。它能够很好地分割小类和大类。这里我们注意到，在训练SegNet-Basic和SegNet时使用了中位频率类平衡[50]。此外，分割的质量总体平滑，很像通常与CRF后处理获得的效果。尽管使用较大训练集获得的结果提高并不奇怪，但使用预训练的编码器网络及其训练集获得的百分比提高表明，这种架构有潜力用于实际应用。我们在互联网上随机测试城市和高速公路图像（见图1）表明SegNet可以吸收大型训练集，并且能够很好地泛化到未见过的图像。它还表明，当足够数量的训练数据可用时，先验（CRF）的贡献可以减少。</p>
<figure>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="C:/Users/17145/AppData/Roaming/Typora/typora-user-images/image-20240322155627073.png" alt="image-20240322155627073">
<figcaption aria-hidden="true">image-20240322155627073</figcaption>
</figure>
<p align="center">
TABLE 2 对SegNet与传统方法在CamVid
11类道路场景分割问题上的定量比较【22】。SegNet在大多数类别上优于所有其他方法，包括那些使用深度信息、视频和/或CRF的方法。与基于CRF的方法相比，SegNet在11个类别中有8个更准确。当在一个大型数据集（含3500张图片）上训练时，它还显示出约10%的类平均准确率改善。特别值得注意的是，对于较小/较薄的类别，准确率有显著提高。*请注意，我们无法获取旧方法的预测数据来计算mIoU和BF指标。
</p>
<p>在表3中，我们比较了SegNet与当前广泛采用的全卷积架构用于分割的性能。与表2中的实验相比，我们没有对包括SegNet在内的任何深度架构使用类平衡来进行训练。这是因为我们发现用中位频率平衡训练较大的模型（如DeconvNet）很困难。我们在4万、8万和超过8万次迭代的基准性能中给出了报告，这相当于大约50、100和超过100个时期。对于最后的测试点，我们还报告了迭代的最大数量（至少150个时期），超过这个数量，我们观察到没有准确性的改善或者出现过拟合。我们在训练阶段报告三个阶段的指标，以揭示指标如何随训练时间变化，特别是对于较大的网络。这很重要，因为它有助于了解在准确性提高时是否需要额外的训练时间。也请注意，对于每次评估，我们都完整地遍历了数据集以获得批量归一化统计数据，然后用这个统计数据评估了测试模型（请参见http://mi.eng.cam.ac.uk/projects/segnet/tutorial.html获取代码）。这些评估在大训练集上执行代价很高，因此我们只在训练阶段报告三个时间点的指标。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/segnet-t3.png"></p>
<p align="center">
TABLE 3
在没有类平衡的情况下，对CamVid测试集上的语义分割深度网络进行定量比较，当使用相同和固定的学习速率进行端到端训练时，像SegNet这样的小型网络能够在更短的时间内学会更好的性能。BF分数衡量了类间边界划分的准确性，对于SegNet和DeconvNet来说，与其他竞争模型相比显著更高。DeconvNet在指标上与SegNet相当，但其计算成本要高得多。另请参见表2，了解SegNet的各个类别准确率。
</p>
<p>从表3中我们立即看到，与其他模型相比，SegNet、DeconvNet在所有指标上均达到最高分。DeconvNet在边界描绘精度上有更高的分数，但SegNet在计算效率上比DeconvNet更高。这可以从表6中的计算统计数据看出。全连接层（转换为卷积层）的FCN、DeconvNet训练更慢，并且与SegNet相比，有相当或更高的正反向传播时间。在这里，我们也注意到，在这些较大的模型中，过拟合并不是问题，因为在与SegNet相当的迭代次数中，他们的指标呈增长趋势。</p>
<p>对于FCN模型，学习去卷积层而不是用双线性插值权重固定它们，特别是BF分数，也在远远较少的时间内实现了更高的指标。这一事实与我们在3.3节的分析一致。令人惊讶的是，DeepLab-LargeFOV经过训练，可以在45×60的分辨率上预测标签，产生有竞争力的性能，尽管它在参数化方面是最小的模型，并且根据表6，也有最快的训练时间。然而，边界精度较差，这一点与其他架构共有。DeconvNet的BF得分高于其他网络，特别是在训练了很长时间之后。鉴于我们在3.3节的分析和它与SegNet类型架构共享的事实。</p>
<p>密集CRF[63]后处理的影响可以在DeepLab-LargeFOV-denseCRF的最后一个时间点看到。全局和mIOU提高，但类平均减少。然而，BF分数得到了很大的提高。请注意，密集CRF超参数是通过在训练集的子集上进行昂贵的网格搜索过程获得的，因为没有可用的验证集。</p>
<p>令人惊讶的是，经过训练以在45×60的分辨率下预测标签的DeepLab-LargeFOV产生了有竞争力的性能，鉴于其在参数化方面是最小的模型，并且根据表6，也具有最快的训练时间。然而，边界精度较差，这一点由其他架构共享。DeconvNet的BF分数高于其他网络，特别是在训练了很长时间之后。鉴于我们在3.3节的分析和它共享SegNet类型架构的事实。</p>
<p>密集CRF[63]后处理的影响可以在DeepLab-LargeFOV-denseCRF的最后一个时间点看到。全局和mIOU得分都有所提高，但类平均得分有所降低。然而，BF分数得到了较大的提升。请注意，这些密集CRF超参数是通过在训练集的一个子集上进行昂贵的网格搜索过程获得的，因为没有可用的验证集。</p>
<h3 id="sun-rgb-d室内场景">4.2 SUN RGB-D室内场景</h3>
<p>SUN
RGB-D[23]是一个非常具有挑战性且庞大的室内场景数据集，包含5285张训练图像和5050张测试图像。这些图像由不同的传感器捕获，因此具有不同的分辨率。任务是对37个室内场景类别进行分割，包括墙壁、地板、天花板、桌子、椅子、沙发等。这项任务之所以困难，是因为对象类别呈现出不同的形状、大小和不同的姿态。由于经常出现部分遮挡，因为每个测试图像中通常存在许多不同类别，这些因素使得它成为最难的分割挑战之一。我们只使用RGB模式进行我们的训练和测试。使用深度模式会需要架构上的改动/重新设计[2]。此外，目前的摄像头质量需要小心后处理以填补遗漏的测量值。它们可能还需要使用多帧的融合来健壮地提取分割特征。因此，我们相信使用深度进行分割值得单独进行研究，这不在本文的范围内。我们还注意到，早期的基准数据集NYUv2[25]被包括为这个数据集的一部分。</p>
<p>道路场景图像在感兴趣类别和它们的空间排列上变化有限。当从移动车辆中拍摄时，相机位置几乎总是与道路表面平行，这限制了视点的变化。这使得深度网络更容易学习如何鲁棒地对它们进行分割。相比之下，室内场景图像更为复杂，因为视点可以有很大的变化，并且场景和空间排列的类别数量上缺乏规律性。另一个困难是由场景中对象类别大小的广泛变化引起的。最近SUN
RGB-D数据集【23】的一些测试样本展示在图5中。我们观察到一些场景有几个大类，而其他一些场景则有密集的杂乱物（底部行和右侧）。外观（纹理和形状）在室内场景中也可以有很大的变化。因此，我们相信这是计算机视觉中分割架构和方法最难的挑战。其他挑战，如Pascal
VOC12【21】中的显著对象分割已经占据了研究人员更多的时间【66】，但我们认为室内场景分割更具挑战性，并且在当前实际应用中更为广泛，例如在增强现实和机器人技术中。为了鼓励在这个方向上进行更多研究，我们推荐使用已知的深度架构在大型SUN
RGB-D数据集上进行研究。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/segnet-f5.png"></p>
<p align="center">
Fig. 5 对最近发布的SUN
RGB-D数据集中RGB室内测试场景的SegNet预测进行定性评估【23】。在这个艰难的挑战中，SegNet预测很好地划分了不同类别的边界，适用于各种场景和视角下的物体类别。总体来说，当物体类别大小合理时，分割质量更好，但当场景更加杂乱时，噪声很多。请注意，场景图像的某些部分没有真实标签，这些部分显示为黑色。在所示的相应深度模型预测中，这些部分并未被掩盖。请注意，这些结果对应于表4中mIoU准确率最高的模型。
</p>
<p>关于室内场景的不同类型如卧室、客厅、实验室、会议室、浴室等样本的SegNet定性结果显示在图5中。我们看到，当类别大小在不同视角下较大时，SegNet获得了合理的预测。这特别有趣，因为输入模态仅为RGB。RGB图像也有助于分割较薄的结构，如椅子和桌子的腿、灯，这些使用当前可用的深度图像传感器难以实现。这可以从图5中的SegNet、DeconvNet结果中看出。它对于分割墙上的装饰物品，如AR任务中的画作也很有用。然而与室外场景相比，分割质量明显更为嘈杂。当杂物增加时，质量显著下降（见中间列的结果样本）。</p>
<p>表4中的定量结果表明，所有深度架构的平均交并比（mIoU）和边界度量都很低。全局和类别平均值（与mIoU相关）也较小。SegNet在G、C、BF度量上胜过所有其他方法，并且其mIoU略低于DeepLab-LargeFOV。在单独实验中，我们使用中位数频率类平衡方法[67]训练了SegNet，度量结果较高（见表4），这与我们在第3.3节的分析一致。有趣的是，使用网格搜索找到的最佳超参数对dense-CRF进行设置，除了DeepLab-LargeFOV的BF得分外，其余都变差了。也许能找到更优的设置，但是由于dense-CRF的大量推理时间，网格搜索过程代价太高。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/segnet-t4.png"></p>
<p align="center">
TABLE 4
在一组5250张室内场景上，对SUNRGB-D数据集上的深度架构进行定量比较。请注意，这些实验中只使用了RGB模态。在这个包含37个类别的复杂任务中，所有架构都表现不佳，特别是由于类别尺寸较小和类别分布的偏斜。DeepLab-LargeFOV，作为最小且最高效的模型，其mIoU略高，但SegNet在G、C、BF分数上表现更好。另请注意，当SegNet采用中值频率类平衡训练时，它的指标达到了71.75、44.85、32.08、14.06（180K）。
</p>
<p>整体性能不佳的一个原因是，这项分割任务中的类别数目很多，其中许多类别只占图像的一小部分且出现频率低。表5清楚地显示，较大的类别有合理的准确率，而较小的类别准确率较低。通过使用更大的数据集和考虑类别分布的训练技术，可以改善这一点。另一个导致性能不佳的原因可能是这些深度架构（都基于VGG架构[6]）无法处理室内场景中的大变异性。我们的这一猜想是基于这样一个事实：最小模型DeepLab-LargeFOV在mIoU上产生了最佳准确率，相比之下，DeconvNet中较大的参数化并没有即使在更长时间的训练（DeconvNet）后提高性能。这表明可能有一个共同的原因导致所有架构的性能不佳。需要更有控制的数据集[68]来验证这一假设。</p>
<figure>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/segnet-t5.png" alt="image-20240322160347380">
<figcaption aria-hidden="true">image-20240322160347380</figcaption>
</figure>
<p align="center">
TABLE 5 在SUN
RGB-D基准数据集中，SegNet对37个室内场景类别的预测的类平均准确率。性能与室内场景中类别的大小高度相关。请注意，类平均准确率与mIoU指标有很强的相关性。
</p>
<h2 id="讨论和未来工作">5 讨论和未来工作</h2>
<p>深度学习模型往往因为有大量数据集的可用性和模型深度及参数化的扩展而取得了日益增加的成功。然而，在实际中，如内存和在训练及测试期间的计算时间这样的因素在从大量模型库中选择模型时是重要的考虑因素。当性能提升不与增加的训练时间相当时，训练时间就成为一个重要的考虑因素，就像我们的实验所示。测试时间内存和计算负载对于在专用嵌入式设备上部署模型很重要，例如，在AR应用中。从整体效率的角度来看，我们认为对于更小、更节省内存、时间效率更高的模型关注不够，这些模型用于实时应用，如道路场景理解和AR。这是我们提出SegNet的主要动机，它显著小且快于其他竞争架构，但我们已经证明它对于任务如道路场景理解是高效的。</p>
<p>分割挑战如Pascal[21]和MS-COCO[42]是对象分割挑战，在这里少数几个类别存在于任何测试图像中。场景分割更具挑战性，因为室内场景的高变异性和需要同时分割大量类别。室外和室内场景分割的任务也更加实际导向，与当前应用如自动驾驶、机器人学和AR紧密相关。我们选择的度量标准用于对各种深度分割架构基准测试，例如边界F1-度量(BF)，是为了补充现有的更偏向于区域准确率的度量标准。从我们的实验和其他独立基准[62]可以清楚看出，从移动汽车捕获的室外场景图像更容易分割，深度架构执行得更稳健。我们希望我们的实验会鼓励研究人员将他们的注意力转向更具挑战性的室内场景分割任务。</p>
<p>我们选择用于对各种深度分割架构进行基准测试的度量指标，如边界F1-度量（BF），是为了补充现有的更倾向于区域精度的度量指标。从我们的实验和其他独立基准[62]来看，从移动汽车中捕获的室外场景图像更容易分割，并且深度架构表现得更为健壮。我们希望我们的实验能鼓励研究者将他们的注意力集中在更具挑战性的室内场景分割任务上。</p>
<p>我们在基准测试不同深度架构的参数化时必须做出的一个重要选择是它们的训练方式。许多这样的架构使用了一系列支持技术和多阶段训练秘籍来在数据集上达到高准确率，但这使得收集它们在时间和内存约束下的真实性能证据变得困难。相反，我们选择进行受控基准测试，在此我们使用批量归一化来实现与相同求解器(SGD)的端到端训练。然而，我们注意到这种方法不能完全解开模型与求解器(优化)在实现特定结果中的效果。这主要是因为训练这些网络涉及到梯度反向传播，这是不完美的，并且优化是一个在极其大的维度中非凸问题。认识到这些不足，我们的希望是这种受控分析补充了其他基准[62]，并揭示了在不同著名架构中涉及的实际权衡。</p>
<p>对于未来，我们希望利用我们从分析中获得的对分割架构的理解，来设计更高效的架构，用于实时应用。我们也对估算深度分割架构预测的模型不确定性感兴趣。</p>
<h2 id="结论">6 结论</h2>
<p>我们介绍了SegNet，一种用于语义分割的深度卷积网络架构。设计SegNet的主要动机是需要一个高效的架构来理解道路和室内场景，这个架构在内存和计算时间上都是高效的。我们分析了SegNet并将其与其他重要的变体进行了比较，以揭示在设计分割架构时涉及的实际权衡，特别是训练时间、内存与准确性的权衡。那些在推理时消耗更多内存但存储了编码网络特征图全集的架构表现最佳。另一方面，SegNet更加高效，因为它仅存储最大池化指数的特征图，并在其解码器网络中使用它们以实现良好的性能。在大型且知名的数据集上，SegNet的表现具有竞争力，为道路场景理解取得了高分。深度分割架构的端到端学习是一个更艰巨的挑战，我们希望看到更多的注意力被支付到这个重要问题上。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://www.chitose.cn">Chitose</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://www.chitose.cn/segnet-paper/">https://www.chitose.cn/segnet-paper/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.chitose.cn" target="_blank">Chitose-Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_2.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/linux/" title="linux"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_10.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">linux</div></div></a></div><div class="next-post pull-right"><a href="/deeplabv3-paper/" title="deeplabv3-paper"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">deeplabv3-paper</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Face.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Chitose</div><div class="author-info__description">Hahaha</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">103</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/chitose-r"><i class="fab fa-github"></i><span>🛴/前往小家..</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/chitose-r" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:171450290@qq.com" target="_blank" title="Email"><i class="fa-solid fa-square-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="/qq/" target="_blank" title="QQ"><i class="fab fa-qq" style="color: #12b7f5;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#segnet-a-deep-convolutional-encoder-decoder-architecture-for-image-segmentation"><span class="toc-text">SegNet:
A Deep Convolutional Encoder-Decoder Architecture for Image
Segmentation</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#segnet%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E5%9B%BE%E5%83%8F%E7%9A%84%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84%E5%88%86%E5%89%B2"><span class="toc-text">SegNet:深度卷积图像的编码器-解码器架构分割</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-text">1 引言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E7%8C%AE%E7%BB%BC%E8%BF%B0"><span class="toc-text">2 文献综述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84"><span class="toc-text">3 架构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E7%A0%81%E5%99%A8%E5%8F%98%E7%A7%8D"><span class="toc-text">3.1 解码器变种</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-text">3.2 训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E6%9E%90"><span class="toc-text">3.3 分析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95"><span class="toc-text">4 基准测试</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%81%93%E8%B7%AF%E5%9C%BA%E6%99%AF%E5%88%86%E5%89%B2"><span class="toc-text">4.1 道路场景分割</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sun-rgb-d%E5%AE%A4%E5%86%85%E5%9C%BA%E6%99%AF"><span class="toc-text">4.2 SUN RGB-D室内场景</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A8%E8%AE%BA%E5%92%8C%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C"><span class="toc-text">5 讨论和未来工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-text">6 结论</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By Chitose</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"></div><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script defer src="/js/cursor.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Python/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🥩 Python (30)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/C/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🕶️ C (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Embedded/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">💳 Embedded (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Pytorch/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📯 Pytorch (9)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Paper/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📰 Paper (30)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/others/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🤡 others (17)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/segmentation/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🔪 segmentation (4)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item" style="visibility: hidden"></div><div class="magnet_item" style="visibility: hidden"></div><a class="magnet_link_more"  href="https://www.chitose.cn/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(33.333333333333336% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>
  function butterfly_clock_anzhiyu_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock_anzhiyu')
    if(parent_div_git) {
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var qweather_key = '6be604177b8a4c3e97c78a352ee324f7';
  var gaud_map_key = '17b299fafade134736e6a1d4acb5ef18';
  var baidu_ak_key = 'undefined';
  var flag = 0;
  var clock_rectangle = '113.34532,23.15624';
  var clock_default_rectangle_enable = 'false';

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_anzhiyu_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_anzhiyu_injector_config();
  }
  </script><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script data-pjax src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.js"></script><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v6.2.0" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly_v4.3.1" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" data-title="本站采用多线部署，主线路托管于Vercel" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-Vercel-brightgreen?style=flat&amp;logo=Vercel" alt=""/></a><a class="github-badge" target="_blank" href="https://dashboard.4everland.org/" style="margin-inline:5px" data-title="本站采用多线部署，备用线路托管于4EVERLAND" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-4EVERLAND-22DDDD?style=flat&amp;logo=IPFS" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="/js/runtime.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023-12-17-2/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_10.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023-12-17-2/" alt="">第二篇文章</a><div class="blog-slider__text">这是第二篇文章</div><a class="blog-slider__button" href="2023-12-17-2/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Pytorch-6/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_4.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-19</span><a class="blog-slider__title" href="Pytorch-6/" alt="">Pytorch(6)-张量可微性</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="Pytorch-6/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023-12-17-3/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_7.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023-12-17-3/" alt="">第三篇文章</a><div class="blog-slider__text">这是第三篇文章</div><a class="blog-slider__button" href="2023-12-17-3/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body></html>