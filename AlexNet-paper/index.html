<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>AlexNet-paper | Chitose-Blog</title><meta name="author" content="Chitose"><meta name="copyright" content="Chitose"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="ImageNet Classification with Deep Convolutional Neural Networks  基于深度卷积的ImageNet分类神经网络 2012  摘要 我们训练了一个庞大的深层卷积神经网络，将ImageNet LSVRC-2010比赛中的120万张高分辨率图像分为1000个不同的类别。在测试数据上，我们取得了37.5％和17.0％的前1和前5">
<meta property="og:type" content="article">
<meta property="og:title" content="AlexNet-paper">
<meta property="og:url" content="https://www.chitose.cn/AlexNet-paper/index.html">
<meta property="og:site_name" content="Chitose-Blog">
<meta property="og:description" content="ImageNet Classification with Deep Convolutional Neural Networks  基于深度卷积的ImageNet分类神经网络 2012  摘要 我们训练了一个庞大的深层卷积神经网络，将ImageNet LSVRC-2010比赛中的120万张高分辨率图像分为1000个不同的类别。在测试数据上，我们取得了37.5％和17.0％的前1和前5">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_10.png">
<meta property="article:published_time" content="2024-02-13T22:46:50.000Z">
<meta property="article:modified_time" content="2024-02-13T22:46:50.000Z">
<meta property="article:author" content="Chitose">
<meta property="article:tag" content="演示">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_10.png"><link rel="shortcut icon" href="https://www.fomal.cc/favicon.ico"><link rel="canonical" href="https://www.chitose.cn/AlexNet-paper/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体中文","cht_to_chs":"你已切换为简体中文","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'AlexNet-paper',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-02-14 06:46:50'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.css" /><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<link rel="stylesheet" href="https://www.fomal.cc/static/css/runtime.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.0.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Face.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">77</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/todolist/"><i class="fa-fw fas fa-link"></i><span> 计划</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_10.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Chitose-Blog"><span class="site-name">Chitose-Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 列表</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-envelope-open"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/todolist/"><i class="fa-fw fas fa-link"></i><span> 计划</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">AlexNet-paper</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-02-13T22:46:50.000Z" title="发表于 2024-02-14 06:46:50">2024-02-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-02-13T22:46:50.000Z" title="更新于 2024-02-14 06:46:50">2024-02-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Paper/">Paper</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">10.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>33分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="AlexNet-paper"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1
id="imagenet-classification-with-deep-convolutional-neural-networks">ImageNet
Classification with Deep Convolutional Neural Networks</h1>
<blockquote>
<h1
id="基于深度卷积的imagenet分类神经网络-2012">基于深度卷积的ImageNet分类神经网络
2012</h1>
</blockquote>
<h2 id="摘要">摘要</h2>
<p>我们训练了一个庞大的深层卷积神经网络，将ImageNet
LSVRC-2010比赛中的120万张高分辨率图像分为1000个不同的类别。在测试数据上，我们取得了37.5％和17.0％的前1和前5的错误率，这比以前的先进水平要好得多。具有6000万个参数和650,000个神经元的神经网络由五个卷积层组成，其中一些随后是最大池化层，三个全连接层以及最后的1000个softmax输出。为了加快训练速度，我们使用非饱和神经元和能高效进行卷积运算的GPU实现。为了减少全连接层中的过拟合，我们采用了最近开发的称为“dropout”的正则化方法，该方法证明是非常有效的。我们还在ILSVRC-2012比赛中使用了这种模式的一个变种，取得了15.3％的前五名测试失误率，而第二名的成绩是26.2％。</p>
<blockquote>
<h3 id="主要内容">主要内容</h3>
<p>（1）表示了用了<strong>一个深度卷积神经网络</strong>来进行图片分类，取得了一个非常好的效果。</p>
<p>（2）深度卷积网络由<strong>60million个参数</strong>，<strong>65w个神经元</strong>，以及<strong>五个卷积层</strong>和<strong>三个全连接层</strong>，<strong>一个1000路的softmax层</strong>组成。</p>
<p>（3）为了加快训练，用到了<strong>非饱和激活函数ReLU和卷积运算的GPU</strong>加速实现。</p>
<p>（4）用了<strong>Dropout</strong>这个随机失活方法来减少完全联通层的过拟。</p>
</blockquote>
<h2 id="引言">1 引言</h2>
<p>当前目标识别的方法基本都使用了机器学习的方法。为了提高这些方法的性能，我们可以收集更大的数据集，学习得到更加强大的模型，然后使用更好的方法防止过拟合。直到现在，相比于成千上百的图像，带标签的图像数据集相对较小（如<strong>NORB</strong>[16]，<strong>Caltech-101/256</strong>[8,9]，以及<strong>CIFAR-10/100</strong>[12]）。这种规模的数据集能使得<strong>简单的识别任务得到很好地解决</strong>，特别是如果他们进行带标签的转换来增广数据集。例如，当前MINIST数字识别任务最小的错误率（&lt;0.3%
）已经接近人类水平[4]。但是<strong>现实世界中的目标呈现出相当大的变化性，因此学习去识别它们就必须要使用更大的训练数据集。</strong>事实上，人们也已广泛地认识到小图像数据集的缺点（如Pinto等[21]），但直到最近，收集包含数百万图像的带标签数据集才成为可能。新的更大的数据集包括由数十万张全分割图像的<strong>LabelMe</strong>[23]和包含超过22000类的1500万张带标签高分辨率图像<strong>ImageNet</strong>[6]组成。</p>
<p>为了从数以百万计的图像中学习出数千种的目标，我们需要一个<strong>具有很强学习能力的模型</strong>。然而，目标识别任务的巨大复杂性意味着，即使在ImageNet这样大的数据集也不能完成任务，因此我们的模型也要有许多先验知识来弥补所有我们没有的数据。卷积神经网络（CNNs）就形成了一种这样类别的模型[16,11,13,18,15,22,26]。可以通过改变网络的深度和广度控制CNN的学习能力，并且它们都能对图像的本质做出强大而又正确的判别（即统计的稳定性和像素位置的依赖性）。因此，相比于相似大小的标准前馈神经网络，CNNs的连接和参数更少，因此更易训练，尽管它们理论上的最优性能可能略差点。尽管CNNs具有一些新颖的特性，和更有效率的局部结构，但大规模地应用于高分辨率图像消耗资源仍然过多。幸运的是，如今<strong>GPU</strong>以及高度优化的<strong>二维卷积计算</strong>，已经足够强大地去帮助大规模CNNs的训练，并且最新的数据集如ImageNet包含足够多的带标签样本，能够训练出不会严重过拟合的模型。</p>
<p>本文具体贡献如下：基于ILSVRC-2010和ILSVRC-2012比赛中用到的ImageNet的子集本文训练出了<strong>至今为止一个最大的卷积神经网络[**2]并且得到了迄今基于这些数据集最好的结果。本文实现了一种</strong>高度优化的二维卷积的GPU运算<strong>以及</strong>卷积神经网络训练中所有其他运算<strong>，这些都已公开提供；本文网络中包含了</strong>大量的不常见和新的特征<strong>来提升网络性能，减少训练时间，详见第三节；即使有120万带标签的训练样本，网络的大小使得过拟合仍成为一个严重的问题，因此本文使用了</strong>许多有效的防止过拟合的技术<strong>，详见第四节；本文最终的网络包含</strong>五层卷积层和三层全连接层**，而这个深度似乎很重要：我们发现移除任何一层卷积层（每一层包含的参数个数不超过整个模型参数个数的1%）都会导致较差的结果。</p>
<p>最后，网络的大小主要受限于GPU的内存大小和我们愿意忍受的训练时间长度。本文的网络在两个GTX
580 3GB
GPU上训练了五到六天。本文所有的实验表明，如果有更快的GPU、更大的数据集，结果可以更好。</p>
<blockquote>
<h3 id="主要内容-1">主要内容</h3>
<p>（1）引出ImageNet这个数据集，介绍它很大很好。</p>
<p>（2）对于ImageNet可以采用<strong>CNN</strong>来作为本文的模型。</p>
<p>（3）介绍了本篇论文主要贡献。</p>
<blockquote>
<p>相关问题一：Labelme和ImageNet</p>
<ul>
<li>前者由数十万张<strong>完全分割的图</strong>像组成。</li>
<li>后者由<strong>超过22,000个类别</strong>的超过1500万张高分辨率标记图像组成。</li>
</ul>
</blockquote>
<h3 id="主要贡献">主要贡献</h3>
<p>（1）基于<strong>ILSVRC-2010</strong>和<strong>ILSVRC-2012</strong>比赛中用到的<strong>ImageNet</strong>的子集，本文训练出了至今为止一个最大的卷积神经网络并且得到了迄今基于这些数据集最好的结果。</p>
<p>（2）本文实现了<strong>一种高度优化的二维卷积的GPU运算</strong>以及<strong>卷积神经网络训练中所有其他运算</strong>，这些都已公开提供。</p>
<p>（3）本文网络中包含了<strong>大量的少见和新的特征</strong>来提升网络性能，<strong>减少训练时间</strong>。</p>
<p>（4）本文使用了许多有效的<strong>防止过拟合</strong>的技术。</p>
<p>（5）本文最终的网络包含<strong>五层卷积层和三层全连接层</strong>，而这个<strong>深度</strong>似乎很重要：我们发现移除任何一层卷积层（每一层包含的参数个数不超过整个模型参数个数的1%）都会导致较差的结果。</p>
</blockquote>
<h2 id="数据集">2 数据集</h2>
<p>ImageNet数据集包含有大概22000种类别共1500多万带标签的高分辨率图像。这些图像是从网络上收集得来，由亚马逊的Mechanical
Turkey的众包工具进行人工标记。从2010年开始，作为Pascal视觉目标挑战的一部分，ImageNet大规模视觉识别挑战（ImageNet
Large-Scale Visual Recognition
Challenge，ILSVRC）比赛每年都会举行。ILSVRC采用ImageNet的子集，共包含一千个类别，每个类别包含大约1000幅训练图像、50幅验证图像、150幅测试图像。总的来说，大约有120万张训练图像，5万张验证图像以及15万张测试图像。</p>
<p>ILSVRC-2010是ILSVRC唯一一个测试集标签公开的版本，因此这个版本就是本文大部分实验采用的数据集。由于我们也以我们的模型参加了ILSVRC-2012的比赛，在第6节本文也会列出在这个数据集上的结果，该测试集标签不可获取。ImageNet通常使用两种错误率：top-1和top-5，其中top-5错误率是指正确标签不在模型认为最有可能的前五个标签中的测试图像的百分数。</p>
<p>ImageNet包含不同分辨率的图像，但是本文的模型要求固定的输入维度。因此，本文将这些<strong>图像下采样为256x256</strong>
。<strong>给定一幅矩形图像，本文采用的方法是首先重新调整图像使得短边长度为256，然后裁剪出中央256x256
的区域。</strong>除了将图像减去训练集的均值图像(<strong>训练集和测试集都减去训练集的均值图像</strong>)，本文不做任何其他图像预处理。因此本文直接在每个像素的原始RGB值上进行训练。</p>
<blockquote>
<h3 id="主要内容-2">主要内容：</h3>
<p>（1）介绍了所使用到的ImageNet数据集</p>
<p>（2）介绍了本文所采用的图像处理的方法</p>
<p>（3）指明没有以任何方式做预处理比如抽取特征、抽取SIFT特征等等，直接将原始图片输入到神经网络模型，就能实现想要的功能。</p>
<h3 id="imagenet">ImageNet</h3>
<p><strong>ImageNet是一种数据集，而不是神经网络模型。</strong>斯坦福大学教授李飞飞为了解决机器学习中过拟合和泛化的问题而牵头构建的数据集。该数据集从2007年开始手机建立，直到2009年作为论文的形式在CVPR
2009上面发布。直到目前，该数据集仍然是深度学习领域中图像分类、检测、定位的最常用数据集之一。</p>
<h3 id="图像处理方法">图像处理方法：</h3>
<p>（1）ImageNet这个数据集不像其他数据集一样，它没有对数据进行裁剪。所以我们要先对数据集进行裁剪，裁剪为：**256*256**的尺寸大小。</p>
<p>（2）<strong>具体裁剪方法：</strong>先对原始图片进行缩放，将短边变成256的大小，另一个长边在这一步操作中也会根据长宽比进行调整，然后第二步从图片中心对长边进行两侧的裁剪，得到256*256的尺寸大小。</p>
</blockquote>
<h2 id="网络架构">3 网络架构</h2>
<p>图2概括了我们所提出网络的结构。它包含八个学习层——五个卷积层和三个全连接层。下面，我们将描述一些所提出网络<a
target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=框架&amp;spm=1001.2101.3001.7020">框架</a>中新颖或不寻常的地方。
3.1-3.4节按照重要顺序排序。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Alexnet-Figure2.png" /></p>
<blockquote>
<p>图2展示了我们卷积神经网络架构的示意图，明确显示了两个GPU之间职责的划分。一个GPU负责运行图示顶部的层部分，而另一个GPU运行底部的层部分。这两个GPU仅在某些层进行通信。网络的输入是150,528维，而网络剩余层中神经元的数量分别为253,440；186,624；64,896；64,896；43,264；4,096；4,096；以及1,000。</p>
</blockquote>
<h3 id="非线性激活函数relu">3.1 非线性激活函数ReLU</h3>
<p>通常使用一个关于输入的函数模拟神经元的输出,这种标准函数是或者。在梯度下降训练时间上，这些饱和的非线性函数比不饱和非线性函数更慢。根据Nair和Hinton[20]，本文将具有这种非线性特征的神经元称为修正线性单元（ReLUs:
Rectified Linear
Units）。使用ReLUs的深度卷积神经网络训练速度比同样情况下使用单元的速度快好几倍。图1表示使用特定的四层卷积网络在数据集CIFAR-10上达到25%错误率所需的迭代次数。这个图表明如果使用传统的饱和神经元模型我们不可能利用这么大规模的神经网络对本文工作进行试验。</p>
<p>本文不是第一个考虑在CNNs中寻找传统神经模型替代方案的。例如，Jarrett等[11]考虑使用非线性函数，在数据集Caltech-101上，与基于局部平均池化的对比归一化结合取得了很好地效果。但是，在这个数据集上他们主要关心的就是防止过拟合，而本文用ReLUs主要是对训练集的拟合进行加速。快速学习对由大规模数据集上训练出大模型的性能有相当大的影响。
<img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Alexnet-Figure1.png" /></p>
<blockquote>
<p>图1。在CIFAR-10数据集上，一个采用ReLU激活函数（实线表示）的四层卷积神经网络达到25%的训练误差率的速度，比采用双曲正切（tanh）激活函数的等效网络（虚线表示）快六倍。每个网络的学习率都被独立选择，以使训练尽可能快速进行。没有使用任何类型的正则化。虽然展示的效果大小随网络架构而变化，但采用ReLU激活函数的网络一致地比采用饱和神经元的等效网络学习速度快数倍。</p>
</blockquote>
<blockquote>
<h3 id="传统方法及不足">传统方法及不足</h3>
<p>Sigmoid
是常用的非线性的激活函数，它能够把输入的连续实值<strong>“压缩”到0和1之间</strong>。特别的，<strong>如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1</strong>。</p>
<p>作者把非线性激活函数（ReLU）用在了模型里，发现训练速度显著提高，原因在于传统用的是饱和非线性激活函数，例如tanh，训练时如果进入到饱和区域，那么会因为梯度变化过小而难以训练；而ReLU是一种非饱和非线性激活函数，接受阈是0~<img
src="https://latex.csdn.net/eq?%5Cinfty"
alt="" />，不存在tanh的问题。</p>
<h3 id="本文改进方法">本文改进方法</h3>
<ul>
<li>本文将具有这种非线性特征的神经元称为非线性激活函数（ReLUs: Rectified
Linear
Units）。Alex用ReLU代替了Sigmoid，发现使用ReLU得到的SGD的收敛速度会比
sigmoid或tanh 快很多。</li>
<li><strong>ReLU的有效性体现在两个方面：</strong> 1. 克服梯度消失的问题
；2.
提高训练速度。这两个方面是相辅相成的，因为克服了梯度消失问题，所以训练才会快。</li>
</ul>
<h3 id="本文的改进结果">本文的改进结果</h3>
<p><strong>本文用ReLUs主要是对训练集的拟合进行加速。</strong>快速学习对在数据集上训练的大模型的性能有很大影响。</p>
</blockquote>
<h3 id="用多个gpu训练">3.2 用多个GPU训练</h3>
<p>单个GTX 580
GPU只有3GB内存，这限制了可以在其上训练的网络的最大尺寸。事实证明，120万个训练样本足以训练那些因规模太大而不适合使用一个GPU训练的网络。因此，我们将网络分布在两个GPU上。目前的GPU很适合于跨GPU并行化操作，因为它们能够直接读写对方的内存，而无需通过主机内存。我们采用的并行化方案基本上将半个内核（或神经元）放在各个GPU上，另外还有一个技巧：GPU只在某些层间进行通信。这意味着，例如，第3层的内核从第2层的所有内核映射（kernel
maps）中获取输入。然而，第4层中的内核又仅从位于同一GPU上的第3层中的那些内核映射获取输入。选择连接模式对于交叉验证是一个不小的问题，但这使得我们能够精确调整通信量，直到它的计算量的达到可接受的程度。</p>
<p>由此产生的架构有点类似于Cire¸san等人使用的“柱状”CNN[5]，除了我们的每列不是独立的之外（见图2）。与一个GPU上训练的每个卷积层只有一半的内核数量的网络相比，该方案分别将我们的top-1和top-5错误率分别降低了1.7％和1.2％。双GPU网络的训练时间比单GPU网络更少。</p>
<blockquote>
<h3 id="传统方法及不足-1">传统方法及不足</h3>
<ul>
<li>单个GTX580
GPU只有3GB内存，这限制了可以在其上训练的网络的最大大小。</li>
<li>实验表明使用120万训练样本训练网络已足够，但是这个任务对一个GPU来说太大了。</li>
</ul>
<h3 id="本文改进方法-1">本文改进方法</h3>
<p><strong>方法：</strong>采用两个GPU</p>
<p><strong>原因：</strong>当前的GPU都能很方便地进行交叉GPU并行，因为它们可以直接相互读写内存，而不用经过主机内存。</p>
<h3 id="采用技巧">采用技巧</h3>
<p>1.在每一个GPU上放二分之一的核(或者神经元)。</p>
<p>2.只有某些层才能进行GPU之间的通信。</p>
<h3 id="本文改进结果">本文改进结果</h3>
<p><strong>（1）错误率下降：</strong>与在一个GPU上训练的网络相比，这种组合让本文的top-1和top-5错误率分别下降了1.7%和1.2%。</p>
<p><strong>（2）训练时间少：</strong>本文的两个GPU网络训练时间比一个GPU的时间要略少。</p>
</blockquote>
<h3 id="局部归一化">3.3 局部归一化</h3>
<p>ReLU具有理想的属性，它们不需要对输入进行归一化来防止它们饱和。如果至少有一些训练实例为ReLU产生了正的输入，那么这个神经元就会学习。然而，我们还是发现下面的这种归一化方法有助于泛化。设表示第个内核计算位置的ReLU非线性单元的输出，而响应归一化（Local
Response Normalization）的输出值定义为： <img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Alexnet-3.3.png" /></p>
<blockquote>
<h3 id="传统方法及不足-2">传统方法及不足</h3>
<p>ReLUs理想特性：ReLUS有一个理想的特性，即它们不需要输入规范化来防止它们饱和。</p>
<h3 id="本文改进方法-2">本文改进方法</h3>
<ul>
<li>在ReLU层之前我们应用了局部归一化得到了一个更好的效果：用Aix, yx, y
表示在(X,Y)位置应用核然后应用RELU
非线性所产生的神经元的活动，即响应归一化活动Bi。其中和在同一空间位置的n个“相邻”核映射上运行，n是层中核的总数。</li>
<li>局部响应归一化处理方法类似于生物神经元的横向抑制机制，可以理解为将局部响应最大的再放大，并抑制其他响应较小的(放大局部显著特征,作用还是提高鲁棒性)。在用ReLU非线性层之后用到局部响应归一化在特定的层。</li>
</ul>
<h3 id="本文改进结果-1">本文改进结果</h3>
<ul>
<li><strong>（1）错误率降低：</strong>响应规范化将Top-1和Top-5错误率分别降低了1.4%和1.2%。</li>
<li><strong>（2）在CIFAR-10数据集上验证了该方案的有效性：</strong>一个四层CNN
在未归一化的情况下获得了13%的测试错误率，在归一化的情况下获得了11%的测试错误率。</li>
</ul>
</blockquote>
<h3 id="重叠池化">3.4 重叠池化</h3>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/AlexNet-3.4.png" /></p>
<blockquote>
<h3 id="传统方法及不足-3">传统方法及不足</h3>
<ul>
<li>传统上，邻接池化单元归纳的邻域不重叠。</li>
<li>更精确地说，一个池化层可以被认为是由池化单元网格组成的，池化单元网格间隔s个像素，每一个概括以池化单元的位置为中心的大小为z×z的邻域。如果我们设置s=z，我们就得到了CNNS中常用的局部池化层。</li>
</ul>
<h3 id="本文改进办法">本文改进办法</h3>
<p><strong>采用了重叠池化层：</strong>如果我们设置s&lt;z，我们得到重叠合用。这就是我们在整个网络中使用的s=2和z=3。</p>
<h3 id="本文改进效果">本文改进效果</h3>
<p>（<strong>1）错误率下降：</strong>与非重叠方案
S=2,Z=2相比，该方案的top-1和top-5错误率分别降低了0.4%和0.3%。</p>
<p><strong>（2）过度拟合的差异更大：</strong>我们通常在训练过程中观察到，使用重叠池的模型发现过度拟合的差异更大</p>
</blockquote>
<h3 id="整体架构">3.5 整体架构</h3>
<p>现在我们准备描述我们的CNN的整体架构。如图2所示，我们的网络包含8个带权重的层；前5层是卷积层，剩下的3层是全连接层。最后一层全连接层的输出是1000维softmax的输入，softmax会产生1000类标签的分布。我们的网络最大化多项逻辑回归的目标，这等价于最大化预测分布下训练样本正确标签的对数概率的均值。</p>
<p>第2，4，5卷积层的核只与位于同一GPU上的前一层的核映射相连接（看图2）。第3卷积层的核与第2层的所有核映射相连。全连接层的神经元与前一层的所有神经元相连。第1，2卷积层之后是响应归一化层。3.4节描述的这种最大池化层在响应归一化层和第5卷积层之后。ReLU非线性应用在每个卷积层和全连接层的输出上。</p>
<p>第1卷积层使用96个核对224 × 224 × 3的输入图像进行滤波，核大小为11 × 11
×
3，步长是4个像素（核映射中相邻神经元感受野中心之间的距离）。第2卷积层使用用第1卷积层的输出（响应归一化和池化）作为输入，并使用256个核进行滤波，核大小为5
× 5 ×
48。第3，4，5卷积层互相连接，中间没有接入池化层或归一化层。第3卷积层有384个核，核大小为3
× 3 ×
256，与第2卷积层的输出（归一化的，池化的）相连。第4卷积层有384个核，核大小为3
× 3 × 192，第5卷积层有256个核，核大小为3 × 3 ×
192。每个全连接层有4096个神经元。</p>
<blockquote>
<h3 id="网络架构-1">网络架构</h3>
<p><strong>第一层卷积层</strong>使用96个大小为11x11x3的卷积核对224x224x3的输入图像以4个像素为步长（这是核特征图中相邻神经元感受域中心之间的距离）进行滤波。<strong>第二层卷积层</strong>将第一层卷积层的输出（经过响应归一化和池化）作为输入，并使用256个大小为5x5x48的核对它进行滤波。<strong>第三层、第四层和第五层</strong>的卷积层在没有任何池化或者归一化层介于其中的情况下相互连接。<strong>第三层卷积层</strong>有384个大小为3x3x256的核与第二层卷积层的输出（已归一化和池化）相连。<strong>第四层卷积层</strong>有384个大小为3x3x192的核，<strong>第五层卷积层</strong>有256个大小为
的核。每个全连接层有4096个神经元。</p>
<p>1.因为在<strong>两个GPU</strong>上运行，所以网络结构被一切为二，上下两部分各自训练各自的，各有各的参数核，结构都是一样的；</p>
<p>2.整个结构有<strong>八层</strong>，<strong>前五层为卷积层，后三层为全连接层</strong>，<strong>最后再跟一个1000路的分类激活函数softmax</strong>，相当于多个logistic回归来进行多元分类。</p>
<p>3.<strong>二、四、五层只与自己之前的核有关系</strong>，就是只与自己这个GPU前一层训练的输出有关系。<strong>第三层卷积层与前一层的两个GPU训练出来的都有关系</strong>，在通道维度上做了一个融合。全连接层就与前一层中所有神经元相连。</p>
<p>4.<strong>局部归一化（LRN）</strong>应用在了第一层和第二层的卷积层。</p>
<p>5.<strong>最大池化层</strong>应用在了有局部归一化的层以及第五卷积层。</p>
<p>6.这八个卷积层每一层都应用了<strong>ReLU函数。</strong></p>
<p>7.这些层的顺序：<strong>局部归一化放在ReLU之前，然后最大池化层跟在ReLU之后</strong>。</p>
<h3 id="规律">规律</h3>
<p>我们输入的图片从一个又高又宽又扁的一个形状，慢慢变为了一个宽和高都很小，但是很长的一个张量，这是说我们的空间信息被压缩了也就是从一开始的224变为了后面的13，也就是13中的一个像素能表示之后一大片像素。</p>
<p>通道数变多也就是变长了，通道数可以理解为对于一个模式的识别，例如通道数为192那么说明可以识别图中192个模式，例如猫腿、爪子这种模式。所以说整个过程就是空间信息被压缩，但是语义信息空间慢慢增加。</p>
<h3 id="运作流程">运作流程：</h3>
<p><strong>conv1：</strong>输入→卷积→ReLU→局部响应归一化→重叠最大池化层</p>
<p><strong>conv2：</strong>卷积→ReLU→局部响应归一化→重叠最大池化层</p>
<p><strong>conv3：</strong>卷积→ReLU</p>
<p><strong>conv4：</strong>卷积→ReLU</p>
<p><strong>conv5：</strong>卷积→ReLU→重叠最大池化层(经过这层之后还要进行flatten展平操作)</p>
<p><strong>FC1：</strong>全连接→ReLU→Dropout</p>
<p><strong>FC2：</strong>全连接→ReLU→Dropout</p>
<p><strong>FC3</strong>(可看作softmax层)：全连接→ReLU→Softmax</p>
</blockquote>
<h2 id="reducing-overfitting减少过拟合"><strong>4.Reducing
Overfitting—减少过拟合</strong></h2>
<p>本文的神经网络结构有6千万个参数。尽管ILSVRC的1000个类别使得每一个训练样本利用10bit的数据就可以将图像映射到标签上，但是如果没有大量的过拟合，是不足以学习这么多参数的。接下来，本文描述了两种减少过拟合的主要的方法。</p>
<h3 id="数据增强">4.1 数据增强</h3>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Alexnet-4.1.png" /></p>
<blockquote>
<p>减少图像数据过度拟合的最简单也是最常见的方法是使用保留标签的变换人为地放大数据集。这里用了两种方式：</p>
<h3
id="第一种数据增强的形式包括生成平移图像和水平翻转图像">第一种数据增强的形式包括生成<strong>平移图像</strong>和<strong>水平翻转图像</strong>。</h3>
<p><strong>具体方法：</strong>做法就是从256x256的图像中提取随机的224x224大小的块（以及它们的水平翻转），然后基于这些提取的块训练网络。这个让我们的训练集增大了2048倍（(256-224)25个224x224*2=2048）。在测试时，网络通过提取5个224x224块（四个边角块和一个中心块）以及它们的水平翻转（因此共十个块）做预测，然后网络的softmax层对这十个块做出的预测取均值。</p>
<h3
id="第二种形式的数据增强包括改变训练图像中rgb通道的强度">第二种形式的数据增强包括改变训练图像中RGB通道的强度。</h3>
<p><strong>具体方法：</strong>特别的，本文对整个ImageNet训练集的RGB像素值进行了PCA。对每一幅训练图像，本文加上多倍的主成分，倍数的值为相应的特征值乘以一个均值为0标准差为0.1的高斯函数产生的随机变量。</p>
<p>该方案近似地抓住了自然图像的一个重要性质，即物体的同一性对光照强度和颜色的变化是不变的。
该方案将TOP-1错误率降低1%以上。</p>
</blockquote>
<h3 id="随机失活法">4.2 随机失活法</h3>
<p>将许多不同模型的预测结合起来是降低测试误差[1,
3]的一个非常成功的方法，但对于需要花费几天来训练的大型神经网络来说，这似乎太昂贵了。然而，有一个非常有效的模型结合版本，它只花费两倍的训练成本。这种最近引入的技术，叫做“dropout”[10]，它会以0.5的概率对每个隐层神经元的输出设为0。那些“失活的”的神经元不再进行前向传播并且不参与反向传播。因此每次输入时，神经网络会采样一个不同的架构，但所有架构共享权重。这个技术减少了复杂的神经元互适应，因为一个神经元不能依赖特定的其它神经元的存在。因此，神经元被强迫学习更鲁棒的特征，它在与许多不同的其它神经元的随机子集结合时是有用的。在测试时，我们使用所有的神经元但它们的输出乘以0.5，对指数级的许多失活网络的预测分布进行几何平均，这是一种合理的近似。</p>
<p>我们在图2中的前两个全连接层使用失活。如果没有失活，我们的网络表现出大量的过拟合。失活大致上使要求收敛的迭代次数翻了一倍。</p>
<blockquote>
<h3 id="引出dropout">引出dropout：</h3>
<p>结合多种不同模型的预测结果是一种可以降低测试误差的非常成功的方法，但是这对于已经要花很多天来训练的大规模神经网络来说显得太耗费时间了。但是，有一种非常有效的模型结合的方法，训练时间只需要原先的<strong>两倍</strong>。最新研究的技术，叫做“<strong>dropout</strong>”。</p>
<h2 id="思想">思想</h2>
<p>全连接层由于参数过于庞大，因此很容易出现过拟合，那么每次迭代的时候把一些神经元以概率p失活，这样每次迭代时都是一个新的模型，显著提高了<strong>健壮性</strong>（Robust），某种意义可以看做通过集成不同的模型来提高泛化能力</p>
<h3 id="方法">方法</h3>
<p>它将每一个隐藏神经元的输出以50%的概率设为0。以这种方式“dropped
out”的神经元既不参与前向传播，也不参与反向传播。每次做完dropout，相当于从原始的网络中找到一个更瘦的网络，有了dropout之后，可以将一个大网络看作多个小网络的组合，<strong>dropout能够有效地防止过拟合。</strong></p>
<h3 id="结果">结果</h3>
<p>1.因此每次有输入时，神经网络采样一个不同的结构，但是所有这些结构都共享权值。</p>
<p>2.这种技术减少了神经元复杂的共同适应，因为一个神经元不是依赖于特定的其他神经元存在的。</p>
<p>3.因此迫使要学到在连接其他神经元的多个不同随机子集的时候更鲁棒性（稳定性）的特征。</p>
<p>4.在测试时，本文使用所有的神经元，但对其输出都乘以了0.5，对采用多指数dropout网络生成的预测分布的几何平均数来说这是一个合理的近似。</p>
<h3 id="本文中的应用">本文中的应用：</h3>
<p>在图2中的前两个全连接层使用dropout。如果不采用dropout，本文的网络将会出现大量的过拟合。dropout大致地使达到收敛的迭代次数增加了一倍。</p>
</blockquote>
<h2 id="学习细节">5 学习细节</h2>
<h3 id="sgd随机梯度下降法">5.1 SGD随机梯度下降法</h3>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Alexnet-5.1.png" /></p>
<blockquote>
<p>我们使用<strong>随机梯度下降法（SGD）</strong>训练我们的模型，批量大小为128，动量（momentum）为0.9（对传统SGD增加了动量这个观点，来解决传统SGD的一些问题，例如优化过程非常不平滑或者梯度下降很低效的时候），权值（weight
decay）为0.0005（可以理解为是一个L2的正则化项，用在优化算法上而不是模型上）。</p>
<p>我们发现，这种少量的权值对模型的学习很重要。</p>
</blockquote>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Alexnet-Figure3.png" /></p>
<blockquote>
<p>图3：由第一卷积层从224×224×3的输入图像中学习得到的96个尺寸为11×11×3的卷积核。顶部的48个卷积核在GPU
1上学习得到，而底部的48个卷积核在GPU 2上学习得到。详情见第6.1节。</p>
</blockquote>
<h3 id="初始化参数">5.2 初始化参数</h3>
<p>我们使用标准差为0.01、均值为0的高斯分布来初始化各层的权重。我们使用常数1来初始化了网络中的第二个、第四个和第五个卷积层以及全连接层中的隐含层中的所有偏置参数。这种初始化权重的方法通过向ReLU提供了正的输入，来加速前期的训练。我们使用常数0来初始化剩余层中的偏置参数。</p>
<blockquote>
<p>（1）从标准差为0.01，均值为0的高斯分布初始化每一层的权重。</p>
<p>（2）将第2、4、5个卷积层和全连接起来的隐藏层的神经元偏置初始化为常数1（这样的初始化通过为ReLU提供正的输入加速了初期阶段网络的学习）。</p>
<p>（3）将剩余层的神经元偏置初始化为常数0。</p>
</blockquote>
<h3 id="学习速率">5.3 学习速率</h3>
<p>我们对所有层都使用相同的学习率，在训练过程中又手动进行了调整。我们遵循的启发式方法是：以当前的学习速率训练，验证集上的错误率停止降低时，将学习速率除以10.学习率初始时设为0.01，并且在终止前减少3次。我们使用120万张图像的训练集对网络进行了大约90次迭代的训练，这在两块NVIDIA
GTX 580 3GB GPU上花费了大约5到6天的时间。</p>
<blockquote>
<p>所有层均相等，在训练中会人为调整。我们遵循的启发式方法是，<strong>当验证集的错误率不再随当前学习率提高时，将学习率除以
10。</strong>学习率初始化为0.01并在终止前减少三次。也有自动的方法，例如Resnet，训练120轮epoch，初始学习率也是设为0.01，每30轮降低十倍。本文是训练了90个epoch，每一次是120w张图片。</p>
<p>当然现在我们都不采用十倍十倍去降低了，我们采用更平滑的降低方式，例如利用cos函数去降低。</p>
<p>如下图，蓝色线为本文中的降低方式，十倍十倍去降，红色线是我们现在用的，一开始学习率设的大一些，慢慢下降，这样更高效。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Alexnet-cos.png" /></p>
</blockquote>
<h2 id="实验结果"><strong>6. 实验结果</strong></h2>
<p>我们在ILSVRC-2010上的结果概括为表1。我们的神经网络取得了<code>top-1 37.5%</code>，<code>top-5 17.0%</code>的错误率。在ILSVRC-2010竞赛中最佳结果是<code>top-1 47.1%</code>，<code>top-5 28.2%</code>，使用的方法是对6个在不同特征上训练的稀疏编码模型生成的预测进行平均，从那时起已公布的最好结果是<code>top-1 45.7%</code>，<code>top-5 25.7%</code>，使用的方法是平均在Fisher向量（FV）上训练的两个分类器的预测结果，Fisher向量是通过两种密集采样特征计算得到的[24]。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Alexnet-Table1.png" /></p>
<blockquote>
<p>表1：ILSVRC
2010测试集上的结果比较。斜体字表示其他研究者达到的最佳结果。</p>
</blockquote>
<p>我们也用我们的模型参加了ILSVRC-2012竞赛并在表2中报告了我们的结果。由于ILSVRC-2012的测试集标签不可以公开得到，我们不能报告我们尝试的所有模型的测试错误率。在这段的其余部分，我们会使用验证误差率和测试误差率互换，因为在我们的实验中它们的差别不会超过0.1%（看图2）。本文中描述的CNN取得了<code>top-5 18.2%</code>的错误率。五个类似的CNN预测的平均误差率为16.4%。为了对ImageNet
2011秋季发布的整个数据集（1500万图像，22000个类别）进行分类，我们在最后的池化层之后有一个额外的第6卷积层，训练了一个CNN，然后在它上面进行“fine-tuning”，在ILSVRC-2012取得了16.6%的错误率。对在ImageNet
2011秋季发布的整个数据集上预训练的两个CNN和前面提到的五个CNN的预测进行平均得到了15.3%的错误率。第二名的最好竞赛输入取得了26.2%的错误率，他的方法是对FV上训练的一些分类器的预测结果进行平均，FV在不同类型密集采样特征计算得到的。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Alexnet-Table2.png" /></p>
<blockquote>
<p>表2：ILSVRC-2012验证集和测试集上的错误率比较。斜体字表示其他研究者达到的最佳结果。带有星号*的模型是使用ImageNet
2011秋季发布的整个数据集进行“预训练”的，以进行分类。详情见第6节。</p>
</blockquote>
<p>最后，我们也报告了我们在ImageNet 2009秋季数据集上的误差率，ImageNet
2009秋季数据集有10,184个类，890万图像。在这个数据集上我们按照惯例用一半的图像来训练，一半的图像来测试。由于没有建立测试集，我们的数据集分割有必要不同于以前作者的数据集分割，但这对结果没有明显的影响。我们在这个数据集上的的top-1和top-5错误率是67.4%和40.9%，使用的是上面描述的在最后的池化层之后有一个额外的第6卷积层网络。这个数据集上公开可获得的最好结果是78.1%和60.9%[19]。</p>
<blockquote>
<ul>
<li>ILSVRC-2010上，本文网络的<strong>测试集top-1和top-5的错误率分别为37.5%和17.0%</strong>。</li>
<li>ILSVRC-2012上，本文中所描述的CNN的top-5错误率是18.2%。五个相似的CNN的平均预测结果的错误率是16.4%。在最后一个池化层上增加第六个卷积层，使用整个ImageNet
Fall
2011的数据（15M图像，22000种类别）作为分类数据预训练得到的一个CNN，再经过微调，用ILSVRC-2012对该CNN进行测试得到的错误率为16.6%。对上述的五个在整个Fall
2011数据集上预训练过的CNN，得到的预测求平均得到的错误率结果为15.3%。</li>
</ul>
</blockquote>
<h4 id="qualitative-evaluations定性评估">6.1Qualitative
Evaluations—定性评估</h4>
<p>图3显示了网络的两个数据连接层学习到的卷积核。网络学习到了大量的频率核、方向选择核，也学到了各种颜色点。注意两个GPU表现出的专业化，3.5小节中描述的受限连接的结果。GPU
1上的核主要是没有颜色的，而GPU
2上的核主要是针对颜色的。这种专业化在每次运行时都会发生，并且是与任何特别的随机权重初始化（以GPU的重新编号为模）无关的。</p>
<p>在图4的左边部分，我们通过在8张测试图像上计算它的top-5预测定性地评估了网络学习到的东西。注意即使是不在图像中心的目标也能被网络识别，例如左上角的小虫。大多数的top-5标签似乎是合理的。例如，对于美洲豹来说，只有其它类型的猫被认为是看似合理的标签。在某些案例（格栅，樱桃）中，网络在意的图片焦点真的很含糊。</p>
<p>探索网络可视化知识的另一种方式是思考最后的4096维隐藏层在图像上得到的特征激活。如果两幅图像生成的特征激活向量之间有较小的欧式距离，我们可以认为神经网络的更高层特征认为它们是相似的。图4表明根据这个度量标准，测试集的5张图像和训练集的6张图像中的每一张都是最相似的。注意在像素级别，检索到的训练图像与第一列的查询图像在L2上通常是不接近的。例如，检索的狗和大象似乎有很多姿态。我们在补充材料中对更多的测试图像呈现了这种结果。</p>
<p>通过两个4096维实值向量间的欧氏距离来计算相似性是效率低下的，但通过训练一个自动编码器将这些向量压缩为短二值编码可以使其变得高效。这应该会产生一种比将自动编码器应用到原始像素上[14]更好的图像检索方法，自动编码器应用到原始像素上的方法没有使用图像标签，因此会趋向于检索与要检索的图像具有相似边缘模式的图像，无论它们是否是语义上相似。</p>
<p><img
src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Alexnet-Figure4.png" /></p>
<h2 id="探讨">7 探讨</h2>
<p>我们的结果表明一个大型深度卷积神经网络在一个具有高度挑战性的数据集上使用纯有监督学习可以取得破纪录的结果。值得注意的是，如果移除一个卷积层，我们的网络性能会降低。例如，移除任何中间层都会引起网络损失大约2%的top-1性能。因此深度对于实现我们的结果非常重要。</p>
<p>为了简化我们的实验，我们没有使用任何无监督的预训练，尽管我们希望它会有所帮助，特别是在如果我们能获得足够的计算能力来显著增加网络的大小而标注的数据量没有对应增加的情况下。到目前为止，我们的结果已经提高了，因为我们的网络更大、训练时间更长，但为了匹配人类视觉系统的下颞线（视觉专业术语）我们仍然有许多数量级要达到。最后我们想在视频序列上使用非常大的深度卷积网络，视频序列的时序结构会提供非常有帮助的信息，这些信息在静态图像上是缺失的或远不那么明显。</p>
<blockquote>
<p>1.本文的结果表明一个大规模深度卷积神经网络在具有高度挑战性的数据集上<strong>仅用监督学习</strong>就能够获得破纪录的好结果。</p>
<p>2.卷积核可以学习到<strong>频率、方向和颜色</strong>特征；</p>
<p>3.<strong>更强大GPU</strong>及<strong>更多的数据</strong>可进一步提高模型性能。</p>
<p>4.<strong>深度</strong>可决定网络能力，如果去掉一个卷积层，那么准确率会下降2%。</p>
<p>5.图片缩放细节，<strong>对短边先缩放</strong>；</p>
<p>6.<strong>ReLU不需要对输入进行标准化来防止饱和现象</strong>，而sigmoid、tanh激活函数有必要对输入做标准化；</p>
<p>7.<strong>没有使用无监督进行预训练</strong>。这个是有一定历史背景的，在Alexnet网络提出之前有监督学习打不过无监督学习，但是在Alexnet提出之后，引起了有监督学习的热潮，直到最新的语言模型bert的提出，才慢慢的将人们又拉回了无监督学习。</p>
<p>8.<strong>网络结构具有相关性</strong>，不能轻易移除某一层；</p>
<p>9.本文的结果已经有所提高，但我们仍然有很多需求来进行时空下人类视觉系统的研究。最终我们想要将非常大规模地深度卷积网络应用于<strong>视频序列</strong>的处理，视频序列中的时间结构提供了许多有用的信息，而这些信息在静态图中丢失了或者不是很明显。</p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://www.chitose.cn">Chitose</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://www.chitose.cn/AlexNet-paper/">https://www.chitose.cn/AlexNet-paper/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.chitose.cn" target="_blank">Chitose-Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_10.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/DenseNet-paper/" title="DenseNet-paper"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_1.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">DenseNet-paper</div></div></a></div><div class="next-post pull-right"><a href="/Todo/" title="Todo"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_4.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Todo</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/Face.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Chitose</div><div class="author-info__description">Hahaha</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">77</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/chitose-r"><i class="fab fa-github"></i><span>🛴/前往小家..</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/chitose-r" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:171450290@qq.com" target="_blank" title="Email"><i class="fa-solid fa-square-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="/qq/" target="_blank" title="QQ"><i class="fab fa-qq" style="color: #12b7f5;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到我的博客！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#imagenet-classification-with-deep-convolutional-neural-networks"><span class="toc-text">ImageNet
Classification with Deep Convolutional Neural Networks</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%9A%84imagenet%E5%88%86%E7%B1%BB%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-2012"><span class="toc-text">基于深度卷积的ImageNet分类神经网络
2012</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-text">摘要</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9"><span class="toc-text">主要内容</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-text">1 引言</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9-1"><span class="toc-text">主要内容</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E8%B4%A1%E7%8C%AE"><span class="toc-text">主要贡献</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">2 数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%86%85%E5%AE%B9-2"><span class="toc-text">主要内容：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#imagenet"><span class="toc-text">ImageNet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95"><span class="toc-text">图像处理方法：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc-text">3 网络架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0relu"><span class="toc-text">3.1 非线性激活函数ReLU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95%E5%8F%8A%E4%B8%8D%E8%B6%B3"><span class="toc-text">传统方法及不足</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E6%94%B9%E8%BF%9B%E6%96%B9%E6%B3%95"><span class="toc-text">本文改进方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E7%9A%84%E6%94%B9%E8%BF%9B%E7%BB%93%E6%9E%9C"><span class="toc-text">本文的改进结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%A8%E5%A4%9A%E4%B8%AAgpu%E8%AE%AD%E7%BB%83"><span class="toc-text">3.2 用多个GPU训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95%E5%8F%8A%E4%B8%8D%E8%B6%B3-1"><span class="toc-text">传统方法及不足</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E6%94%B9%E8%BF%9B%E6%96%B9%E6%B3%95-1"><span class="toc-text">本文改进方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%87%E7%94%A8%E6%8A%80%E5%B7%A7"><span class="toc-text">采用技巧</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E6%94%B9%E8%BF%9B%E7%BB%93%E6%9E%9C"><span class="toc-text">本文改进结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%80%E9%83%A8%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-text">3.3 局部归一化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95%E5%8F%8A%E4%B8%8D%E8%B6%B3-2"><span class="toc-text">传统方法及不足</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E6%94%B9%E8%BF%9B%E6%96%B9%E6%B3%95-2"><span class="toc-text">本文改进方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E6%94%B9%E8%BF%9B%E7%BB%93%E6%9E%9C-1"><span class="toc-text">本文改进结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8D%E5%8F%A0%E6%B1%A0%E5%8C%96"><span class="toc-text">3.4 重叠池化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95%E5%8F%8A%E4%B8%8D%E8%B6%B3-3"><span class="toc-text">传统方法及不足</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E6%94%B9%E8%BF%9B%E5%8A%9E%E6%B3%95"><span class="toc-text">本文改进办法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E6%94%B9%E8%BF%9B%E6%95%88%E6%9E%9C"><span class="toc-text">本文改进效果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84"><span class="toc-text">3.5 整体架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84-1"><span class="toc-text">网络架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%84%E5%BE%8B"><span class="toc-text">规律</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%90%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-text">运作流程：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reducing-overfitting%E5%87%8F%E5%B0%91%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-text">4.Reducing
Overfitting—减少过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-text">4.1 数据增强</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E7%A7%8D%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E7%9A%84%E5%BD%A2%E5%BC%8F%E5%8C%85%E6%8B%AC%E7%94%9F%E6%88%90%E5%B9%B3%E7%A7%BB%E5%9B%BE%E5%83%8F%E5%92%8C%E6%B0%B4%E5%B9%B3%E7%BF%BB%E8%BD%AC%E5%9B%BE%E5%83%8F"><span class="toc-text">第一种数据增强的形式包括生成平移图像和水平翻转图像。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%A7%8D%E5%BD%A2%E5%BC%8F%E7%9A%84%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E5%8C%85%E6%8B%AC%E6%94%B9%E5%8F%98%E8%AE%AD%E7%BB%83%E5%9B%BE%E5%83%8F%E4%B8%ADrgb%E9%80%9A%E9%81%93%E7%9A%84%E5%BC%BA%E5%BA%A6"><span class="toc-text">第二种形式的数据增强包括改变训练图像中RGB通道的强度。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E5%A4%B1%E6%B4%BB%E6%B3%95"><span class="toc-text">4.2 随机失活法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%95%E5%87%BAdropout"><span class="toc-text">引出dropout：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%9D%E6%83%B3"><span class="toc-text">思想</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-text">方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C"><span class="toc-text">结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-text">本文中的应用：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%BB%86%E8%8A%82"><span class="toc-text">5 学习细节</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#sgd%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-text">5.1 SGD随机梯度下降法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%82%E6%95%B0"><span class="toc-text">5.2 初始化参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E9%80%9F%E7%8E%87"><span class="toc-text">5.3 学习速率</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-text">6. 实验结果</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#qualitative-evaluations%E5%AE%9A%E6%80%A7%E8%AF%84%E4%BC%B0"><span class="toc-text">6.1Qualitative
Evaluations—定性评估</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A2%E8%AE%A8"><span class="toc-text">7 探讨</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By Chitose</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"></div><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script defer src="/js/cursor.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Python/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🥩 Python (23)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/C/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🕶️ C (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Embedded/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">💳 Embedded (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Pytorch/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📯 Pytorch (9)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/Paper/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📰 Paper (18)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/others/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🤡 others (16)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://www.chitose.cn/categories/segmentation/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🔪 segmentation (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item" style="visibility: hidden"></div><div class="magnet_item" style="visibility: hidden"></div><a class="magnet_link_more"  href="https://www.chitose.cn/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(33.333333333333336% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>
  function butterfly_clock_anzhiyu_injector_config(){
    var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
    var item_html = '<div class="card-widget card-clock"><div class="card-glass"><div class="card-background"><div class="card-content"><div id="hexo_electric_clock"><img class="entered loading" id="card-clock-loading" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/loading.gif" style="height: 120px; width: 100%;" data-ll-status="loading"/></div></div></div></div></div>';
    console.log('已挂载butterfly_clock_anzhiyu')
    if(parent_div_git) {
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var qweather_key = '6be604177b8a4c3e97c78a352ee324f7';
  var gaud_map_key = '17b299fafade134736e6a1d4acb5ef18';
  var baidu_ak_key = 'undefined';
  var flag = 0;
  var clock_rectangle = '113.34532,23.15624';
  var clock_default_rectangle_enable = 'false';

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_clock_anzhiyu_injector_config();
  }
  else if (epage === cpage){
    butterfly_clock_anzhiyu_injector_config();
  }
  </script><script src="https://widget.qweather.net/simple/static/js/he-simple-common.js?v=2.0"></script><script data-pjax src="https://cdn.cbd.int/hexo-butterfly-clock-anzhiyu/lib/clock.min.js"></script><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v6.2.0" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly_v4.3.1" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" data-title="本站采用多线部署，主线路托管于Vercel" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-Vercel-brightgreen?style=flat&amp;logo=Vercel" alt=""/></a><a class="github-badge" target="_blank" href="https://dashboard.4everland.org/" style="margin-inline:5px" data-title="本站采用多线部署，备用线路托管于4EVERLAND" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-4EVERLAND-22DDDD?style=flat&amp;logo=IPFS" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><script async src="/js/runtime.js"></script><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023-12-17-2/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_1.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023-12-17-2/" alt="">第二篇文章</a><div class="blog-slider__text">这是第二篇文章</div><a class="blog-slider__button" href="2023-12-17-2/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="Pytorch-6/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_8.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-19</span><a class="blog-slider__title" href="Pytorch-6/" alt="">Pytorch(6)-张量可微性</a><div class="blog-slider__text">再怎么看我也不知道怎么描述它的啦！</div><a class="blog-slider__button" href="Pytorch-6/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023-12-17-3/" alt=""><img width="48" height="48" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://cdn.jsdelivr.net/gh/chitose-r/pic_bed@master/img/default_cover_7.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-12-17</span><a class="blog-slider__title" href="2023-12-17-3/" alt="">第三篇文章</a><div class="blog-slider__text">这是第三篇文章</div><a class="blog-slider__button" href="2023-12-17-3/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body></html>